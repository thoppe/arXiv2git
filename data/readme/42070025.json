{
  "read_at": 1462549066, 
  "description": "", 
  "README.md": "# NLP Tags\nA keywords & key-phrases extraction RESTful Web Service written in Java.\n## Overview\nThe code in this project is based on the published research results in the field of Natural Language Processing (NLP). Core functionalities are based on Stanford CoreNLP framework (http://nlp.stanford.edu/) and JUNG (http://jung.sourceforge.net/).\n\nThe project's main goal was to create a functional RESTful Web Service to provide users with the possibility to extract keywords and key-phrases from the text and return results in JSON format. The extraction of keywords and key-phrases is based on graph-based representation of text and application of Social Network Analysis (SNA) metrics.\n\nIt is still work in progress.\n\n### Why?\nIncreasingly, companies, governments and individuals are faced with large amounts of text that are critical for their everyday working and life practices. Decisions need to be made fast, so it is crucial to get important information from text to act accordingly. \n\nAt the same time, a plethora of content recommendation systems are built and massive amount of Internet moguls tend to use \nthat same information from text to get advantage of user feedback on various websites. \n\nThis project has been built with as an attempt to provide a service that takes a piece of text, extracts the most relevant keywords and key-phrases from it and returns them in a programming friendly data structure that can be manipulated in different ways.\n\n### About the Project\nThe work done in this project is largely based on the research reported in *Lahiri, S., Choudhury, S. R., & Caragea, C. (2014). Keyword and Keyphrase Extraction Using Centrality Measures on Collocation Networks. CoRR. Retrieved November 22, 2014*, that can be downloaded from http://arxiv.org/pdf/1401.6571v1.\n\nThe approach in the mentioned paper consists of several steps that lead to the extraction of keywords and key-phrases from any piece of text; these steps are closely followed in this project.\n\nThe project consists of two parts, one concerning the extraction of keywords and other concerning the extraction of key-phrases.\n\n#### To extract keywords from text, the following steps are required:\n1. Division of the provided text into sentences, and subsequently sentences into words\n2. Conversion of words into their basic form ([lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation))\n3. Elimination of lemmas that have empirically been proven as irrelevant for keywords extraction and thus might negatively affect the final outcome:\n  * 5% of the most frequent and 5% of the least frequent lemmas (i.e., top 5% and bottom 5% of lemmas ordered by their frequency) in the given text or\n * Most common English words (stop-words)\n4. Creation of graph where lemmas represent nodes, while an edge between two nodes is established if the corresponding lemmas occur one next to the other in the input text (i.e., if they form a bigram); edges are weighted based on the number of immediate bigrams in the input text\n5. Scoring the graph based on one of the given centrality measures (Degree, Strength, Neighborhood size - order 1, Coreness, Clustering Coefficient, Structural Diversity Index, PageRank, HITS, Betweenness, Closeness, Eigenvector Centrality)\n6. Returning the scored lemmas in JSON format.\n\n#### To extract key-phrases from text, the following steps are required:\n1. Division of the provided text into sentences, and subsequently sentences into words\n2. Conversion of words into their basic form (lemmatisation)\n3. Working out the grammatical structure of sentences by annotating lemmas with Part of Speech (POS) tags\n4. Elimination of grammatical structures that do not represent noun phrases\n5. Creation of a graph where noun phrases represent nodes, while edges are formed between two nodes if the corresponding noun phrases occur in a specified 'window' in the input text; edges are weighted based on the number of common occurrences of the two noun phrases in the text\n6. Scoring the graph based on one of the given centrality measures (Degree, Strength, Neighborhood size - order 1, Coreness, Clustering Coefficient, Structural Diversity Index, PageRank, HITS, Betweenness, Closeness, Eigenvector Centrality)\n7. Returning the scored noun phrases in JSON format.\n\n### Solution\nBased on the steps mentioned above, it was important to find appropriate software libraries, those that deal with NLP tasks and others that are good for creating graphs and scoring edges based on different SNA centrality measures.\n\n#### Keywords\nTo process the input text, the project uses [the Stanford CoreNLP](http://nlp.stanford.edu/software/corenlp.shtml#Usage) library that provides a set of natural language analysis tools that can take raw input text and supply:\n- the base forms of words, \n- parts of speech tags,\n- identification of named entities such as companies, people, etc., \n- normalized dates, times, and numeric quantities,  \n- marking up the structure of sentences in terms of phrases and word dependencies, \n- co-reference resolution (i.e., indication which noun phrases refer to the same entities), \n- indication of sentiment, etc. \n\nThe process starts by chunking the input text into sentences, and then a tokenizer divides text into a sequence of tokens, which roughly correspond to \"words\". \nTokenization of text produces basic text units - tokens - which include dots, commas, regular words ... Tokens can be transformed to lemmas (to the words' basic form). However, not all lemmas are valuable as keywords in the end, so it is important to remove lemmas that are:\n- not nouns,\n- less than 3 characters long,\n- brackets presented in form of \"-lrb-,-rrb-,-lsb-,-rsb-,-lcb-,-rcb-\".\n\nFor identifying nouns, CoreNLP provides MaxentTagger - Part-Of-Speech Tagger (POS Tagger). It is a piece of software that reads text in some language and assigns parts of speech to each word, such as noun, verb, adjective, etc. There are two taggers in distribution and service uses a model able to learn probabilities of POS tags for triples going through sequences in one direction (left to right) *english-left3words-distsim.tagger*. This tagger runs a lot faster than the bidirectional ones, and is recommended for general use. Its accuracy was 96.92% on Penn Treebank WSJ secs. 22-24.\n\nIt associates each lemma with one of the POS tags from the following [list](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). For this project, noun phrases tags -  NN, NNS, NNP and NNPS - are important; all other lemmas are ignored. \n\nThis is the point in the process of extracting keywords where the noun phrases (selected in the previous step) should be filtered, to remove those that have very little likelihood of being keywords. This filtering can be done in two ways: \n1. by eliminating 5% of the most frequent and 5% of the least frequent words (i.e., top 5% and bottom 5% words ordered by their frequency) in the given text and\n2. by eliminating common English words using one of the available stop-words list.\n\n##### Removing top 5% and bottom 5% words ordered by their frequency in the given text\nTo remove words by their frequency in the given text it is important to make a dictionary of the words appearing in the text, and where each word is associated with the number of its occurrences in the text. After sorting thus obtained data structure, it is possible to remove top 5% and bottom 5% words ordered by their frequency.\n\n##### Removing most common English words (stopwords)\nTo remove most common English words, two  publicly available stopword lists are used in this project:\n- [Stopwords list from the Weka framework](http://programcreek.com/java-api-examples/index.php?example_code_path=weka-weka.core-Stopwords.java)\n- A list provided in the [StopAnalyzer](https://lucene.apache.org/core/4_0_0/analyzers-common/org/apache/lucene/analysis/core/StopAnalyzer.html) class of the Apache Lucene Core text search engine library.\n\nThe result of the above described processing steps is a list of lemmas ready for creation of a graph. To create a graph, project uses the [JUNG](http://jung.sourceforge.net/) Java library.\nIterating through list of lemmas, a lemma is added to the graph as new node if it hasn't been already added. Every bigram is presented in the graph by establishing an edge between the two lemmas that form the bigram, while every reoccurrence of the same bigram increments the weight of the corresponding edge by one.\nAfter the graph creation, nodes are scored using [DegreeScorer](http://jung.sourceforge.net/doc/api/edu/uci/ics/jung/algorithms/scoring/DegreeScorer.html) class from the JUNG library; this scoring is based on degree centrality measure, which is defined as the number of links incident upon a node (i.e., the number of ties that node has).\n\nThe developed service regarding keywords allows for specifying:\n- the text to be used for keywords extraction,\n- the extraction method to be applied,\n- the filter used in removing frequent words (stopwords),\n- the number of keywords to be returned (zero for all keywords).\n\nIt is possible to make the call to the service using query string containing three parameters:\n- text - text of the document you want to extract keywords from;\n- method - 'keywords' (extraction of keywords);\n- filter - two choices possible: \n 1. stopwords (extraction of keywords while pre-eliminating stopwords in the given text) or\n 2. frequency (extraction of keywords while pre-eliminating top 5% and bottom 5% words ordered by their frequency in the given  text).\n- number - number of keywords service will return (0 for all).\n\nExample of service call:\n```\nGET /api/v1/tag?text=This is a keywords test.&method=keywords&filter=stopwords&number=10\n```\nwith parameters: text, method, filter and number.\n\n#### Key-phrases\nAs in the keywords extraction part of the project, in order to process the input text for key-phrases extraction, the project uses [the Stanford CoreNLP](http://nlp.stanford.edu/software/corenlp.shtml#Usage) library.\n\nThe process starts by chunking the input text into sentences, and then a tokenizer divides text into a sequence of tokens. \nTokenization of text produces basic text units - tokens - which include dots, commas, regular words ... Tokens can be transformed to lemmas (to the words' basic form). However, not all lemmas are valuable as keywords in the end, so it is important to remove lemmas that are:\n- less than 3 characters long,\n- brackets presented in form of \"-lrb-,-rrb-,-lsb-,-rsb-,-lcb-,-rcb-\".\n\nNext step in extracting key-phrases is putting all tokens that pass conditions above to a list. The list is parsed by [LexParser](http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/LexicalizedParser.html), a class that provides the top-level API and command-line interface to a set of reasonably good treebank-trained parsers. A natural language parser is a program that works out the grammatical structure of sentences, for instance, which groups of words go together (as \"phrases\") and which words are the subject or object of a verb. After parsed, an example sentence \"This is one good example of a sentence.\" looks like: \n```\n(ROOT\n   (S\n     (NP (DT This))\n     (VP (VBZ is)\n       (NP\n         (NP (CD one) (JJ good) (NN example))\n         (PP (IN of)\n           (NP (DT a) (NN sentence)))))\n     (. .)))\n```\nThis is the point where the parsed sentence is processed and all the nodes that don't have the NP (Noun Phrase) tag are filtered out.\nA collocation network in form of a graph is constructed for each document as follows: nodes represent unique noun phrases, and edges link together noun phrases that occur within a specific window of each other. Window size is the median sentence length of a document. Note that every edge between noun phrase 1 and noun phrase 2 is weighted with their co-occurrence frequency. While merging edges, edge weights were incremented.\nAfter the graph creation, nodes are scored using [DegreeScorer](http://jung.sourceforge.net/doc/api/edu/uci/ics/jung/algorithms/scoring/DegreeScorer.html) class from the JUNG library; this scoring is based on degree centrality measure.\n\nThe developed service regarding key-phrases allows for specifying:\n- the text to be used for key-phrases extraction,\n- the extraction method to be applied,\n- the number of key-phrases to be returned (zero for all key-phrases).\n\nIt is possible to make the call to the service using query string containing three parameters:\n- text - text of the document you want to extract keywords from;\n- method - 'keyphrases' (extraction of key-phrases);\n- number - number of key-phrases service will return (0 for all).\n\nExample of service call:\n```\nGET /api/v1/tag?text=This is a keyphrases test.&method=keyphrases&number=10\n```\nwith parameters: text, method and number.\n\n### Acknowledgements\nThis project has been developed as a part of the assignment for the subject *Applications of Artificial Intelligence* at the Faculty of Organizational Sciences, University of Belgrade, Serbia.\n### License\nSee the [LICENSE](LICENSE.md) file for license rights and limitations (Apache License).\n", 
  "id": 42070025
}