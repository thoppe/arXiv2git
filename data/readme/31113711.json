{
  "read_at": 1462551917, 
  "description": "Python implementation of CBOW and skip-gram word vector models, and hierarchical softmax and negative sampling learning algorithms", 
  "readme.md": "A Python implementation of the Continuous Bag of Words (CBOW) and skip-gram neural network architectures, and the hierarchical softmax and negative sampling learning algorithms for efficient learning of word vectors (Mikolov, et al., 2013a, b, c; http://code.google.com/p/word2vec/).\n\nUsage\n-----\nTo train word vectors:\n```\nword2vec.py [-h] -train FI -model FO [-cbow CBOW] [-negative NEG]\n            [-dim DIM] [-alpha ALPHA] [-window WIN]\n            [-min-count MIN_COUNT] [-processes NUM_PROCESSES]\n            [-binary BINARY]\n\nrequired arguments:\n  -train FI                 Training file\n  -model FO                 Output model file\n\noptional arguments:\n  -h, --help                show this help message and exit\n  -cbow CBOW                1 for CBOW, 0 for skip-gram\n  -negative NEG             Number of negative examples (>0) for negative sampling, \n                            0 for hierarchical softmax\n  -dim DIM                  Dimensionality of word embeddings\n  -alpha ALPHA              Starting learning rate\n  -window WIN               Max window length\n  -min-count MIN_COUNT      Min count for words used to learn <unk>\n  -processes NUM_PROCESSES  Number of processes\n  -binary BINARY            1 for output model in binary format, 0 otherwise\n```\nEach sentence in the training file is expected to be newline separated. \n\nImplementation Details\n----------------------\nWritten in Python 2.7.6 and NumPy 1.9.1.\n\nEvaluation\n----------\nAccuracy (%) on the word analogy task compared against the original C implementation (in parentheses). Trained on a preprocessed version of the first 10<sup>8</sup> bytes of the English Wikipedia dump on March 3, 2006 (http://mattmahoney.net/dc/textdata.html).\n\n| Model        | Total         | Semantic      | Syntactic     |\n|:------------ |--------------:| -------------:| -------------:|\n| CBOW HS      | 6.76 (6.90)   | 4.86 (3.61)   | 7.93 (8.93)   |\n| CBOW NS      | 4.52 (6.72)   | 3.94 (3.74)   | 4.88 (8.56)   |\n| Skip-gram HS | 14.76 (14.59) | 11.40 (10.40) | 16.83 (17.18) |       \n| Skip-gram NS | 8.43 (7.72)   | 4.91 (4.62)   | 10.62 (9.63)  |\n\nReferences\n----------\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013a). Distributed representations of words and phrases and their compositionality. Advances in Neural Information Processing Systems. http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013b). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. http://arxiv.org/pdf/1301.3781.pdf\n\nMikolov, T., Yih, W., & Zweig, G. (2013c). Linguistic Regularities in Continuous Space Word Representations. HLT-NAACL. http://msr-waypoint.com/en-us/um/people/gzweig/Pubs/NAACL2013Regularities.pdf\n", 
  "id": 31113711
}