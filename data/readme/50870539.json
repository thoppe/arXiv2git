{
  "read_at": 1462557713, 
  "description": "My machine learning paper notes", 
  "README.md": "# Generating images with recurrent adversarial networks\n  * http://arxiv.org/abs/1602.05110\n  * Motivation: generate good image samples\n  * Abbreviation: GRAN\n  * Similar models: LAGAN and DRAW\n  * Idea:\n    * image generation is an iterative process.\n    * Start with a sample from the distribution of latent variables, feed it to the decoder to get an image. Feed the generated sample to the encoder, which will generate a vector representation of the image. Concatenate that representation with the sample from the distribution and feed that to the decoder, to generate another image. This process is repeated t times, with t fixed aprori. To generate the final image, the t image generated samples are added together and the tanh function is applied to ensure the final image has entries in between 0 and 1.\n    * encoders and decoders can be represented by any function, they use deep convolutional adversarial nets\n  * Evaluation idea:\n    * to evaluate two GAN models, with discriminator D1 and generator G1 and discriminator D2 and generator G2, one cam compare them by seeing how well D1 can discriminate samples from G2 and how well D2 can discriminate samples from G1\n  * Differences between proposed model and DRAW:\n    * in DRAW at each time step, a new sample from the latent space is generated. In GRAN, only one sample is generated and then reused at each time step.\n    * GRAN starts with the decoding phase, not with the encoding phase\n    * GRAN has no attention mechanism\n    * A discriminator from GRAN can differentiate between DRAW generated samples and images from MNIST with a 10% error\n\n# Unitary evolution recurrent neural networks\n  * http://arxiv.org/abs/1511.06464\n  * Motivation: avoid exploding / vanishing gradient in RNNs\n  * To avoid exploding and vanishing gradients, we need to use a matrix for recurrent corrections with eigen values 1 => we parametrize the matrix so that we ensure it is unitary\n  * parametrize matrices with real entries is difficult and either results in a high computational complexity or too simple parametrization (use a matrix with non zero entries for the diagonal as a matrix for recurrent connections)\n  * instead of working in real space, let's work in complex space => easier to find a definition for the matrix of the recurrent connections that is unitary in the complex space. This is computationally feasible and captures enough complexity for learning\n  * memory & cost increase almost linearly with the size of the hidden layer\n  * new activation function, relu for complex numbers\n  * good results for Rnns example tasks (binary addition, etc), but not for a real life example\n\n# Exponential linear units\n  * http://arxiv.org/abs/1511.07289\n  * Motivation: increase speed of learning and classification accuracy\n  * Idea:\n    * avoid vanishing gradient by using identity for positive values, but have negative values which allow to push the mean unit activation towards zero\n    * zero mean activation brings the unit closer to the unit natural gradient\n    * units that have a non-zero mean activation act as bias for the next layer\n  * like batch normalization, they push the mean towards zero, but with less computational footprint\n  * Elu formula:\n    * x(f) = x for x >=0 and a(exp(x) - 1) for x < 0\n    * a is a tunable hyperparameter\n  * Idea behind natural gradient:\n    * The parameter space can be complicated and not euclidean => the direction of the gradient is no longer appropriate => account for the curvature of the space\n    * More [here](http://www.yaroslavvb.com/papers/amari-why.pdf)\n  * Proof in paper:\n    * Paper proves that zero mean activations speed up learning when the natural gradient is used\n    * This applies to all techniques that strive to get zero mean activations (including batch normalization)\n  * Results:\n    * improvement in accuracy for network with more than 5 layers\n\n# Multilingual language processing from bytes\n * http://arxiv.org/abs/1512.00103\n * Motivation: use one model for multiple languages for POS and NAR\n * Idea:\n    * use a sequence to sequence model to predict POS and NAR\n    * feed the model UTF-8 bytes (advantage: small vocabulary size)\n  * Model name: Byte to span\n  * At each time step the model does not predict a triple (start, len, pos), but rather learns to predict one of them at a different time step: t -> start, t+1 -> len, t+2 -> pos. The model learns not to mix them without much difficulty\n  * Mentions that language models benefit from CNNs over characters to generate word embeddings (more [here](http://arxiv.org/abs/1508.06615))\n  * Input segmentation:\n    * They want the model to work on long documents, but RNNs have problems modelling long sequences\n    * Instead, they split the input in segments of length K, using a sliding window\n    * Has the advantage of being able to perform inference in parallel (per split segment)\n    * Start and End predictions have a clear bound\n    * In their experiments, they use k=60\n  * Byte dropout:\n    * Randomly replace a fraction of the input bytes with a special token\n    * makes model ore robust\n    * only done for the input later\n  * Results:\n    * one model for 11 languages with better results than having language individual models\n    * requires no language specific engineering\n\n# Exploiting Similarities among Language for Machine Translation\n  * http://arxiv.org/abs/1309.4168\n  * Motivation: Increase the size of language dictionaries from monolingual data\n  * Idea:\n    * It is easy to procure multilingual data, we can use that to learn embeddings in different languages\n    * Learn a linear mapping from embeddings in the source language to embeddings in the target language from a small source language to target language dictionary\n    * To expand the given source to target language dictionary:\n      * take a word in the source language and compute its embedding\n      * map the embedding to the target language using the learned mapping from the given dictionary\n      * chose the word in the target language with the smallest cosine distance to the mapped embedding\n  * Results:\n    * Good sample translations from English to Spanish\n  * Potential issue: at test time one needs to compute the cosine distance against all the words in the target vocabulary which can be very big. However, this is can be parallelized and clustering method can also be used to improve speed.\n  * Use cases:\n    * find mistakes in dictionaries\n    * expand dictionaries to new words\n\n# A Simple Way to Initialize RNNs of Relu\n  * http://arxiv.org/abs/1504.00941\n  * Motivation: RNNs hard to train due to vanishing & exploding gradients\n  * Idea: use the identity matrix (or a scaled version) to initialize the recurrent connections\n    * activation function: Relu\n    * Initialize biases to 0 and the recurrent connection to the identity matrix.\n  * Advantage: simple architecture compared to LSTMs\n  * Results: comparable with LSTM on a couple of toy examples + speech recognition\n\n# Recurrent neural network regularization\n  * http://arxiv.org/abs/1409.2329\n  * Use dropout for Recurrent Neural Nets for the input connections, but not for recurrent connections\n  * Results:\n    * minimize overfitting for LSTM training\n    * good results for language modelling and image caption\n\n# Neural networks with few multiplications\n  * http://arxiv.org/abs/1510.03009\n  * Motivation: big computational cost of NN training\n  * Idea: avoid multiplication by binarizing weights (forward pass) and convert multiplication into binary shift (backward pass)\n  * mention of Completely Boolean networks: simplify test time computation with acceptable hit in accuracy, but no training time is saved.\n  * Binary connect:\n    * Traditional: y = h (Wx + b)\n    * Idea: sample each weight to be 1 or -1\n    * p(w_{ij}) = (w'_{ij} + 1) / 2, where w'_{ij} is the usual weight value, but constrained to be in between -1 and 1 (by being capped)\n    * random number generation has to be fast and not do a lot of multiplication for this to be worth it\n  * Ternary connect\n    * In NNs a lot of weights are 0 or close to 0 => allow weights to be 0\n    * if w'_{ij} > 0, use P(w_{ij} = 1) =  w'_{ij}, 0 otherwise\n    * if w'_{ij} < 0, use P(w_{ij} = - 1) =  - w'_{ij}, 0 otherwise\n  * Quantized backpropagation:\n    * do bit shifts in order to avoid multiplication, not for the gradient but for the input.\n  * they use batch normalization in their architecture\n  * Results:\n    * qualitative: a bit better than using simple NNs and CNNs\n    * no measure of speed improvements\n\n\n# Generative adversarial networks\n  * http://arxiv.org/abs/1406.2661\n  * Motivation: generate good samples\n  * Used for: start of the art image generation with Laplacian pyramids of GANs\n  * min-max game:\n     * Generator (G) tries to generate good samples\n     * Discriminator (D) tries to learn which samples come from the true distribution of the input and which samples come from G\n     * at the end of training you hope that D cannot distinguish between real samples and samples from G\n  * loss: = E_(x~pdata(x))[log D(x)] + E_(z~pz(z))[log(1 - D(G(z)))]\n  * no need for MCMC and approximations\n  * general framework, D&G not bound to be neural nets\n  * training tricks:\n      * update D k times for each update of G\n      * min log(1-D(G(z))) does poorly at the begging of training because D has an advantage (easy to distinguish between samples from G and samples from the real distribution), so instead max log(D(G(z))), which has a stronger gradient\n  * Theoretic result: the optimal distribution for D is: D*(G(x)) = p_data(x) / (p_data(x) + p_G(x))\n", 
  "id": 50870539
}