{
  "read_at": 1462555956, 
  "description": "", 
  "README.md": "tiny-cnn: A header only, dependency-free deep learning framework in C++11\n========\n\n| **Linux/Mac OS** | **Windows** |\n|------------------|-------------|\n|[![Build Status](https://travis-ci.org/nyanp/tiny-cnn.svg?branch=master)](https://travis-ci.org/nyanp/tiny-cnn)|[![Build status](https://ci.appveyor.com/api/projects/status/s4mow1544tvoqeeu?svg=true)](https://ci.appveyor.com/project/nyanp/tiny-cnn)|\n\ntiny-cnn is a C++11 implementation of deep learning. It is suitable for deep learning on limited computational resource, embedded systems and IoT devices.\n\n* [Features](#features)\n* [Comparison with other libraries](#comparison-with-other-libraries)\n* [Supported networks](#supported-networks)\n* [Dependencies](#dependencies)\n* [Build](#build)\n* [Examples](#examples)\n* [References](#references)\n* [License](#license)\n* [Mailing list](#mailing-list)\n\nsee [Wiki Pages](https://github.com/nyanp/tiny-cnn/wiki) for more info.\n\n## Features\n- fast, without GPU\n    - with TBB threading and SSE/AVX vectorization\n    - 98.8% accuracy on MNIST in 13 minutes training (@Core i7-3520M)\n- header only\n    - Just include tiny_cnn.h and write your model in c++. There is nothing to install.\n- small dependency & simple implementation\n- [can import caffe's model](https://github.com/nyanp/tiny-cnn/tree/master/examples/caffe_converter)\n\n## Comparison with other libraries\n\n||tiny-cnn|[caffe](https://github.com/BVLC/caffe)|[Theano](https://github.com/Theano/Theano)|[TensorFlow](https://www.tensorflow.org/)|\n|---|---|---|---|---|\n|Prerequisites|__Nothing__(Optional:TBB,OpenMP)|BLAS,Boost,protobuf,glog,gflags,hdf5, (Optional:CUDA,OpenCV,lmdb,leveldb etc)|Numpy,Scipy,BLAS,(optional:nose,Sphinx,CUDA etc)|numpy,six,protobuf,(optional:CUDA,Bazel)|\n|Modeling By|C++ code|Config File|Python Code|Python Code|\n|GPU Support|No|Yes|Yes|Yes|\n|Installing|Unnecessary|Necessary|Necessary|Necessary|\n|Windows Support|Yes|No*|Yes|No*|\n|Pre-Trained Model|Yes(via caffe-converter)|Yes|No*|No*|\n\n*unofficial version is available\n\n## Supported networks\n### layer-types\n* fully-connected layer\n* convolutional layer\n* average pooling layer\n* max-pooling layer\n* contrast normalization layer\n* dropout layer\n* linear operation layer\n\n### activation functions\n* tanh\n* sigmoid\n* softmax\n* rectified linear(relu)\n* leaky relu\n* identity\n* exponential linear units(elu)\n\n### loss functions\n* cross-entropy\n* mean-squared-error\n\n### optimization algorithm\n* stochastic gradient descent (with/without L2 normalization and momentum)\n* stochastic gradient levenberg marquardt\n* adagrad\n* rmsprop\n* adam\n\n## Dependencies\n##### Minimum requirements\nNothing.All you need is a C++11 compiler.\n\n##### Requirements to build sample/test programs\n[OpenCV](http://opencv.org/)\n\n## Build\ntiny-cnn is header-ony, so *there's nothing to build*. If you want to execute sample program or unit tests, you need to install [cmake](https://cmake.org/) and type the following commands:\n\n```\ncmake .\n```\n\nThen open .sln file in visual studio and build(on windows/msvc), or type ```make``` command(on linux/mac/windows-mingw).\n\nSome cmake options are available:\n\n|options|description|default|additional requirements to use|\n|-----|-----|----|----|\n|USE_TBB|Use [Intel TBB](https://www.threadingbuildingblocks.org/) for parallelization|OFF*|[Intel TBB](https://www.threadingbuildingblocks.org/)|\n|USE_OMP|Use OpenMP for parallelization|OFF*|[OpenMP Compiler](http://openmp.org/wp/openmp-compilers/)|\n|USE_SSE|Use Intel SSE instruction set|ON|Intel CPU which supports SSE|\n|USE_AVX|Use Intel AVX instruction set|ON|Intel CPU which supports AVX|\n|BUILD_TESTS|Build unist tests|OFF|-**|\n|BUILD_EXAMPLES|Build example projects|ON|-|\n\n*tiny-cnn use c++11 standard library for parallelization by default\n**to build tests, type `git submodule update --init` before build\n\nFor example, type the following commands if you want to use intel TBB and build tests:\n```bash\ncmake -DUSE_TBB=ON -DBUILD_EXAMPLES=ON .\n```\n\n## Customize configurations\nYou can edit include/config.h to customize default behavior.\n\n## Examples\nconstruct convolutional neural networks\n\n```cpp\n#include \"tiny_cnn/tiny_cnn.h\"\nusing namespace tiny_cnn;\nusing namespace tiny_cnn::activation;\n\nvoid construct_cnn() {\n    using namespace tiny_cnn;\n\n    // specify loss-function and optimization-algorithm\n    network<mse, adagrad> net;\n    //network<cross_entropy, RMSprop> net;\n\n    // add layers\n    net << convolutional_layer<tan_h>(32, 32, 5, 1, 6) // 32x32in, conv5x5, 1-6 f-maps\n        << average_pooling_layer<tan_h>(28, 28, 6, 2) // 28x28in, 6 f-maps, pool2x2\n        << fully_connected_layer<tan_h>(14 * 14 * 6, 120)\n        << fully_connected_layer<identity>(120, 10);\n\n    assert(net.in_dim() == 32 * 32);\n    assert(net.out_dim() == 10);\n    \n    // load MNIST dataset\n    std::vector<label_t> train_labels;\n    std::vector<vec_t> train_images;\n    \n    parse_mnist_labels(\"train-labels.idx1-ubyte\", &train_labels);\n    parse_mnist_images(\"train-images.idx3-ubyte\", &train_images);\n    \n    // train (50-epoch, 30-minibatch)\n    net.train(train_images, train_labels, 30, 50);\n    \n    // save\n    std::ofstream ofs(\"weights\");\n    ofs << net;\n    \n    // load\n    // std::ifstream ifs(\"weights\");\n    // ifs >> net;\n}\n```\nconstruct multi-layer perceptron(mlp)\n\n```cpp\n#include \"tiny_cnn/tiny_cnn.h\"\nusing namespace tiny_cnn;\nusing namespace tiny_cnn::activation;\n\nvoid construct_mlp() {\n    network<mse, gradient_descent> net;\n\n    net << fully_connected_layer<sigmoid>(32 * 32, 300)\n        << fully_connected_layer<identity>(300, 10);\n\n    assert(net.in_dim() == 32 * 32);\n    assert(net.out_dim() == 10);\n}\n```\n\nanother way to construct mlp\n\n```cpp\n#include \"tiny_cnn/tiny_cnn.h\"\nusing namespace tiny_cnn;\nusing namespace tiny_cnn::activation;\n\nvoid construct_mlp() {\n    auto mynet = make_mlp<mse, gradient_descent, tan_h>({ 32 * 32, 300, 10 });\n\n    assert(mynet.in_dim() == 32 * 32);\n    assert(mynet.out_dim() == 10);\n}\n```\n\nmore sample, read examples/main.cpp or [MNIST example](https://github.com/nyanp/tiny-cnn/tree/master/examples/caffe_converter) page.\n\n## References\n[1] Y. Bengio, [Practical Recommendations for Gradient-Based Training of Deep Architectures.](http://arxiv.org/pdf/1206.5533v2.pdf) \n    arXiv:1206.5533v2, 2012\n\n[2] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, [Gradient-based learning applied to document recognition.](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n    Proceedings of the IEEE, 86, 2278-2324.\n    \nother useful reference lists:\n- [UFLDL Recommended Readings](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Recommended_Readings)\n- [deeplearning.net reading list](http://deeplearning.net/reading-list/)\n\n## License\nThe BSD 3-Clause License\n\n## Mailing list\ngoogle group for questions and discussions:\n\nhttps://groups.google.com/forum/#!forum/tiny-cnn-users\n", 
  "id": 55021606
}