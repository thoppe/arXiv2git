{
  "read_at": 1462557936, 
  "description": "FLASH: Fast Bayesian Optimization for Data Analytic Pipelines", 
  "README.md": "# FLASH\n\n## What is FLASH?\n\nFLASH is a package to perform Bayesian optimization on tuning data analytic pipelines. Specifically, it is a two-layer Bayesian optimization framework, which first uses a parametric model to select promising algorithms, then computes a nonparametric model to fine-tune hyperparameters of the promising algorithms.\n\nDetails of FLASH are described in the paper:\n\n**FLASH: Fast Bayesian Optimization for Data Analytic Pipelines** [[arXiv](http://arxiv.org/abs/1602.06468)]  \nYuyu Zhang, Mohammad Taha Bahadori, Hang Su, Jimeng Sun\n\nFLASH is licensed under the GPL license, which can be found in the package.\n\n## Installation\n\nFLASH is developed on top of [HPOlib](http://www.automl.org/hpolib.html), a general platform for hyperparameter optimization. Since HPOlib was developed on Ubuntu and currently only supports Linux distributions, FLASH also only works on Linux (we developed and tested our package on Ubuntu 14.04.4 LTS).\n\n**1. Clone repository**\n```bash\ngit clone https://github.com/yuyuz/FLASH.git\n```\n\n**2. Install Miniconda**\n\nTo avoid a variety of potential problems in environment settings, we highly recommend to use Miniconda (Python 2.7).\n\nIf you are using 64-bit Linux system (recommended):\n```bash\nwget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh\nbash Miniconda-latest-Linux-x86_64.sh\n```\n\nIf you are using 32-bit Linux system:\n```bash\nwget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86.sh\nbash Miniconda-latest-Linux-x86.sh\n```\n\nAnswer ``yes`` when the installer asks to prepend the Miniconda2 install location to PATH in your ``.bashrc``. \n\nAfter installation completed, restart terminal or execute ``source ~/.bashrc`` to make sure that conda has taken charge of your Python environment.\n\n**3. Install dependencies**\n\nNow we install dependencies within conda environment:\n```bash\neasy_install -U distribute\nconda install -y openblas numpy scipy matplotlib scikit-learn==0.16.1\npip install hyperopt liac-arff\n```\n\n**4. Install package**\n\nInstall HPOlib and some requirements (``pymongo``, ``protobuf``, ``networkx``). During the installation, please keep your system **connected to the Internet**, such that ``setup.py`` can download optimizer code packages.\n```bash\ncd /path/to/FLASH\npython setup.py install\n```\n\n## Benchmark Datasets\n\nAll the benchmark datasets are publicly available [here](http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets). These datasets were first introduced by [Auto-WEKA](http://www.cs.ubc.ca/labs/beta/Projects/autoweka) and have been widely used to evaluate Bayesian optimization methods.\n\nDue to the file size limit, we are not able to provide all those datasets in our Github repository. In fact, only the ``madelon`` dataset is provided as an example. To deploy a new benchmark dataset, download the zip file from [here](http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets) and then uncompress it. You will get a dataset folder including two files ``train.arff`` and ``test.arff``. Move this folder into the [data directory](https://github.com/yuyuz/FLASH/tree/master/data), just like the dataset folder ``madelon`` we already put there.\n\n## How to Run?\n\nFor benchmark datasets, we build a general data analytic pipeline based on [scikit-learn](http://scikit-learn.org), following the pipeline design of [auto-sklearn](https://github.com/automl/auto-sklearn). We have 4 computational steps with 33 algorithms in this pipeline. Details are discussed in the [paper](http://arxiv.org/abs/1602.06468).\n\nTo run this pipeline on a specific dataset, first you need to correctly set the configuration file (``/path/to/FLASH/benchmarks/sklearn/config.cfg``):\n\n* In the ``HPOLIB`` section, change the ``function`` path according to your local setting.\n* In the ``HPOLIB`` section, change the ``data_path`` according to your local setting.\n* In the ``HPOLIB`` section, change the ``dataset`` name to whichever dataset you have deployed as the input of pipeline.\n\nNow you can tune the pipeline using different Bayesian optimization methods. For each method, we provide a Python script to run the tuning process.\n\nFor our method, it currently has two versions (with different optimizers in the last phase): **FLASH** and **FLASH<sup>*</sup>**.  \nTo run **FLASH**:\n```bash\ncd /path/to/FLASH/benchmarks/sklearn\npython run_flash.py\n```\n\nTo run **FLASH<sup>*</sup>**:\n```bash\ncd /path/to/FLASH/benchmarks/sklearn\npython run_flash_star.py\n```\n\nFor other methods ([SMAC](http://www.cs.ubc.ca/labs/beta/Projects/SMAC), [TPE](http://jaberg.github.io/hyperopt), Random Search), we use the implementations in [HPOlib](http://www.automl.org/hpolib.html) and also provide Python scripts.  \nTo run SMAC:\n```bash\ncd /path/to/FLASH/benchmarks/sklearn\npython run_smac.py\n```\n\nTo run TPE:\n```bash\ncd /path/to/FLASH/benchmarks/sklearn\npython run_tpe.py\n```\n\nTo run Random Search:\n```bash\ncd /path/to/FLASH/benchmarks/sklearn\npython run_random.py\n```\n\n## Advanced Configurations\n\nIn the configuration file (``/path/to/FLASH/benchmarks/sklearn/config.cfg``), you can set quite a few advanced configurations.\n\nThe configuration items in the ``HPOLIB`` section are effective for all the optimization methods above:\n\n* Set ``use_caching`` as ``1`` to enable pipeline caching, ``0`` to disable caching.\n* ``cv_folds`` specifies the number of cross validation folds during the optimization.\n* For other items such as ``number_of_jobs`` and ``result_on_terminate``, refer to the HPOlib [manual](http://www.automl.org/manual.html).\n\nThe configuration items in the ``LR`` section are only effective for FLASH and FLASH<sup>*</sup>:\n\n* Set ``use_optimal_design`` as ``1`` to enable optimal design for initialization, ``0`` to use random initialization.\n* ``init_budget`` specifies the number of iterations for Phase 1 (initialization).\n* ``ei_budget`` specifies the number of iterations for Phase 2 (pruning).\n* ``bopt_budget`` specifies the number of iterations for Phase 3 (fine-tuning).\n* ``ei_xi`` is the trade-off parameter x in EI and EIPS functions, which balances the exploitation and\nexploration.\n* ``top_k_pipelines`` specifies the number of best pipeline paths to select at the end of Phase 2 (pruning).\n", 
  "id": 51542533
}