{
  "id": 14445155, 
  "read_at": 1462556831, 
  "README.txt": "pyaixi\r\n======\r\n\r\nDescription\r\n-----------\r\n\r\nA pure Python implementation of the Monte Carlo-AIXI-Context Tree Weighting (MC-AIXI-CTW)\r\nartificial intelligence algorithm.\r\n\r\nThis is an approximation of the AIXI universal artificial intelligence algorithm, which\r\ndescribes a model-based, reinforcement-learning agent capable of general learning.\r\n\r\n\r\nA more in-depth description of the MC-AIXI-CTW algorithm can be found here:\r\n\r\nJ.Veness, K.S.Ng, M.Hutter, W.Uther, D.Silver,\r\nA Monte Carlo AIXI Approximation,\r\nJournal of Artificial Intelligence Research, 40 (2011) 95-142\r\nhttp://dx.doi.org/10.1613/jair.3125\r\nFree TechReport version: http://arxiv.org/abs/0909.0801\r\nBibTeX: http://www.hutter1.net/official/bib.htm#aixictwx\r\n\r\n\r\nMotivation\r\n----------\r\n\r\nProviding a pure Python implementation of the MC-AIXI-CTW algorithm is intended to:\r\n\r\n- help make the implementation of AIXI-approximate algorithms more accessible to people\r\n  without a C++ background\r\n\r\n- permit easier use of the MC-AIXI-CTW algorithm (and components) in other Python-based\r\n  AI projects, and\r\n\r\n- permit faster prototyping of new AIXI-approximate algorithms via Python's comparative\r\n  linguistic simplicity.\r\n\r\n\r\nGetting started\r\n---------------\r\n\r\nTo try the example `Rock Paper Scissors` environment, run the following in the\r\nbase directory of this package.\r\n\r\nFrom the Linux/Unix/Mac console:\r\n\r\npython aixi.py -v conf/rock_paper_scissors_fast.conf\r\n\r\n\r\nOn Windows:\r\n\r\npython aixi.py -v conf\\rock_paper_scissors_fast.conf\r\n\r\n\r\nOr if you have PyPy (e.g. version 1.9) installed on Linux/Unix/Mac:\r\n\r\npypy-c1.9 aixi.py -v conf/rock_paper_scissors_fast.conf\r\n\r\n\r\nNOTE: it is highly recommended to use the PyPy http://pypy.org Python interpreter to\r\nrun code from this package, as this typically provides an order-of-magnitude run-time\r\nimprovement over using the standard CPython interpreter.\r\n\r\n(This is unfortunately still an order of magnitude slower than the C++ version, though.)\r\n\r\n\r\nThis example will perform 500 interactions of the agent with the environment, with the agent\r\nexploring the environment by trying permitted actions at random, and learning from\r\nthe related observations and rewards.\r\n\r\nThen, the agent will use what it has learnt to maximise its reward in the following\r\n500 interactions. (Exploration is typically quite quick, while using that gained knowledge\r\nto choose the best action possible is typically much slower.)\r\n\r\n\r\nFor this particular environment, an average reward greater than 1 means the agent is winning\r\nmore than it is losing.\r\n\r\n(A score ranging from 1.02 to 1.04 is typical, depending on the random seed given.)\r\n\r\n\r\nFurther example environments can be found in the `environments` directory:\r\n\r\n - coin_flip            - A simulation of a biased coin flip\r\n - extended_tiger       - An extended version of the Tiger-or-Gold door choice problem.\r\n - kuhn_poker           - A simplified, zero-sum version of poker.\r\n - maze                 - A two-dimensional maze.\r\n - rock_paper_scissors  - Rock Paper Scissors.\r\n - tic_tac_toe          - Tic Tac Toe\r\n - tiger                - A choice between two doors. One door hides gold; the other, a tiger.\r\n\r\nSimilarly-named environment configuration files for these environments can be found in the\r\n`conf` directory, and run by replacing `rock_paper_scissors_fast.conf` in the commands\r\nlisted above with the name of the appropriate configuration file.\r\n\r\n\r\nScript usage\r\n------------\r\n\r\nUsage: python aixi.py [-a | --agent <agent module name>]\r\n                      [-d | --explore-decay <exploration decay value, between 0 and 1>]\r\n                      [-e | --environment <environment module name>]\r\n                      [-h | --agent-horizon <search horizon>]\r\n                      [-l | --learning-period <cycle count>]\r\n                      [-m | --mc-simulations <number of simulations to run each step>]\r\n                      [-o | --option <extra option name>=<value>]\r\n                      [-p | --profile]\r\n                      [-r | --terminate-age <number of cycles before stopping the run>]\r\n                      [-t | --ct-depth <maximum depth of predicting context tree>]\r\n                      [-x | --exploration <exploration factor, greater than 0>]\r\n                      [-v | --verbose]\r\n                      [<environment configuration file name to load>]\r\n\r\n\r\nAdding new environments\r\n-----------------------\r\n\r\nThe environments in the `environments` directory all inherit from\r\na base class, `environment.Environment`, found in the base package directory.\r\n\r\nNew environments will need to inherit this class, and provide the methods\r\nof this class (as well as any internal logic) to interact with the agent.\r\n\r\nYou'll also need to construct a new configuration file for this environment,\r\nmaking sure to give the name of your new environment in the `environment` key.\r\n\r\n\r\nAdding new agents\r\n-----------------\r\n\r\nThe only (for now) provided agent class can be found in the `agent` directory:\r\n\r\n - mc_aixi_ctw - an agent implementing the Monte Carlo-AIXI-Context Tree Weighting algorithm.\r\n\r\n\r\nThe prediction algorithm used by this agent can be found in the `prediction` directory:\r\n\r\n - ctw_context_tree - an implementation of Context Tree Weighting context trees.\r\n\r\n\r\nThe search algorithm used is found in the `search` directory:\r\n\r\n - monte_carlo_search_tree - an implementation of Monte Carlo search trees.\r\n\r\n\r\nNew agents need to inherit from the base `agent.Agent` class, and provide the methods\r\nlisted within to interact with the currently-configured environment.\r\n\r\nTo use your own agent instead of the default `mc_aixi_ctw` agent in a configuration file,\r\nuse the `agent` key to specify the Python module name of your agent.\r\n\r\nAlternatively, you can override the default/the configuration file value, by using\r\nthe '-a'/'--agent' option on the command line.\r\n\r\n\r\nSimilar projects\r\n----------------\r\n\r\nThis package is based on the C++ implementation of the MC-AIXI-CTW algorithm seen here:\r\n\r\nhttps://github.com/moridinamael/mc-aixi\r\n\r\n\r\nAnother implementation of MC-AIXI-CTW can be found here:\r\n\r\nJoel Veness's personal page: http://jveness.info/software/default.html\r\n\r\n\r\nLicense\r\n-------\r\n\r\nCreative Commons Attribution ShareAlike 3.0 Unported. (CC BY-SA 3.0)\r\n\r\nPlease see `LICENSE.txt` for details.\r\n\r\nIf permitted in your legal domain (as this package is arguably a substantive\r\nderivative of another CC BY-SA 3.0 package, hence the licensing terms above,\r\nand the legal compatibility of CC BY-SA 3.0 with other open-source licences is currently\r\nunknown), the author of this package permits alternate licensing under your\r\nchoice of either the LGPL 3.0 or the GPL 3.0.\r\n\r\n\r\nContact the author\r\n------------------\r\n\r\nFor further assistance or to offer constructive feedback, please contact the author,\r\nGeoff Kassel, via:\r\n\r\ngeoffkassel_at_gmail_dot_com", 
  "README.md": "pyaixi\r\n======\r\n\r\nDescription\r\n-----------\r\n\r\nA pure Python implementation of the Monte Carlo-AIXI-Context Tree Weighting (MC-AIXI-CTW)\r\nartificial intelligence algorithm.\r\n\r\nThis is an approximation of the AIXI universal artificial intelligence algorithm, which\r\ndescribes a model-based, reinforcement-learning agent capable of general learning.\r\n\r\n\r\nA more in-depth description of the MC-AIXI-CTW algorithm can be found here:\r\n\r\nJ.Veness, K.S.Ng, M.Hutter, W.Uther, D.Silver,\r\nA Monte Carlo AIXI Approximation,\r\nJournal of Artificial Intelligence Research, 40 (2011) 95-142\r\nhttp://dx.doi.org/10.1613/jair.3125\r\nFree TechReport version: http://arxiv.org/abs/0909.0801\r\nBibTeX: http://www.hutter1.net/official/bib.htm#aixictwx\r\n\r\n\r\nMotivation\r\n----------\r\n\r\nProviding a pure Python implementation of the MC-AIXI-CTW algorithm is intended to:\r\n\r\n- help make the implementation of AIXI-approximate algorithms more accessible to people\r\n  without a C++ background\r\n\r\n- permit easier use of the MC-AIXI-CTW algorithm (and components) in other Python-based\r\n  AI projects, and\r\n\r\n- permit faster prototyping of new AIXI-approximate algorithms via Python's comparative\r\n  linguistic simplicity.\r\n\r\n\r\nGetting started\r\n---------------\r\n\r\nTo try the example `Rock Paper Scissors` environment, run the following in the\r\nbase directory of this package.\r\n\r\nFrom the Linux/Unix/Mac console:\r\n\r\npython aixi.py -v conf/rock_paper_scissors_fast.conf\r\n\r\n\r\nOn Windows:\r\n\r\npython aixi.py -v conf\\rock_paper_scissors_fast.conf\r\n\r\n\r\nOr if you have PyPy (e.g. version 1.9) installed on Linux/Unix/Mac:\r\n\r\npypy-c1.9 aixi.py -v conf/rock_paper_scissors_fast.conf\r\n\r\n\r\nNOTE: it is highly recommended to use the PyPy http://pypy.org Python interpreter to\r\nrun code from this package, as this typically provides an order-of-magnitude run-time\r\nimprovement over using the standard CPython interpreter.\r\n\r\n(This is unfortunately still an order of magnitude slower than the C++ version, though.)\r\n\r\n\r\nThis example will perform 500 interactions of the agent with the environment, with the agent\r\nexploring the environment by trying permitted actions at random, and learning from\r\nthe related observations and rewards.\r\n\r\nThen, the agent will use what it has learnt to maximise its reward in the following\r\n500 interactions. (Exploration is typically quite quick, while using that gained knowledge\r\nto choose the best action possible is typically much slower.)\r\n\r\n\r\nFor this particular environment, an average reward greater than 1 means the agent is winning\r\nmore than it is losing.\r\n\r\n(A score ranging from 1.02 to 1.04 is typical, depending on the random seed given.)\r\n\r\n\r\nFurther example environments can be found in the `environments` directory:\r\n\r\n - coin_flip            - A simulation of a biased coin flip\r\n - extended_tiger       - An extended version of the Tiger-or-Gold door choice problem.\r\n - kuhn_poker           - A simplified, zero-sum version of poker.\r\n - maze                 - A two-dimensional maze.\r\n - rock_paper_scissors  - Rock Paper Scissors.\r\n - tic_tac_toe          - Tic Tac Toe\r\n - tiger                - A choice between two doors. One door hides gold; the other, a tiger.\r\n\r\nSimilarly-named environment configuration files for these environments can be found in the\r\n`conf` directory, and run by replacing `rock_paper_scissors_fast.conf` in the commands\r\nlisted above with the name of the appropriate configuration file.\r\n\r\n\r\nScript usage\r\n------------\r\n\r\nUsage: python aixi.py [-a | --agent <agent module name>]\r\n                      [-d | --explore-decay <exploration decay value, between 0 and 1>]\r\n                      [-e | --environment <environment module name>]\r\n                      [-h | --agent-horizon <search horizon>]\r\n                      [-l | --learning-period <cycle count>]\r\n                      [-m | --mc-simulations <number of simulations to run each step>]\r\n                      [-o | --option <extra option name>=<value>]\r\n                      [-p | --profile]\r\n                      [-r | --terminate-age <number of cycles before stopping the run>]\r\n                      [-t | --ct-depth <maximum depth of predicting context tree>]\r\n                      [-x | --exploration <exploration factor, greater than 0>]\r\n                      [-v | --verbose]\r\n                      [<environment configuration file name to load>]\r\n\r\n\r\nAdding new environments\r\n-----------------------\r\n\r\nThe environments in the `environments` directory all inherit from\r\na base class, `environment.Environment`, found in the base package directory.\r\n\r\nNew environments will need to inherit this class, and provide the methods\r\nof this class (as well as any internal logic) to interact with the agent.\r\n\r\nYou'll also need to construct a new configuration file for this environment,\r\nmaking sure to give the name of your new environment in the `environment` key.\r\n\r\n\r\nAdding new agents\r\n-----------------\r\n\r\nThe only (for now) provided agent class can be found in the `agent` directory:\r\n\r\n - mc_aixi_ctw - an agent implementing the Monte Carlo-AIXI-Context Tree Weighting algorithm.\r\n\r\n\r\nThe prediction algorithm used by this agent can be found in the `prediction` directory:\r\n\r\n - ctw_context_tree - an implementation of Context Tree Weighting context trees.\r\n\r\n\r\nThe search algorithm used is found in the `search` directory:\r\n\r\n - monte_carlo_search_tree - an implementation of Monte Carlo search trees.\r\n\r\n\r\nNew agents need to inherit from the base `agent.Agent` class, and provide the methods\r\nlisted within to interact with the currently-configured environment.\r\n\r\nTo use your own agent instead of the default `mc_aixi_ctw` agent in a configuration file,\r\nuse the `agent` key to specify the Python module name of your agent.\r\n\r\nAlternatively, you can override the default/the configuration file value, by using\r\nthe '-a'/'--agent' option on the command line.\r\n\r\n\r\nSimilar projects\r\n----------------\r\n\r\nThis package is based on the C++ implementation of the MC-AIXI-CTW algorithm seen here:\r\n\r\nhttps://github.com/moridinamael/mc-aixi\r\n\r\n\r\nAnother implementation of MC-AIXI-CTW can be found here:\r\n\r\nJoel Veness's personal page: http://jveness.info/software/default.html\r\n\r\n\r\nLicense\r\n-------\r\n\r\nCreative Commons Attribution ShareAlike 3.0 Unported. (CC BY-SA 3.0)\r\n\r\nPlease see `LICENSE.txt` for details.\r\n\r\nIf permitted in your legal domain (as this package is arguably a substantive\r\nderivative of another CC BY-SA 3.0 package, hence the licensing terms above,\r\nand the legal compatibility of CC BY-SA 3.0 with other open-source licences is currently\r\nunknown), the author of this package permits alternate licensing under your\r\nchoice of either the LGPL 3.0 or the GPL 3.0.\r\n\r\n\r\nContact the author\r\n------------------\r\n\r\nFor further assistance or to offer constructive feedback, please contact the author,\r\nGeoff Kassel, via:\r\n\r\ngeoffkassel_at_gmail_dot_com", 
  "description": ""
}