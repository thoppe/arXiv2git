{
  "read_at": 1462546209, 
  "description": "Deep Recurrent Neural Network for Audio Source Separation", 
  "README.md": "**Deep Recurrent Neural Network for Audio Source Separation**\n\nA Matlab implementation of DRNN for monaural audio source separation based on the joint optimization of masking functions and discriminative training criteria of Huang et al. [1].\n\nWith this framework one can also set several kinds of inicializations for DRNN, such as: \"Xavier\" [2], \"He\" [3] and \"IRNN\" [4].\n\n**Getting Started**\n\n1. training: run_train.m (run_train.sh is only used to run the scrip into the server)\n \n2. testing: run_test.m\n\nTo try the codes on your data: put your data into a waves/ folder. The waves/ folder should be into the same root folder as DRNN4ASS/.\n\nInside DRNN4ASS/ the directories DRNN4ASS/models and DRNN4ASS/resultWaves should also be created to store the models and the output .wav files.\n\nLook at the unit test parameters below codes/ or the parameters used in run_train.m and run_test.m. \n\n**Dependencies**\n\n1. The package is modified based on deeplearningsourceseparation (Reference: https://sites.google.com/site/deeplearningsourceseparation/) that at its time is based on rnn-speech-denoising (Reference: https://github.com/amaas/rnn-speech-denoising).\n\n2. Mark Schmidt's minFunc package for convex optimization (Reference: http://www.di.ens.fr/~mschmidt/Software/minFunc.html).\n\n3. Mark Hasegawa-Johnson's HTK write and read functions that are used to handle the features files (Reference: http://www.isle.illinois.edu/sst/software/).\n\n4. HTK for computing features (Reference: http://htk.eng.cam.ac.uk/).\n\n5. Signal processing functions from Labrosa (Reference: http://labrosa.ee.columbia.edu/).\n\n6. BSS Eval toolbox Version 3.0 for evaluation (Reference: http://bass-db.gforge.inria.fr/bss_eval/).\n\n**References**\n\n[1] Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson, Paris Smaragdis (2015). \"Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation\" to appear in IEEE/ACM Transactions on Audio, Speech, and Language Processing.\n\n[2] Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In International conference on artificial intelligence and statistics (pp. 249-256).\n\n[3] He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. arXiv preprint arXiv:1502.01852.\n\n[4] Le, Q. V., Jaitly, N., & Hinton, G. E. (2015). A Simple Way to Initialize Recurrent Networks of Rectified Linear Units. arXiv preprint arXiv:1504.00941.\n\n**Author of the modification**\n\nJordi Pons (idrojsnop@gmail.com)\n", 
  "id": 39386630
}