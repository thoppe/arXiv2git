{
  "read_at": 1462554035, 
  "description": "spca is an R package for Sparse Principal Component Analysis ", 
  "readme.R": "## ----, echo = FALSE------------------------------------------------------\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  fig.path = \"README-\"\n)\n\n## ----example, fig.path='README_files/'-----------------------------------\nlibrary(spca)\ndata(bsbl)\n#Ordinary PCA\nbpca = pca(bsbl, screeplot = FALSE, kaiser.print = TRUE)\n\n#-Sparse PCA\nbbe1 <- spcabe(bsbl, nd = 4, thresh = 0.25, unc = FALSE)\n\n#-summary output\nsummary(bbe1) \n#-# Explaining over 96% of the variance explained by PCA with 2, 3, 3 and 1 variables.\n\n#-print percentage contributions\nbbe1\n#-# Simple combinations of offensive play in career and in season are most important.\n#-# Defensive play appears only in 3rd component.\n\n#-plot solution\nplot(bbe1, plotloadvsPC = TRUE, pc = bpca, mfr = 2, mfc = 2, \n               variablesnames = TRUE)\n#-# Explaining the variance pretty closely to PCA with much fewer variables.\n\n", 
  "README.rmd": "---\ntitle: \"spca package\"\nauthor: \"Giovanni Merola<br>\nRMIT International University Vietnam<br>\nemail: lsspca@gmail.com<br>\nrepository: https://github.com/merolagio/spca\"\ndate: \"`r format(Sys.time(), '%d %B %Y')`\"\noutput:\n  rmarkdown::html_document:\n    toc: true\n    theme: united\n    highlight: haddock\n---\n\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  fig.path = \"README-\"\n)\n```\n\n# spca  \n<!--    -->\n[![Build Status](https://travis-ci.org/merolagio/spca.png?branch=master)](https://travis-ci.org/merolagio/spca)\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->  \n\n\n### Intro  \n`spca` is an R package for running Sparse Principal Component Analysis. It implements the LS SPCA approach that computes the Least Squares estimates of sparse PCs. the LS SPCA solutions maximise the variance of the data explained,  Unlike other existing SPCA methods. Details can be found in [Merola, 2014. arXiv](http://arxiv.org/abs/1406.1381v2 \"Pre-print\") and in the forthcoming paper in *Australia and New Zealand Journal of Statistics*. \n\nI had difficulties publishing the LS SPCA paper, possibly because LS SPCA improves on existing methods. This is confirmed by the fact that Technometrics' chief editor, Dr Qiu, rejected the paper endorsing a report stating that: **the LS criterion is a new measure used ad-hoc**   :-D This on top of a number of other blatantly wrong arguments. Dr Qiu added in his rejection letter that the algorithms are not scalable. Now, this is arbitrary because computational efficiency is not in the scope and aims of Technometrics, In a resubmission a reviewer from *ANZJS* asked me to **compare the about 20 existing SPCA methods with mine on more datasets** (only because I show that my solutions maximise the variance explained and theirs don't!) Thankfully, the editors of this journal accepted my refusal to do so.   \n\n### Sparse Principal Component Analysis\nPrincipal Component Analysis is used for analysing a multivariate dataset with two or three uncorrelated components that explain the most variance of the data. \n\nIn some situations more than three components are used. But this simply reduces a multivariate problem into a lower dimensional one, which is still difficult to analyse.\n\nSPCA aims to obtain interpretable components.  In Factor Analysis literature there is plenty of discussion about the  definition of interpretable and simple solutions (as qualities and mathematical measures).\n\n* **Simplicity** can be defined in different terms linked to **sparsness**, **variance explained** and **size of the loadings**, for example. \n\n* **interpretability** is, instead, also linked to **which variables are included** in the solution  and is not measurable.\n    * it usually requires **expert knowledge**.\n\nFor these reasons, usually there exist different competing solutions and it is necessary to choose the *best* ones among these. You can think of this as a sort of model selection in regression analysis.\n\n### Use of the package\n\n**spca aims to obtain interpretable solutions**\n\nSPCA solutions, not unlikely regression models, can be evaluated under different criteria. The package `spca` can  easily produce plots and tables useful for comparing solutions under satisfying different criteria.\n\n**spca is implemented as an exploratory data analysis tool** \n\nThe cardinality of the components can be chosen interactively after inspecting trace and plots of solutions of different cardinality.\n\nSeveral solutions can be easily computed so as to:\n\n* have uncorrelated components or not.\n\n* have a minimal cardinality. \n\n* reproduce a given proportion of the variance explained by the full PCs. \n\n* have only contributions larger than a given threshold.\n\nSolutions computed under different settings can be visualised in different ways with pots and summaries, also coparatitevely.\n\n`spca` can be helpful also in a confirmatory stage of the analysis, since the sparse components can be constrained to be made up of only chosen variables. If a partition (a list) of indices is known before, it can be passed as the `startind` argument to `spca::spca()`\n\nBeside this quick tour of the package, there are vignettes with examples and explanations. You can start with `vignette(\"Introduction to spca\", package = \"spca\")`, which is similar to this document but more detailed.` Other vignettes contain an extended example and a navigable help. \n\n### Optimisation Models  \nFinding the optimal indices for an *spca* solution is an intractable NP-hard problem.  \n\nTherefore, we find the solutions through two greedy algorthms: Branch-and-Bound (**BB**) and Backward Elimination (**BE**).\n\n* **BB** searches for the solutions that sequentially maximise the variance explained under the constraints. The solutions may not be a global maximum when more than one component is computed. The BB algorithm is a modification of Farcomeni's (2010) (thanks!).\n\n* **BE** has the goal of attaining larger contributions while minimising the LS criteria. It sequentially eliminates the smallest contributions (in absolute value) from a non-sparse solution. This will generally lead to explaining less variance than the **BB** search. However, the **BE** algorithm is much faster than the **BB** one, and the solutions usually have large loadings.\n\nThe **BE** algorthm is illustrated in `vignettes(\"BE algorithm\", package = \"spca\")`\n\n### Computing the solutions\n`spca` uses simple S3 methods. The exported functions and methods are given below.\n\n## Functions\nThe workhorses of the package are the functions `uspca` and `cspca`, which compute the uncorrelated and correlated solutions for a given set of indices. These are not available but the function\n\n- ` spca::spca `\n\nimplements them for given indices of a nember of components.\n\nSets of indices can be found through the **BB** and **BE** searches with:\n\n- ` spca::spcabb ` \n\nand \n\n- `spca::spcabe`\n\n`help(spcabb)` and `help(spcabe)` provide examples of using spca and the utilities. Calling `vignettes(\"Advanced Example\", package = \"spca\")` a complete example and details on the methods will be displayed.\n\n## Methods\nThe package contains the methods for plotting, printing and comparing spca solutions given below.\n\n- `choosecard`: interactive method for choosing the cardinality. It plots and prints statistics for comparing solutions of different cardinality.\n\n- print`: shows a formatted matrix of sparse loadings, or *contributions*, of a solution. Contributions are scaled to percentages while loadings are scaled to unit sum of squares,\n\n- `showload`: prints only the non-zero sparse loadings. This is useful when the number of variables is large.\n\n- `summary`: shows formatted summary statistics of a solution\n\n- `plot`: plots the cumulative variance explained by the sparse solutions with that explained by the PCs, which is its upper bound. It can also plot the sparse contributions in different ways.\n\n- `compare`: plots and prints comparisons of two or more *spca* objects.\n\n\n## Class spca \n`spca` functions return objects of class *spca* to which methods are applied. The minimal spca object contains the following elements:\n\n- *loadings*   A matrix with the loadings scaled to unit L_2 norm in the columns.\n- *vexp*\t A vector with the % variance explained by each component.\n- *vexpPC*\t A vector with the % variance explained by each principal component.\n- *ind*\t A list of the indices of the sparse loadings\n\n### Minimal Example\n\nThe naming of the arguments in R is not simple, mainly because different conventions have been used over the years. I tried to follow [Harley Wickam's suggestions](http://r-pkgs.had.co.nz/style.html), not completely. I tried to use consistent argument names in different functions and to give meaningful names, starting differently so that `R`'s useful feature of partial matching the arguments can be exploited. In the following example I sometime use partial arguments names.\n\n```{r example, fig.path='README_files/'}\nlibrary(spca)\npackageVersion(\"spca\")\ndata(bsbl)\n#Ordinary PCA\nbpca = pca(bsbl, screeplot = FALSE, kaiser.print = TRUE)\n\n#-Sparse PCA (using the Backward Elimination method `spcabe`) \nbbe1 <- spcabe(bsbl, nd = 4, thresh = 0.25, unc = FALSE)\n\n#-summary output: \nmessage(\"variance explained, cardinality e minimum contribution\")\nsummary(bbe1) \n#-# Explaining over 96% of the variance explained by PCA with 2, 3, 3 and 1 variables.\n\n#-print percentage contributions\nmessage(\"Sparse contributions\")\nbbe1\n#-# Simple combinations of offensive play in career and in season are most important.\n#-# Defensive play appears only in 3rd component.\n\n#-plot sparse solution againts full PCs\nplot(bbe1, plotloadvsPC = TRUE, pc = bpca, mfr = 2, mfc = 2, \n               variablesnames = TRUE)\n#-# Explaining the variance pretty closely to PCA with much fewer variables.\n```\n\n### Installing the package\n\n* the latest released version from CRAN with\n\n```R\ninstall.packages(\"spca\")\n````\n\n* The latest development version from github with\n\n```R\nif (packageVersion(\"spca\") < 0.4.0) {\n  install.packages(\"devtools\")\n}\ndevtools::install_github(\"merolagio/spca\")\n```\n\n###Comments\nThis is the first release and will surely contain some bugs, even though I tried to test it. Please do let me know if you find any or can suggest improvements. You can use the *Github* tools for [submitting issues](https://github.com/merolagio/spca/issues/new ) or contributions.\n\nFor now most of the plots are produced with the basic plotting functions. In a later release i will produce them with ggplot2 (it requires learning the package :-).\n\nThe code is implemented in R, so it will not work for large datasets. Optimizing R is a difficult task because one is using functions written for generic tasks. This is surely a problem with matrix algebra, also due to the poor documentation available. Packages like `ff` and `bigmemory` may be useful to analyze very large matrices that would exhaust rhe RAM, once they are fully developed and documented. Working on my tetra-core laptop, I found useful using the `foreach` and `doParallel` packages to speed up the computations. These, however, will not help the RAM. \n\nI have in mind to develop C routines at least for the matrix algebra. Anybody willing to help, please, let me know. \n", 
  "id": 26708396, 
  "README.md": "---\ntitle: \"spca package\"\nauthor: \"Giovanni Merola<br>\nRMIT International University Vietnam<br>\nemail: lsspca@gmail.com<br>\nrepository: https://github.com/merolagio/spca\"\ndate: \"15 February 2015\"\noutput:\n  rmarkdown::html_document:\n    toc: true\n    theme: united\n    highlight: haddock\n---\n\n\n\n\n# spca  \n<!--    -->\n[![Build Status](https://travis-ci.org/merolagio/spca.png?branch=master)](https://travis-ci.org/merolagio/spca)\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->  \n\n\n### Intro  \n`spca` is an R package for running Sparse Principal Component Analysis. It implements the LS SPCA approach that computes the Least Squares estimates of sparse PCs. the LS SPCA solutions maximise the variance of the data explained,  Unlike other existing SPCA methods. Details can be found in [Merola, 2014. arXiv](http://arxiv.org/abs/1406.1381v2 \"Pre-print\") and in the forthcoming paper in *Australia and New Zealand Journal of Statistics*. \n\nI had difficulties publishing the LS SPCA paper, possibly because LS SPCA improves on existing methods. This is confirmed by the fact that Technometrics' chief editor, Dr Qiu, rejected the paper endorsing a report stating that: **the LS criterion is a new measure used ad-hoc**   :-D This on top of a number of other blatantly wrong arguments. Dr Qiu added in his rejection letter that the algorithms are not scalable. Now, this is arbitrary because computational efficiency is not in the scope and aims of Technometrics, In a resubmission a reviewer from *ANZJS* asked me to **compare the about 20 existing SPCA methods with mine on more datasets** (only because I show that my solutions maximise the variance explained and theirs don't!) Thankfully, the editors of this journal accepted my refusal to do so.   \n\n### Sparse Principal Component Analysis\nPrincipal Component Analysis is used for analysing a multivariate dataset with two or three uncorrelated components that explain the most variance of the data. \n\nIn some situations more than three components are used. But this simply reduces a multivariate problem into a lower dimensional one, which is still difficult to analyse.\n\nSPCA aims to obtain interpretable components.  In Factor Analysis literature there is plenty of discussion about the  definition of interpretable and simple solutions (as qualities and mathematical measures).\n\n* **Simplicity** can be defined in different terms linked to **sparsness**, **variance explained** and **size of the loadings**, for example. \n\n* **interpretability** is, instead, also linked to **which variables are included** in the solution  and is not measurable.\n    * it usually requires **expert knowledge**.\n\nFor these reasons, usually there exist different competing solutions and it is necessary to choose the *best* ones among these. You can think of this as a sort of model selection in regression analysis.\n\n### Use of the package\n\n**spca aims to obtain interpretable solutions**\n\nSPCA solutions, not unlikely regression models, can be evaluated under different criteria. The package `spca` can  easily produce plots and tables useful for comparing solutions under satisfying different criteria.\n\n**spca is implemented as an exploratory data analysis tool** \n\nThe cardinality of the components can be chosen interactively after inspecting trace and plots of solutions of different cardinality.\n\nSeveral solutions can be easily computed so as to:\n\n* have uncorrelated components or not.\n\n* have a minimal cardinality. \n\n* reproduce a given proportion of the variance explained by the full PCs. \n\n* have only contributions larger than a given threshold.\n\nSolutions computed under different settings can be visualised in different ways with pots and summaries, also coparatitevely.\n\n`spca` can be helpful also in a confirmatory stage of the analysis, since the sparse components can be constrained to be made up of only chosen variables. If a partition (a list) of indices is known before, it can be passed as the `startind` argument to `spca::spca()`\n\nBeside this quick tour of the package, there are vignettes with examples and explanations. You can start with `vignette(\"Introduction to spca\", package = \"spca\")`, which is similar to this document but more detailed.` Other vignettes contain an extended example and a navigable help. \n\n### Optimisation Models  \nFinding the optimal indices for an *spca* solution is an intractable NP-hard problem.  \n\nTherefore, we find the solutions through two greedy algorthms: Branch-and-Bound (**BB**) and Backward Elimination (**BE**).\n\n* **BB** searches for the solutions that sequentially maximise the variance explained under the constraints. The solutions may not be a global maximum when more than one component is computed. The BB algorithm is a modification of Farcomeni's (2010) (thanks!).\n\n* **BE** has the goal of attaining larger contributions while minimising the LS criteria. It sequentially eliminates the smallest contributions (in absolute value) from a non-sparse solution. This will generally lead to explaining less variance than the **BB** search. However, the **BE** algorithm is much faster than the **BB** one, and the solutions usually have large loadings.\n\nThe **BE** algorthm is illustrated in `vignettes(\"BE algorithm\", package = \"spca\")`\n\n### Computing the solutions\n`spca` uses simple S3 methods. The exported functions and methods are given below.\n\n## Functions\nThe workhorses of the package are the functions `uspca` and `cspca`, which compute the uncorrelated and correlated solutions for a given set of indices. These are not available but the function\n\n- ` spca::spca `\n\nimplements them for given indices of a nember of components.\n\nSets of indices can be found through the **BB** and **BE** searches with:\n\n- ` spca::spcabb ` \n\nand \n\n- `spca::spcabe`\n\n`help(spcabb)` and `help(spcabe)` provide examples of using spca and the utilities. Calling `vignettes(\"Advanced Example\", package = \"spca\")` a complete example and details on the methods will be displayed.\n\n## Methods\nThe package contains the methods for plotting, printing and comparing spca solutions given below.\n\n- `choosecard`: interactive method for choosing the cardinality. It plots and prints statistics for comparing solutions of different cardinality.\n\n- print`: shows a formatted matrix of sparse loadings, or *contributions*, of a solution. Contributions are scaled to percentages while loadings are scaled to unit sum of squares,\n\n- `showload`: prints only the non-zero sparse loadings. This is useful when the number of variables is large.\n\n- `summary`: shows formatted summary statistics of a solution\n\n- `plot`: plots the cumulative variance explained by the sparse solutions with that explained by the PCs, which is its upper bound. It can also plot the sparse contributions in different ways.\n\n- `compare`: plots and prints comparisons of two or more *spca* objects.\n\n\n## Class spca \n`spca` functions return objects of class *spca* to which methods are applied. The minimal spca object contains the following elements:\n\n- *loadings*   A matrix with the loadings scaled to unit L_2 norm in the columns.\n- *vexp*\t A vector with the % variance explained by each component.\n- *vexpPC*\t A vector with the % variance explained by each principal component.\n- *ind*\t A list of the indices of the sparse loadings\n\n### Minimal Example\n\nThe naming of the arguments in R is not simple, mainly because different conventions have been used over the years. I tried to follow [Harley Wickam's suggestions](http://r-pkgs.had.co.nz/style.html), not completely. I tried to use consistent argument names in different functions and to give meaningful names, starting differently so that `R`'s useful feature of partial matching the arguments can be exploited. In the following example I sometime use partial arguments names.\n\n\n```r\nlibrary(spca)\npackageVersion(\"spca\")\n#> [1] '0.6.1.9000'\ndata(bsbl)\n#Ordinary PCA\nbpca = pca(bsbl, screeplot = FALSE, kaiser.print = TRUE)\n#> [1] \"number of eigenvalues larger than 1 is 3\"\n\n#-Sparse PCA (using the Backward Elimination method `spcabe`) \nbbe1 <- spcabe(bsbl, nd = 4, thresh = 0.25, unc = FALSE)\n\n#-summary output: \nmessage(\"variance explained, cardinality e minimum contribution\")\n#> variance explained, cardinality e minimum contribution\nsummary(bbe1) \n#>            Comp1 Comp2 Comp3 Comp4\n#> PVE        44.4% 24.9% 10.3% 5.6% \n#> PCVE       44.4% 69.3% 79.6% 85.2%\n#> PRCVE      96.4% 96.1% 96.6% 97.1%\n#> Card       2     3     3     1    \n#> Ccard      2     5     8     9    \n#> PVE/Card   22.2% 8.3%  3.4%  5.6% \n#> PCVE/Ccard 22.2% 13.9% 10%   9.5% \n#> Converged  0     0     0     0    \n#> MinCont    31.5% 26.3% 28.2% 100%\n#-# Explaining over 96% of the variance explained by PCA with 2, 3, 3 and 1 variables.\n\n#-print percentage contributions\nmessage(\"Sparse contributions\")\n#> Sparse contributions\nbbe1\n#> Percentage Contributions\n#>        Comp1 Comp2 Comp3 Comp4\n#> TAB_86  31.5  35.2            \n#> HR_86               28.2      \n#> RUN_86        26.3            \n#> RUN          -38.5            \n#> RUNB    68.5                  \n#> PO_86                      100\n#> ASS_86             -40.9      \n#> ERR_86             -30.9      \n#>        ----- ----- ----- -----\n#> PCVE   44.4  69.3  79.6  85.2 \n#> \n#-# Simple combinations of offensive play in career and in season are most important.\n#-# Defensive play appears only in 3rd component.\n\n#-plot sparse solution againts full PCs\nplot(bbe1, plotloadvsPC = TRUE, pc = bpca, mfr = 2, mfc = 2, \n               variablesnames = TRUE)\n```\n\n![plot of chunk example](README_files/example-1.png) ![plot of chunk example](README_files/example-2.png) \n\n```r\n#-# Explaining the variance pretty closely to PCA with much fewer variables.\n```\n\n### Installing the package\n\n* the latest released version from CRAN with\n\n```R\ninstall.packages(\"spca\")\n````\n\n* The latest development version from github with\n\n```R\nif (packageVersion(\"spca\") < 0.4.0) {\n  install.packages(\"devtools\")\n}\ndevtools::install_github(\"merolagio/spca\")\n```\n\n###Comments\nThis is the first release and will surely contain some bugs, even though I tried to test it. Please do let me know if you find any or can suggest improvements. You can use the *Github* tools for [submitting issues](https://github.com/merolagio/spca/issues/new ) or contributions.\n\nFor now most of the plots are produced with the basic plotting functions. In a later release i will produce them with ggplot2 (it requires learning the package :-).\n\nThe code is implemented in R, so it will not work for large datasets. Optimizing R is a difficult task because one is using functions written for generic tasks. This is surely a problem with matrix algebra, also due to the poor documentation available. Packages like `ff` and `bigmemory` may be useful to analyze very large matrices that would exhaust rhe RAM, once they are fully developed and documented. Working on my tetra-core laptop, I found useful using the `foreach` and `doParallel` packages to speed up the computations. These, however, will not help the RAM. \n\nI have in mind to develop C routines at least for the matrix algebra. Anybody willing to help, please, let me know. \n"
}