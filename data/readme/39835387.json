{
  "read_at": 1462546105, 
  "description": "Deep Filter Banks for Texture Recognition, Description and Segmentation (CVPR15)", 
  "README.md": "# Deep filter banks for texture recognition, description, and segmentation\n\nThe provided code evaluates R-CNN and FV-CNN descriptors on various texture and material datasets (DTD, FMD, KTH-TIPS2b, ALOT), as well as for other datasets: objects (PASCAL VOC 2007), scenes (MIT Indoor), and fine-grained (CUB 200-2011). The results of these experiments are described in Table 1 and 2 of ** Cimpoi15 ** and Tables 3, 4, 5, and 6 of ** Cimpoi15a. **\n \n\n##   Getting starded\n\nAfter downloading the code, make sure that the dependencies are resolved (see below).\n\nYou also have to download the datasets you want to evaluate on, and link to them or copy them under data folder, in the location of your repository. Download the models (VGG-M, VGG-VD and AlexNet) in `data/models`. It is slightly faster to download them manually from here: [http://www.vlfeat.org/matconvnet/pretrained](http://www.vlfeat.org/matconvnet/pretrained).\n\nOnce done, run the `run_experiments.m` file.\n\nIn `run_experiments.m` you could remove (or add) dataset names to the `datasetList` cell. Make sure you adjust the number of splits accordingly. The datasets are specified as `{'dataset_name', <num_splits>}` cells.\n\n### Dependencies\n\nThe code relies on [vlfeat](http://www.vlfeat.org/), and [matconvnet](http://www.vlfeat.org/matconvnet), which should be downloaded and built before running the experiments.\nRun git submodule update -i in the repository download folder.\n\nTo build `vlfeat`, go to `<deep-fbanks-dir>/vlfeat` and run make; ensure you have MATLAB executable and mex in the path.\n\nTo build `matconvnet`, go to `<deep-fbanks-dir>/matconvnet/matlab` and run `vl_compilenn`; ensure you have CUDA installed, and nvcc in the path.\n\nFor LLC features (Table 3 in arxiv paper), please download the code from [http://www.robots.ox.ac.uk/~vgg/software/enceval_toolkit](http://www.robots.ox.ac.uk/~vgg/software/enceval_toolkit) and copy the following to the code folder (no subfolders!)\n\n* `enceval/enceval-toolkit/+featpipem/+lib/LLCEncode.m`\n* `enceval/enceval-toolkit/+featpipem/+lib/LLCEncodeHelper.cpp`\n* `enceval/enceval-toolkit/+featpipem/+lib/annkmeans.m`\n\nCreate the corresponding dcnnllc encoder type (see the examples provided in `run_experiments.m` for BOVW, VLAD or FV).\n\n### Paths and datasets\n\nThe `<dataset-name>_get_database.m` files generate the `imdb` structure for each dataset. Make sure the datasets are copied or linked to manually in the data folder for this to work.\n\nThe datasets are stored in individual folders under data, in the current code folder, and experiment results are stored in `data/exp01` folder, in the same location as the code. Alternatively, you could make data and experiments symbolic links pointing to convenient locations.\n\nPlease be aware that the descriptors are stored on disk (in cache folder, under `data/exp01/<experiment-dir>`), and may require large amounts of free space (especially FV-CNN features).\n\n\n### Dataset and evaluation\n\nDescribable Textures Dataset (DTD) is publicly available for download at:\n[http://www.robots.ox.ac.uk/~vgg/data/dtd](http://www.robots.ox.ac.uk/~vgg/data/dtd). You can also download the precomputed DeCAF features for DTD, the paper and evaluation results.\n\nOur additional annotations for OpenSurfaces dataset are publicly available for download at:\n[http://www.robots.ox.ac.uk/~vgg/data/wildtex](http://www.robots.ox.ac.uk/~vgg/data/wildtex)\n\nCode for CVPR14 paper (and Table 2 in arXiv paper):\n[http://www.robots.ox.ac.uk/~vgg/data/dtd/download/desctex.tar.gz](http://www.robots.ox.ac.uk/~vgg/data/dtd/download/desctex.tar.gz)\n\n###   Citation\n\nIf you use the code and data please cite the following in your work:\n\nFV-CNN code and additional annotaitons for the OpenSurfaces dataset:\n\n\t@article{Cimpoi15a,\n  \tAuthor       = \"Cimpoi, M. and Maji, S., Kokkinos, I. and Vedaldi, A.\",\n  \tTitle        = \"Deep Filter Banks for Texture Recognition, Description, and Segmentation\"\n  \tJournal      = \"arXiv preprint arXiv:1507.02620\",\n  \tYear         = \"2015\",\n\t}\n\n\t@inproceedings{Cimpoi15,\n  \tAuthor       = \"Cimpoi, M. and Maji, S. and Vedaldi, A.\",\n  \tTitle        = \"Deep Filter Banks for Texture Recognition and Segmentation\",\n  \tBooktitle    = \"IEEE Conference on Computer Vision and Pattern Recognition\",\n  \tYear         = \"2015\",\n\t}\n\nDTD dataset and IFV + DeCAF baseline:\n\n\t@inproceedings{cimpoi14describing,\n  \tAuthor       = \"M. Cimpoi and S. Maji and I. Kokkinos and S. Mohamed and A. Vedaldi\",\n  \tTitle        = \"Describing Textures in the Wild\",\n  \tBooktitle    = \"IEEE Conference on Computer Vision and Pattern Recognition\",\n  \tYear         = \"2014\",\n\t}\n\n", 
  "id": 39835387
}