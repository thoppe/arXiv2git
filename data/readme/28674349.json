{
  "read_at": 1462557206, 
  "description": "", 
  "README.md": "DimmWitted [![Build Status](https://travis-ci.org/HazyResearch/dimmwitted.svg?branch=master)](https://travis-ci.org/HazyResearch/dimmwitted)\n==\n\nDimmWitted is a high-performance execution engine for statistical analytics\nand is the core of [DeepDive](deepdive.stanford.edu).To see the type of analytics that is enabled by DimmmWitted, you can find examples in our paper\nhttp://arxiv.org/abs/1403.7550.\n\n# Installation\n\nWe first describe how to install DimmWitted.\n\n##Dependencies\n\nDimmWitted is designed to be compiled on Linux\nor MacOS. Before downloading the system, you need\none C++ compiler, and we have successfully compiled\nDimmWitted using\n\n  - clang++ (Apple LLVM version 6.0, clang-600.0.45.3)\n  - g++ 4.8.2 (On Linux)\n\nNote that DimmWitted needs a compiler with C++0x support.\nTo let DimmWitted know which compiler you are using during compilation, you can use options\nlike\n\n    CXX=g++-4.8 ...\n\n##Downloading and Compiling DimmWitted\n\nNow you are ready to install DimmWitted. First, you can\ncheck out the most recent version of the code by\n\n    git clone https://github.com/HazyResearch/dimmwitted\n\nBy default, this will create a folder called dimmwitted, from now\non, lets use the name `DW_HOME` to refer to this folder.\n\nThere are two ways that DimmWitted can be used. We write\na logsitic regression application with DimmWitted that\nthats as input the same format as LIBLINEAR and output the\nsame format. DimmWitted can also be used to write different \nother applications. We describe these two use cases as follows.\n\n###Logistic Regression Application\n\nTo compile the logistic regression application, you can compile\nDimmWitted as\n\n    cd DW_HOME\n    make lr\n    \nThis produces two binary files, `dw-lr-train` and `dw-lr-test`.\nTo train a model, you can try to run\n\n    ./dw-lr-train -s 0.01 -e 100 -r 0.0001 ./test/a6a  \n\nwhere ./test/a6a is one example input from LIBLINEAR. `-s`\nspecifies the stepsize, `-e` specifies the number of epoches\nto run, and `-r` specifies the regularization parameter (l2).\nThis will produces a model file `test/a6a.model`.\n\nTo test this model on test data, you can run\n\n    ./dw-lr-test ./test/a6a.t ./test/a6a.model ./test/a6a.output\n    \nwhere `./test/a6a.t` is one example test input of LIBLINEAR,\n`./test/a6a.model` is the trained model, and `./test/a6a.output`\nis the output that is of the same format as LIBLINEAR. Running\nthis command will give you\n\n    #elements=317325; #examples=21341; #n_features_test=124; #n_features_train=123\n    #n_features=124\n    Start testing...\n    | Running on 8 Cores...\n    [DimmWitted FUNC=1] TIME=0.000622 secs THROUGHPUT=  7.86 GB/sec.\n    Testing loss=0.33\n    | Running on 8 Cores...\n    [DimmWitted FUNC=0] TIME=0.000459 secs THROUGHPUT=  10.6 GB/sec.\n    Testing acc =0.847\n    Dumping the result to ./test/a6a.output...\n\nwhere you can see the testing loss and testing accuracy. Running LIBLINER\non the same dataset obtains similar accuracy.\n\nYou can find more data at http://ntucsu.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html\nCurrently, this example application only supports binary logistic\nregression (with label +1 and -1). You can write other applications\nby yourself, which will be the topic as you keep reading this document.\n\n###DimmWitted as Library\n\nDimmWitted can be used to write different applications, and\nwe include an example one in the repositary. To compile\nthis example, you can type in\n\n    cd DW_HOME\n    make\n\nThis will generate a binary file with the name `example`, to\nvalidate whether the installation is successful, you can try\nto run this binary file by typing in\n\n    ./example\n\nThis should output:\n\n    TIME=0.062495001 secs THROUGHPUT=12.208008 GB/sec.\n    1.2478489    loss=0.50147917\n    TIME=0.063390002 secs THROUGHPUT=12.035643 GB/sec.\n    1.2431277    loss=0.50159193\n    TIME=0.063591003 secs THROUGHPUT=11.9976 GB/sec.\n    1.2588885    loss=0.50123058\n    TIME=0.062996998 secs THROUGHPUT=12.110727 GB/sec.\n    1.2576657    loss=0.50125708\n    TIME=0.062763996 secs THROUGHPUT=12.155686 GB/sec.\n    SUM OF MODEL (Should be ~1.3-1.4): 1.2576657\n\nThis binary contains one example of using DimmWitted to train\na logistic regression model on a synthetic data set. To check\nwhether DimmWitted works properly on your machine for this application, \nyou should see the last line to be similar to \n\n    SUM OF MODEL (Should be ~1.3-1.4): 1.2576657\n\nNote that the number 1.26 might vary, but it should not be too far\naway from 1.3-1.4.\n\n##Testing\n\nDimmWitted contains a set of unit tests to better make sure\nit works properly on your machine. To run our suite of test\ncases, you first need to compile googletest by typing in\n\n    make test_dep\n\nAnd then to run test case, you can type in\n\n    make runtest\n\nPlease allow couple minutes for the test to run, and ideally you\nshould see\n\n    ...\n    [----------] Global test environment tear-down\n    [==========] 12 tests from 2 test cases ran. (91775 ms total)\n    [  PASSED  ] 12 tests.\n\n##What's Next?\n\nNow you have successfully installed DimmWitted. The next step\nis to learn how to write an application inside DimmWitted.\nYou can find examples in the `examples` folder and the walkthrough\n[here](https://github.com/HazyResearch/dimmwitted/wiki).\n\n", 
  "id": 28674349
}