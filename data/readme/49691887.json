{
  "read_at": 1462552749, 
  "description": "Theano implementation of T1-T2 gradient-based method for tuning continuous hyperparameters.", 
  "README.md": "# T1-T2 Hyperparameter Tuning\n\nTheano implementation of T1-T2 method for tuning continuous hyperparameters.\nPaper: [http://arxiv.org/abs/1511.06727](http://arxiv.org/abs/1511.06727)\n\n![click me][traject]\n\nCurrently supporting:\n- architectures: mlp, cnn\n- datasets: [mnist](http://yann.lecun.com/exdb/mnist/), [svhn](http://ufldl.stanford.edu/housenumbers/), [cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html) and [not_mnist](http://yaroslavvb.blogspot.fi/2011/09/notmnist-dataset.html) data sets\n- regularization: batch normalization; various versions of additive and multiplicative gaussian noise; \nL1, L2, Lmax, soft Lmax penalties; drop-out, per-batch drop-out\n- training regularizers: all gaussian noise, L2, soft Lmax; \nparametrized per unit, per map (for cnn), per layer, per network\n- optimizers: SGD, momentum, adam\n- T2 gradients: via L-op, via finite difference\n- monitoring: various network activation and parameter statistics, gradient norms and angles \n\nThis version was implemented partially as an exercise, more efficient implementation will be developed in [keras](https://github.com/fchollet/keras/).\n\n\nTest performance with initial random hyperparameters vs. same model just tuned hyperparameter:\n- [MNIST](https://github.com/jelennal/t1t2/blob/master/pics/beforeafter_in_scale(mnist).png) \n- [SVHN](https://github.com/jelennal/t1t2/blob/master/pics/beforeafter_1(svhn).png) \n\n(different symbols correspond to different experiment setups: varying network architecture, number and degree of freedom of hyperparameters; x-axis: test error before tuning, y-axis: test error after tuning)\n\n[traject]: https://github.com/jelennal/t1t2/blob/master/pics/trajectories%20in%20hyperspace%20(mnist).png \"Hyperparameter values during training with T1-T2, illustrated in hyperparameter space.\"\n", 
  "id": 49691887
}