{
  "read_at": 1462555481, 
  "description": "Optimization using Stochastic quasi-Newton methods", 
  "README.md": "# minSQN : Stochastic Quasi-Newton Optimization in MATLAB\n\nAuthors: [Nitish Shirish Keskar](http://users.iems.northwestern.edu/~nitish/) and [Albert S. Berahas](https://sites.google.com/a/u.northwestern.edu/albertsberahas/home)\n\nPlease contact us if you have any questions, suggestions, requests or bug-reports.\n\n## Introduction\n\nThis is a package for solving an unconstrained minimization\nproblem of the form,\nmin f(x) = sum_i f_i(x).\n\nminSQN allows for the user to solve large-scale (sum-of-functions)\noptimization problems using one of 11 Stochastic Quasi-Newton methods.\n\n\n\nThe following table summarizes all the methods that minSQN contains. The\nmethods are classified in terms of:\n- Hyperparameters\n- Length of LBFGS memory (limited/inf) [if inf, BFGS method used]\n- Powell damping\n- Hessian damping\n- Curvature pair update (y)\n\n```\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|  Method  | Hyperparameters | LBFGS Memory | Powell Damping | Hessian Damping |     Curvature pair     |          Reference          |\n|          |                 | (finite/inf) |      (Y/N)     |      (Y/N)      |       (y) update       |                             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|    SQN   |     alpha, L    |    finite    |        N       |        N        | Hessian-vector product |     Byrd et. al. (2014)     |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|   DSQN   |     alpha, L    |    finite    |        Y       |        N        | Hessian-vector product |              --             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|   oBFGS  |   alpha, delta  |      inf     |        N       |        Y        |  Gradient differencing |  Schraudolph et. al. (2007) |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|  oLBFGS  |   alpha, delta  |    finite    |        N       |        N        |  Gradient differencing | Schraudolph et. al. (2007), |\n|          |                 |              |                |                 |                        |   Mokhtari et. al. (2014)   |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|  D-oBFGS |   alpha, delta  |      inf     |        Y       |        Y        |  Gradient differencing |              --             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n| D-oLBFGS |   alpha, delta  |    finite    |        Y       |        N        |  Gradient differencing |              --             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|    RES   |   alpha, delta  |      inf     |        N       |        Y        |  Gradient differencing |   Mokhtari et. al. (2014)   |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|   L-RES  |   alpha, delta  |    finite    |        N       |        N        |  Gradient differencing |                             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|  SDBFGS  |   alpha, delta  |      inf     |        Y       |        Y        |  Gradient differencing |     Wang et. al. (2014)     |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n| L-SDBFGS |   alpha, delta  |    finite    |        Y       |        Y        |  Gradient differencing |              --             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|   adaQN  |     alpha, L    |    finite    |        N       |        N        | Hessian-vector product |    Keskar et. al. (2015)    |\n|          |                 |              |                |                 |   accumulated Fisher   |                             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n```\n\n### Features\nThe minSQN package\n\n* is written in pure-MATLAB with minimal dependencies and emphasizes simplicity, extendibility and cross-platform compatibility. \n* allows the user to run 11 different stochastic quasi-Newton methods which are able to solve a vast array of problems (both convex and non-convex). \n* comes with an automatic hyperparameter tuning mechanism thus obviating the need for manually tuning the parameters for any of the included methods. \n\n## Citation\nIf you use minSQN for your research, please cite the Github repository:\n\n```\n@misc{minSQN2016,\n   author = \"Nitish Shirish Keskar and Albert S. Berahas\",\n   title = \"{minSQN}: {S}tochastic {Q}uasi-{N}ewton {O}ptimization in {MATLAB}\",\n   year = \"2016\",\n   url = \"https://github.com/keskarnitish/minSQN/\",\n   note = \"[Online]\"\n }\n```\n\n\n\n## Usage Guide\n\nThe algorithm can be run using the syntax \n\n```\nlogger = minSQN(problem,options,[hyperparameters]);\n```\nwhere,\n* `problem` is an object pertaining to a specific loss function and data set. \n* `options` is a struct containing the necessary parameters for use in the optimization algorithms. \n* `hyperparameters` is an array of hyperparameters necessary for the optimization algorithms such as the step-size, damping constants and aggregation lengths. This is an optional argument. If it is not specified, minSQN uses its inbuilt automatic tuner (which we describe next) to find hyperparameters. \n\n### Automatic Tuning:\n\nAll method above have certain hyperparameters that need to be set or\ntuned. The second column in the table above indicates what\nhyperparameters are needed for each of the methods.\n\nIn minSQN, we provide an automatic tuning mechanism that randomly samples hyperparameters (as in Bergstra et. al. (2012)) from a prespecified range of hyperparameter values, solves the problem several times, and returns the best optimization run and hyperparameter setting. The number of tuning steps is determined in the options (default is 10).  \n\n\n\n\n### Example (no tuning):\nTo solve a problem using minSQN, the user must follow 4 steps:\n\n1. Construct the problem class (Logistic Regression and Least Squares are included with the minSQN code. Others can be coded easily using our template)\n2. Generate default options using `GenOptions()` and over-write them as needed\n3. Set the hyperparameters necessary for the specific method (For instance, SGD requires the step-size, RES requires the step-size and the damping constant and SQN requires the step-size and the aggregation length)\n4. Run minSQN\n\n\n```\nX = randn(2000,500);\ny = 2*(randn(2000,1) > 0.5) - 1;\nproblem = lossFunctions.LogReg(X,y);\n\noptions = GenOptions();\noptions.method = 'SQN';\nsqn_log_untuned = minSQN(problem,options,[5e-2,5]);\n```\n\nThe output of `minSQN` would be:\n\n```\nsqn_log_untuned = \n\n              fhist: [21x1 double]\n    hyperparameters: [0.050000000000000 5]\n             w_star: [500x1 double]\n```\nwhere `fhist` is the history of average loss function values over each epoch, `hyperparameters` returns the provided hyperparameters in the case when they are provided and returns their tuned values if the automatic tuning was chosen (see next example). `w_star` is the value of the iterate at the end of the optimization.  \n\n### Example (with tuner):\nThe process for running the methods with automatic tuning is similar to above except no hyperparameters are passed as input (as in Step 3 above). \n```\nX = randn(2000,500);\ny = 2*(randn(2000,1) > 0.5) - 1;\nproblem = lossFunctions.LogReg(X,y);\n\noptions = GenOptions();\noptions.method = 'SQN';\nsqn_log_tuned = minSQN(problem,options);\n```\n\nThe output of `minSQN` in this example would be:\n```\nsqn_log_tuned = \n\n              fhist: [21x1 double]\n    hyperparameters: [0.006962319523931 26]\n             w_star: [500x1 double]\n```             \n`fhist` and `w_star` are the function values and final iterate as explained in the previous example. In this case, `hyperparameters` is the value of the best hyperparameters as chosen by the automatic tuning mechanism. \n\nPlease refer to `demo.m` for a short demonstration of using `minSQN` for solving a small Logistic Regression problem using three different methods and plotting the results. For a detailed documentation of minSQN and its associated functions, use MATLAB's `help`. For instance, to obtain details about the different options and their significance, use `help GenOptions`.\n\n\n\n\n\n## References:\nSGD:\n- Bottou, L., 1998. Online learning and stochastic approximations. On-line learning in neural networks, 17(9), p.142.\n \nSQN:\n- Byrd, R.H., Hansen, S.L., Nocedal, J. and Singer, Y., 2014.\nA stochastic quasi-Newton method for large-scale optimization.\narXiv preprint arXiv:1401.7020.\n\noBFGS:\n- Schraudolph, N.N., Yu, J. and Gunter, S., 2007. A stochastic\nquasi-Newton method for online convex optimization. In\nInternational Conference on Artificial Intelligence and Statistics\n(pp. 436-443).\n\noLBFGS:\n- Schraudolph, N.N., Yu, J. and Gunter, S., 2007. A stochastic\nquasi-Newton method for online convex optimization. In\nInternational Conference on Artificial Intelligence and Statistics\n(pp. 436-443).\n- Mokhtari, A. and Ribeiro, A., 2014. Global convergence of online\nlimited memory bfgs. arXiv preprint arXiv:1409.2045.\n\nRES:\n- Mokhtari, A. and Ribeiro, A., 2014. Res: Regularized stochastic\nbfgs algorithm. Signal Processing, IEEE Transactions on, 62(23),\npp.6089-6104.\n\nSDBFGS:\n- Wang, X., Ma, S. and Liu, W., 2014. Stochastic Quasi-Newton\nMethods for Nonconvex Stochastic Optimization. arXiv preprint\narXiv:1412.1196.\n\nadaQN:\n- Keskar, N.S. and Berahas, A.S., 2015. adaQN: An Adaptive\nQuasi-Newton Algorithm for Training RNNs. arXiv preprint\narXiv:1511.01169.\n\n\n\n\n\n\n\n\n\n\n\n\n", 
  "id": 53980806
}