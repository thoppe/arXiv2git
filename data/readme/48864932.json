{
  "read_at": 1462549934, 
  "description": "This is a Torch implementation of [\"Deep Residual Learning for Image Recognition\",Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun](http://arxiv.org/abs/1512.03385) the winners of the 2015 ILSVRC and COCO challenges.", 
  "README.md": "Deep Residual Learning for Image Recognition\n============================================\n\nThis is a Torch implementation of [\"Deep Residual Learning for Image Recognition\",Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun](http://arxiv.org/abs/1512.03385) the winners of the 2015 ILSVRC and COCO challenges.\n\n**What's working:** CIFAR converges, as per the paper.\n\n**What's not working yet:** Imagenet. I also have only implemented Option\n(A) for the residual network bottleneck strategy.\n\nTable of contents\n-----------------\n\n- [CIFAR: Effect of model size](#cifar-effect-of-model-size)\n- [CIFAR: Effect of model architecture on shallow networks](#cifar-effect-of-model-architecture)\n  - [...on deep networks](#cifar-effect-of-model-architecture-on-deep-networks)\n- [Imagenet: Others' preliminary model architecture experiments](#imagenet-effect-of-model-architecture-preliminary)\n- [CIFAR: Effect of alternate solvers (RMSprop, Adagrad, Adadelta)](#cifar-alternate-training-strategies-rmsprop-adagrad-adadelta)\n  - [...on deep networks](#cifar-alternate-training-strategies-on-deep-networks)\n- [CIFAR: Effect of batch normalization momentum](#effect-of-batch-norm-momentum)\n\nChanges\n-------\n- 2016-02-01: Added others' preliminary results on ImageNet for the architecture. (I haven't found time to train ImageNet yet)\n- 2016-01-21: Completed the 'alternate solver' experiments on deep networks. These ones take quite a long time.\n- 2016-01-19:\n  - **New results**: Re-ran the 'alternate building block' results on deeper networks. They have more of an effect.\n  - **Added a table of contents** to avoid getting lost.\n  - **Added experimental artifacts** (log of training loss and test error, the saved model, the any patches used on the source code, etc) for two of the more interesting experiments, for curious folks who want to reproduce our results. (These artifacts are hereby released under the zlib license.)\n- 2016-01-15:\n  - **New CIFAR results**: I re-ran all the CIFAR experiments and\n  updated the results. There were a few bugs: we were only testing on\n  the first 2,000 images in the training set, and they were sampled\n  with replacement. These new results are much more stable over time.\n- 2016-01-12: Release results of CIFAR experiments.\n\nHow to use\n----------\n- You need at least CUDA 7.0 and CuDNN v4.\n- Install Torch.\n- Install the Torch CUDNN V4 library: `git clone https://github.com/soumith/cudnn.torch; cd cudnn; git co R4; luarocks make` This will give you `cudnn.SpatialBatchNormalization`, which helps save quite a lot of memory.\n- Install nninit: `luarocks install nninit`.\n- Download\n  [CIFAR 10](http://torch7.s3-website-us-east-1.amazonaws.com/data/cifar-10-torch.tar.gz).\n  Use `--dataRoot <cifar>` to specify the location of the extracted CIFAR 10 folder.\n- Run `train-cifar.lua`.\n\nCIFAR: Effect of model size\n---------------------------\n\nFor this test, our goal is to reproduce Figure 6 from the original paper:\n\n![figure 6 from original paper](https://i.imgur.com/q3lcHic.png)\n\nWe train our model for 200 epochs (this is about 7.8e4 of their\niterations on the above graph). Like their paper, we start at a\nlearning rate of 0.1 and reduce it to 0.01 at 80 epochs and then to\n0.01 at 160 epochs.\n\n###Training loss\n![Training loss curve](http://i.imgur.com/XqKnNX1.png)\n\n###Testing error\n![Test error curve](http://i.imgur.com/lt2D5cA.png)\n\n| Model                                 | My Test Error | Reference Test Error from Tab. 6 | Artifacts |\n|----|----|----|----|\n| Nsize=3, 20 layers                    | 0.0829 | 0.0875 | [Model](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141709-AnY56THQt7/model.t7), [Loss](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141709-AnY56THQt7/Training%20loss.csv) and [Error](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141709-AnY56THQt7/Testing%20Error.csv) logs, [Source commit](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141709-AnY56THQt7/Source.git-current-commit) + [patch](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141709-AnY56THQt7/Source.git-patch) |\n| Nsize=5, 32 layers                    | 0.0763 | 0.0751 | [Model](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141709-rewkex7oPJ/model.t7), [Loss](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141709-rewkex7oPJ/Training%20loss.csv) and [Error](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141709-rewkex7oPJ/Testing%20Error.csv) logs, [Source commit](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141709-rewkex7oPJ/Source.git-current-commit) + [patch](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141709-rewkex7oPJ/Source.git-patch) |\n| Nsize=7, 44 layers                    | 0.0714 | 0.0717 | [Model](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141710-HxIw7lGPyu/model.t7), [Loss](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141710-HxIw7lGPyu/Training%20loss.csv) and [Error](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141710-HxIw7lGPyu/Testing%20Error.csv) logs, [Source commit](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141710-HxIw7lGPyu/Source.git-current-commit) + [patch](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141710-HxIw7lGPyu/Source.git-patch) |\n| Nsize=9, 56 layers                    | 0.0694 | 0.0697 | [Model](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141710-te4ScgnYMA/model.t7), [Loss](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141710-te4ScgnYMA/Training%20loss.csv) and [Error](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141710-te4ScgnYMA/Testing%20Error.csv) logs, [Source commit](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141710-te4ScgnYMA/Source.git-current-commit) + [patch](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601141710-te4ScgnYMA/Source.git-patch) |\n| Nsize=18, 110 layers, fancy policy1   | 0.0673 | 0.06612 | [Model](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601142006-5T5D1DO3VP/model.t7), [Loss](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601142006-5T5D1DO3VP/Training%20loss.csv) and [Error](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601142006-5T5D1DO3VP/Testing%20Error.csv) logs, [Source commit](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601142006-5T5D1DO3VP/Source.git-current-commit) + [patch](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601142006-5T5D1DO3VP/Source.git-patch) |\n\nWe can reproduce the results from the paper to typically within 0.5%.\nIn all cases except for the 32-layer network, we achieve very slightly\nimproved performance, though this may just be noise.\n\n1: For this run, we started from a learning rate of 0.001 until the\nfirst 400 iterations. We then raised the learning rate to 0.1 and\ntrained as usual. This is consistent with the actual paper's results.\n\n2: Note that the paper reports the best run from five runs, as well as\nthe mean. I consider the mean to be a valid test protocol, but I don't\nlike reporting the 'best' score because this is effectively training\non the test set. (This method of reporting effectively introduces an\nextra parameter into the model--which model to use from the\nensemble--and this parameter is fitted to the test set)\n\nCIFAR: Effect of model architecture\n-----------------------------------\n\nThis experiment explores the effect of different NN architectures that\nalter the \"Building Block\" model inside the residual network.\n\nThe original paper used a \"Building Block\" similar to the \"Reference\"\nmodel on the left part of the figure below, with the standard\nconvolution layer, batch normalization, and ReLU, followed by another\nconvolution layer and batch normalization. The only interesting piece\nof this architecture is that they move the ReLU after the addition.\n\nWe investigated two alternate strategies.\n\n![Three different alternate CIFAR architectures](https://i.imgur.com/uRMBOaS.png)\n\n- **Alternate 1: Move batch normalization after the addition.**\n  (Middle) The reasoning behind this choice is to test whether\n  normalizing the first term of the addition is desirable. It grew out\n  of the mistaken belief that batch normalization always normalizes to\n  have zero mean and unit variance. If this were true, building an\n  identity building block would be impossible because the input to the\n  addition always has unit variance. However, this is not true. BN\n  layers have additional learnable scale and bias parameters, so the\n  input to the batch normalization layer is not forced to have unit\n  variance.\n\n- **Alternate 2: Remove the second ReLU.** The idea behind this was\n  noticing that in the reference architecture, the input cannot\n  proceed to the output without being modified by a ReLU. This makes\n  identity connections *technically* impossible because negative\n  numbers would always be clipped as they passed through the skip\n  layers of the network. To avoid this, we could either move the ReLU\n  before the addition or remove it completely. However, it is not\n  correct to move the ReLU before the addition: such an architecture\n  would ensure that the output would never decrease because the first\n  addition term could never be negative. The other option is to simply\n  remove the ReLU completely, sacrificing the nonlinear property of\n  this layer. It is unclear which approach is better.\n\nTo test these strategies, we repeat the above protocol using the\nsmallest (20-layer) residual network model.\n\n(Note: The other experiments all use the leftmost \"Reference\" model.)\n\n![Training loss](http://i.imgur.com/qDDLZLQ.png)\n\n![Testing error](http://i.imgur.com/fTY6TL5.png)\n\n| Architecture                        | Test error |\n|-----------------------------------|----------|\n| ReLU, BN before add (ORIG PAPER reimplementation)    | 0.0829 |\n| No ReLU, BN before add              | 0.0862 |\n| ReLU, BN after add                  | 0.0834 |\n| No ReLU, BN after add               | 0.0823 |\n\nAll methods achieve accuracies within about 0.5% of each other.\nRemoving ReLU and moving the batch normalization after the addition\nseems to make a small improvement on CIFAR, but there is too much\nnoise in the test error curve to reliably tell a difference.\n\nCIFAR: Effect of model architecture on deep networks\n----------------------------------------------------\n\nThe above experiments on the 20-layer networks do not reveal any\ninteresting differences. However, these differences become more\npronounced when evaluated on very deep networks. We retry the above\nexperiments on 110-layer (Nsize=19) networks.\n\n![Training loss](http://i.imgur.com/RANDrXl.png)\n\n![Testing error](http://i.imgur.com/sldN4cK.png)\n\nResults:\n\n- For deep networks, **it's best to put the batch normalization before\n  the addition part of each building block layer**. This effectively\n  removes most of the batch normalization operations from the input\n  skip paths. If a batch normalization comes after each building\n  block, then there exists a path from the input straight to the\n  output that passes through several batch normalizations in a row.\n  This could be problematic because each BN is not idempotent (the\n  effects of several BN layers accumulate).\n\n- Removing the ReLU layer at the end of each building block appears to\n  give a small improvement (~0.6%)\n\n| Architecture                        | Test error | Artifacts |\n|-----------------------------------|----------|---|\n| ReLU, BN before add (ORIG PAPER reimplementation)    |  0.0697 | [Model](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181920-jmOtpiNPQa/model.t7), [Loss](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181920-jmOtpiNPQa/Training%20loss.csv) and [Error](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181920-jmOtpiNPQa/Testing%20Error.csv) logs, [Source commit](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181920-jmOtpiNPQa/Source.git-current-commit) + [patch](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181920-jmOtpiNPQa/Source.git-patch) |\n| No ReLU, BN before add              |  0.0632 | [Model](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181924-V2wDg0NKDK/model.t7), [Loss](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181924-V2wDg0NKDK/Training%20loss.csv) and [Error](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181924-V2wDg0NKDK/Testing%20Error.csv) logs, [Source commit](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181924-V2wDg0NKDK/Source.git-current-commit) + [patch](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181924-V2wDg0NKDK/Source.git-patch) |\n| ReLU, BN after add                  |  0.1356 | [Model](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181922-8VYWhyuTuA/model.t7), [Loss](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181922-8VYWhyuTuA/Training%20loss.csv) and [Error](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181922-8VYWhyuTuA/Testing%20Error.csv) logs, [Source commit](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181922-8VYWhyuTuA/Source.git-current-commit) + [patch](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181922-8VYWhyuTuA/Source.git-patch) |\n| No ReLU, BN after add               |  0.1230 | [Model](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181923-Qfp5mTA2u9/model.t7), [Loss](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181923-Qfp5mTA2u9/Training%20loss.csv) and [Error](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181923-Qfp5mTA2u9/Testing%20Error.csv) logs, [Source commit](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181923-Qfp5mTA2u9/Source.git-current-commit) + [patch](https://mjw-xi8mledcnyry.s3.amazonaws.com/experiments/201601181923-Qfp5mTA2u9/Source.git-patch) |\n\nImageNet: Effect of model architecture (preliminary)\n----------------------------------------------------\n[@ducha-aiki is performing preliminary experiments on imagenet.](https://github.com/gcr/torch-residual-networks/issues/5)\nFor ordinary CaffeNet networks, @ducha-aiki found that putting batch\nnormalization after the ReLU layer may provide a small benefit\ncompared to putting it before.\n\n> Second, results on CIFAR-10 often contradicts results on ImageNet. I.e., leaky ReLU > ReLU on CIFAR, but worse on ImageNet.\n\n@ducha-aiki's more detailed results here: https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md\n\n\nCIFAR: Alternate training strategies (RMSPROP, Adagrad, Adadelta)\n-----------------------------------------------------------------\n\nCan we improve on the basic SGD update rule with Nesterov momentum?\nThis experiment aims to find out. Common wisdom suggests that\nalternate update rules may converge faster, at least initially, but\nthey do not outperform well-tuned SGD in the long run.\n\n![Training loss curve](http://i.imgur.com/0ZxQZ7k.png)\n\n![Testing error curve](http://i.imgur.com/oLzwLDo.png)\n\nIn our experiments, vanilla SGD with Nesterov momentum and a learning\nrate of 0.1 eventually reaches the lowest test error. Interestingly,\nRMSPROP with learning rate 1e-2 achieves a lower training loss, but\noverfits.\n\n| Strategy                                      | Test error |\n|---------------------------------------------|----------|\n| Original paper: SGD + Nesterov momentum, 1e-1 | 0.0829     |\n| RMSprop, learrning rate = 1e-4                | 0.1677     |\n| RMSprop, 1e-3                                 | 0.1055     |\n| RMSprop, 1e-2                                 | 0.0945     |\n| Adadelta1, rho = 0.3                          | 0.1093     |\n| Adagrad, 1e-3                                 | 0.3536     |\n| Adagrad, 1e-2                                 | 0.1603     |\n| Adagrad, 1e-1                                 | 0.1255     |\n\n1: Adadelta does not use a learning rate, so we did not use the same\nlearning rate policy as in the paper. We just let it run until\nconvergence.\n\nSee\n[Andrej Karpathy's CS231N notes](https://cs231n.github.io/neural-networks-3/#update)\nfor more details on each of these learning strategies.\n\nCIFAR: Alternate training strategies on deep networks\n-----------------------------------------------------\n\nDeeper networks are more prone to overfitting. Unlike the earlier\nexperiments, all of these models (except Adagrad with a learning rate\nof 1e-3) achieve a loss under 0.1, but test error varies quite wildly.\nOnce again, using vanilla SGD with Nesterov momentum achieves the\nlowest error.\n\n![Training loss](http://i.imgur.com/ZvMfLtk.png)\n\n![Testing error](http://i.imgur.com/B8PMIQw.png)\n\n| Solver                                    | Testing error |\n|-------------------------------------------|--------|\n| Nsize=18, Original paper: Nesterov, 1e-1  | 0.0697 |\n| Nsize=18, RMSprop, 1e-4                   | 0.1482 |\n| Nsize=18, RMSprop, 1e-3                   | 0.0821 |\n| Nsize=18, RMSprop, 1e-2                   | 0.0768 |\n| Nsize=18, RMSprop, 1e-1                   | 0.1098 |\n| Nsize=18, Adadelta                        | 0.0888 |\n| Nsize=18, Adagrad, 1e-3                   | 0.3022 |\n| Nsize=18, Adagrad, 1e-2                   | 0.1321 |\n| Nsize=18, Adagrad, 1e-1                   | 0.1145 |\n\nEffect of batch norm momentum\n-----------------------------\n\nFor our experiments, we use batch normalization using an exponential\nrunning mean and standard deviation with a momentum of 0.1, meaning\nthat the running mean and std changes by 10% of its value at each\nbatch. A value of 1.0 would cause the batch normalization layer to\ncalculate the mean and standard deviation across only the current\nbatch, and a value of 0 would cause the batch normalization layer to\nstop accumulating changes in the running mean and standard deviation.\n\nThe strictest interpretation of the original batch normalization paper\nis to calculate the mean and standard deviation across the entire\ntraining set at every update. This takes too long in practice, so the\nexponential average is usually used instead.\n\nWe attempt to see whether batch normalization momentum affects\nanything. We try different values away from the default, along with a\n\"dynamic\" update strategy that sets the momentum to 1 / (1+n), where n\nis the number of batches seen so far (N resets to 0 at every epoch).\nAt the end of training for a certain epoch, this means the batch\nnormalization's running mean and standard deviation is effectively\ncalculated over the entire training set.\n\nNone of these effects appear to make a significant difference.\n\n![Test error curve](http://i.imgur.com/3M1P79N.png)\n\n| Strategy | Test Error |\n|----|----|\n| BN, momentum = 1 just for fun      |  0.0863 |\n| BN, momentum = 0.01                |  0.0835 |\n| Original paper: BN momentum = 0.1  |  0.0829 |\n| Dynamic, reset every epoch.        |  0.0822 |\n\n\n\nTODO: Imagenet\n--------------\n", 
  "id": 48864932
}