{
  "read_at": 1462546401, 
  "description": "Implementation of ROCK* algorithm (Gaussian kernel regression + natural gradient descent) for optimisation", 
  "README.md": "ROCK* - the algorithm in this program combines Gaussian kernel regression and natural gradient descent and is shown to have better convergence than FDSA, SPSA, IW-PGPE, REINFORCE, CMAES, AMalGaM, and CEM: \n\nJemin Hwangbo, Christian Gehring, Hannes Sommer, Roland Siegwart and Jonas Buchli.2014. ROCK* - Efficient Black-box Optimization for Policy Learning. Humanoids 2014, Madrid, Spain\nhttp://www.adrl.ethz.ch/archive/Humanoid2014_ROCKSTAR.pdf\n\nThe natural gradient is described here:\nS. Amari, and S.C. Douglas. 1998. Why natural gradient? IEEE 1213-1216\nand examined in:\nJames Martens.2015. New perspectives on the natural gradient method. http://arxiv.org/pdf/1412.1193v4.pdf\n\nand the covariant matrix adaptation (CMA-ES-like) is from here:\nNikolas Hansen and Andreas Ostermeier. 1996. Adapting arbitrary normal mutation distributions in evolution strategies: the covariance matrix adaptation. IEEE 312-317\n\nThe objective function is like the one presented in:\n\nhttps://chessprogramming.wikispaces.com/Texel%27s+Tuning+Method\n\nCoefficient a = 0.007 (in centipawns) from:\nVladimir Medvedev. Opredeliaem vesa shakhmatn'ikh figur regressionn'im analizom (Determination of chess pieces weights by regression analysis).\nhttp://habrahabr.ru/post/254753/", 
  "id": 38736747
}