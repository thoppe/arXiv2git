{
  "read_at": 1462509837, 
  "description": "Repository for my course task. Winnow algorithm in P2P.", 
  "README.md": "Gossip Learning Framework (GoLF)\n================================\n\nThis open source benchmarking framework allows you to *build* your own\n__P2P learning algorithm__ and *evaluate* it in a simulated but\nrealistic -- where you can model message delay, drop or churn --\nnetworked environment. Moreover it contains the prototype\nimplementations of some well-known machine learning algorithms like SVM\nand Logistic Regression. (More will be coming soon.)\n\nThe project is related to our academic research and it is partially\nsupported by the Future and Emerging Technologies programme FP7-COSI-ICT\nof the European Commission through project\n[QLectives](http://www.qlectives.eu/) (grant no.: 231200). Some related\npublications can be found on our personal homepages\n([here](http://www.inf.u-szeged.hu/~ormandi/index.php?menu=publications)\nand [here](http://www.inf.u-szeged.hu/~ihegedus/publ.php)) and on\n[arXiv](http://arxiv.org/abs/1109.1396).\n\n\nGetting Started\n===============\n\nThis framework includes some predefined learning scenarios based on the\nprototype implementation of the machine learning algorithms and the\nwell-known [Iris learning database](http://archive.ics.uci.edu/ml/datasets/Iris). To play with\nthem you have to perform the following steps:\n\n* __getting the source__: First you have to download the source code of\nthe framework. Probably the easiest way to do that is cloning this git\nrepository by typing `git clone git://github.com/RobertOrmandi/Gossip-Learning-Framework.git`. \nAdditional possibilities are to download as [zip archive](https://github.com/RobertOrmandi/Gossip-Learning-Framework/zipball/master)\nor as [tar.gz archive](https://github.com/RobertOrmandi/Gossip-Learning-Framework/tarball/master).\n\n* __building it__: The building process is supported with *ant*. To create a jar you have\nto type `ant` in the root directory of the\nproject. This will produce *gossipLearning.jar* in the *bin*\ndirectory of the project. (All of the libraries which are necessary for\nbuilding or running the project are included in the *lib* directory of\nthe project.)\n\n* __running a predefined simulation__: To run a simulation applying one of the predefined scenarios on the \n[Iris](http://archive.ics.uci.edu/ml/datasets/Iris) dataset you have to type the \nfollowing code snippet: `res/script/run.sh training_db evaluation_db 100 scenario result` \n(assuming a standard UNIX environment with java and gnuplot installed).The parameters of the \n`run.sh` are pretty intuitive and you can find examples in the package.\nThe first two parameters refer to the training and evaluation datasets, respectively, presented in [SVMLight\nformat](http://svmlight.joachims.org/). You can use the `res/db/iris_setosa_versicolor_train.dat` and \n`res/db/iris_setosa_versicolor_eval.dat` files respectively. The third parameter defines the\nnumber of iterations. The fourth one describes the simulation environment.\nBasically this is a [Peersim](http://peersim.sourceforge.net/) \nconfiguration file template (configuration file with some variables that are \ninstantiated based on the used training set). Here you can use the \n`res/config/no_failure_applying_more_learners_voting10.txt` configuration file.\nThe results are generated in the *res/results* directory given in the fifth parameter \n(it has to be created before the call of `run.sh`). \nMake sure to delete previously generated results before you rerun the simulation!\nIn the *res* directory of the\nproject you can find additional training datasets (*db* subdirectory) and other\nconfiguration templates (*config* subdirectory). \n\n* __understanding the results__: The result graphs can be found in the\n*res/results/* directory. It should be similar to\n[this](http://www.inf.u-szeged.hu/rgai/~ormandi/iris_setosa_versicolor.png)\nfigure. Each curve belongs to a certain type of learning algorithm\n(see labels) and each point of the curves corresponds to a point in time (see label of x-axis).\nEach point shows the averaged 0-1 error over the\ndifferent machine learning models stored by the nodes of the network\nmeasured on a separate (i.e. not known by the learning algorithm)\nevaluation set. As you can see, each line drops down after a certain point\nin time which means each algorithm converges.\n\nThis is just the tip of the iceberg since the framework provides an\n*API* which makes it extensible, i.e. you can implement new learning\nalgorithms or protocols. Or you can define other network scenarios using\nthe configuration mechanism of\n[Peersim](http://peersim.sourceforge.net/).\n\n\nFurther Reading\n===============\n\nSince the GoLF is built on the top of\n[Peersim](http://peersim.sourceforge.net/), *for the deeper understanding\nof the underlying mechanism* you should be __familiar with Peersim__.\nYou should understand the following tutorials:\n[this](http://peersim.sourceforge.net/tutorial1/tutorial1.pdf) and\n[this](http://peersim.sourceforge.net/tutorial2/tutorial2.pdf).\nThis is also necessary for understanding the configuration\nfiles of GoLF.\n\n*To develop a new algorithm or protocol* you have to know the\n__details of the Gossip Learning Framework__. This was described in\n[this paper](http://arxiv.org/abs/1109.1396) and a slightly simplified version\ncan be found in the wiki of the project.\n\nYou are almost done. But before *you start development* be sure you\nunderstand __the inner design concepts of the implementation of GoLF__.\nYou can read about this part of the project wiki where a class diagram\nis also shown.\n\n*To set up your development environment* you should\n__read our step-by-step guide__ which can be found here specifically\nfor Eclipse IDE.\n\n\nCitation\n========\n\nIf you use GoLF in your scientific work or just you want to refer to\nGoLF somewhere, please cite the following\n[paper](http://dx.doi.org/10.1007/978-3-642-23400-2_49). The full\ncitation is\n\n\t@inproceedings{ormandi2011asynchronP2PDM,\n\t  author = {R{\\'o}bert Orm{\\'a}ndi and Istv{\\'a}n Heged\\H{u}s and M{\\'a}rk Jelasity},\n\t  title = {Asynchronous Peer-to-Peer Data Mining with Stochastic Gradient Descent},\n\t  booktitle = {17th International European Conference on Parallel and Distributed Computing (Euro-Par 2011)},\n\t  year = {2011},\n\t  pages = {528-540},\n\t  series = {Lecture Notes in Computer Science},\n\t  volume = {6852},\n\t  publisher = {Springer-Verlag},\n\t  ee = {http://dx.doi.org/10.1007/978-3-642-23400-2_49},\n\t  bibsource = {http://www.inf.u-szeged.hu/~ormandi/papers/ormandi2011asynchronP2PDM.bib}\n\t}\n\n", 
  "id": 4483051
}