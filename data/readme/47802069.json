{
  "read_at": 1462550366, 
  "description": "Wrapper functions for scikit-learn to use wu-svm the same as using its customized libsvm.", 
  "README.txt": "WU-SVM\nVersion: 0.1 Alpha (March 2014)\n\nPlease contact Stephen Tyree (swtyree@wustl.edu), Gabriel Hope (ghope@wustl.edu) or Nicholas Kolkin (n.kolkin@wustl.edu) with any questions, feedback, or bug reports.\n\nThis tool is meant to be an easy to use and very fast SVM solver that leverages\nmulticore processors and/or the parallel capabilities of GPU's to create accurate\nmodels in a fraction of the time required by traditional implementations. It \nrelies on external BLAS/CUBLAS libraries, so it's speed will improve organically\nas hardware and BLAS implementations become more advanced.\n\n//////////////////////////////////////////\n//            REQUIREMENTS:             //\n//////////////////////////////////////////\n  Basic Multicore:\n    CMake (Version 2.8 or higher): http://www.cmake.org/cmake/resources/software.html\n    BLAS/LAPACK Library\n      Download Sources:\n\n        Standard: http://www.netlib.org/lapack/#_software \n    \n        (faster better optimized implementation, will greatly improve speed)\n        ACML: http://developer.amd.com/tools-and-sdks/cpu-development/amd-core-math-library-acml/acml-downloads-resources/\n        MKL: http://software.intel.com/en-us/intel-mkl\n\n  Additional Requirements for GPU Version:\n    Nvidia GPU\n    CUDA (version 5.0 or later): https://developer.nvidia.com/cuda-downloads\n\n\n///////////////////////////////////////\n//          INSTALLATION:            //\n///////////////////////////////////////\n\n  (0) Install requirements\n     (a) Install CMake\n     (b) Install Blas/Lapack Library\n     (c) (GPU version only) Install CUDA (also make sure CUDA drivers are installed for your GPU)\n\n  (1) Navigate to directory containing build.sh (included in download)\n\n  (2) run build.sh (our wrapper for running CMake) or use CMake directly on the src director\n    \n  (2.5) Troubleshooting 'build.sh' in text editor\n\n      (a) set 'CMAKE_PATH' equal to the path to CMake (often unnescessary if CMake is installed correctly)\n      \t  e.g 'CMAKE_PATH =../tools/cmake-2.8.12.1/bin/cmake'\n\n      (b) set 'SRC_PATH' equal to the path where you've stored the source code\n      \t  e.g 'SRC_PATH=../lasp_release_0.1/src/'\n\n      (c) (GPU version only) set 'CUDA_BIN_PATH' equal to the path to your cuda installation\n      \t  e.g 'CUDA_BIN_PATH=/usr/local/cuda-5.5'\n\t  \n      (d) Additional options for building/installation are detailed in 'build.sh' (e.g. OS specific options)\n\n\n  (3) run demo.sh to ensure SP_SVM has correctly installed\n      you should be able to see the SVM create the model,\n      and the it should get an accuracy on the test set of\n      around 85% \n\n\n///////////////////////////////////////\n//          USAGE TRAIN_MC:          //\n///////////////////////////////////////\n\nUsage: train_mc [options] [training data file] [model output file]\n\noptions:\n\n-k kernel: set type of kernel (default = RBF)\n         0 -- Radial Basis Function: exp(-gamma*|u-v|^2)\n         1 -- Linear: u'*v\n         2 -- Polynomial: (gamma*u'*v + coef)^degree\n         3 -- Sigmoid: tanh(gamma*u'*v + coef)\n\n-r coef: sets coefficient value used in some kernel functions (default = 0)\n\n-d degree: sets degree value uesd in some kernel functions (default = 3)\n\n-g gamma: sets gamma value used in some kernel functions (default = 1 / # of features)\n\n-c C: sets c parameter for SVM (default = 1)\n\n-n nb_cand: sets nb_cand (default = 10)\n\n     This sets the number of training example from which the each support vector is chosen.\n     For example if nb_cand=10, the the SVM chooses the most valuable support vector from\n     examples 1-10, then the next one from example 11-20, the next from 21-30, and so on\n\n-s set_size: sets size (default = 5000)\n\n     This sets the maximum number of support vectors the SVM is allowed to train to\n\n-i max_iter: sets max number of backtracking iterations(default = 20)\n\n     If a newton step is too large, controls the maximum amount of backtracking allowed,\n     if you are seeing divergence one option is to try increasing this value\n\n-v verbosity: sets output level (default = 1)\n\n   -v 0: no output\n   -v 1: progress indicator\n   -v 2: training set updates only\n   -v 3: full output\n\n-m max_new_basis: sets maximum number of vectors in new basis (default = 800)\n\n       sets the maximum number of new basis vectors chosen before completely retraining the model\n\n-x stopping_criterion: sets the stopping criterion (default = 5e-6)\n        \n       the stopping criterion is the number of new points classified correctly, divided by the\n       total number of training points. The SVM stops training once it's improvement is below\n       the threshold set with this parameter\n\n-j jump_set: sets the size of the starting training set (default = 100)\n\n       We initialize training with moderate number of basis vectors chosen (default 100), so that\n       we start with a feasibly complex model for most problems (rather than wasting time computing\n       overly simple ones), the number of basis vectors we start with can be set with this parameter\n\n-b probability_estimates: use Platt scaling to produce probability estimates (default 0)\n\n--gpu (-u) gpu: uses CUDA to accelerate computation\n\n--omp_threads (-T) Open MP threads: sets the maximum number of openMP threads allowed (only used if built openMP)\n\n--no_cache (-K) no cache: avoid caching the full kernel matrix\n\n       Significantly reduces the amount of memory used at the expense of slower training speed\n\n--random (-f) randomize: randomizes training set selection\n\n       Can speed training, potentially at the expense of accuracy (speeds up training if heuristically\n       choosing basis vectors is the bottleneck)\n\n--version (-q) version: displays version number and exits\n\n-h help: displays this message\n\n\n///////////////////////////////////////\n//          USAGE CLASSIFY_MC:          //\n///////////////////////////////////////\n\nThree arguments required.\n\nusage: [options] ./classify_mc dataFile modelFile outputFile\n\noptions:\n\n  -v verbosity: sets output level (default = 1)\n     -v 0: no output\n     -v 1: prints accuracy\n\n  --gpu (-u) gpu: uses CUDA to accelerate computation\n\n  --dag (-d) dag: use dag model for multiclass classification\n\n  --version (-q) version: displays version number and exits\n\n  -h help: displays this message\n\n\n\n/////////////////////////////////////////\n//              EXAMPLE:               //\n/////////////////////////////////////////\n\n  ./train_mc --gpu -v 3 -s 2000 -c 1.0 -g 0.05 adult.test adult.train adult.model\n  \n  trains a classifier on the gpu with verbosity level 3, with a maximum of 2000 support vectors, using an\n  RBF kernel (by default) with c = 1.0 and gamma = 0.05\n\n  ./classify_mc adult.test adult.model adult.labels\n\n  tests the classifier trained above and outputs the labels\n\n\n/////////////////////////////////////////\n//            DATA FORMAT:             //\n/////////////////////////////////////////\n\n  Input and output is in lib-SVM compatible format. See Lib-SVM website for more details \n  on format specifications.\n\n  http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n\n\n/////////////////////////////////////////\n//             CITATIONS:              //\n/////////////////////////////////////////\n\nIf you use WU-SVM in a published work, please cite the following technical report:\n\n@article{tyree2014parallel,\n  title={Parallel Support Vector Machines in Practice},\n  author={Tyree, Stephen and Gardner, Jacob R. and Weinberger, Kilian Q. and Agrawal, Kunal and Tran, John},\n  journal={arXiv preprint arXiv:1404.1066},\n  year={2014}\n}\n\n//////////////////////////////////////////\n//               LICENSE:               //\n//////////////////////////////////////////\n\nCopyright (c) 2014, Washington University in St. Louis\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in the\n      documentation and/or other materials provided with the distribution.\n    * Neither the name of Washington University in St. Louis nor the\n      names of its contributors may be used to endorse or promote products\n      derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \nARE DISCLAIMED. IN NO EVENT SHALL WASHINGTON UNIVERSITY BE LIABLE FOR ANY \nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES \n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; \nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND \nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT \n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF \nTHIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n", 
  "id": 47802069
}