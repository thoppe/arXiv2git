{
  "read_at": 1462545888, 
  "description": null, 
  "README.txt": "****************************************************************\n\nThis is the implementation of RadoBoost following the paper:\n\nR. Nock, G. Patrini and A. Friedman\n\"Rademacher Observations, Private Data and Boosting\"\nInternational Conference on Machine Learning, 2015\n\nThis program is provided as is, without any warranty whatsoever. \nUse it at your own risks, and enjoy :).\n\nQuestions: to R. Nock at cord.krichna@gmail.com\n\n****************************************************************\n\n** Datasets format\n\nRadoBoost is provided with four UCI datasets. It should be clear\nhow to format a new dataset, just make two files, .data and .features\nwhere you indicate the nature of each observed feature, AND which\none is the class (and also how you represent class values from the observed data).\n\n** Executing the program\n\nHow to compile:\tas any Java program, it should compile with a bare \njavac compiler.\n\nHow to run (example):\tjava -d64 -Xmx5000m Experiments -R resource_example1.txt,\n\nwhere resource_example1.txt contains the parameters to run the algorithms.\nThe program indicates at run time the memory load. Should help to properly parameterise\na 64bits JVM.\n\nKey parameters for this file are:\n\n@MAX_RADO_SIZE,1000  --> fixes the number of rados to generate in each fold.\n(Warning: the as written in the ICML paper, the code (in LinearBoost.java, tag \"RAD-C1\") contains a\ntest that controls that the number of rados does not exceed the training size / 2.\nRemove this test if you want a larger number of rados)\n\n@ALGORITHM,XXX,YYY,Z,T,UUU[,VVV]\nlike in @ALGORITHM,@AdaBoost,1000,0,0,@ALL\n\n* XXX = type of algorithm to run (two choices: @RadoBoost or @AdaBoost)\n* YYY = number of boosting rounds\n* Z = If 0, then uses r_t as in the paper (eq. (9)). If 1, then clamps r_t\n  (i.e. if |r_t| < SCALE_MU_THRESHOLD (in LinearBoost.java, tag \"RAD-C2\"), then fixes |r_t| = SCALE_MU_THRESHOLD;\n  this accelerates the convergence of AdaBoost if SCALE_MU_THRESHOLD is not too large or too small; see \n  the supplementary information in the ArXiv version for more details)\n* T = If 0, then uses AdaBoost.R weight update (for RadoBoost, this is the choice of the paper in eq. (11)).\n  If 1, then uses AdaBoost weight update. Note that you can use Adaboost weight update with RadoBoost as well\n  (we did not use this option for the paper)\n* UUU = size of training sample (examples/rados) used to train the algorithm. If @MAX, then uses all the\n  available data to train the algorithm. If @MAX, uses the @MAX_RADO_SIZE number. \n* For RadoBoost, VVV gives the method to generate rados. Two choices (in LinearBoost.java, tag \"RAD-C3\"): \n  @RANDOM = plain random rados;\n  @DIFFPRIVK = picks a binary attribute at random and generate rados according to the Rademacher rejection\n  sampling algorithm in Algorithm 2. In this case, epsilon is then given (delta corresponds to any value\n  satisfying (77) in the long ArXiv version of the paper, http://arxiv.org/pdf/1502.02322v2.pdf)\n\n", 
  "id": 40108611
}