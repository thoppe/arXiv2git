{
  "read_at": 1462557494, 
  "description": "Train a deep learning net with OpenStreetMap features and satellite imagery.", 
  "README.md": "# DeepOSM\n\nDetect roads and features in satellite imagery, by training neural networks with OpenStreetMap (OSM) data. The gist:\n\n* Download a chunk of satellite imagery\n* Download OSM data that shows roads/features for that area\n* Generate training and evaluation data\n\nRead below to run the code. [I am blogging my work journal too](http://trailbehind.github.io/DeepOSM/). \n\nContributions are welcome. Open an issue if you want to discuss something to do, or [email me](mailto:andrew@gaiagps.com).\n\n## Background on Data - NAIPs and OSM PBF\n\nFor training data, DeepOSM cuts tiles out of [NAIP images](http://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/), which provide 1 meter per pixel resolution, with RGB+infrared data bands.\n\nFor training labels, DeepOSM uses PBF extracts of OSM data, which contain features/ways in binary format, which can be munged with Python.\n\nThe [NAIPs come from a requester pays bucket on S3 set up by Mapbox](http://www.slideshare.net/AmazonWebServices/open-data-innovation-building-on-open-data-sets-for-innovative-applications), and the OSM extracts come [from geofabrik](http://download.geofabrik.de/).\n\n## Install Requirements\n\n### AWS Credentials\n\nYou need AWS credentials to download NAIPs from an S3 requester-pays bucket. This only costs a few cents for a bunch of images, but you need a credit card on file.\n\n * get your [AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from AWS](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html)\n\n * export them as environment variables (and maybe add to your bash or zprofile)\n\n    export AWS_ACCESS_KEY_ID='FOO'\n\n    export AWS_SECRET_ACCESS_KEY='BAR'\n\n### Install Docker\n\nFirst, [install a Docker Binary](https://docs.docker.com/engine/installation/).\n\nI also needed to set my VirtualBox default memory to 12GB. libosmium needed 4GB, and the neural net needed even more. This is easy:\n\n * start Docker, per the install instructions\n * stop Docker\n * open VirtualBox, and increase the memory of the VM Docker made\n\n### Run Scripts\n\nStart Docker, then run:\n\n```bash\nmake dev\n```\n\n### Download NAIP, PBF, and Analyze\n\nInside Docker, the following Python script will work. It will download all source data, tile it into training/test data and labels, train the neural net, and generate image and text output. The default data is three NAIPs, which gets tiled into NxNx4 bands of data (RGB-IR bands). The training labels derive from PBF files that overlap the NAIPs.\n\n#### Dry Run, Predicts Poorly (random, about 50%)\n\n    python src/run_analysis.py\n\nFor output, it will produce some console logs, and then JPEGs of the ways, labels, and predictions overlaid on the tiff. \n\n#### Full-Data Run (as good as 75%)\n\nIf you then re-run the analysis, but set some parameters, it will be more accurate on a CPU (~70%), after running all night. On the second run, the data prep won't take nearly as long, but the neural net training takes much longer.\n\n    python src/run_analysis.py --bands=0001 --model=mnist --training_batches=6000\n\nFull usage is as follows, or use -h on the command line:\n\n```\nrun_analysis.py [-h] \n                [--tile_size TILE_SIZE]\n                [--training_batches TRAINING_BATCHES]\n                [--batch_size BATCH_SIZE] \n                [--bands BANDS]\n                [--extract_type EXTRACT_TYPE]\n                [--cache_way_bmp CACHE_WAY_BMP]\n                [--clear_way_bmp_cache CLEAR_WAY_BMP_CACHE]\n                [--render_results RENDER_RESULTS] \n                [--model MODEL]\n```\n\n![NAIP with Ways and Predictions](https://pbs.twimg.com/media/Cg2F_tBUcAA-wHs.png)\n\n### Jupyter Notebook\n\nAlternately, development/research can be done via jupyter notebooks:\n\n```bash\nmake notebook\n```\n\nTo access the notebook via a browser on your host machine, find the IP VirtualBox is giving your default docker container by running:\n\n```bash\ndocker-machine ls\n\nNAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER    ERRORS\ndefault   *        virtualbox   Running   tcp://192.168.99.100:2376           v1.10.3\n```\n\nThe notebook server is accessible via port 8888, so in this case you'd go to:\nhttp://192.168.99.100:8888\n\n### Readings\n\n* [TensorFlow](https://www.tensorflow.org/) - using this for the deep learning, do multilayer, deep CNN\n* [Learning to Detect Roads in High-Resolution Aerial\nImages](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.232.1679&rep=rep1&type=pdf) (Hinton) \n* [Machine Learning for Aerial Image Labeling](https://www.cs.toronto.edu/~vmnih/docs/Mnih_Volodymyr_PhD_Thesis.pdf)- Minh's 2013 thesis, student of Hinton's\nbest/recent paper on doing this, great success with these methods\n* Similar Efforts with OSM Data\n    * [OSM-Crosswalk-Detection](https://github.com/geometalab/OSM-Crosswalk-Detection) - uses Keras to detect crosswalks, a class project (Fall 2015)\n    * [OSM-HOT-ConvNet](https://github.com/larsroemheld/OSM-HOT-ConvNet) - attempted use for disaster response, author thinks it's only 69% accurate at pixel level (Fall 2016)\n* [Parsing Natural Scenes and Natural Language\nwith Recursive Neural Networks (RNNs)](http://ai.stanford.edu/~ang/papers/icml11-ParsingWithRecursiveNeuralNetworks.pdf)\n* Links from the Tensorflow site\n    * [MNIST Data and Background](http://yann.lecun.com/exdb/mnist/)\n    * all the other links to Nielsen's book and [Colah's blog](http://colah.github.io/posts/2015-08-Backprop/)\n* Deep Background\n    * [original Information Theory paper by Shannon](http://worrydream.com/refs/Shannon%20-%20A%20Mathematical%20Theory%20of%20Communication.pdf)\n\n\n### Papers that Cite Hinton/Mnih, to Review\n\nI am reviewing these papers from Google Scholar that both cite the key papers and seem relevant to the topic. \n\n* http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6602035&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6602035\n\n* http://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W13/html/Paisitkriangkrai_Effective_Semantic_Pixel_2015_CVPR_paper.html\n\n* http://www.tandfonline.com/doi/abs/10.1080/15481603.2013.802870\n\n* https://www.computer.org/csdl/proceedings/icpr/2014/5209/00/5209d708-abs.html\n\n* http://opticalengineering.spiedigitallibrary.org/article.aspx?articleid=1679147\n\n* http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1354584\n\n* http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.309.8565\n\n* https://www.itc.nl/library/papers_2012/msc/gem/shaoqing.pdf\n\n* http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7326745&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7326745\n\n* http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2191094\n\n* http://arxiv.org/abs/1509.03602\n\n* http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7112625&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7112625\n\n* http://www.sciencedirect.com/science/article/pii/S0924271615001690\n\n* http://arxiv.org/abs/1405.6137\n\n* https://www.itc.nl/external/ISPRS_WGIII4/ISPRSIII_4_Test_results/papers/Onera_2D_label_Vaih.pdf\n\n* http://link.springer.com/chapter/10.1007/978-3-319-23528-8_33#page-1\n\n* http://arxiv.org/abs/1508.06163\n\n* http://www.mdpi.com/2072-4292/8/4/329\n\n* http://arxiv.org/abs/1510.00098\n\n* http://link.springer.com/article/10.1007/s10489-016-0762-6\n\n* http://www.tandfonline.com/doi/abs/10.1080/01431161.2015.1054049\n\n* http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7393563&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7393563\n\n* http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Mattyus_Enhancing_Road_Maps_ICCV_2015_paper.html\n\n* http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zheng_Minimal_Solvers_for_ICCV_2015_paper.html\n\n* http://arxiv.org/abs/1405.6136\n\n* http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.681.1695&rep=rep1&type=pdf\n\n* http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7120492&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7120492\n\n* http://www.tandfonline.com/doi/abs/10.3846/20296991.2014.890271\n\n* http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7362660&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7362660\n\n* http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7414402&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7414402\n\n* http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6663455&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6663455\n\n* http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7337372&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7337372\n\n* https://www.researchgate.net/profile/Moslem_Ouled_Sghaier/publication/280655680_Road_Extraction_From_Very_High_Resolution_Remote_Sensing_Optical_Images_Based_on_Texture_Analysis_and_Beamlet_Transform/links/55c0d9da08ae092e96678ff3.pdf\n\n* http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7159022&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7159022\n\n* http://www.sciencedirect.com/science/article/pii/S0303243415300283\n\n* http://dl.acm.org/citation.cfm?id=2666389\n\n* http://www.ijicic.org/ijicic-15-04045.pdf\n\n### Papers - Relevant Maybe\n\n* [Uses a large window to improve predictions, trying to capture broad network topology](https://www.inf.ethz.ch/personal/ladickyl/roads_gcpr14.pdf)\n\n* [Automatically extract roads with no human labels. Not that accurate, could work for preprocessing to detect roads in unlab](https://www.researchgate.net/publication/263892800_Tensor-Cuts_A_simultaneous_multi-type_feature_extractor_and_classifier_and_its_application_to_road_extraction_from_satellite_images)\n\n\n### Papers - Not All that Relevant\n\n* [Uses map data and shapes of overpasses to then detect pictures of the objects? Seems like a cool paper to read if it was free.](http://dl.acm.org/citation.cfm?id=2424336)\n\n* [New technique for classification of sub-half-meter data into different zones](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6827949&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6827949)\n\n* [Couldn't access text, focused on usig lidar data](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6238909&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6238909)\n\n* [Proposes a way to extract network topology, and maybe this can be used as a post processor?](http://www.cv-foundation.org/openaccess/content_cvpr_2013/html/Wegner_A_Higher-Order_CRF_2013_CVPR_paper.html)\n\n\n\n### Original Idea\n\nThis was the general idea to start, and working with TMS tiles sort of worked (see first 50 or so commits), so DeepOSM got switched to better data:\n\n![Deep OSM Project](https://gaiagps.mybalsamiq.com/mockups/4278030.png?key=1e42f249214928d1fa7b17cf866401de0c2af867)", 
  "id": 51717338
}