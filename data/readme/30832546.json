{
  "read_at": 1462551875, 
  "description": "Webcrawler for arxiv.com", 
  "README.md": "## Webcrawler for arxiv.com\n\n### Purpose\nThe webcrawler was written in order to gain experience with python as well as to be able to build a bipartite author-paper network with timestamps (date the paper was published)\n\n### Explanation in respect to the admins of arxiv.com\nFirst of all I want to explain that the arxiv website forbids extensive crwaling of their website.\nThe webpage should be available for human activities not onlinefor robots.\nWith respect to this I set a delay of several seconds.\n\n### What does it do?\nBeginning from one article on the arxiv website (given in startingurl) the web crwaler searches up to 5 authors and for each author we get a list of at most 10 papers in which he or she participated.\nThe program queues every link to a paper.\n\nThe program saves the date, the subject, the authors(5 at most) and the arxiv ID to local mongoDB instance\n", 
  "id": 30832546
}