{
  "README.Rmd": "---\noutput:\n  md_document:\n    variant: markdown_github\n---\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  fig.path = \"README-\"\n)\n```\n\ndeeplearning \n=====\n\n#### Create and train deep neural network of ReLU type with SGD and batch normalization\n\n### About\nThe deeplearning package is an R package that implements deep neural networks in R. It employes Rectifier Linear Unit functions as its building blocks and trains a neural network with stochastic gradient descent method with batch normalization to speed up the training and promote regularization. Neural networks of such kind of architecture and training methods are state of the art and even achieved suplassing human-level performance in ImageNet competition. The deeplearning package is inspired by another R package darch which implements layerwise Restricted Boltzmann Machine pretraining and dropout and uses its class DArch as the default class. \n\n### Installtion \n\nInstall deeplearning from CRAN \n```\ninstall.packages(\"deeplearning\")\n```\n\nOr install it from github \n```\ndevtools::install_github(\"rz1988/deeplearning\")\n\n```\n\n### Use deeplearning \n\nUsing the deeplearning package is designed to be easy and fun. It only takes two steps to run your first neural network. \n\nIn step one, the user will create a new neural network. You will need to specify the strucutre of the neural network which are the number of layers and neurons in the network and the type of activation functions. The default activation is rectifier linear unit function for the hidden layers but you can also use other types of activation such as sigmoidal function or write your own activation function.\n\nIn step two, the user will train the neural network with a training input and a traing target. There are a number of other training parameters. For how to choose these training parameters please refer to https://github.com/rz1988/deeplearning. \n\n### Examples \n\n#### Train a neural networ for regression \n\n```\ninput <- matrix(runif(1000), 500, 2)\ninput_valid <- matrix(runif(100), 50, 2)\ntarget <- rowSums(input + input^2)\ntarget_valid <- rowSums(input_valid + input_valid^2)\n\n\n# create a new deep neural network for classificaiton\ndnn_regression <- new_dnn(\n                          c(2, 50, 50, 20, 1),  # The layer structure of the deep neural network.\n                                                # The first element is the number of input variables.\n                                                # The last element is the number of output variables.\n                          hidden_layer_default = rectified_linear_unit_function, \n                          # for hidden layers, use rectified_linear_unit_function\n                          output_layer_default = linearUnitDerivative # for regression, use linearUnitDerivative function\n                          )\n\ndnn_regression <- train_dnn(\n                     dnn_regression,\n\n                     # training data\n                     input, # input variable for training\n                     target, # target variable for training\n                     input_valid, # input variable for validation\n                     target_valid, # target variable for validation\n\n                     # training parameters\n                     learn_rate_weight = exp(-8) * 10, # learning rate for weights, higher if use dropout\n                     learn_rate_bias = exp(-8) * 10, # learning rate for biases, hihger if use dropout\n                     learn_rate_gamma = exp(-8) * 10, # learning rate for the gamma factor used\n                     batch_size = 10, # number of observations in a batch during training. Higher for faster training. Lower for faster convergence\n                     batch_normalization = T, # logical value, T to use batch normalization\n                     dropout_input = 0.2, # dropout ratio in input.\n                     dropout_hidden = 0.5, # dropout ratio in hidden layers\n                     momentum_initial = 0.6, # initial momentum in Stochastic Gradient Descent training\n                     momentum_final = 0.9, # final momentum in Stochastic Gradient Descent training\n                     momentum_switch = 100, # after which the momentum is switched from initial to final momentum\n                     num_epochs = 300, # number of iterations in training\n\n                     # Error function\n                     error_function = meanSquareErr, # error function to minimize during training. For regression, use meanSquareErr\n                     report_classification_error = F # whether to print classification error during training\n)\n\n# the prediciton by dnn_regression\npred <- predict(dnn_regression)\n\n# calculate the r-squared of the prediciton\nrsq(dnn_regression)\n\n# calcualte the r-squared of the prediciton in validation\nrsq(dnn_regression, input = input_valid, target = target_valid)\n```\n\n#### Train a neural network for classification \n\n```\n\ninput <- matrix(runif(1000), 500, 2)\ninput_valid <- matrix(runif(100), 50, 2)\ntarget <- (cos(rowSums(input + input^2)) > 0.5) * 1\ntarget_valid <- (cos(rowSums(input_valid + input_valid^2)) > 0.5) * 1\n\n# create a new deep neural network for classificaiton\ndnn_classification <- new_dnn(\n  c(2, 50, 50, 20, 1),  # The layer structure of the deep neural network.\n                        # The first element is the number of input variables.\n                        # The last element is the number of output variables.\n  hidden_layer_default = rectified_linear_unit_function, # for hidden layers, use rectified_linear_unit_function\n  output_layer_default = sigmoidUnitDerivative # for classification, use sigmoidUnitDerivative function\n)\n\ndnn_classification <- train_dnn(\n  dnn_classification,\n\n  # training data\n  input, # input variable for training\n  target, # target variable for training\n  input_valid, # input variable for validation\n  target_valid, # target variable for validation\n\n  # training parameters\n  learn_rate_weight = exp(-8) * 10, # learning rate for weights, higher if use dropout\n  learn_rate_bias = exp(-8) * 10, # learning rate for biases, hihger if use dropout\n  learn_rate_gamma = exp(-8) * 10, # learning rate for the gamma factor used\n  batch_size = 10, # number of observations in a batch during training. Higher for faster training. Lower for faster convergence\n  batch_normalization = T, # logical value, T to use batch normalization\n  dropout_input = 0.2, # dropout ratio in input.\n  dropout_hidden = 0.5, # dropout ratio in hidden layers\n  momentum_initial = 0.6, # initial momentum in Stochastic Gradient Descent training\n  momentum_final = 0.9, # final momentum in Stochastic Gradient Descent training\n  momentum_switch = 100, # after which the momentum is switched from initial to final momentum\n  num_epochs = 100, # number of iterations in training\n\n  # Error function\n  error_function = crossEntropyErr, # error function to minimize during training. For regression, use crossEntropyErr\n  report_classification_error = T # whether to print classification error during training\n)\n\n# the prediciton by dnn_regression\npred <- predict(dnn_classification)\n\nhist(pred)\n\n# calculate the r-squared of the prediciton\nAR(dnn_classification)\n\n# calcualte the r-squared of the prediciton in validation\nAR(dnn_classification, input = input_valid, target = target_valid)\n\n# print the layer weights\n# this function can print heatmap, histogram, or a surface\nprint_weight(dnn_regression, 1, type = \"heatmap\")\n\nprint_weight(dnn_regression, 2, type = \"surface\")\n\nprint_weight(dnn_regression, 3, type = \"histogram\")\n```\n\n#### References \nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, 2013, Dropout: A Simple Way to Prevent Neural Networks from Overfitting, Journal of Machine Learning Research 15 (2014) 1929-1958\n\nSergey Ioffe, Christian Szegedy, 2015, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 2015, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, arXiv\n\nX. Glorot, A. Bordes, and Y. Bengio, 2011,Deep sparse rectifier networks. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, pages 315-323\n\n\nDrees, Martin (2013). \"Implementierung und Analyse von tiefen Architekturen\nin R\". German. Master's thesis. Fachhochschule Dortmund.\n\nRueckert, Johannes (2015). \"Extending the Darch library for deep\narchitectures\". Project thesis. Fachhochschule Dortmund.\nURL: [saviola.de](http://static.saviola.de/publications/rueckert_2015.pdf)\n\n\n \n", 
  "read_at": 1462555917, 
  "description": "A deep neural network package for R", 
  "README.md": "<!-- README.md is generated from README.Rmd. Please edit that file -->\ndeeplearning\n============\n\n#### Create and train deep neural network of ReLU type with SGD and batch normalization\n\n### About\n\nThe deeplearning package is an R package that implements deep neural networks in R. It employes Rectifier Linear Unit functions as its building blocks and trains a neural network with stochastic gradient descent method with batch normalization to speed up the training and promote regularization. Neural networks of such kind of architecture and training methods are state of the art and even achieved suplassing human-level performance in ImageNet competition. The deeplearning package is inspired by another R package darch which implements layerwise Restricted Boltzmann Machine pretraining and dropout and uses its class DArch as the default class.\n\n### Installtion\n\nInstall deeplearning from CRAN\n\n    install.packages(\"deeplearning\")\n\nOr install it from github\n\n    devtools::install_github(\"rz1988/deeplearning\")\n\n### Use deeplearning\n\nUsing the deeplearning package is designed to be easy and fun. It only takes two steps to run your first neural network.\n\nIn step one, the user will create a new neural network. You will need to specify the strucutre of the neural network which are the number of layers and neurons in the network and the type of activation functions. The default activation is rectifier linear unit function for the hidden layers but you can also use other types of activation such as sigmoidal function or write your own activation function.\n\nIn step two, the user will train the neural network with a training input and a traing target. There are a number of other training parameters. For how to choose these training parameters please refer to <https://github.com/rz1988/deeplearning>.\n\n### Examples\n\n#### Train a neural networ for regression\n\n    input <- matrix(runif(1000), 500, 2)\n    input_valid <- matrix(runif(100), 50, 2)\n    target <- rowSums(input + input^2)\n    target_valid <- rowSums(input_valid + input_valid^2)\n\n\n    # create a new deep neural network for classificaiton\n    dnn_regression <- new_dnn(\n                              c(2, 50, 50, 20, 1),  # The layer structure of the deep neural network.\n                                                    # The first element is the number of input variables.\n                                                    # The last element is the number of output variables.\n                              hidden_layer_default = rectified_linear_unit_function, \n                              # for hidden layers, use rectified_linear_unit_function\n                              output_layer_default = linearUnitDerivative # for regression, use linearUnitDerivative function\n                              )\n\n    dnn_regression <- train_dnn(\n                         dnn_regression,\n\n                         # training data\n                         input, # input variable for training\n                         target, # target variable for training\n                         input_valid, # input variable for validation\n                         target_valid, # target variable for validation\n\n                         # training parameters\n                         learn_rate_weight = exp(-8) * 10, # learning rate for weights, higher if use dropout\n                         learn_rate_bias = exp(-8) * 10, # learning rate for biases, hihger if use dropout\n                         learn_rate_gamma = exp(-8) * 10, # learning rate for the gamma factor used\n                         batch_size = 10, # number of observations in a batch during training. Higher for faster training. Lower for faster convergence\n                         batch_normalization = T, # logical value, T to use batch normalization\n                         dropout_input = 0.2, # dropout ratio in input.\n                         dropout_hidden = 0.5, # dropout ratio in hidden layers\n                         momentum_initial = 0.6, # initial momentum in Stochastic Gradient Descent training\n                         momentum_final = 0.9, # final momentum in Stochastic Gradient Descent training\n                         momentum_switch = 100, # after which the momentum is switched from initial to final momentum\n                         num_epochs = 300, # number of iterations in training\n\n                         # Error function\n                         error_function = meanSquareErr, # error function to minimize during training. For regression, use meanSquareErr\n                         report_classification_error = F # whether to print classification error during training\n    )\n\n    # the prediciton by dnn_regression\n    pred <- predict(dnn_regression)\n\n    # calculate the r-squared of the prediciton\n    rsq(dnn_regression)\n\n    # calcualte the r-squared of the prediciton in validation\n    rsq(dnn_regression, input = input_valid, target = target_valid)\n\n#### Train a neural network for classification\n\n\n    input <- matrix(runif(1000), 500, 2)\n    input_valid <- matrix(runif(100), 50, 2)\n    target <- (cos(rowSums(input + input^2)) > 0.5) * 1\n    target_valid <- (cos(rowSums(input_valid + input_valid^2)) > 0.5) * 1\n\n    # create a new deep neural network for classificaiton\n    dnn_classification <- new_dnn(\n      c(2, 50, 50, 20, 1),  # The layer structure of the deep neural network.\n                            # The first element is the number of input variables.\n                            # The last element is the number of output variables.\n      hidden_layer_default = rectified_linear_unit_function, # for hidden layers, use rectified_linear_unit_function\n      output_layer_default = sigmoidUnitDerivative # for classification, use sigmoidUnitDerivative function\n    )\n\n    dnn_classification <- train_dnn(\n      dnn_classification,\n\n      # training data\n      input, # input variable for training\n      target, # target variable for training\n      input_valid, # input variable for validation\n      target_valid, # target variable for validation\n\n      # training parameters\n      learn_rate_weight = exp(-8) * 10, # learning rate for weights, higher if use dropout\n      learn_rate_bias = exp(-8) * 10, # learning rate for biases, hihger if use dropout\n      learn_rate_gamma = exp(-8) * 10, # learning rate for the gamma factor used\n      batch_size = 10, # number of observations in a batch during training. Higher for faster training. Lower for faster convergence\n      batch_normalization = T, # logical value, T to use batch normalization\n      dropout_input = 0.2, # dropout ratio in input.\n      dropout_hidden = 0.5, # dropout ratio in hidden layers\n      momentum_initial = 0.6, # initial momentum in Stochastic Gradient Descent training\n      momentum_final = 0.9, # final momentum in Stochastic Gradient Descent training\n      momentum_switch = 100, # after which the momentum is switched from initial to final momentum\n      num_epochs = 100, # number of iterations in training\n\n      # Error function\n      error_function = crossEntropyErr, # error function to minimize during training. For regression, use crossEntropyErr\n      report_classification_error = T # whether to print classification error during training\n    )\n\n    # the prediciton by dnn_regression\n    pred <- predict(dnn_classification)\n\n    hist(pred)\n\n    # calculate the r-squared of the prediciton\n    AR(dnn_classification)\n\n    # calcualte the r-squared of the prediciton in validation\n    AR(dnn_classification, input = input_valid, target = target_valid)\n\n    # print the layer weights\n    # this function can print heatmap, histogram, or a surface\n    print_weight(dnn_regression, 1, type = \"heatmap\")\n\n    print_weight(dnn_regression, 2, type = \"surface\")\n\n    print_weight(dnn_regression, 3, type = \"histogram\")\n\n#### References\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, 2013, Dropout: A Simple Way to Prevent Neural Networks from Overfitting, Journal of Machine Learning Research 15 (2014) 1929-1958\n\nSergey Ioffe, Christian Szegedy, 2015, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 2015, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, arXiv\n\nX. Glorot, A. Bordes, and Y. Bengio, 2011,Deep sparse rectifier networks. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, pages 315-323\n\nDrees, Martin (2013). \"Implementierung und Analyse von tiefen Architekturen in R\". German. Master's thesis. Fachhochschule Dortmund.\n\nRueckert, Johannes (2015). \"Extending the Darch library for deep architectures\". Project thesis. Fachhochschule Dortmund. URL: [saviola.de](http://static.saviola.de/publications/rueckert_2015.pdf)\n", 
  "id": 53913209
}