{
  "read_at": 1462556085, 
  "description": "Implementation of caption-image retrieval from the paper \"Order-Embeddings of Images and Language\"", 
  "README.md": "# order-embeddings\n\nTheano implementation of caption-image retrieval from the paper [\"Order-Embeddings of Images and Language\"](http://arxiv.org/abs/1511.06361).\n\nSimilar to [visual-semantic-embedding](https://github.com/ryankiros/visual-semantic-embedding) of which this repository is a fork, \nwe map images and their captions into a common vector space. The main difference, as explained in the paper, is that we model\nthe caption-image relationship as an (asymmetric) partial order rather than a symmetric similarity relation.\n\nThe code differs from visual-semantic-embedding in a number of other ways, including using 10-crop averaged VGG features for the \nimage representation, and adding a visualization server.\n\n\n## Dependencies\n\nThis code is written in python. To use it you will need:\n\n* Python 2.7\n* Theano 0.7\n* A recent version of [NumPy](http://www.numpy.org/) and [SciPy](http://www.scipy.org/)\n\n## Replicating the paper\n\n### Getting data\n\nDownload the dataset files (1 GB), including 10-crop [VGG19 features](http://www.robots.ox.ac.uk/~vgg/research/very_deep/), by running\n\n    wget http://www.cs.toronto.edu/~vendrov/order/coco.zip\n   \nNote that we use the [splits](http://cs.stanford.edu/people/karpathy/deepimagesent/) produced by Andrej Karpathy. The full COCO dataset\ncan be obtained [here](http://mscoco.org/).\n    \nUnzip the downloaded file - if not in the project directory, you'll need to change the `datasets_dir` variable in `paths.py`.\n\n**note for Toronto users**: just run `ln -s /ais/gobi1/vendrov/order/coco data/coco` instead\n    \n### Evaluating pre-trained models\n\nDownload two pre-trained models (the full model and the symmetric baseline, 124 MB) and associated visualization data by running\n\n    wget http://www.cs.toronto.edu/~vendrov/order/models.zip\n    \nUnzip the file in the project directory, and evaluate by running \n\n```python\n\n    import tools, evaluation\n    model = tools.load_model('snapshots/order')\n    evaluation.ranking_eval_5fold(model, split='test')\n```\n    \n\n## Computing image and sentence vectors\n\nSuppose you have a list of strings that you would like to embed into the learned vector space. To embed them, run the following:\n\n    sentence_vectors = tools.encode_sentences(model, s, verbose=True)\n    \nWhere `s` is the list of strings. Note that the strings should already be pre-tokenized, so that `str.split()` returns the tokens.\n\nAs the vectors are being computed, it will print some numbers. The code works by extracting vectors in batches of sentences that have the same length - so the number corresponds to the current length being processed. If you want to turn this off, set verbose=False when calling encode.\n\nTo encode images, run the following instead:\n\n    image_vectors = tools.encode_images(model, im)\n    \nWhere `im` is a NumPy array of VGG features. Note that the VGG features were scaled to unit norm prior to training the models.\n\n## Training new models\n\nTo train your own models, simply run \n\n    import train\n    train.trainer(**kwargs)\n\nAs the model trains, it will periodically evaluate on the development set and re-save the model each time performance on the development set increases. Once the models are saved, you can load and evaluate them in the same way as the pre-trained models.\n\n`train.trainer` has many hyperparameters; see `driver.py` for the ones used in the paper. Descriptions of each hyperparameter follow:\n\n\n#### Saving / Loading\n* **name**: a string describing the model, used for saving + visualization\n* **save_dir**: the location to save model snapshots\n* **load_from**: location of model from which to load existing parameters\n* **dispFreq**: How often to display training progress (in batches)\n* **validFreq**: How often to evaluate on the development set\n\n#### Data\n* **data**: The dataset to train on (currently only 'coco' is supported)\n* **cnn**: The name of the CNN features to use, if you want to evaluate different image features\n\n#### Architecture\n* **dim**: The dimensionality of the learned embedding space (also the size of the RNN state)\n* **dim_image**: The dimensionality of the image features. This will be 4096 for VGG\n* **dim_word**: The dimensionality of the learned word embeddings\n* **encoder**: The type of RNN to use to encode sentences (currently only 'gru' is supported)\n* **margin**: The margin used for computing the pairwise ranking loss\n\n#### Training\n* **optimizer**: The optimization method to use (currently only 'adam' is supported)\n* **batch_size**: The size of a minibatch.\n* **max_epochs**: The number of epochs used for training\n* **lrate**: Learning rate\n* **grad_clip**: Magnitude at which to clip the gradient\n\n    \n## Training on different datasets\n\nTo train on a different dataset, put tokenized sentences and image features in the same format as those provided for COCO,\nadd the relevant paths to `paths.py`, and modify `datasets.py` to handle your dataset correctly.\n\nIf you're training on Flickr8k or Flickr30k, just put [Karpathy's](http://cs.stanford.edu/people/karpathy/deepimagesent/) `dataset_flickr{8,30}k.json` file in the dataset directory, and run the scripts `generate_captions.py` and `extract_cnn_features.py`. The latter script requires a working [Caffe installation](http://caffe.berkeleyvision.org/installation.html), as well as the VGG19 [model spec and weights](https://gist.github.com/ksimonyan/3785162f95cd2d5fee77).\n\nThe evaluation (`evaluation.py`) and batching (`datasource.py`) assume that there are exactly 5 captions per image; if your dataset doesn't have this property, you will need to modify them.\n\n## Visualizations\n\nYou can view plots of training errors and ranking metrics, as well as ROC curves for Image Retrieval, by running the visualization server.\nSee the `vis` directory for more details.\n\n## Reference\n\nIf you found this code useful, please cite the following paper:\n\nIvan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun. **\"Order-Embeddings of Images and Language.\"** *arXiv preprint arXiv:1511.06361 (2015).*\n\n    @article{vendrov2015order,\n      title={Order-embeddings of images and language},\n      author={Vendrov, Ivan and Kiros, Ryan and Fidler, Sanja and Urtasun, Raquel},\n      journal={arXiv preprint arXiv:1511.06361},\n      year={2015}\n    }\n\n## License\n\n[Apache License 2.0](http://www.apache.org/licenses/LICENSE-2.0)\n\n\n\n\n", 
  "id": 44820527
}