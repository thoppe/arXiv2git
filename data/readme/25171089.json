{
  "read_at": 1462511191, 
  "description": "Fast matrix multiplication", 
  "README.md": "Fast matrix multiplication\n--------\nAustin R. Benson and Grey Ballard\n\nThis software contains implementations of fast matrix multiplication algorithms for\nsequential and shared-memory parallel environments.\n\nTo cite this work, please use:\n\nAustin R. Benson and Grey Ballard.  \"A framework for practical parallel fast matrix multiplication\". In Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), 2015. \n\nAn extended version of the paper is available on [arxiv](http://arxiv.org/pdf/1409.2908v1.pdf).\n\n\nLicense\n--------\nCopyright 2014 Sandia Corporation. Under the terms of Contract DE-AC04-94AL85000 with Sandia Corporation, the U.S. Government retains certain rights in this software.\n\nThis software is released under the BSD 2-Clause license. Please see the LICENSE file.\n\n\nSetup\n--------\nThe code requires:\n* Intel MKL\n* Compiler supporting C++11 and OpenMP\n\nThe Makefile depends on an included file that specifies the compiler and the run-time mode.\nYou must specify this file in the first line of the Makefile.\nFor an example, see the file `make.incs/make.inc.edison`, which contains the information for running\non NERSC's Edison machine.\nThe `MODE` variable specifies sequential or parallel mode.\nThe `DEFINES` variable can specify the type of parallelism if running in parallel mode.\nThe `DEFINES` variable also specifies the naming convention for BLAS routines.\nBy default, names are considered to be like \"dgemm\".\nHowever, by defining BLAS_POST, names are considered to be like \"dgemm_\", i.e., the routines have a trailing underscore.\nThe `MKL_ROOT` variable must be set for your machine.\n\nWe did most testing using the Intel compiler (icpc).\nDepending on the version of g++, the OpenMP task constructs can be different and the hybrid shared-memory\nparallel code may crash.  Sequential mode, DFS parallel, and BFS parallel should work with g++.\n\n\nBuilding examples\n--------\nFirst, use the code generator to generate the algorithms:\n          \n\t  cd codegen\n\t  bash gen_all_algorithms.sh 0\n\nSome simple codes that use the fast algorithms are in the `examples` directory.\nFor example, you can build and run the (4, 3, 3) algorithm:\n\n    make fast433\n    ./build/fast433\n\n\nBuilding tests\n--------\nWe now assume that all of the algorithms have been gernated with the code generator (see above).\nThe tests are built and run with:\n\n    make matmul_tests\n    ./build/matmul_tests -all 1\n\nThe tests are just for correctness of the algorithms, not for performance.\nYou should see output like:\n\n    STRASSEN_1: 257, 500, 55\n    Maximum relative difference: 3.87253e-15\n\nThis test runs one step of Strassen's algorithm, multiplying a 257 x 500 matrix with a 500 x 55 matrix.\nThe maximum relative difference is an error measure:\n\n    max_{ij} |C_{ij} - D_{ij}| / |C_{ij}|,\n\nwhere C is the result computed with the fast algorithm and D is the result computed with the classical algorithm.\nFor all of the exact fast algorithms, the error should be around 1e-14 or 1e-15.\nThe approximate algorithms (e.g., Bini's) have larger error.\nTypically, additional recursive steps leads to a larger error.\n\n\nBuilding with different parallel methods\n--------\nThe BFS, DFS, and HYBRID parallel algorithms are compile-time options.\nIn your make include file in the `make.incs` directory, to use DFS:\n\n    MODE=openmp\n    DEFINES += -D_PARALLEL_=1\n\nSwitch the `_PARALLEL_` define to 2 for BFS or 3 for HYBRID.\nFor an example, run\n\n    make fast424\n    ./build/fast424\n\n\nDGEMM curve benchmarks\n--------\nBuild and run the benchmark for the dgemm curves:\n      \n      make dgemm_curves\n      ./build/dgemm_curves 1  # N x N x N\n      ./build/dgemm_curves 2  # N x 800 x N\n      ./build/dgemm_curves 3  # N x 800 x 800\n\nThe output is a semi-colon separated list, where each item loooks like:\n\n    M K N num_trials time;\n\nThe M, K, and N terms specify the matrix dimensions: M x K multiplied by K x N.\nThe time is in milliseconds and is the total time to run num_trials multiplies.\nFor example,\n\n    1200 800 1200 5 104.87;\n\nmeans that it took 104.87 milliseconds to multiply a 1200 x 800 matrix by a 800 x 1200 matrix five times.\nTo build with parallelism enabled, you need to define the `_PARALLEL_` (see `make.incs/make.inc.linux`).\nTo run without dynamic threads (i.e., mkl_set_dynamic(0)), append a second argument, e.g.:\n\n   ./build/dgemm_curves 1 1  # Square timings without dynamic thread allocation\n\n\nFast algorithms benchmarks\n--------\nBuild benchmarking code for all of the fast algorithms:\n\n    make matmul_benchmarks\n\nThe build takes a while because we are compiling all of the algorithms.\nTo run a small test to benchmark MKL against Strassen with one, two, and three levels of recursion:\n\n    ./build/matmul_benchmarks -square_test 1\n\nThe output format is specified in `data/README.md`.\n\nTo run all of the benchmarks for the tall-and-skinny matrix multiplied by a small square matrix (N x k x k for fixed k):\n\n    ./build/matmul_benchmarks -ts_square_like 1\n", 
  "id": 25171089
}