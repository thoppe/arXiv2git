{
  "read_at": 1462549324, 
  "description": "", 
  "README.md": "# skip-thoughts\n\nSent2Vec encoder from the paper [Skip-Thought Vectors](http://arxiv.org/abs/1506.06726). The training code and data will be released at a later date.\n\n## Dependencies\n\nThis code is written in python. To use it you will need:\n\n* Python 2.7\n* Theano 0.7\n* A recent version of [NumPy](http://www.numpy.org/) and [SciPy](http://www.scipy.org/)\n* [scikit-learn](http://scikit-learn.org/stable/index.html)\n* [NLTK 3](http://www.nltk.org/)\n* [Keras](https://github.com/fchollet/keras) (for Semantic-Relatedness experiments only)\n\n## Getting started\n\nYou will first need to download the model files and word embeddings. The embedding files (utable and btable) are quite large (>2GB) so make sure there is enough space available. The encoder vocabulary can be found in dictionary.txt.\n\n    wget http://www.cs.toronto.edu/~rkiros/models/dictionary.txt\n    wget http://www.cs.toronto.edu/~rkiros/models/utable.npy\n    wget http://www.cs.toronto.edu/~rkiros/models/btable.npy\n    wget http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz\n    wget http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz.pkl\n    wget http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz\n    wget http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz.pkl\n\nNOTE to Toronto users: You should be able to run the code as is from any machine, without having to download.\n\nOnce these are downloaded, open skipthoughts.py and set the paths to the above files (path_to_models and path_to_tables). Now you are ready to go. Make sure to set the THEANO_FLAGS device if you want to use CPU or GPU.\n\nOpen up IPython and run the following:\n\n    import skipthoughts\n    model = skipthoughts.load_model()\n\nNow suppose you have a list of sentences X, where each entry is a string that you would like to encode. To get vectors, just run the following:\n\n    vectors = skipthoughts.encode(X)\n\nvectors is a numpy array with as many rows as the length of X, and each row is 4800 dimensional (combine-skip model, from the paper). The first 2400 dimensions is the uni-skip model, and the last 2400 is the bi-skip model. We highly recommend using the combine-skip vectors, as they are almost universally the best performing in the paper experiments.\n\nAs the vectors are being computed, it will print some numbers. The code works by extracting vectors in batches of sentences that have the same length - so the number corresponds to the current length being processed. If you want to turn this off, set verbose=False when calling encode.\n\nThe rest of the document will describe how to run the experiments from the paper. For these, create a folder called 'data' to store each of the datasets.\n\n## TREC Question-Type Classification\n\nDownload the dataset from http://cogcomp.cs.illinois.edu/Data/QA/QC/ (train_5500.label and TREC_10.label) and put these into the data directory. To obtain the test set result using the best chosen hyperparameter from CV, run the following:\n\n    import eval_trec\n    eval_trec.evaluate(model, evalcv=False, evaltest=True)\n\nThis should give you a result of 92.2%, as in the paper. Alternatively, you can set evalcv=True to do 10-fold cross-validation on the training set. It should find the same hyperparameter and report the same accuracy as above.\n\n## Image-Sentence Ranking\n\nThe file eval_rank.py is used for the COCO image-sentence ranking experiments. To use this, you need to prepare 3 lists: one each for training, development and testing. Each list should consist of 3 entries. The first entry is a list of sentences, the second entry is a numpy array of image features for the corresponding sentences (e.g. OxfordNet/VGG) and the third entry is a numpy array of skip-thought vectors for the corresponding sentences.\n\nTo train a model, open eval_rank.py and set the hyperparameters as desired in the trainer function. Then simply run:\n\n    import eval_rank\n    eval_rank.trainer(train, dev)\n\nwhere train and dev are the lists you created. The model will train for the maximum numbers of epochs specified and periodically compute ranks on the development set. If the ranks improve, it will save the model. After training is done, you can evaluate a saved model by calling the evaluate function:\n\n    eval_rank.evaluate(dev, saveto, evaluate=True)\n\nThis will load a saved model from the 'saveto' path and evaluate on the development set (alternatively, past the test list instead to evaluate on the test set).\n\nPre-computed COCO features will be made available at a later date, once I find a suitable place to host them. Note that this ranking code is generic, it can be applied with other tasks but you may need to modify the evaluation code accordingly.\n\n## Semantic-Relatedness\n\nDownload the SemEval 2014 Task 1 (SICK) dataset from http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools (training data, trial data and test data with annotations) and put these into the data directory. Then run the following:\n\n    import eval_sick\n    eval_sick.evaluate(model, evaltest=True)\n\nThis will train a model using the trial dataset to early stop on Pearson correlation. After stopping, it will evaluate the result on the test set. It should output the following results:\n\n    Test Pearson: 0.858463714763\n    Test Spearman: 0.791613731617\n    Test MSE: 0.26871638445\n\nFor this experiment, you will need to have Keras installed in order for it to work.\n\n## Paraphrase Detection\n\nDownload the Microsoft Research Paraphrase Corpus and put it in the data directory. There should be two files, msr_paraphrase_train.txt and msr_paraphrase_test.txt. To obtain the test set result using the best chosen hyperparameter from CV, run the following:\n\n    import eval_msrp\n    eval_msrp.evaluate(model, evalcv=False, evaltest=True, use_feats=True)\n\nThis will evaluate on the test set using the best chosen hyperparamter from CV. I get the following results:\n\n    Test accuracy: 0.75768115942\n    Test F1: 0.829526916803\n\nAlternatively, turning on evalcv will perform 10-fold CV on the training set, and should output the same result after.\n\n## Binary classification benchmarks\n\nThe file eval_classification.py is used for evaluation on the binary classification tasks (RT, CR, SUBJ and MPQA). You can download these datasets from http://nlp.stanford.edu/~sidaw/home/projects:nbsvm . If you want to use NB features, you will need to get nbsvm.py from https://github.com/mesnilgr/nbsvm . Included is a function for nested cross-validation, since it is standard practice to report 10-fold CV on these datasets. See the nested CV docstring for additional details on how to format the datasets and use the code.\n\n## A note about the EOS (End-of-Sentence) token\n\nBy default the EOS token is not used when encoding, even though it was used in training. We found that this results in slightly better performance across all tasks, assuming the sentences end with proper puctuation. If this is not the case, we highly recommend using the EOS token (which can be applied with use_eos=True in the encode function). For example, the semantic-relatedness sentences have been stripped of periods, so we used the EOS token in those experiments. If ever in doubt, consider it as an extra hyperparameter.\n\n## Reference\n\nIf you found this code useful, please cite the following paper:\n\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. **\"Skip-Thought Vectors.\"** *arXiv preprint arXiv:1506.06726 (2015).*\n\n    @article{kiros2015skip,\n      title={Skip-Thought Vectors},\n      author={Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},\n      journal={arXiv preprint arXiv:1506.06726},\n      year={2015}\n    }\n\n## License\n\n(TBD)\n", 
  "id": 38311460
}