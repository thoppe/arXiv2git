{
  "read_at": 1462548319, 
  "description": "", 
  "README.md": "# End-to-End Generative Dialogue\n\n A neural conversational model.\n\n----\n## Running Code\n\nBefore anything can be run, the MovieTriples dataset is first required. \n\nFirst create the data directory\n```\nmkdir data\n```\nand copy into the directory the MovieTriples dataset. \n\nYour directory should look like:\n```\n.\n+-- data\t     \n|   +-- MovieTriples\n|        +-- ...\n|        ...\n+-- src\n...\n```\n\nCode is run from the /src folder\n```\ncd End-To-End-Generative-Dialogue/src\n```\n#### Preprocessing Code\n\n```\npython preprocess.py\n```\nFor micro dataset\n```\npython preprocess.py --seqlength 5 # For micro dataset (~500 sentences)\n```\n#### Running the model\n```\nth train.lua -data_file data/conv-train.hdf5 -val_data_file data/conv-val.hdf5 -save_file conv-model -gpuid 1 #Runs on gpu\n\nth run_beam.lua -model conv-model.t7 -src_file data/dev_src_words.txt -targ_file data/dev_targ_words.txt -output_file pred.txt -src_dict data/src.dict -targ_dict data/targ.dict\n```\n\n### Running Code in Parallel\n\n#### Locally\n\nTo run a worker with 4 parallel clients on your own computer:\n```\nth train.lua -data_file data/conv-train.hdf5 -val_data_file data/conv-val.hdf5 -save_file conv-model -parallel -n_proc 4\n\n```\n#### Locally through localhost\n\nTo run a worker with 1 parallel client on your own computer running through localhost (which is more similar to how things will work when running through the google server). There is only 1 parallel client since it requires that you input your password while connecting to your own computer through ssh. I didn't want to deal with passwords so I just spawn one worker,input the password, and see if it works. There is no point to use this in practice since its just slightly more inefficient than the previous command. Use this as a benchmark for developing the remote server training. \n\nIn order for this to work, you must first **enable Remote Login in System Preferences/Sharing**\n\n**Note**: You have to specify the location of the src folder from the home directory of your computer:\n\ni.e. **PATH_TO_SRC = Desktop/GoogleDrive/FinalProject/Singularity/src/**\n```\nth train.lua -data_file data/conv-train.hdf5 -val_data_file data/conv-val.hdf5 -save_file conv-model -parallel -n_proc 1 -localhost -extension PATH_TO_SRC\n\n```\n#### DEV: Running remotely on gcloud servers\n\n##### Setup an ssh key to connect to our servers\n\nYou must first set up an ssh key to connect to the servers. \n\nReplace USERNAME with your own username.\n\ni.e. USERNAME = michaelfarrell\n\n```\nssh-keygen -t rsa -f ~/.ssh/gcloud-sshkey -C USERNAME\n```\nHit enter twice and a key should have been generated.\n\n```\ncat ~/.ssh/gcloud-sshkey.pub\n```\nAnd then copy the key that is printed out.\n\nNext you must add the key to the set of public keys. \n\n- Login to our google compute account. \n- Go to compute engine dashboard\n- Go to metdata tab\n- Go to ssh-key subtab\n- Click edit\n- Add the key you copied as a new line\n\nRestrict access:\n\n```\nchmod 400 ~/.ssh/gcloud-sshkey\n```\n\n##### Generate an instance group of machines if you have not yet done so\n\nNext create your own instance group if you have not created one already. \n\n- Go to the 'Instance groups' tab\n- Create instance group\n- Give the group a name, i.e. training-group-dev\n- Give a description\n- Set zone to us-central1-b\n- Use instance template\n- Choose \"mike-instance-template-1\"\n- Set the number of instances\n- Create\n- Wait for the instances to launch\n- Once there is a green checkmark, click on the new instance\n- You can connect to one of the servers by running\n\n```\nssh -o \"StrictHostKeyChecking no\" -i ~/.ssh/gcloud-sshkey USERNAME@IP_ADDR\n```\nwhere username is your username, i.e. michaelfarrell\n\nand IP_ADDR is the ip address of the machine listed under \"External ip\", i.e. 104.197.9.84 \n\nthe -o \"StrictHostKeyChecking no\" automatically adds the host to your list and does not prompt yes or no.\n\nIf you get an error like this:\n```\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n```\nthen you'll want to\n```\nvim ~/.ssh/known_hosts\n```\nand delete the last few lines that were added. They should look like some ip address and then something that starts with AAAA. You can delete lines in vim by typing dd to delete the current line.\n\nThis normally happens when you restart the servers and they change ip addresses or something.\n\n##### Running the remote server\n\nCurrently attempting to run with the parallel workers running remotely on the servers with the code below.\n```\nth train.lua -data_file data/conv-train.hdf5 -val_data_file data/conv-val.hdf5 -save_file conv-model -parallel -n_proc 4 -remote -extension End-To-End-Generative-Dialogue/src/\n\n```\n### Notes:\n\nNB: the MovieTriples dataset is not publicly available. Training on arbitrary dialogue will be supported soon.\n\n----\n## Primary contributors\n\n[Kevin Yang](https://github.com/kyang01)\n\n[Michael Farrell](https://github.com/michaelfarrell76)\n\n[Colton Gyulay](https://github.com/cgyulay)\n\n----\n## Relevant links\n\n- https://medium.com/chat-bots/the-complete-beginner-s-guide-to-chatbots-8280b7b906ca#.u1jngyhzc\n- https://www.youtube.com/watch?v=IK0t38Al4_E\n- https://github.com/julianser/hed-dlg\n- https://docs.google.com/document/d/1KKP8ZRZJbZweazZvz4cHZkvVnzFQApJJEySpZ5JLdwc/edit\n- http://arxiv.org/pdf/1507.04808.pdf\n- http://arxiv.org/pdf/1511.06931v6.pdf\n- https://www.aclweb.org/anthology/P/P15/P15-1152.pdf\n- http://arxiv.org/pdf/1603.09457v1.pdf\n- https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/\n- https://www.reddit.com/r/MachineLearning/comments/3ukvc6/datasets_of_one_to_one_conversations/\n- http://arxiv.org/pdf/1412.3555v1.pdf\n- https://github.com/clementfarabet/lua---parallel\n- http://www.aclweb.org/anthology/P02-1040.pdf\n- http://victor.chahuneau.fr/notes/2012/07/03/kenlm.html\n- https://cloud.google.com/compute/docs/troubleshooting\n\n----\n## TODO\n\n#### Preprocessing (preprocess.py)\n\n- Add subTle datset cleaning to preprocessing code (and any other additional datasets we may need)\n- Modify preprocessing code to have longer sequences (rather than just (U_1, U_2, U_3), have (U_1, ..., U_n) for some n. With this we could try to add more memory to the model we currently have now)\n- Modify preprocessing code to return entire conversations (rather than fixing n, have the entire back and forth of a conversation together. This could be useful for trying to train a model more specific to our objective. This could be used for testing how the model does for a specific conversation )\n- Finish cleaning up file (i.e. finish factoring code. I started this but things are going to be modified when subTle is added so I never finished. It shouldn't be bad at all)\n\n#### Parallel (parallel_functions.lua)\n- Add way to do localhost without password on server\n- Get working on google servers\n- Make sure server setup is correctly done\n\n#### General \n\n- Start result collection of some sort. Maybe have some datasheet and when we run a good model we record the results?\n- run each of the models for 10 epochs-ish? -> save the model, record results ^\n- implement RNN model\n- experiment with HRED model\n- heirarchical model \n- add in error rate stuff when reporting\n\n----\n## Acknowledgments\n\nOur implementation utilizes code from the following:\n\n* [Yoon Kim's seq2seq-attn repo](https://github.com/harvardnlp/seq2seq-attn)\n* [Element rnn library](https://github.com/Element-Research/rnn)\n* [Facebook's neural attention model](https://github.com/facebook/NAMAS)\n", 
  "id": 56800398
}