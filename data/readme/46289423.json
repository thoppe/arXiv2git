{
  "read_at": 1462554159, 
  "description": "Weight initialisation schemes for Torch7 neural network modules", 
  "README.md": "# nninit\n\nParameter initialisation schemes for Torch7 neural network modules. Works with `nn`, and therefore `nngraph`. Allows arbitrary indexing of weights/biases/parameters. Supported modules:\n\n- nn.Linear / nn.LinearNoBias\n- nn.LookupTable\n- nn.TemporalConvolution\n- nn.SpatialConvolution / cudnn.SpatialConvolution\n- nn.VolumetricConvolution / cudnn.VolumetricConvolution\n\nReadme contents:\n\n- [Installation](#installation)\n- [Usage](#usage)\n- [Example](#example)\n- [Development](#development)\n- [Acknowledgements](#acknowledgements)\n\n## Installation\n\n```sh\nluarocks install nninit\n```\n\n## Usage\n\n**`nninit`** adds an `init` method to `nn.Module`, with the following API:\n\n```lua\nmodule:init(accessor, initialiser, ...)\n```\n\nThe [`accessor`](#accessors) argument is used to extract the tensor to be initialised from the module. The [`initialiser`](#initialisers) argument is a function that takes the module, tensor, and further options; it adjusts the tensor and returns the module, allowing `init` calls to be chained. `nninit` comes with several initialiser functions. `...` represents additional arguments for the initialiser function.\n\n### Accessors\n\nThe `accessor` argument is used to extract the tensor to be initialised from the module. It can either be a string, table, or function. \n\n#### string\n\nThe tensor is accessed as a property of the module. For example:\n\n```lua\nmodule:init('weight', nninit.constant, 1)\n```\n\n#### table\n\nThe tensor is first accessed as a property of the module from the first element, and a subtensor is then extracted using Torch's [indexing operator](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor--dim1dim2--or--dim1sdim1e-dim2sdim2e-) applied to the second element. For example:\n\n```lua\nmodule:init({'weight', {{1, 5}, {}}}, nninit.uniform, -1, 1)\n```\n\n#### function\n\nThe tensor must be returned as the result of the function applied to the module. For example:\n\n```lua\nmodule:init(function(m) return m.weight:narrow(1, 1, 10) end, nninit.normal, 0, 0.01)\n```\n\n### Initialisers\n\n#### nninit.copy(module, tensor, init)\nCopies the `init` tensor to the tensor to be initialised.\n\n#### nninit.constant(module, tensor, val)\nFills tensor with the constant `val`.\n\n#### nninit.addConstant(module, tensor, val)\nAdds to current tensor with the constant `val`.\n\n#### nninit.mulConstant(module, tensor, val)\nMultiplies current tensor by the constant `val`.\n\n#### nninit.normal(module, tensor, mean, stdv)\nFills tensor ~ N(`mean`, `stdv`).\n\n#### nninit.addNormal(module, tensor, mean, stdv)\nAdds to current tensor with ~ N(`mean`, `stdv`).\n\n#### nninit.uniform(module, tensor, a, b)\nFills tensor ~ U(`a`, `b`).\n\n#### nninit.addUniform(module, tensor, a, b)\nAdds to current tensor with ~ U(`a`, `b`).\n\n#### nninit.eye(module, tensor)\n**Only supports the module weights as the tensor. Relies on the module type to determine appropriate identity.**  \nFills weights with the identity matrix (for linear layers/lookup tables).  \nFills filters with the Dirac delta function (for convolutional layers). Normalises by the number of input layers.\n\n#### nninit.xavier(module, tensor, [{[dist], [gain]}])\nFills tensor with `stdv = gain * sqrt(2 / (fanIn + fanOut))`. Uses the uniform distribution by default.  \nOptional named parameters [`dist`](#dists) and [`gain`](#gains) can be passed in via a table.  \nAlso known as Glorot initialisation.\n\n> Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In *International Conference on Artificial Intelligence and Statistics*.\n\n#### nninit.kaiming(module, tensor, [{[dist], [gain]}])\nFills tensor with `stdv = gain * sqrt(1 / fanIn)`. Uses the normal distribution by default.  \nOptional named parameters [`dist`](#dists) and [`gain`](#gains) can be passed in via a table.  \nAlso known as He initialisation.\n\n> He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. *arXiv preprint arXiv:1502.01852*.\n\n#### nninit.orthogonal(module, tensor, [{[gain]}])\n**Only supports tensors with at least 2 dimensions.**  \nFills tensor with a (normally distributed) random orthogonal matrix.  \nOptional named parameter [`gain`](#gains) can be passed in via a table.\n\n> Saxe, A. M., McClelland, J. L., & Ganguli, S. (2013). Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. *arXiv preprint arXiv:1312.6120*.\n\n#### nninit.sparse(module, tensor, sparsity)\nSets `(1 - sparsity)` percent of the tensor to 0, where `sparsity` is between 0 and 1. For example, a `sparsity` of 0.2 drops out 80% of the tensor.\n\n> Martens, J. (2010). Deep learning via Hessian-free optimization. In *Proceedings of the 27th International Conference on Machine Learning (ICML-10)*.\n\n### Dists\n\nThe 2 types of distribution supported are `'normal'` and `'uniform'`.\n\n### Gains\n\nGains can be calculated depending on the succeeding nonlinearity. If `gain` is a number it is used directly; if `gain` is a string the following mapping is used. By default gains (where applicable) are set to 1.\n\n| Gain      | Parameters | Mapping                     |\n|-----------|------------|-----------------------------|\n| 'linear'  |            | 1                           |\n| 'sigmoid' |            | 1                           |\n| 'relu'    |            | sqrt(2)                     |\n| 'lrelu'   | leakiness  | sqrt(2 / (1 + leakiness^2)) |\n\nIf the `gain` must be calculated from additional parameters, `gain` must be passed as table with the string as the first element as well as named parameters. For example:\n\n```lua\nmodule:init('weight', nninit.kaiming, {gain = {'lrelu', leakiness = 0.3}})\n```\n\n## Example\n\n```lua\nlocal nn = require 'nn'\nrequire 'cunn'\nlocal cudnn = require 'cudnn'\nrequire 'rnn'\nlocal nninit = require 'nninit'\n\nlocal getBias = function(module)\n  return module.bias\nend\n\nlocal batchSize = 5\nlocal imgSize = 16\nlocal nChannels = 3\nlocal nFilters = 8\nlocal rho = 6\nlocal hiddenSize = 2\n\nlocal cnn = nn.Sequential()\ncnn:add(cudnn.SpatialConvolution(nChannels, nFilters, 2, 2):init('weight', nninit.eye)\n                                                           :init('weight', nninit.mulConstant, 1/2)\n                                                           :init('weight', nninit.addNormal, 0, 0.01)\n                                                           :init(getBias, nninit.constant, 0))\ncnn:add(nn.View(nFilters*15*15))\ncnn:add(nn.Linear(nFilters*15*15, nFilters):init('weight', nninit.kaiming, {\n  dist = 'uniform',\n  gain = {'lrelu', leakiness = 0.3}\n}))\ncnn:add(nn.RReLU(1/3, 1/3))\ncnn:add(nn.Linear(nFilters, 6):init('weight', nninit.orthogonal, {gain = 'relu'}))\ncnn:add(cudnn.ReLU())\ncnn:add(nn.Linear(6, 4):init('weight', nninit.xavier, {dist = 'normal', gain = 1.1}))\ncnn:add(nn.Linear(4, hiddenSize):init('weight', nninit.sparse, 0.2)\n                                :init(getBias, nninit.constant, 0))\n\nlocal model = nn.Sequential()\nmodel:add(nn.Sequencer(cnn))\nlocal lstm = nn.FastLSTM(hiddenSize, hiddenSize, rho)\n-- Note that chaining will pass through the module initialised, never parents\nlstm.i2g:init({'bias', {{3*hiddenSize+1, 4*hiddenSize}}}, nninit.constant, 1) -- High forget gate bias\nmodel:add(nn.Sequencer(lstm))\nmodel:cuda()\n\nlocal inputs = {}\nfor i = 1, rho do\n  table.insert(inputs, torch.ones(batchSize, nChannels, imgSize, imgSize):cuda())\nend\nprint(model:forward(inputs))\n```\n\n## Development\n\nTo develop **nninit**/use it to test new initialisation schemes, `git clone`/download this repo and use `luarocks make rocks/nninit-scm-1.rockspec` to install **nninit** locally.\n\n## Acknowledgements\n\n- [Lasagne](https://github.com/Lasagne/Lasagne)\n- [Purdue e-Lab Torch Toolbox](https://github.com/e-lab/torch-toolbox)\n", 
  "id": 46289423
}