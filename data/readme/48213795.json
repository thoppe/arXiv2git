{
  "read_at": 1462550356, 
  "description": "Entropy estimator", 
  "README.md": "#Tutorial\n\n##Compile and run\nCheck out all source code, including the Makefile and .txt files.\n\nType ```make``` to compile the sources, you will get executable file *\"entropy\"*.\n\nType ```./entropy -k=100000 -fin=fin_sample.txt``` or ```./entropy -k=100000 -hist=hist_sample.txt``` to experiment on the fingerprint \"fin\\_sample.txt\" or histogram \"hist\\_sample.txt\" respectively, which are both the statistic of 3,000 samples generated by the uniform distribution over 100,000 symbols. \n\n### Program arguments\n\n#### Main arguments:\n\n* ```-k=number```: set alphabet size. \n* ```-fin=filename```: set fingerprint input file. \n  Each line of file consists of two numbers: frequency *j*, the number of symbols that appears exactly *j* times.\n* ```-hist=filename```: set histogram input file. \n  Each line of file consists of only one number: frequency *j*. Symbols are not needed.\n* *k* must be provided. \n  Either *fin* or *hist* must be provided.\n  If both *fin* and *hist* are provided, only *fin* will be read.\n  \n#### Optional arguments:\n\n* ```-L=number```: set polynomial degree. Default *L=1.6 log k*.\n* ```-M=number```: set the right endpoint of approximation interval. Default *M=3.5 log k*.\n* ```-N=number```: set the threshold to apply the polynomial estimator. Default *N=1.6 log k*.\n* The parameters above can be combined, e.g., ```./entropy -k=100000 -fin=fin_sample.txt -L=18 -M=40 -N=18```.\n* Type ```./entropy -help``` or ```./entropy -h``` to see the list of arguments.\n\n\n\n## More for developers\nFor developer who want to write a new test scratch to test the entropy estimator, follow the example *main_test.cpp*.\nAfter ```make``` you will also see the executable file *\"test\"*.\n\n### Entropy estimator class: *Entropy*\nWork flow: \n\n1. Set alphabet size *k* and several parameters: \n  * **setDegree(int L)**: the degree of polynomial is L.\n  * **setInterval(int M)**: the approximation interval is [0,M/n], where n is the sample size.\n  * **setThreshold(int N)**: the threshold to use polynomial estimator is when the histogram is at most N.\n2. Input fingerprint or histogram. Use one of the following: \n  * **setFin( filename )**: each line of file consists of two numbers: frequency *j*, the number of symbols that appears exactly *j* times.\n  * **setFin( freq, count )**: input two vectors of the same length: *freq[i]* represents frequency, and *count[i]* counts the number of symbols that appear exactly *freq[i]* times. \n  * **setHist( filename )**: each line of file consists of one number: frequency *j*. Symbols are not needed.\n  * **setHist( freq )**: input one vector with frequencies.\n3. Output various estimates: \n  * **estimate()**: our polynomial estimator\n  * **estimate_plug()**: plug-in estimator\n  * **estimate\\_Miller\\_Madow()**: Miller-Madow estimator\n\n\n\n### Synthetic data experiments\nFor those who want to generate synthetic samples within C++, you can refer to the standard [random number generator facilities](http://www.cplusplus.com/reference/random/).\nThere are examples to generate random numbers according to different types of distributions.\nIf the distribution is not yet provided, you can use the general [discrete distribution](http://www.cplusplus.com/reference/random/discrete\\_distribution/).\nThere are several ways to [construct discrete distributions](http://www.cplusplus.com/reference/random/discrete\\_distribution/discrete\\_distribution/).\n\n\n## Reference\nFor detailed explanation of parameters, please refer to our paper [Minimax rates of entropy estimation on large alphabets via best polynomial approximation, arXiv:1407.0381](http://arxiv.org/abs/1407.0381).\nThe parameters described in the paper are: *L=c<sub>0</sub> log k, M=c<sub>1</sub> log k, N=c<sub>2</sub> log k*.\nAs in the paper, the default values are *L=1.6 log k, M=3.5 log k, N=1.6 log k*.\n", 
  "id": 48213795
}