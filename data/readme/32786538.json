{
  "read_at": 1462544830, 
  "description": "hw1", 
  "README.txt": "# hw1\n\nUsage\nTo prepare dataset\n>> THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python read_data.py\nTo train\n>> THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python train_nn.py\nTo generate predict result\n>> THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python predict_single.py\nTo blend models and predict result\n>> THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python predict_blending.py\n\n(choose CPU or GPU base on your device)\n\n\n========Version discription====================================== \n\nTable\nVersion     Validate accuracy        Submmit accuracy     \nzero        0.586875                 0.622            (MFCC,2 layer) \n0.01        0.595                    0.626            (MFCC,2 layer,ReLU) \n0.01-1      0.604                    X                (FBANK,3 layer,ReLU) \n0.01-2      0.614 (@2000 epoch)      X                (FBANK,4 layer,ReLU)\n0.02        0.602 (@275  epoch)      X                (FBANK,2 layer,PReLU)\n0.02-1      0.622 (@2000 epoch)      X                (FBANK,4 layer,PReLU)\n0.03        0.628 (@2000 epoch)      0.63684          (as 0.02-1,Momentum)\n0.03b\t    0.671 (@2000 epoch)      0.65078          (as 0.03,39 Phonemes)\n0.03c1      0.682 (@1000 epoch)      X                (as 0.03b,4L wider node)\n0.03c2      0.690 (@279 epoch)       0.65481          (as 0.03b,5L)    \n0.03d       0.692 (@378 epoch)       0.66202          (as 0.03c2,L2Regression)\n0.03d1      0.700 (@511 epoch)       ?                (as 0.03d,7L)\n0.03e       0.692 (@328 epoch)       0.66081          (as 0.03e?,FBANK+MFCC)\n0.03e1      0.706 (@551 epoch)       0.62023          (as 0.03e,7L)   \n0.03g       ?                        ?                (as 0.03e1,LR decay)\n0.04        0.75 (train accuracy)    0.67867          (8 model blending, all 5L FBANK+MFCC)\n0.04a       ?                        ?                (major revision,DropOut)\n0.04e       0.783 (@744 epoch)       0.72683          (combine 3 frames)\n\nZero edition: create by Jan \n\tit takes about 8 mins on GTX760 for 200 epoch \n\tprediction_3.csv on kaggle \n\taccuracy 0.622 \n\nVer 0.01: modify by PHHung \n\tit takes about 50 mins on GTX760 for 200 epoch \n\tprediction_4.csv on kaggle\n\taccuracy 0.626 \n\nVer 0.02: modify by HYTseng\n\tI train it on CPU XD. \nI would update the timing and accuracy when I get the GPU computation power.\n        Major modify: PReLU: http://arxiv.org/pdf/1502.01852v1.pdf\n\nVer 0.03: modify by PHHung\n        it takes about 3 hr on GTX760 for 2000 epoch\n        Major modify: momentum\n\taccuracy 0.636 on kaggle\n\nVer 0.03a: Modified by Jan\n        Built our own softmax function but left theano's in place as it's faster.\n\nVer 0.03b: Modified by Jan\n\tPrediction changed to 39 phonemes, should increase performance  \n\taccuracy 0.650 on kaggle\n\nVer 0.03c: Modified by PHHung\n\t1.save model as \"model_best.save\" when there is a better model\n\t2.separate training & prediction to two different file \n\t=>you can terminate training process whenever you like, \n          and then go for prediction\n\t3.deeper & wider model\n    \t    \nVer 0.03d: Modified by PHHung\n\tadd L2 regression in cost function,hope to prevent overfiting\n\t0.03d1: 7layer in/128/256/512/1024/512/256/128/out \n\nVer 0.03e: by HYTseng\n\t(need modify both train_nn.py and predict.py)\n\tinput with FBANK+MFCC, still overfitting, time to think about drop out?\n\nVer 0.03f: Modified by PHHung\n\t//add DropOut : http://arxiv.org/pdf/1207.0580.pdf\n\nVer 0.03g: Jan / fix for the learning rate decay\n\nVer 0.03h: Jan / learning rate update now epoch-wise\n\nVer 0.04: by HYTseng\n\tmodel blending\n\nVer 0.04a: by PHHung\n\tMajor revision\n\t1.add DropOut\n\t2.Separate layer's detail from train_nn to Layer_Buffet\n\t  (define your model more easily)\n\t3.delete predict.py add predict_single.py\n\nVer 0.04b: Jan / training set subselection has its own random number generator\n\nVer 0.04c: Jan / permutation of training data\n\nVer 0.04d: Jan / added bagging function to draw random subsets from the trainig data\n\nVer 0.04e: HY  / combine 3 frames in one, FBANK feature -> 69*3 = 207-d feature\n                 batch_size 512, LR_D = 0.99995, 5 hidden_layers = [256 512 256 128 64]\n                 first layer drop out with p = 0.1\n                 train with bagging\nVer 0.04g: Jan\n        Make sure that a sentence is either in the validation or in the training set\n\nVer 0.04h: Jan\n        Bootstrap sample aggregating now fills the bootstrap samples up by oversampling\n\nVer 0.04i: Jan\n        Disabled default bootstrap oversampling to save memory\n\nVer. 0.04j by Jan\n        Set temporary data to None to save memory\n==================================================================\n\nToDo:\n>bagging\n>matplot function\n>batch normalization?\n>Use MFCC and FBANK together?\n\nDone:\n>model average (uniform)\n>momentum gd\n>softmax\n>learning rate decay\n>DropOut\n", 
  "id": 32786538
}