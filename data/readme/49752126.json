{
  "read_at": 1462552320, 
  "description": "Gated Recurrent Unit with Low-rank matrix factorization", 
  "README.md": "# lowrank-gru\nGated Recurrent Unit with Low-rank matrix factorization\n\nPaper: http://arxiv.org/abs/1603.03116\n\nA reparametrization of the Gated Recurrent Unit (Cho et al. 2014, http://arxiv.org/abs/1406.1078) where the recurrent matrices are constrained to be low-rank. Reduces data complexity and memory usage.\n\nThis code is based on the code for the Unitary Evolution Recurrent Neural Networks (Arjovsky et al. 2015 http://arxiv.org/abs/1511.06464) in order to facilitate a direct comparison. Original repository is https://github.com/amarshah/complex_RNN .\n\nOur model performs comparably or better than Unitary Evolution Recurrent Neural Networks on all the tasks we tested (memory, addition and randomly-permuted sequential MNIST) for similar number of parameters.\n\nNotes:\n\nRequires Theano: http://www.deeplearning.net/software/theano/\n\nFile fftconv.py is derived from Theano and is therefore under Theano licence. This file is needed only for the baseline uRNN model and not for out Low-rank GRU and Low-rank plus diagonal GRU models.\n\n", 
  "id": 49752126
}