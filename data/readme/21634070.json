{
  "README": "               ------------------------------------------------\n                  xFitter   --- PDF fit program from HERA.\n               ------------------------------------------------\n\nxFitter is an open source QCD fit framework desinged to extract PDFs \nand assess the impact of new data. The xFitter project is is a \ncommon initiative by the H1 and ZEUS collaborations and extended \nto the LHC collaborations to provide precision QCD analyses. \nxFitter has been used as one of the main software packages for the \ndetermination of the HERA proton parton densities (PDFs), HERAPDFs. \nxFitter has been used to produce the ATLAS-epWZ12 (NNLO, available \nin LHAPDF5.9.1 and LHAPDFv6.1.X), LHeC (NLO) PDF sets. For further \ndetails please check xfitter.org web page.\n\n\nThe current package includes code to fit DIS inclusive cross section \ndata, Drell-Yan, jet and ttbar processes (using APPLGRID and FastNLO\ninterfaces). The program is distributed under the GPL v3 license, see\nLICENCE file for more details. The program uses the QCD evolution \npackage QCDNUM developed by M. Botje and includes other parts of the code:\n-- VFNS from R. Thorne, G. Watt (MSTW) @ LO, NLO, NNLO\n-- VFNS from F. Olness (ACOT) @ LO, NLO and NNLO, NNNLO corrections for FL \n-- VFNS from APFEL (FONLL) @ LO, NLO and NNLO\n-- FFNS from S. Alekhin (ABM) @ NLO, NNLO (pole and running heavy quark masses)\n-- DY LO+k-factor calculation from A. Sapronov\n-- PDF error estimation from J. Pumplin\n-- DIS electroweak corrections from H. Spiesberger with Jegerlehner's \n   hadronic parametric contribution (based on e+,e- data)\n-- Bayesian reweighting tool from A. Guffanti (a la NNPDF) and based on\n   EIGENVECTORS from G. Watt (a la MSTW).\n-- DIPOLE models (GBW, IIM, BGS)\n-- TDM (uPDFs) as an alternative to DGLAP formalism (J. Jung)\n-- Diffractive PDFs (W. Slominski)\n-- total ttbar production cross sections via HATHOR (S. Moch et al.)\n-- differential ttbar production cross sections with DiffTop (M. Guzzi, S. Moch et al.)\n-- MNR calculation for heavy quark production (Mangano, Nason and Ridolfi, \n   implemented by O.Zenaiev)\n\nIf the results obtained with the program are to be included in a scientific \npublication, please use the citations as suggested by the REFERENCES file. \n\nFor support information, please visit https://wiki-zeuthen.desy.de/xFitter/xFitter\n\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n1) Installation and Usage Instructions: please refer to the INSTALLATION file.\n=====================\n\n2) BRIEF DESCRIPTION\n=====================\n  a) Steering cards\n--------------------\n    The software behaviour is controlled by two files with steering commands.\n    These files have predefined names:\n\n      steering.txt   --  controls main \"stable\" (un-modified during \n                         minimisation) parameters. The file also contains\n                         names of data files to be fitted to, definition \n                         of kinematic cuts                        \n                        \n      minuit.in.txt  --  controls minimisation parameters and minimisation \n                         strategy. Standard Minuit commands can be provided\n                         in this file\n\n      ewparam.txt    --  controls electroweak parameters.\n\n\n  b) Inclusion of data files\n-------------------------------\n    Inclusion of the data files is controlled by &InFiles namelist in the \n    steering.txt file. For example, by default the following four HERA-I\n    files are included:\n\n&InFiles\n    NInputFiles = 4\n    InputFileNames(1) = 'datafiles/H1ZEUS_NC_e-p_HERA1.0.dat'\n    InputFileNames(2) = 'datafiles/H1ZEUS_NC_e+p_HERA1.0.dat'\n    InputFileNames(3) = 'datafiles/H1ZEUS_CC_e-p_HERA1.0.dat'\n    InputFileNames(4) = 'datafiles/H1ZEUS_CC_e+p_HERA1.0.dat'\n&End\n\n    To include more files:\n       -- Increase NInputFiles\n       -- Specify  InputFileNames()\n\nanother option would be:\n\n    NInputFiles = 4\n    InputFileNames =\n 'datafiles/hera/H1ZEUS_NC_e-p_HERA1.0.corr.dat'\n 'datafiles/hera/H1ZEUS_NC_e+p_HERA1.0.corr.dat'\n 'datafiles/hera/H1ZEUS_CC_e-p_HERA1.0.corr.dat'\n 'datafiles/hera/H1ZEUS_CC_e+p_HERA1.0.corr.dat'\n\n-> then the order does matter of the files listed.\n\n   Inclusion of the statistical or systematic correlations of the data \n   in the fit is done via &InCorr namelist:\n\n&InCorr\n  ! Number of correlation (statistical, systematical or full) files\n    NCorrFiles = 1\n    CorrFileNames(1) = 'datafiles/hera/H1_NormInclJets_HighQ2_99-07___H1_NormInclJets_HighQ2_99-07.corr'\n&End\n   in this case the statistical correlations for H1_NormInclJets_HighQ2_99-07 data file are included\n   (the method also allows to include correlations between data sets via file names, i.e:\n    H1_NormInclJets_HighQ2_99-07___H1_InclJets_HighQ2_99-00.dat.corr)\n\n\n   As additional option for data sets with covariance matrix, it is possible\n   to convert covariance matrix to nuisance parameter representation\n   (following the prescription suggested by J. Gao and P. Nadolsky  in arXiv:1401.0013):\n\n&CovarToNuisance\n   ! Global switch for using nuisance param representation for covariance mat.\n  LConvertCovToNui = .true.\n\n   ! Tolerance -- zero means exact transformation\n  Tolerance = 0.0\n\n   ! The following lines allow to adjust error scaling properties \n   ! (default: :M - multiplicative, A - additive)\n  DataName     = 'CMS electon Asymmetry rapidity', 'CMS W muon asymmetry'\n  DataSystType = ':A', ':A'\n&End\n\n  c) Data files format \n--------------------------\n\n   Experimental data are provided by the standard ASCII text files. The files\n   contain a \"header\" which describes the data format and the \"data\" in terms\n   of a 2-dimensional table. Each line of the data table corresponds to a\n   data point, the meaning of the columns is specified in the file header.\n\n   For example, a header for HERA-I combined H1-ZEUS data for e+p neutral \n   current scattering cross section is given in the file\n\n       datafiles/H1ZEUS_NC_e-p_HERA1.0.dat\n\n   The format of the file follows standard \"namelist\" conventions. Comments \n   start with exclamation mark.  Pre-defined variables are:\n\n      Name        --- (string) provides a name of the data set\n\n      Reaction    --- (string) reaction type of the data set. Reaction type is used \n                      to trigger corresponding theory calculation. The following \n                      reaction types  are currently supported by the xFitter:\n\n                        'NC e+-p'  -- double differential NC ep scattering\n                        'CC e+-p'  -- double differential CC ep scattering\n                        'NC e+-p charm' -- charm production in the NC ep scattering\n\n                        'CC pp'    -- single differential d sigma (W^{+,-})/d eta\n                                      production and W asymmetry at pp and ppbar \n                                      colliders (LO+kfactors and APPLGRID interface)\n                        'NC pp'    -- single differential d sigma Z/d y_Z at pp and\n                                      ppbar colliders\n                                      (LO+kfactors and APPLGRID interface)\n\n                        'pp jets APPLGRID' -- pp->inclusive jet production, using\n                                     APPLGRID \n\n                        'FastNLO ep jets' -- ep jets calculated with the help of\n                                     fastnlo v.2.0 table\n                        'FastNLO ep jets normalised' -- fastnlo ep jets normalised\n                                     to inclusive DIS cross section\n\n                        'muon p'   -- proton structure function in the muon-proton \n                                      DIS scattering \n\t\t\t'DUMMY'    -- Dummy reaction type to be used for testing the \n\t\t\t\t      data format. In this case the central values of data\n\t\t\t\t      are ignored and theory predictions are used, chi2 will be zero.\n\n\n      NData       --- (integer) specifies number of data points in the file. \n                     This corresponds to the number of table rows which \n                     follow after the header.\n\n      NColumn     --- (integer) number of columns in the data table.\n\n      ColumnType  --- (array of strings)\n                      Defines layout of the data table. The following column types\n\t\t      are pre-defined: 'Flag', 'Bin', 'Sigma', 'Error' and\n\t\t      'Dummy' The keywords are case sensitive. 'Flag' cortrols\n\t\t      the treatment of specific bin (0/1 - exclude/include the\n\t\t      bin in the fit, 1 by default), 'Bin' correspond to an\n\t\t      abstract bin definition, 'Sigma' corresponds to the data\n\t\t      measurement, 'Error' - to various type of uncertainties\n\t\t      and 'Dummy' indicates that the column should be ignored.\n\n      ColumnName  --- (array of strings)\n                      Defines names of the columns. The meaning of the name depends\n\t\t      on the ColumnType. For ColumnType 'Flag' it is 'binFlag',\n\t\t      For ColumnType 'Bin', ColumnName gives a name of the\n\t\t      abstract bin. The abstract bins can contain any variable\n\t\t      names, but some of them must be present for correct cross\n\t\t      section calculation. For example, 'x', 'Q2' and 'y' are\n\t\t      required for DIS NC cross-section calculation.\n \n                      For ColumnType 'Sigma', ColumnName provides a label for \n                      the observable, which can be any string.\n \n                      For ColumnType 'Error', the following names have special meaning:\n                       'stat'  -- specifies column with statistical uncertainties\n                       'uncor' -- specifies column with uncorrelated uncertainties  \n                       'total' -- specifies column with total uncertainties. \n                                  Total uncertainties are not used in the fit,\n                                  however there is an additional check is performed\n                                  if 'total' column is specified: sum in quadrature\n                                  of statistical, uncorrelated and correlated \n                                  systematic uncertainties is compared to the total\n                                  and a warning is issued if they differ significantly.\n                       'ignore' - specifies column to be ignored (for special studies).\n                      Other names specifies columns of correlated systematic \n                      uncertainty. For a given data file, each column of the correlated\n                      uncertainty must have unique name. To specify correlation across\n                      data files, same name must be used for different files.  \n                       \n      SystScales  --- (array of float)\n                      For special studies, systematic uncertainties can be scaled\n                      The numbering of uncertainties starts from the first column\n                      with the ColumnType 'Error'. For example, setting \n\n                           SystScale(1) = 2.  \n\n                      in datafiles/H1ZEUS_NC_e-p_HERA1.0.dat would scale stat. \n                      uncertainty by factor of two.\n                       \n\n      Percent     --- (array of bool) For each uncertainty specify if it is given in \n                      absolute (\"false\") or in percent (\"true\").  The numbering of \n                      uncertainties starts from the first column with the \n                      ColumnType 'Error' (see example above).\n\n      NInfo       --- (integer) Calculation of the cross-section predictions may \n                      require  additional information about the data set. The number of \n                      information strings is given by NInfo\n\n      CInfo       --- (array of strings) Names of the information strings. \n                      Several of them are predefined for different cross-section \n                      calculations.\n \n      DataInfo    --- (array of float) Values, corresponding to CInfo names.\n\n      IndexDataset -- (integer) Internal H1 Fitter index of the data set. Provide unique\n                      numbers to get extra info for chi2/dof for each data set.\n      \t              To index new data sets please refer to the table available in www.xfitter.org\n\t\n      TheoryInfoFile --- (string) Optional additional theory file with extra \n                     information for cross-section calculation. This could be k-factors,\n                     APPLGRID file or FastNLO table.\n  \n      TheoryType --- (string) Theory file type: 'kfactor', 'applgrid', 'FastNLO' or \n\t\t     'expression'. The last one gives more flexibility in\n\t\t     theory definition, allowing to set a simple formula in\n\t\t     'TheorExpr' string variable with preliminary defined\n\t\t     terms in 'TermName', 'TermType' (can be 'kfactor',\n\t\t     'applgrid' or 'virtgrid') and 'TermSource' (the files from\n\t\t     where the predictions are taken). TermInfo can be a\n\t\t     special option string for fast cross section evaluation.\n\t\t     See what options are supported for each theory type. The\n\t\t     expression recognises simple arithmetic operations\n\t\t     (+,-,/,*) and 'sum()' function, returning predictions\n\t\t     summed over bins. Example:\n                     --------------------------\n                      TheoryType     = 'expression'\n                      TermName = 'A1', 'K' \n                      TermType = 'applgrid','kfactor'\n                      TermInfo = '',''\n                      TermSource = 'path/to/grid.root' , \n                                   'path/to/kfactor.txt'\n                      TheorExpr= 'K*A1/sum(A1)'\n\t\t     --------------------------\n\t\t     The expression also recognises numerical terms, e.g.\n\t\t     'k*A+0.1' (due to technical limitations, no spaces \n\t\t     are allowed in 'TheorExpr' value). By default the numeric result \n\t\t     of the expression is divided by the bin width. In order\n\t\t     to obtain initial values or use 'sum()' operation (integral\n\t\t     of the differential distribution e.g. for normalization purposes) \n\t\t     one should add '_norm' suffix to the the TermType of 'applgrid' and\n\t\t     'virtgrid'. For more information on the 'virtgrid' definition\n\t\t     please see program's manual.\n      \n      NKFactor   --- (integer) For kfactor files, number of columns in\n                     TheoryInfoFile\n\n      KFactorNames - (array of strings) For kfactor files, names of columns in \n                     TheoryInfoFile\n\n      PlotDesc   --- contains options for drawing tools, i.e.:\n            PlotN         - number of plots for data set(s), i.e. SubPlots\n            PlotDefColumn - data variable used to divide the data in SubPlots\n            PlotVarColumn - variable providing bin center information (to be used only if bin edges are missing)\n            PlotDefValue  - ranges of PlotDefColumn used to divide the data in SubPlots\n            PlotOptions(N)- additinal information displayed on the plots like experiment, process, axes titles, example: \n            PlotOptions(1)='Experiment:H1 ZEUS@ExtraLabel:e^{-}p CC @XTitle: x @YTitle: d#sigma/dx @Title:Q^{2} = 300  @Xlog@Ylog'\n\n\n  c.a) FastNLO specific data format\n-----------------------------------\n\n  In this subsection we describe data format specific for FastNLO implementation.\n  The program included FastNLO Toolkit for the new format tables (v. 3.2+). The old\n  FastNLO table format can be still accessed with the help of APPLGRID (this is not\n  tested in the xFitter enviroment though). The reader supports both flexible\n  and non-flexible scales tables. For flexible tables, scales can be defined\n  through the CInfo mechanism in the data file. Below more details on different\n  data file variables are given.\n\n      Reaction - for the fastnlo jet cross sections this should be 'FastNLO ep jets'\n  or 'FastNLO ep jets normalised'. The latter refers to jet cross sections normalised\n  to inclusive DIS cross sections (definition of the normalisation phase space needs\n  to be done for each data point, see the 'ColumnName' field).\n\n      ColumnName - There are some specific names that are recognised internally by\n  the code:\n         'Z0Corr': Allows to inform the program of the size of the Z0 exchange correction.\n                 If it is given, each point calculated by the FastNLO code will be\n                 multiplied by the Z0Corr value.\n         'NPCorr': Allows to inform the program of the size of the non-perturbative correction.\n                 If it is given, each point calculated by the FastNLO code will be\n                 multiplied by the NPCorr value. Z0Corr and NPCorr can be added \n                 simultaneously, and in this case the calculated cross sections\n                 will be multiplied by the product of (Z0Corr*NPCorr).\n         'q2min', 'q2max', 'ymin', 'ymax', 'xmin', 'xmax': These can be used to define \n                 DIS phase space for the normalisation used in the 'FastNLO ep jets normalised'\n                 case. Out of these three (q2, y, x) exactly two sets should be defined\n                 to inambiguisly define the DIS phase space.\n         \n      CInfo, DataInfo - Following info fields are required to calculated desired cross sections\n                 (some can be ommited for 'FastNLO ep jets normalised' case):\n\n         'PublicationUnits': Output of the FastNLO code can be given in units used in \n                 the relevant publication table or in a standarized units. To use \n                 publication units one needs to set PublicationUnits to 1. In order\n                 to use absolute units, it needs to be set to 0.\n\n         'MurDef', 'MufDef': Here user can define the scale definition used by\n                 the FastNLO code for variable scale tables. The renormalisation\n                 scale (MurDef) and factorisation scale (MufDef) definitions\n                 can be set independently. The required value follows \n                 the FastNLO standard and should be equal to :\n                    0 :   mu^2 = Q^2 \n                    1 :   mu^2 = pt^2 \n                    2 :   mu^2 = ( Q^2 + pt^2 )\n                    3 :   mu^2 = ( Q^2 + pt^2 ) / 2 \n                    4 :   mu^2 = ( Q^2 + pt^2 ) / 4 \n                    5 :   mu^2 = (( Q + pt ) / 2 )^2\n                    6 :   mu^2 = (( Q + pt ))^2\n                    7 :   mu^2 = max( Q^2, pt^2)\n                    8 :   mu^2 = min( Q^2, pt^2) \n                    9 :   mu^2 = (scale1 * exp(0.3 * scale2)) ^2\n\n         'lumi(e-)/lumi(tot)': This needs to be defined for 'FastNLO ep jets normalised'\n                 option. The normalisation depends on the ratio of the e+ and e- data \n                 used to calculate the cross sections. This ratio should rather be\n                 given in a format (lumi{e-} / (lumi{e-} + lumi{e+}) and assume\n                 values between [0. 1.].\n\n         'UseZMVFNS': Should be defined for 'FastNLO ep jets normalised'. The calculation\n                 of the integrated inclusive DIS cross sections could be time consuming.\n                 This option provides an opportunity to use a \"Zero Mass Variable Flavour\n                 Number Scheme\" approximation which is very fast and possibly provides\n                 enough precision for the normalisation purposes. ZMVNS is used if 'UseZMVFNS'=1. \n                 If 'UseZMVFNS'=0., the same scheme as defined in a global steering.txt file\n                 in the variable 'HF_SCHEME'\n\n      TheoryInfoFile - Should be a path to a FastNLO table in a version 2.0+\n      TheoryType -  Should be set to 'FastNLO'\n\n\n  d) Minuit cards\n--------------------------\n\n  The minuit card contains the list of parameters used in the fits.\n  The default card (minuit.in.txt) located in the trunk is linked to the \n  STANDARD PARAMETRISATION form as used for HERAPDF2.0 (14 free parameters).\n  \t\n\n  STANDARD PARAMETRISATION has the form: \n  A * x**B * (1 - x)**C * (1 + D *x + E * x**2 + F * x**3) - Ap * x**Bp * (1 - x)**Cp   \n\n\n  and it parametrises the following PDFs: \n  uval, dval, Ubar(=ubar+cbar), Dbar(=dbar+sbar), gluon\n \n\n Other optional minuit cards are stored in the input_steering/: \n\t- CTEQ minuit card\n    - CTEQHERA - hybrid: valence like CTEQ, rest like HERAPDF\n\t- CHEBYSHEV minuit card: uval, dval, Sea(=Ubar+Dbar), gluon\n\t- BiLog - bi-lognormal parametrisation\n    - DIFFRACTION - parametrisation optimised for fits with diffractive DIS data\n    - DIPOLE for dipole model fits (fixing all or all but gluon PDFs)\n    - GENETIC - switches on the multi solution finding tool\n    - kt-factorisation - parametrisation for uPDF fits \n\nIMPORTANT:\n   Make sure that choosen minuit.in.txt corresponds to your selection in the steering.txt\n\n\nExplanation of the minuit.in.txt format:  \n\n\nset title\nnew  13p HERAPDF\nparameters\n   1    'Ag'                       0.0000      0.\n   2    'Bg'                      -0.226958    1.126400e-03\n.....\n\n-  The first 3 lines set title and announces MINUIT the list of parameters\t\n-  The index of parameters is the first column and it is hardwired to the source code.\n\n1 -10 gluon parameters     \n11-20 uval  parameters\n21-30 dval  parameters\n31-40 Ubar  parameters\n41-50 Dbar  parameters\n51-60 U     parameters\n61-70 D     parameters\n71-80 Sea   parameters\n81-90 Delta parameters\n91-100 other parameters: alphas (95), fs=Dbar/str (96), fc=Ubar/ch (97)\n\n\n- second column represents just user defined names \n- third column: input value for the parameter\n- forth column: step size (usually chosen of the same order as of the error)\n\tIMPORTANT:\n\t-> if step size value is 0. then this parameter is FIXED\n- fifth colum: lower boundary of the fit parameter\n- sixth column: upper boundary of the fit parameter\n\t-> if boundaries are not mentioned then there are no boundaries!\n\n\nOnly parameters that have the step size non-zero are let to vary in the fit (free parameters)\nAnother way to fix the parameters is simply by typing at the end of the \nlist of parameters (make sure there is one line free between):\n\nFIX 96 --> this one fixes parameter 96\n\nCommands taken by minuit:\ncall fcn 3  ->  fit is not performed, only 1 iteration, useful for testing\n\t\tMinuit parameters ARE NOT minimized\n\nmigrad \t     -> fit is performed (default number of calls 2000).\nmigrad 20000 -> fit is performed up to 20000 calls, then terminates.\nhesse\t     -> hessian estimate of the MINUIT parameters (more reliable than MINUIT)\n\n\n\n\n- The output of the fit is stored in the output/ directory: minuit.out.txt\n\n  Statements to watch in minuit.out.txt:\n  FCN=   575.16     -> this is total chisquare\n  FROM MIGRAD   STATUS=CONVERGED  -> this is desirable for a fit that converged\n  FCN=   575.16     FROM HESSE     STATUS=OK  --> this is desirable for a fit that converged \n \t\t\t\t\t\t  and errors estimated with HESSE method\n  EDM=  0.12E-04    STRATEGY= 1      ERROR MATRIX ACCURATE \n\n\nAdditional Option that works only with  ./configure --enable-genetic\n\ngenetic  \n(for details please see below)\n\n  d.a) GENETIC tool\n--------------------------\n  \n   Genetic option in MUNUIT card is useful when one needs to assure \n   that the MINUIT has found a global minima and not a local one.\n   Once activated, this option will initialise the scan of the parametrisation \n   parameters and will store multi-solutions found in the output directories \n   named output/genetic.*\n   An example of the minuit.in.txt is available in input_steering/ \n   directory (minuit.in.txt.GENETIC).\n   NOTE: due to time constraints it is recomended to use RT FAST or \n   ZMVFNS scheme when using this option.\n\n\n  e) Applying cuts\n--------------------------\n  The namelist &Cuts, located inside the steering.txt file can be used to apply\n  simple process dependent cuts. The cuts are limitted to bin variables.\n  Simple low and high limits are allowed. For example, a cut on Q2>3.5 for\n  NC ep scattering is specified as\n\n  ! Rule #1: Q2 cuts\n   ProcessName(1)     = 'NC e+-p'\n   Variable(1)        = 'Q2'\n   CutValueMin(1)     = 3.5 \n   CutValueMax(1)     = 1000000.0\n\n  Maximum 100 cuts can be used by default.\n\n\n  f) Choosing the heavy flavour scheme\n--------------------------\n  Several schemes are available for heavy quarks:\n\n  -VFNS (Variable Flavour Number Schemes):\n     RT-VFS                                 [from Robert Thorne]\n     ZMVFNS                                 [qcdnum]   \n     ACOT (ACOT-Full, ACOT-ZM, S-ACOT-Chi)  [from Fred Olness]\n     FONLL\t      \t       \t\t    [as implemented in APFEL]\n\n  -FFNS (Fixed Flavour Number Scheme)       [qcdnum]\n   also available in ABM (openqcdrad-2.0b4) [from Sergey Alekhin]\n      \n  IMPORTANT if running with FFNS (nf=3): \n  - only neutral current DIS data should be used in FF scheme due to missing NLO \n    coefficient functions in charged current (W+c) process, valence quarks \n    in this case should to be fixed in minuit.in.txt file\n    In FF ABM implementation the charged current coefficients are available\n    therefore valence parameters do not need to be fixed\n  - alpha_s(Q2) in FFNS is 3-flavour and recommended to be set to value of 0.105 \n    such that is not too high at low energies\n  - the scale in FFNS is defined as mu^2 = Q^2 + 4m_h^2 by default, can be \n    changed in HQScale in steering.txt (scale variation in ABM not yet implemented)\n  - the pole mass definition for heavy quarks is set in ABM by default, \n    the running mass definition (arXiv:1011.5790v1) can be switched in \n    by setting HF_SCHEME = 'FF ABM RUNM' in steering.txt \n\n\n  g) Understanding the output\n------------------------------\n  The results of the minimization are printed to the standard output and written\n  to the files in the output/ directory (name of the directory can be changed to\n  other than the default in the steering.txt). \n\n  The quality of the fit can be judged based on total chi2 per degrees of freedom.\n  It is printed for each iteration as \n                      Iteration   Chi2   NDF       Chi2/NDF\n   xfitter f,ndf,f/ndf      3      588.64 579        1.02\ntogether with PDFs:\nuv:    3.7171    0.6656    4.6522    0.0000    9.6938    0.0000    0.0000    0.0000    0.0000    0.0000\ndv:    2.1893    0.6656    4.2914    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000\nUb:    0.1122   -0.1651    2.5820    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000\nDb:    0.1626   -0.1651    2.4048    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000\nGL:    6.7729    0.2138    9.0138    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000\nST:    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000\n\n  The resulting chi2 is reported for each data set and for correlated \n  systematic uncertainties separately. This information is printed and written\n  to the output/Results.txt file. The Results.txt file contains additional \n  information about shifts of the correlated systematic uncertainties.\n\n  The minimization information from the minuit is stored in the output/minuit.out.txt\n  file. The verboseness level of this information can be changed by minuit commands\n  in the minuit.in.txt file. Make sure that the minuit does not report any errors\n  or warnings at the end of minimisation.\n  \n  Point by point comparison of the data and predictions after the minimization \n  is provided in output/fittedresults.txt file. The file reports three columns\n  corresponding to three first bins of the input tables, data value, sum in \n  quadrature of statistical and uncorrelated systematic uncertainty, total\n  uncertainty, the predicted value, after applying correlated systematic shifts,\n  pull between data and theory (calculated as (data-theory)/uncorrelated_error),\n  data set index. Similar information is stored in pulls.first.txt and pulls.last.txt\n  ( dataset index, first bin, second bin, third bin, data, theory, pull), however\n  theory is not adjusted for systematic error shifts in this case.\n\n  The output PDFs are stored in forms of tables in output/pdfs_q2val_XX.txt files.\n  Each of the files reports values of gluon, and quark PDFs as a function of x\n  for fixed Q^2 points. The Q^2 values and x grid are specified by \n  &Output namelist in the steering.txt\n  \n  The PDF information and data to theory comparisons can be plotted using \n  the bin/xfitter-draw program. The program requires the fit output directory as an argument. \n  Calling the program with more directories as arguments provides comparison of the PDFs \n  obtained in the various fits. \n  For a full list of the available options of xfitter-draw please type: bin/xfitter-draw --help\n  \n  Finally, the xFitter package provides PDFs in LHAPDF format (version 5 and 6). \n  To obtain the LHAPDF5.X grid file, run tools/tolhapdf.cmd script. The script \n  produces PDFs.LHgrid file which can be read by lhapdf version lhapdf-5.8.6.tar.gz\n  or later. The LHAPDF6.X version grids are produced atomatically in hf_pdf directory\n\n\n\nh) PDF type\n------------------------------\n\n Currently there are two PDF types which can be fitted in xFitter:\n 'proton' for fitting proton data and 'lead' for fitting lead data\n (cannot be used in the combination with proton data). The PDF type \n is set in the steering.txt with a flag PDFType:\n\n PDFType = 'proton'\n\n\ni) parametrisation style\n------------------------------\n\n There are various types of parametric functional form supported by xFitter. \n They are accessed via the steering flag called PDFStyle:\n \n  PDFStyle = 'HERAPDF'\n\n  The following options can be selected from the steering.txt  with a predefined string:\n\n   'HERAPDF' -- HERAPDF-like with uval, dval, Ubar, Dbar, glu evolved pdfs\n   'CTEQ'        -- CTEQ-like parameterisation\n   'CTEQHERA'    -- Hybrid: valence like CTEQ, rest like HERAPDF\n   'CHEB'        -- CHEBYSHEV parameterisation based on glu,sea, uval,dval evolved pdfs\n   'LHAPDFQ0'    -- use lhapdf library to define pdfs at starting scale and evolve with local qcdnum parameters\n   'LHAPDF'      -- use lhapdf library to define pdfs at all scales\n   'DDIS'        -- use Diffractive DIS \n   'BiLog'       -- bi-lognormal parametrisation \n\n\nj) Options for the chi2 choice:\n------------------------------\n\n  The form of the chi2 function in xFitter is based on nuisance parameters \n  or the covariance matrix. The form and the scaling properties of the uncertainties \n  are controlled globally by the CHI2SettingsName and Chi2Settings variables:\n\n  CHI2SettingsName = 'StatScale', 'UncorSysScale', 'CorSysScale', 'UncorChi2Type', 'CorChi2Type'\n  Chi2Settings     = 'Poisson'  , 'Linear',        'Linear'     , 'Diagonal'     , 'Hessian'\n\n  Variables 'StatScale', 'UncorSysScale' and 'CorSysScale' allow to chose different \n  scaling rules for statistical, uncorrelated and correlated systematic uncertainties,\n  'UncorChi2Type' and 'CorChi2Type' selects the treatment of the systematic uncertainties\n  (e.g. Hessian, Matrix or Offset method can be chosen for the correlated systematics).\n\n  Extra corrections can be applied via Chi2ExtraParam flag (they are set off by default)\n  Chi2ExtraParam = 'PoissonCorr'\n  !  'PoissonCorr'            : extra log correction accounting for changing uncertainties\n  !  'FirstIterationRescale' : re-scale uncertainties at the first iteration only\n  !  'ExtraSystRescale'      : additional re-scaling of stat. uncertainty to account for syst. shifts.\n\n------------------------------\n\n\n3) FITTING uPDF (TMD)\n=====================\n  *************************************************\n  *   fitting uPDF (TMD) gluon to HERA data       *\n  *   using the CASCADE framework                 *\n  *         H. Jung (DESY)                        *\n  *         hanes.jung@desy.de                    *\n  *************************************************\n                 \n 0. set environment variables (please see INSTALLATION file) and run\n     \n    ./configure --enable-updf --enable-lhapdf --enable-checkBounds\n\n    NOTE: by default uPDF code uses cteq66 PDFs for the starting \n          distribution for the valence quarks (Cascade/src/evolve_tmd.F), \n          please make sure you have it downloaded and linked.\n\n 1. use steering and minuit input files from \"input_steering\":\n\n    cp input_steering/steering.txt.kt-factorisation steering.txt\n    cp input_steering/minuit.in.txt.kt-factorisation minuit.in.txt\n    cp input_steering/steer-ep-CASCADE steer-ep\n    cp input_steering/steer_gluon-evolv steer_gluon-evolv\n   \n 2. edit steering.txt:\n    &CCFMFiles: give name for output grid file for uPDF.&xFitter \n    &xFitter \n    TheoryType = 'uPDF4'  | fit calculating kernel on fly, grid of sigma_hat\n\n   all other parameters are standard\n\n 3. run the program:\n    bin/xfitter\n   \n 4. plotting F2 fit results:\n    bin/xfitter-draw output ! will draw F2 results\n\n\n4) USING NNPDF REWEIGHTING PROGRAM\n=====================\n * *************************************************************** * \n * NNPDF subpackage - Reweighting program of NNPDF fitting group   *\n *                                                                 *\n * Description of NNPDF method to create NNPDF PDF sets:           *\n * arXiv:1002.4407 [hep-ph]                                        *    \n *                                                                 *\n * Description of reweighting method:                              * \n * arXiv:1012.0836 [hep-ph],                                       *\n * arXiv:1108.1758 [hep-ph]                                        *\n *                                                                 *\n * kristin.lohwasser@cern.ch                                       *\n * Alberto.Guffanti@nbi.dk                                         *  \n * *************************************************************** * \n\n                 Running NNPDF reweighting\n\n 0) General NNPDF philosophy\n ---------------------------------\n \n The NNPDF collaboration releases PDF sets consisting of 100 or 1000 PDF replicas, \n whose mean prediction for a given observable corresponds to the central NNPDF \n prediction and the RMS of those replicas for the observable is the NNPDF error.\n \n The NNPDF reweighting calculates the chi2 between a new data set and the old NNPDF \n replicas in order to determine which replicas are still able to describe the new\n data (they are kept) and which ones fail (they are thrown out).\n \n The output of the procedure is a new, updated NNPDF set in LHAPDF format with a \n reduced number of replicas that describe the old and the new data well. \n Some additional check plots which give clues about the validity of the procedure \n for the given new data set are also provided.\n \n \n1) RUNNING the NNPDF reweighting\n ---------------------------------\n \n In order to use the reweighting technique, first the LHAPDF library has to be installed\n and linked as described in the INSTALLATION file. \n NOTE: reweighting currently is working with LHAPDF6.1.1 (or higher) version only!\n\n First, in the xFitter steering files, as RunningMode the following parameter has to \n be chosen: 'LHAPDF Analysis'. This will write out the following files into the output \n directory:\n\n  NNPDF-style PDFs:\n    pdf BAYweights.dat (Bayesian) and pdf GKweights.dat (Giele-Keller reweighting)\n  Hessian PDFs:\n    pdf vector cor.dat, pdf shifts.dat, pdf rotate.dat (can be used to either \n    perform reweighting or profiling, for more details please see Manual) \n\na) To get the results as LHAPDF files, the xfitter-process has to be run in order:\n\n  bin/xfitter-process reweight <number_output_replicas> <pdf_weights> <pdf_dir_in> <pdf_dir_out>\n\n  where <number_output_replicas> is the number of PDF sets that the replica \n    should contain after the reweighting,\n  <pdf_weights> refers to BAYweights.dat or GKweights.dat output files,\n  <pdf_dir_in>  is the directory of the input PDF set,\n  <pdf_dir_out> is the directory of the output PDF set.\n\n  two checks plots are automatically created when running the reweighting:\n  ./weights.pdf --> weight distributions (used in the reweighting procedure - replicas with high \n weights are kept, low weight replicas are thrown out)\n\n ./palpha.pdf (only for Bayesian weighting) --> distribution of the probability, that the uncertainties \n of the new data should be re-scaled by a factor of alpha. The rescaling factor alpha should \n therefore ideally be 1. It is essentially a measure of the compatibility of \n the new data with the old data (it should be around around 1, if it is larger than that, \n say around 1.7, then then the new data are incompatible with the ones included in the \n fit - 0.5 for example however suspiciously good).\n\nb) To plot the results as comparions with the input data, the bin/xfitter-draw program can be run just as for the other fits, e.g. using the command:\n \n  bin/xfitter-draw reweight-BAY:output:\"BAYreweighted\" reweight-GK:output:\"GKreweighted\"\n\n\n5) DESCRIPTION OF DiffDIS PACKAGE\n   FOR THE DIFFRACTIVE FIT TO DIS\n=====================\n\n General description\n ---------------------------------\n \n Diffractive DIS data are fitted within the 'proton vertex factorisation' approach \n where the diffractive DIS is mediated by the exchange of hard Pomeron and a \n secondary Reggeon.\n The model was used in previous HERA fits, see e.g.\n 1. ZEUS Collaboration, S. Chekanov, et al., Nucl. Phys. B 831 (2010) 1.\n 2. H1 Collaboration, A. Aktas, et al., Eur. Phys. J. C 48 (2006) 715.\n\n The model supplied by the DiffDIS package provides values of the 'reduced cross \n section', sigma_r = F2 - y^2/(1+(1-y)^2) FL\n which is expected to be the experimentally meausured quantity.\n (Actually, the ZEUS data files ZEUS-LPS_2009.dat and ZEUS-LRG_2009.dat \n contain xPom*sigma_r.)\n The structure functions F2 and FL are calculated at NLO with heavy quarks \n treated according to the Thorn-Roberts GM-VFNS.\n \n Relevant formulae and notation can be found in the above mentioned papers and \n in the attached diffit.pdf file. The Eqs. numbers in the following correspond \n to the latter.\n \n F2 and FL are calculated from DPDFs given by Eq. (18). The Reggeon PDFs, f^R \n are taken as those of GRV pion. The fluxes are given by Eqs. (9,10) and they \n require following parameters, defined in plug_DDIS.h:\n   Flux_tmin, Flux_tmax -- t limits for the integrated flux\n   Pomeron_tslope -- Pomeron flux t-slope (b)\n   Pomeron_a0 -- Pomeron intercept\n   Pomeron_a1 -- Pomeron slope\n   Reggeon_tslope -- Reggeon flux t-slope (b)\n   Reggeon_a0 -- Reggeon intercept\n   Reggeon_a1 -- Reggeon slope\n   Reggeon_factor -- A_R of Eq. (10a)\n\n The values of these parameters are predefined in plug_DDIS.h and can also be read \n from the DDIS.coca file.  A_P of Eq. (10a) is set to 1 --- it is absorbed into \n the initial Pomeron parametrization, Eq. (19).\n \n Example run\n ---------------------------------\n \n This example reproduces the ZEUS-C fit results of Ref. [1].\n \n Here the fitted parameters include:\n  -- A_i of Eq. (19) for the gluon and light quarks --- they correspond to Ag, Bg, Cg \n     and Auv, Buv, Cuv of the minuit.in.txt file,\n  -- Pomeron_a0, Reggeon_a0 and Reggeon_factor --- they are declared and initialized \n     in the 'ExtraMinimisationParameters' section of the steering.txt file.\n In order to reproduce the original results the ewparam.txt file is modified to contain \n the appropriate heavy quark masses.\n \n The three above mentioned files are stored as\n input_steering/minuit.in.txt.DIFFRACTION\n input_steering/steerig.txt.DIFFRACTION\n input_steering/ewparam.txt.DIFFRACTION\n and must be copied to minuit.in.txt, steerig.txt and ewparam.txt, respectively, \n before running the program.\n \n ====================================\n \n", 
  "read_at": 1462551057, 
  "description": "", 
  "id": 21634070
}