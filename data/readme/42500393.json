{
  "read_at": 1462548698, 
  "description": "Python implementation of Markov Jump Hamiltonian Monte Carlo", 
  "README.md": "# Markov Jump Hamiltonian Monte Carlo\nPython implementation of Markov Jump HMC\n\nMarkov Jump HMC is described in the paper\n\n> A. Berger, M. Mudigonda, M. R. DeWeese and J. Sohl-Dickstein <br>\n> A Markov Jump Process for More Efficient Hamiltonian Monte Carlo <br>\n> *arXiv preprint [arXiv:1509.03808](http://arxiv.org/abs/1509.03808)*, 2015\n\n## Example Python Code\n\n```python\nfrom mjhmc.samplers.markov_jump_hmc import MarkovJumpHMC\nimport numpy as np\n\n# Define the energy function and gradient\ndef E(X, sigma=1.):\n    \"\"\" Energy function for isotropic Gaussian \"\"\"\n    return np.sum(X**2, axis=0).reshape((1,-1))/2./sigma**2\n    \ndef dEdX(X, sigma=1.):\n    \"\"\" Energy function gradient for isotropic Gaussian \"\"\"\n    return X/sigma**2\n\n# Initialize the sample locations -- 2 dimensions, 100 indepedent sampling particles\nXinit = np.random.randn(2,100)\n\n# Initialize the sampler\nmjhmc = MarkovJumpHMC(Xinit, E, dEdX, epsilon=0.1, beta=0.1)\n# Perform 10 sampling steps for all 100 particles\n# Returns an array of samples with shape (ndims, num_steps * num_particles), in this case (2, 1000)\nX = mjhmc.sample(num_steps = 10)\n```\n\n## Dependencies\n### Required\n* numpy\n* scipy\n* pandas\n\n## Optional\n* matplotlib\n* nosetests\n* seaborn (for making pretty plots)\n* spearmint (for hyperparameter optimization)\n\n", 
  "id": 42500393
}