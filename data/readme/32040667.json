{
  "read_at": 1462544793, 
  "description": "Randomized Solvers for Large-scale Least-Squares Problems with Spark", 
  "README.md": "# Randomized Solvers for Large-scale Least-Squares Problems with Spark\n\nThis is a collection of codes that implement algorithms for solving large-scale least-squares problems using randomized numerical linear algebra with Spark via Python API.\n\nby *Jiyan Yang* (jiyanyang12@gmail.com)\n\n## About\nGiven *A*(*n*-by-*d*) and *b*(*n*-by-*1*), the least-squares regression problem is to solve:\n    min_*x* ||*Ax-b*||_2.\nWhen solving least-squares problems, randomized numerical linear algebra algorithms first compute a sketch for the linear system, then use it in one of the following two ways to get either low-precision or high-precision solutions:\n+ Low-precision solvers:\n    solve the subproblem induced by computing a sketch\n+ High-precision solvers:\ncompute a preconditioner using the sketch and invoke LSQR to solve the preconditioned problem\n\nFor sketch, there are two available choices:\n+ projection:\nperform a random projection on *A*\n+ sampling:\nusing random projection to estimate the leverage scores first and use them to construct a sampling sketch\n\nFor projection method, there are four choices available:\n+ cw:\nsparse count-sketch like transform (http://arxiv.org/abs/1207.6365)\n+ gaussian:\ndense Gaussian transform\n+ rademacher:\ndense Rademacher transform\n+ srdht:\nsubsampled randomized discrete Hartley transform\n\n## Folders\n+ `src/`: contains all the source codes\n+ `data/`: default path to local files storing datasets\n+ `test/`: contains basic test codes\n+ `N_file/`: stores matrices obtained by the sketches that can be reused in the future\n+ `result/`: stores the computed solutions and total running time\n+ `log/`: stores Python logging files (stage updates and computed accuracies will be logged)\n+ `spark_log/`: stores the Spark log files (if the flag `--save_logs` in on)\n\n## Input\n  Current implementation assumes that the augmented matrix [*A* *b*] is stored in plain text format with file name `FILENAME.txt` (meaning the last column is the response vector b). It can be loaded from one of the following three sources:\n+ local: from local disc (default)\n+ HDFS: from Hadoop file system (using `--hdfs`)\n+ S3: from Amazon S3 file system (using `--s3`)\nFor the purpose of evaluating the computed solutions, files named `FILENAME_x_opt.txt` and `FILENAME_f_opt.txt` which store the optimal solution vector and objective value should be provided in the local folder `data_dir` (see below); otherwise they will be computed in the program. To generate larger matrices, one can use the option `--nrepetitions NUM` to creat a larger matrix by stacking the original one vertically `NUM` times.\n\n## Output\n  A file which stores the computed solutions and total running time will be stored (default file name is ls.out) in the `result/` subdirectory. This file can be opened by the cPickle module.\n\n## Configuration\n  The Spark configurations can be set via the script `run_ls.sh` from which the Spark job is submitted. Two `.cfg` files storing general setting of the program and Python logging setting respectively are needed to be set.\n  \n  The default filename for general setting is `conf/setting.cfg` in which there are three sections, namley, `local_directories`, `hdfs` and `s3`. In `local_directories` section, two directories are needed to be set so that the files can be properly loaded and saved. \n+ `data_dir`: path to local data files\n+ `spark_logs_dir`: path to the folder that stores the Spark log files (if the flag `--save_logs` in on)\nIn `hdfs` and `s3` sections, paths leading the dataset should be provided if either file system if used. For S3, `key_id` and `secret_key` are also required.\n\nThe default configuration filename for Python logging module is `conf/logging.cfg`. Configurations (e.g., location of the log file) can be set via this file.\n  \nConfiguration files with names other than the default ones can be passed into the program using `--setting_filename settingConfFilename` and `--logging_filename settingConfFilename`.\n\n## Usage\n```sh\n$ ./run_ls.sh [-h] --dims m n [--nrepetitions numRepetitions]\n                 [--stack stackType] [--npartitions numPartitions]\n                 [--setting_filename settingConfFilename]\n                 [--logging_filename loggingConfFilename] [-c] [--hdfs | --s3]\n                 [--low-precision | --high_precision]\n                 [--projection | --sampling]\n                 [-p {cw,gaussian,rademacher,srdht}] [-r projectionSize]\n                 [-s samplingSize] [-q numIters] [-k numTrials] [-t]\n                 [--save_logs] [--output_filename outputFilename] [--load_N]\n                 [--save_N] [--debug]\n                 dataset\n```\nType `./run_ls.sh -h` for help message.\n\n## Examples\nSome toy datasets are placed in the folder `data/` along with a file `gen_nonunif_bad_mat.py` for generating datasets. Below are a few examples showing how the program can be executed.\n\n```sh\n./run_ls.sh nonunif_bad_1000_10 --dims 1000 10 --low --proj -p cw -r 100 -k 3 -c\n./run_ls.sh nonunif_bad_1000_50 --dims 1000 50 --low --proj -p gaussian -r 200 -k 3 -t --save_N\n./run_ls.sh nonunif_bad_1000_50 --dims 1000 50 --low --samp -p gaussian -s 400 -r 200 -k 3 -t --load_N --save_N\n./run_ls.sh nonunif_bad_1000_50 --dims 1000 50 --high --proj -p gaussian -r 200 -q 5 -k 3 -t --load_N --save_logs\n./run_ls.sh nonunif_bad_1000_50 --dims 1000 50 --high --samp -p rademacher -s 200 -r 300 -q 3 -k 3 -t --nrepetition 5 --save_logs\n./run_ls.sh nonunif_bad_1000_10 --dims 1000 10 --high --proj -p srdht -r 200 --nrep 10 -q 3 -k 1 -t --load_N --save_logs --save_N --hdfs\n```\n\n## Reference\nJiyan Yang, Xiangrui Meng, and Michael W. Mahoney, [Implementing Randomized Matrix Algorithms in Parallel and Distributed Environments](http://arxiv.org/abs/1502.03032).\n\n## License\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\n    \nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n", 
  "id": 32040667
}