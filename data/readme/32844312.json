{
  "read_at": 1462543858, 
  "description": "A highly extensible deep learning framework", 
  "README.md": "deepy: A highly extensible deep learning framework based on Theano\n===\n[![Build](https://travis-ci.org/zomux/deepy.svg)](https://travis-ci.org/zomux/deepy)\n[![Quality](https://img.shields.io/scrutinizer/g/zomux/deepy.svg)](https://scrutinizer-ci.com/g/zomux/deepy/?branch=master)\n[![PyPI version](https://badge.fury.io/py/deepy.svg)](https://badge.fury.io/py/deepy)\n[![Requirements Status](https://requires.io/github/zomux/deepy/requirements.svg?branch=master)](https://requires.io/github/zomux/deepy/requirements/?branch=master)\n[![Documentation Status](https://readthedocs.org/projects/deepy/badge/?version=latest)](http://deepy.readthedocs.org/en/latest/)\n[![MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/zomux/deepy/blob/master/LICENSE)\n\n*deepy* is a deep learning framework for designing models with complex architectures.\n\nMany important components such as LSTM and Batch Normalization are implemented inside.\n\nAlthough highly flexible, *deepy* maintains a clean high-level interface.\n\nFrom deepy 0.2.0, you can easily design very complex computational graphs such as Neural Turing Machines.\n\nExample codes will be added shortly.\n\n## Dependencies\n\n- Python 2.7 (Better on Linux)\n- numpy\n- theano\n- scipy for L-BFGS and CG optimization\n\n### Tutorials (Work in progress)\n\n[http://deepy.readthedocs.org/en/latest/](http://deepy.readthedocs.org/en/latest/)\n\nClean interface\n===\n```python\n# A multi-layer model with dropout for MNIST task.\nfrom deepy import *\n\nmodel = NeuralClassifier(input_dim=28*28)\nmodel.stack(Dense(256, 'relu'),\n            Dropout(0.2),\n            Dense(256, 'relu'),\n            Dropout(0.2),\n            Dense(10, 'linear'),\n            Softmax())\n\ntrainer = MomentumTrainer(model)\n\nannealer = LearningRateAnnealer(trainer)\n\nmnist = MiniBatches(MnistDataset(), batch_size=20)\n\ntrainer.run(mnist, controllers=[annealer])\n```\n\nExamples\n===\n\n### Enviroment setting\n\n- CPU\n```\nsource bin/cpu_env.sh\n```\n- GPU\n```\nsource bin/gpu_env.sh\n```\n\n### MNIST Handwriting task\n\n- Simple MLP\n```\npython experiments/mnist/mlp.py\n```\n- MLP with dropout\n```\npython experiments/mnist/mlp_dropout.py\n```\n- MLP with PReLU and dropout\n```\npython experiments/mnist/mlp_prelu_dropout.py\n```\n- Maxout network\n```\npython experiments/mnist/mlp_maxout.py\n```\n- Deep convolution\n```\npython experiments/mnist/deep_convolution.py\n```\n- Elastic distortion\n```\npython experiments/mnist/mlp_elastic_distortion.py\n```\n- Recurrent visual attention model\n   - [Result visualization](http://raphael.uaca.com/experiments/recurrent_visual_attention/Plot%20attentions.html)\n```\npython experiments/attention_models/baseline.py\n```\n\n### Variational auto-encoders\n\n- Train a model\n```\npython experiments/variational_autoencoder/train_vae.py\n```\n\n- Visualization the output when varying the 2-dimension latent variable\n```\npython experiments/variational_autoencoder/visualize_vae.py\n```\n\n- Result of visualization\n\n![](https://raw.githubusercontent.com/uaca/deepy/master/experiments/variational_autoencoder/visualization.png)\n\n### Language model\n\n#### Penn Treebank benchmark\n\n- Baseline RNNLM (Full-output layer)\n```\npython experiments/lm/baseline_rnnlm.py\n```\n- Class-based RNNLM\n```\npython experiments/lm/class_based_rnnlm.py\n```\n- LSTM based LM (Full-output layer)\n```\npython experiments/lm/lstm_rnnlm.py\n```\n\n#### Char-based language models\n\n- Char-based LM with LSTM\n```\npython experiments/lm/char_lstm.py\n```\n- Char-based LM with Deep RNN\n```\npython experiments/lm/char_rnn.py\n```\n\n### Deep Q learning\n\n- Start server\n```\npip install Flask-SocketIO\npython experiments/deep_qlearning/server.py\n```\n- Open this address in browser\n```\nhttp://localhost:5003\n```\n\n### Auto encoders\n\n- Recurrent NN based auto-encoder\n```\npython experiments/auto_encoders/rnn_auto_encoder.py\n```\n- Recursive auto-encoder\n```\npython experiments/auto_encoders/recursive_auto_encoder.py\n```\n\n### Train with CG and L-BFGS\n\n- CG\n```\npython experiments/scipy_training/mnist_cg.py\n```\n- L-BFGS\n```\npython experiments/scipy_training/mnist_lbfgs.py\n```\nOther experiments\n===\n\n### DRAW\n\nSee https://github.com/uaca/deepy-draw\n\n```\n# Train the model\npython mnist_training.py\n# Create animation\npython animation.py experiments/draw/mnist1.gz\n```\n\n![](https://github.com/uaca/deepy-draw/raw/master/plots/mnist-animation.gif)\n\n### Highway networks\n\n- http://arxiv.org/abs/1505.00387\n```\npython experiments/highway_networks/mnist_baseline.py\npython experiments/highway_networks/mnist_highway.py\n```\n\n### Effect of different initialization schemes\n\n```\npython experiments/initialization_schemes/gaussian.py\npython experiments/initialization_schemes/uniform.py\npython experiments/initialization_schemes/xavier_glorot.py\npython experiments/initialization_schemes/kaiming_he.py\n```\n\n\n---\n\nSorry for that deepy is not well documented currently, but the framework is designed in the spirit of simplicity and readability.\nThis will be improved if someone requires.\n\n**Raphael Shu, 2016**\n", 
  "id": 32844312
}