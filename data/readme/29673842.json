{
  "read_at": 1462551598, 
  "description": "", 
  "README.md": "# Deep Reinforcement Learning\nThis is the python code for my master thesis titled 'Deep Reinforcement Learning For Arcade Games' at Denmarks Technical University.\n\nMy thesis is inspired by this DeepMind article http://arxiv.org/abs/1312.5602. My aim is to first replicate the article and use the results as a baseline. Subsequently I will attempt to improve the final score, the learning rate and/or the generalisability by: \n- Using a range for reward instead of fixing them to -1,0,1\n- Implementing prioritised sweeping for the experience replay\n- Experimenting with different CNN architectures (filter sizes, pooling layers, hyper parameters, etc.)\n- Implementing and experimenting with the saddle-free newton method (http://arxiv.org/abs/1406.2572)\n- Experimenting with different games and investigating the applicability of various setups/architectures for transfer learning\n- Implementing RNN layers and experimenting with various configurations for final layers\n\nThe tools I am using are\n[Deeppy](https://github.com/andersbll/deeppy),\n[CUDArray](https://github.com/andersbll/cudarray) and\n[Arcade Learning Environment](http://www.arcadelearningenvironment.org/).\n\nMy code is based on [Replicating DeepMind](https://github.com/kristjankorjus/Replicating-DeepMind).\n\nI have switched to Theano half-way through the project. This repository is therefore not up-to-date. My Theano implementation can be found here: [Deep Reinforcement Learning with Theano](https://github.com/ijosc/theano_deep_rl)\n", 
  "id": 29673842
}