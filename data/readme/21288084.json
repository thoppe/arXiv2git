{
  "read_at": 1462547352, 
  "description": "", 
  "README.md": "NNForSKLearn\n============\n\n= Neural Network =\n\n== Where to get it? ==\nI have uploaded it to a [https://github.com/FrederikDiehl/NNForSKLearn github repository]. Simply clone it.\n\n== What is it? ==\nIt's a fully-layer-connected feed-forward neural network with mini-batches, [http://arxiv.org/pdf/1207.0580.pdf dropout] and [http://papers.nips.cc/paper/5032-adaptive-dropout-for-training-deep-neural-networks.pdf standout]. It uses matrix-matrix multiplication for speed.\n\n== How to use it? ==\n\nUse the NeuralNetwork as in the normal sklearn classifiers.\n\nThere are, however, some peculiarities:\n* There are no comments. I did not intend to actually publish it.\n* There is no documentation. See above.\n* It only uses one core. I may change that in the summer, but that'll be too late for your projects.\n* Use np.matrix. Everything else will almost certainly crash.\n* If it doesn't work, try transposing things.\n* Set batchsize to the maximum possible (which is > the number of samples). You'll want this for maximum performance.\n* It requires sklearn and numpy. Also, python3.\n* There is, as usual, no guarantee of any kind.\n* architecture sets the hidden layers: [50, 20, 30] is a hidden layer of 50 neurons followed by one with 20 neurons and one with 30. Input and output layers are done automatically from your data.\n* It uses a sigmoid-approximation for the activation functions.\n", 
  "id": 21288084
}