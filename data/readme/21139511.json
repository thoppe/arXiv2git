{
  "read_at": 1462547268, 
  "description": "mostly changes in RBM code. including PCD", 
  "README.md": "**THIS DOCUMENTATION DOES NOT CORRESPOND WITH CURRENT CODE**\n\n# RBM Toolbox\n\nRBM toolbox is a MATLAB toolbox for online training of RBM and stacked RBM's.\n\n * Support for training RBM's with class labels including:\n    * Generative training objective [2,7]\n    * Discriminative training objective [2,7]\n    * Hybrid training objective [2,7]\n    * Semi-supervised learning [2,7]\n * CD - k (contrastive divergence k) [5]\n * PCD (persistent contrastive divergence) [6]\n * RBM/DBN sampling functions (pictures / movies)\n * RBM/DBN Classification support [2,7]\n * Regularization: L1, L2, sparsity, early-stopping, dropout [1],dropconnect[10], momentum [3] \n\nThe code in the toolbox is partly based on the DeepLearnToolbox by Rasmus Berg Palm. \n\n\nThis README first describes a classification RBM and the notation used, then settings in the toolbox are explained and finally some usage examples are given.\n\n% Classification RBMs\nRBM's are exaplained in [3]. Classification RBM's (cRBM) differ from *regular* RBM's in that they augment the data to the visible units of the RBM. A normal RBM models `p(x)` a  cRBM models either `p(x,y)`or `p(x|y)`. The labels are passed to the RBM by augmenting them to the x values, a single input to the cRBM is then **x**<sup>n</sup><sub>input</sub> = {**x**<sup>n</sup>, **y**<sup>n</sup>}. A cRBM is shown below, the picture is taken from [7], which also has a more through explanation of the cRBM.\n\n<html>\n<img src=\"/uploads/classRBM.png\" height=\"300\" width=\"400\">    \n\nWe use the notation given in the picture. Biases are:\n\n* __b__: x_visible bias\n* __c__: hidden layer bias\n* __d__: y_visible bias\n\n# Settings\nSettings in the toolbox are generally controlled with the `opts` struct. An `opts` struct with default values can be created with\n\n```MATLAB\nopts = dbncreateopts();\n```\n\nThe DBN network is then created and trained with:\n\n```MATLAB\nsizes = [50]\ndbn = dbnsetup(sizes,x_train,opts);\ndbn = dbntrain(dbn,x_train,opts);\n```\n\nHere sizes specifies the sizes of the hidden layers in the RBM's. In the example a single RBM with 50 hidden units is created. If sizes is is a vector, e.g `sizes = [50 100]`, a RBM with 50 hidden units and a RBM with 100 hidden units are stacked. Sizes of visible layers are inferred from the data.\n\n## Training Objectives\nThe toolbox support three different training objectives:\n\n * Generative    : optimizes `-log( p(x,y) )` \n * Discrminative : optimizes `-log( p(y | x) )`\n * Hybrid        : optimizes `-alpha * log( p(x,y) ) - (1-alpha) * log( p(y | x) )`\n\nFurthermore semisupervised training is available. In semisupervised training unlabled data is used in conjunction with the labeled data. \n\n\nRBM weights are updated using the equation:\n\n`grads = alpha * grads_generative + (1-alpha) * grads_discriminative  + beta * grads_semisupervised`\n\nFrom the equation it follows that:\n\n * `opts.alpha = 0` is discriminative training\n * `opts.alpha = 1` is generative training\n * `opts.alpha = ]0;1[` is hybrid training\n\n Semisupervised training is added by setting `opts.beta > 0`. In semisupervised training we partly train on unlabeled data. \n The toolbox uses y values sampled from `p(y|x)` as labels for the semisupervised samples.  The procedure is described in [7].\n\n## Specifying training, validation and semisupervised data\n\n * `x_train`        : pass x training data to the toolbox with `dbntrain(dbn,x_train,opts)`\n * `opts.y_train`   : y training data  \n * `opts.x_val`     : x validation data (optional)\n * `opts.y_val`     : y validation data (optional)\n * `opts.x_semisup` : x semisupervised data (optional)\n\n During training training and validatiion performance is monitored. The number of epochs between calcualtion of training and validaiton performance is controlled with `opts.testinterval`.\n\n If the traning set is large it might be costly to evaluate the training performance using the whole training set. `opts.traintestbatch` controls the number of samples used for calculation of training performance. \n\n## Learning rate and momentum\n\nLearning rate is set with:   \n\n```MATLAB\n% Decaying learning rate, t = current epoch\neps               = 0.001;    % initial learning rate\nf                 = 0.99;      % learning rate decay\nopts.learningrate = @(t,momentum) eps.*f.^t*(1-momentum);\n\n% Constant learning rate\nopts.learningrate =  0.01;\n```\n\nMomentum is set with:\n\n```MATLAB\n% Momentum with ramp-up, t = current epoch\nT             = 50;     % momentum ramp up epoch\np_f           = 0.9;    % final momentum\np_i           = 0.5;    % initial momentum\nopts.momentum = @(t) ifelse(t < T, p_i*(1-t/T)+(t/T)*p_f, p_f);\n\n% Constant momentum\nopts.momentum =  0.9;\n```\n\n## Sampling statistics\n\nThe toolbox support Contrastive divergence (`CD`)[5] and persistent contrastive divergcence (`PCD`) [6] for collecting statistics. \n\n * Contrastive divergence: `opts.traintype = 'CD'`\n * Persistent contrastive divergence:  `opts.traintype = 'PCD'`\n\nThe number of Gibbs steps before the statistics is collected is controlled with `opts.cdn`. `opts.cdn` is eiether a scalar in wich case the same number of gibbs steps is used for all epochs. \n\nChoose the sampling method with `opts.traintype`. For `PCD` the number of persistent chains is controlled with `opts.npcdchains`. \n `opts.npcdchains` must be less than the the number of samples and the number of semisupervised samples, the default number of chains is 100.\n\n The number of Gibbs steps before the negative statistics is collected is controlled with `opts.cdn`. `opts.cdn` is specified similarly to the learning rate and momentum, i.e:\n\n```MATLAB\n% Increasing gibbs steps at fifth epoch\nT = 5;\nopts.cdn = @(epoch) ifelse(t < T,1,5);\n\n% Constant CDn\nopts.cdn =  1;\n```\n\n## Initial Weight Sizes\n\nInitial weights are either sampled from a normal distribution [3] or from a uniform distribution [7], the behavior is controlled through `opts.init_type`:\n\n * `opts.init_type = 'gauss'`    : N(0, 0.01)\n * `opts.init_type = 'cRBM'`     : Unif(-M^-0.5, M^-0.5), is max(n_columns, n_rows) of weight matrix. \n * `opts.init_type = @(m,n) func : Handle to funtion returning a M-by-N matrix.\n\n## Regularization\n  \nThe following regularization options are implemented:\n\n * `opts.L1`: specify the regularization weight\n * `opts.L2`: specify the regularization weight\n * `opts.sparsity`: implemented as in [7]. Specify the sparsity being subtracted from biases after each weight update.\n * `opts.dropout`: dropout on hidden units. Specify the 1-probability of being dropped. see [1]\n * `opts.dropconnect`: dropout on connections, specify 1-probability of connection being zeroed, see [10]\n * Early-stopping\n\n#### Dropout Weights\nIn dropout the hidden units are dropped with `1-opts.dropout`. During each weight update rows of the incoming weights and biases to the hidden units are clamped to zero. In dropout rows of W are clamped to zero. The picture below shows the dropout W (left) and the original W (right). Black is a wieght value equal to zero and white is a weight value > 0. Hidden biases, c, of dropped units are clamped to zero.    \n\n<html>\n<img src=\"/uploads/dropout.png\" height=\"200\" width=\"500\"> \n\n\n\n#### DropConnect Weights\nDropConnect drops connections between visible and hidden units with probability `1-opts.dropconnect`. The picture shows W with dropconnect enabled (left) and the original weights (right). Hidden biases, c, are also dropped with probability `1-opts.dropconnect`.    \n\n<html>\n<img src=\"/uploads/dropconnect.png\" height=\"200\" width=\"500\"> \n\n####\nEarly stopping is always enabled. The early stopping patience is set with `opts.patience`. If you want to disable early stopping set `opts.patience = Inf`. \n\n## Using the CPU / GPU\n\n`opts.gpu` switches between CPU and GPU. GPU requires the MATLAB Parallel Computing Toolbox.\n\n * `opts.gpu = 1` : Use GPU. Requires that `opts.thisgpu` is set to a reference to the selected GPU (use `opts.thisgpu = gpuDevice()`).\n * `opts.gpu = 0` : Use CPU. \n * `opts.gpu = -1`: For testing\n\n# Examples\n\nReproducing results from [7], specifically the results from the table reproduced below:\n\n| Model  |Objective                                         | Errror (%)    | Example  |\n|---     |---                                               |---            |---       |\n|        | Generative(lr = 0.005, H = 6000)                 |   3.39        |    4     |\n|ClassRBM| Discriminative(lr = 0.05, H = 500)               |   1.81        |    5     |\n|        | Hybrid(alpha = 0.01, lr = 0.05, H = 1500)        |   1.28        |    6     |\n|        | Sparse Hybrid( idem + H = 3000, sparsity=10^-4)  |   1.16        |    7     |\nlr = learning rate\nH = hidden layer size\n\n\n\nThe networks were trained on the MNIST data set.\nTo reproduce the results the following settings where used:\n * online learning, i.e a batchsize of 1. \n * Initial weights are taken from uniform samples in the interval [-m^(-0.5), m^(-0.5)] where m is max([n_rows, n_columns)] of the matrix being initilaized. \n * Early stopping with patience of 15.\n * MNIST training set was randomly split into a training set of 50000 samples and a validation 10000 samples. The original test set was used. \n\nWeight initalization is important, try experiment by supplying your own initalization functions. \n\n\n## Example 4 - Discriminative \n\n```MATLAB\n%% Example 4 - Discriminative training\n% Tries to reproduce discriminative result from table 1 in \n% \"Learning algorithms for the classification Restricted boltzmann machine\"\nname = 'example_4';\nrng('default');rng(101);\n [train_x,val_x,test_x,train_y,val_y,test_y] = setupmnist(101,1);\nf = fullfile(pwd,[name '.mat'])\n\n% Setup DBN\nsizes = [500 ];   % hidden layer size\n\nopts = dbncreateopts();\nopts.alpha = 0; % 0 = discriminative, 1 = generative\nopts.beta = 0;\nopts.gpu   = 0;                  % use GPU other optsion are 0: CPU, -1: CPU test\nopts.cdn = 1;   \nopts.thisgpu = [];              % ref to gpu,  must be set if opts.gpu =1\nopts.gpubatch = size(train_x,1); \nopts.outfile = [name '_intermediate.mat'];\nopts.patience = 15;\nopts.numepochs = 1000;\nopts.testinterval = 1;\nopts.init_type = 'cRBM';\nopts.classRBM = 1;\nopts.y_train = train_y;\nopts.x_val = val_x;\nopts.y_val = val_y;\n\n\n%% Set learningrate and momentum\nopts.learningrate = @(t,momentum) 0.05;\nopts.momentum = @(t) 0;\n\n\n[dbn, opts]  = dbnsetup(sizes, train_x, opts);  % train function \n\nrbm = dbn.rbm{1};\nopts.gpu\nopts.numepochs\ndisp(rbm);\n\nfprintf('\\n\\n')\n\nrbm = rbmtraingpu(rbm,train_x,opts);\n\nsave(f,'rbm','opts');\n```\n\n## Example 5 - Generative\n\n```MATLAB\n%% Example 5 - generative training\n% Tries to reproduce discriminative result from table 1 in \n% \"Learning algorithms for the classification Restricted boltzmann machine\"\nname = 'example_5';\nrng('default');rng(101);\n [train_x,val_x,test_x,train_y,val_y,test_y] = setupmnist(101,1);\nf = fullfile(pwd,[name '.mat'])\n\n% Setup DBN\nsizes = [6000 ];   % hidden layer size\nopts = dbncreateopts();\nopts.alpha = 1; % 0 = discriminative, 1 = generative\nopts.beta = 0;\nopts.gpu   = 0;                  % use GPU other optsion are 0: CPU, -1: CPU test\nopts.cdn = 1;   \nopts.thisgpu = [];              % ref to gpu,  must be set if opts.gpu =1\nopts.gpubatch = size(train_x,1); \nopts.outfile = [name '_intermediate.mat'];\nopts.patience = 4;\nopts.numepochs = 1000;\nopts.testinterval = 5;\nopts.init_type = 'cRBM';\nopts.classRBM = 1;\nopts.y_train = train_y;\nopts.x_val = val_x;\nopts.y_val = val_y;\n\n\n%% Set learningrate and momentum\nopts.learningrate = @(t,momentum) 0.005;\nopts.momentum = @(t) 0;\n\n\n[dbn, opts]  = dbnsetup(sizes, train_x, opts);  % train function \n\nrbm = dbn.rbm{1};\nopts.gpu\nopts.numepochs\ndisp(rbm);\n\nfprintf('\\n\\n')\n\nrbm = rbmtraingpu(rbm,train_x,opts);\nsave(f,'rbm','opts');\n```\n\n## Example 6 - Hybrid training \n\n```MATLAB\n%% Example 6 - Hybrid\n% Tries to reproduce discriminative result from table 1 in \n% \"Learning algorithms for the classification Restricted boltzmann machine\"\nname = 'example_6';\nrng('default');rng(101);\n [train_x,val_x,test_x,train_y,val_y,test_y] = setupmnist(101,1);\nf = fullfile(pwd,[name '.mat'])\n\n% Setup DBN\nsizes = [1500 ];   % hidden layer size\nopts = dbncreateopts();\nopts.alpha = 0.01; % 0 = discriminative, 1 = generative\nopts.beta = 0;\nopts.gpu   = 0;                  % use GPU other optsion are 0: CPU, -1: CPU test\nopts.cdn = 1;   \nopts.thisgpu = [];              % ref to gpu,  must be set if opts.gpu =1\nopts.gpubatch = size(train_x,1); \nopts.outfile = [name '_intermediate.mat'];\nopts.patience = 15;\nopts.numepochs = 1000;\nopts.testinterval = 1;\nopts.init_type = 'cRBM';\nopts.classRBM = 1;\nopts.y_train = train_y;\nopts.x_val = val_x;\nopts.y_val = val_y;\n\n\n%% Set learningrate and momentum\nopts.learningrate = @(t,momentum) 0.05;\nopts.momentum = @(t) 0;\n\n\n[dbn, opts]  = dbnsetup(sizes, train_x, opts);  % train function \n\nrbm = dbn.rbm{1};\nopts.gpu\nopts.numepochs\ndisp(rbm);\n\nfprintf('\\n\\n')\n\nrbm = rbmtraingpu(rbm,train_x,opts);\nsave(f,'rbm','opts');\n```\n\n\n## Example 7 - Hybrid training with sparsity\n\n```MATLAB\n%% Example 7 - Sparse hybrid\n% Tries to reproduce discriminative result from table 1 in \n% \"Learning algorithms for the classification Restricted boltzmann machine\"\nname = 'example_6';\nrng('default');rng(101);\n [train_x,val_x,test_x,train_y,val_y,test_y] = setupmnist(101,1);\nf = fullfile(pwd,[name '.mat'])\n\n% Setup DBN\nsizes = [3000 ];   % hidden layer size\nopts = dbncreateopts();\nopts.alpha = 0.01; % 0 = discriminative, 1 = generative\nopts.beta = 0;\nopts.gpu   = 0;                  % use GPU other optsion are 0: CPU, -1: CPU test\nopts.cdn = 1;   \nopts.sparsity = 10^-4;\nopts.thisgpu = [];              % ref to gpu,  must be set if opts.gpu =1\nopts.gpubatch = size(train_x,1); \nopts.outfile = [name '_intermediate.mat'];\nopts.patience = 15;\nopts.numepochs = 1000;\nopts.testinterval = 1;\nopts.init_type = 'cRBM';\nopts.classRBM = 1;\nopts.y_train = train_y;\nopts.x_val = val_x;\nopts.y_val = val_y;\n\n\n%% Set learningrate and momentum\nopts.learningrate = @(t,momentum) 0.05;\nopts.momentum = @(t) 0;\n\n\n[dbn, opts]  = dbnsetup(sizes, train_x, opts);  % train function \n\nrbm = dbn.rbm{1};\nopts.gpu\nopts.numepochs\ndisp(rbm);\n\nfprintf('\\n\\n')\n\nrbm = rbmtraingpu(rbm,train_x,opts);\nsave(f,'rbm','opts');\n```\n\n\n# TODO\n\n * Add Annealed Importance Sampling (AIS) [8]\n * add Normalization to RBM [9]\n *  Parallel tempering for training of restricted Boltzmann machines.\n# \n\n# References\n\n[1] Srivastava Nitish, G. Hinton, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting,\" J. Mach. Learn. Res., vol. 5(Jun), no. 2, p. 1929-1958, 2014.    \n[2] H. Larochelle and Y. Bengio, \"Classification using discriminative restricted Boltzmann machines,\" in Proceedings of the 25th international conference on Machine learning. ACM,, 2008.     \n[3] G. Hinton, \"A practical guide to training restricted Boltzmann machines,\" Momentum, vol. 9, no. 1, p. 926, 2010.    \n[4] G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, \"Improving neural networks by preventing co-adaptation of feature detectors,\" arXiv Prepr., vol. 1207.0580, no. Hinton, Geoffrey E., et al. \"Improving neural networks by preventing co-adaptation of feature detectors.\" arXiv preprint arXiv:1207.0580 (2012)., Jul. 2012.    \n[5] G. Hinton, \"Training products of experts by minimizing contrastive divergence,\" Neural Comput., vol. 14, no. 8, pp. 1771-1800, 2002.     \n[6] T. Tieleman, \"Training restricted Boltzmann machines using approximations to the likelihood gradient,\" in Proceedings of the 25th international conference on Machine learning. ACM, 2008.    \n[7] H. Larochelle, M. Mandel, R. Pascanu, and Y. Bengio, \"Learning algorithms for the classification restricted boltzmann machine,\" J. Mach. Learn. Res., vol. 13, no. 1, pp. 643-669, 2012.    \n[8] R. Salakhutdinov and I. Murray, \"On the quantitative analysis of deep belief networks,\" in Proceedings of the 25th international conference on Machine learning. ACM,, 2008.    \n[9] Y. Tang and I. Sutskever, \"Data normalization in the learning of restricted Boltzmann machines,\" Dep. Comput. Sci. Toronto Univ., vol. UTML-TR-11, 2011.     \n[10] L. Wan, M. Zeiler, S. Zhang, Y. Le Cun, and R. Fergus, \"Regularization of Neural Networks using DropConnect,\" in Proceedings of The 30th International Conference on Machine Learning, 2013, pp. 1058-1066. \n\nCopyright (c) 2014, Soren Kaae Sonderby (skaaesonderby@gmail.com) All rights reserved.", 
  "id": 21139511
}