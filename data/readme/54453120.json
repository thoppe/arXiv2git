{
  "read_at": 1462555963, 
  "description": "Collector for corpus of conversations, usable by Lumen Reasoner", 
  "README.md": "# Lumen Trainer\n\nCollector for corpus of conversations, usable by Lumen Reasoner.\n\n5 steps to use Lumen Trainer:\n\n1. Collecting raw corpus\n    Alternatively: Download raw corpus snapshot\n2. Preprocessing raw corpus into train-ready corpus\n3. Training the model\n4. Testing the trained model\n5. Predicting\n\n## Collecting Raw Corpus\n\n1. Create PostgreSQL database `buzz_buzz_dev`\n2. Inside database `buzz_buzz_dev`, create schema `buzz`.\n3. Clone [soluvas-buzz](https://github.com/soluvas/soluvas-buzz).\n4. In `soluvas-buzz/twitter-collector/config`, put `lumen.TwitterApp.jsonld` and `lumen-lumenrobot.TwitterAuthorization.jsonld`\n    from `Dropbox/Lumen/config`\n5. To collect from `@IndosatCare`, do:\n\n        ./twitter-collector.sh -a lumen -u lumenrobot -p helpdesk_indosatcare indosatcare\n\n    To collect from `@Telkomsel`, do:\n    \n        ./twitter-collector.sh -a lumen -u lumenrobot -p helpdesk_telkomsel telkomsel\n\n## Download Raw Corpus Snapshot\n\nHendy only: Get from `Dropbox\\Big_Stuff\\buzz_backup\\daily\\postgresql`\n\n## Spark Preparation\n\n1. You need to copy `hadoop-2.7.1/winutils.exe` (download from https://github.com/steveloughran/winutils) into `D:\\spark\\bin`\n    (see http://stackoverflow.com/a/34184857/122441, https://blogs.msdn.microsoft.com/arsen/2016/02/09/resolving-spark-1-6-0-java-lang-nullpointerexception-not-found-value-sqlcontext-error-when-running-spark-shell-on-windows-10-64-bit/)\n\n2. Execute: (this will add _Everyone_ to `D:\\tmp\\hive`'s Security)\n\n        D:\n        cd \\spark\n        bin\\winutils chmod777 \\tmp\\hive\n\n3. Download http://search.maven.org/remotecontent?filepath=org/postgresql/postgresql/9.4.1208/postgresql-9.4.1208.jar into\n    D:\\spark\\lib\n\n## Preprocessing Raw Corpus into Train-Ready Corpus\n\n### Select and Join into Cases Dataset\n\n1. Execute:\n\n        set HADOOP_HOME=D:\\spark\n        set SPARK_CLASSPATH=D:\\spark\\lib\\postgresql-9.4.1208.jar\n        D:\n        cd \\spark\n        bin\\spark-shell --packages com.databricks:spark-csv_2.10:1.4.0\n\n2. In Spark Shell, get the DataFrame.\n    Currently `buzz.twitterstatus` table causes Spark to throw `SQLException: Unsupported type 1111` ([SPARK-7869](https://issues.apache.org/jira/browse/SPARK-7869))\n    due to `geography` columns, so we wrapped the table in a Spark-friendly view called `twitterstatus_simple`.\n    \n    Read from JDBC:\n    \n        val twitterstatus = sqlContext.read.format(\"jdbc\").options(\n            Map(\"driver\" -> \"org.postgresql.Driver\",\n                \"url\" -> \"jdbc:postgresql://localhost/buzz_buzz_dev?user=postgres&password=bippo\",\n                \"dbtable\" -> \"buzz.twitterstatus_simple\")).load()\n\n    TODO: `mediacontent` should be in different table!\n\n    **Spark Crash Course (Optional): Inspecting the Raw Dataset**\n    \n    Now you can get number of loaded:\n    \n        twitterstatus.count()\n        \n    Inspect schema and some rows:\n    \n        twitterstatus.printSchema()\n        twitterstatus.show()\n        twitterstatus.take(3)\n\n    Filter a bit: (check [DataFrame API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame))\n    \n        twitterstatus.select(\"project_id\", \"userscreenname\", \"inreplytoscreenname\", \"text\").show()\n        twitterstatus.filter(\"project_id = 'helpdesk_telkomsel'\").select(\"project_id\", \"userscreenname\", \"inreplytoscreenname\", \"text\").show()\n        twitterstatus.filter(\"project_id = 'helpdesk_telkomsel' AND userscreenname = 'Telkomsel'\").groupBy($\"inreplytostatusid\").agg(count($\"id\").alias(\"replystatuscount\"), concat_ws(\" \", collect_list($\"text\")).alias(\"reply\")).show()\n\n3. Now we can get all the replies:\n    \n        val replies = (twitterstatus.filter(\"project_id = 'helpdesk_telkomsel' AND userscreenname = 'Telkomsel'\")\n            .groupBy($\"inreplytostatusid\")\n            .agg(count($\"id\").alias(\"replystatuscount\"), concat_ws(\" \", collect_list($\"text\")).alias(\"reply\")))\n        \n4. And join them to the original statuses:\n    \n        val inquiries = twitterstatus.filter(\"project_id = 'helpdesk_telkomsel'\").select($\"statusid\", $\"userscreenname\", $\"text\".as(\"inquiry\")) \n        val cases0 = (replies.join(inquiries, replies(\"inreplytostatusid\") === inquiries(\"statusid\"))\n            .select($\"statusid\", $\"userscreenname\", $\"inquiry\", $\"reply\", $\"replystatuscount\"))\n        cases0.show()\n\n5. Naive whitespace cleaning to make the TSV output correct:\n\n        import org.apache.spark.sql.functions._\n        val cases = cases0.withColumn(\"inquiry\", regexp_replace($\"inquiry\", \"\\\\s+\", \" \")).withColumn(\"reply\", regexp_replace($\"reply\", \"\\\\s+\", \" \"))\n        cases.show()\n        \n5. Output to Parquet and TSV:\n    \n        import java.io._\n        import org.apache.commons.io._\n        FileUtils.deleteDirectory(new File(\"telkomsel_cases.parquet\"))\n        FileUtils.deleteDirectory(new File(\"telkomsel_cases.tsv\"))\n        cases.coalesce(1).write.save(\"telkomsel_cases.parquet\")\n        cases.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").save(\"telkomsel_cases.tsv\")\n        \n6. Backup Parquet and TSV file to Hendy's Dropbox: (Use your own folder)\n    Unfortunately the Parquet has dynamic filename, please copy it manually.\n\n        FileUtils.copyFile(new File(\"D:/spark/telkomsel_cases.tsv/part-00000\"), \n            new File(\"C:/Users/ceefour/Dropbox/Lumen/Reasoning/helpdesk/telkomsel_cases.tsv\"))\n\n### Tokenizing the Dataset\n\nYou should now have a **cases** dataset such as `C:/Users/ceefour/Dropbox/Lumen/Reasoning/helpdesk/telkomsel_cases.parquet`.\n\nWe need to process `*_cases.tsv` file by transforming `inquiry` to `inquiry_clean` and `reply` to `reply_clean`.\nMost processing are case-insensitive unless marked otherwise:\n\n0. Read the cases dataset\n\n        val cases = sqlContext.read.load(\"C:/Users/ceefour/Dropbox/Lumen/Reasoning/helpdesk/telkomsel_cases.parquet\")\n            \n1. Make lowercase (both)\n\n        import org.apache.spark.sql.functions._\n        val tokendf1 = cases.withColumn(\"inquirymain\", lower($\"inquiry\")).withColumn(\"replymain\", lower($\"reply\"))\n\n2. Remove @telkomsel (inquiry only)\n\n        val tokendf2 = tokendf1.withColumn(\"inquirymain\", regexp_replace($\"inquirymain\", \"@telkomsel\", \"\"))\n        tokendf2.select(\"inquirymain\").head()\n\n3. Remove all @userscreenname mentions (both)\n\n        val tokendf3 = (tokendf2.withColumn(\"inquirymain\", regexp_replace($\"inquirymain\", \"(?<![a-z0-9_])@[a-z0-9_]+\", \"\"))\n            .withColumn(\"replymain\", regexp_replace($\"replymain\", \"(?<![a-z0-9_])@[a-z0-9_]+\", \"\")) )\n        tokendf3.select(\"inquirymain\", \"replymain\").head()\n\n4. Remove `(?<![a-z0-9_])-[a-z]+` (reply only). This is the staff name of the replier.\n\n        val tokendf4 = tokendf3.withColumn(\"replymain\", regexp_replace($\"replymain\", \"(?<![a-z0-9_])-[a-z]+\", \"\"))\n        tokendf4.select(\"replymain\").head()\n\n5. Remove \"[\\d]\" (reply only). This is the tweet number in multi-tweet reply.\n\n        val tokendf5 = tokendf4.withColumn(\"replymain\", regexp_replace($\"replymain\", \"\\\\[\\\\d\\\\]\", \"\"))\n        tokendf5.select(\"replymain\").head(10)\n\n6. Remove \"(mbak|mas|pak|bu)( \\w+|[.,])\" (reply only)\n\n        val tokendf6 = tokendf5.withColumn(\"replymain\", regexp_replace($\"replymain\", \"\"\"(mbak|mas|pak|bu)( \\w+|[.,])\"\"\", \"\"))\n        tokendf6.select(\"reply\", \"replymain\").head(10)\n\n7. Remove http/s, 0812345xxx (both)\n\n        val tokendf7 = ( tokendf6.withColumn(\"inquirymain\", regexp_replace( regexp_replace($\"inquirymain\", \"\"\"http(s?)://\\S+\"\"\", \"\"), \"\"\"0[0-9x]+\"\"\", \"\") )\n            .withColumn(\"replymain\", regexp_replace( regexp_replace($\"replymain\", \"\"\"http(s?)://\\S+\"\"\", \"\"), \"\"\"0[0-9x]+\"\"\", \"\") ) )\n        tokendf7.select(\"replymain\", \"reply\").head(10)\n        // ensure filtered\n        tokendf7.filter($\"inquirymain\".like(\"%http%\")).head(1)\n        tokendf7.filter($\"inquirymain\".like(\"%0812%\")).head(1)\n        tokendf7.filter($\"replymain\".like(\"%http%\")).head(1)\n\n8. Separate word boundaries. Remove a single letter or a word consisting of only numerics. Remove all non-word tokens.\n\n        val tokendf8 = (tokendf7.withColumn(\"inquirymain\", regexp_replace( regexp_replace( regexp_replace($\"inquirymain\", \"\"\"(?<!\\s)\\b(?!\\s)*\"\"\", \" \"), \"\"\"\\b(\\w|[0-9]+)\\b\"\"\", \" \"), \"\"\"\\W+\"\"\", \" \") )\n            .withColumn(\"replymain\", regexp_replace( regexp_replace( regexp_replace($\"replymain\", \"\"\"(?<!\\s)\\b(?!\\s)*\"\"\", \" \"), \"\"\"\\b(\\w|[0-9]+)\\b\"\"\", \" \"), \"\"\"\\W+\"\"\", \" \") ) )\n        tokendf8.select(\"inquirymain\", \"replymain\").head(10)\n\n9. Tokenize. (should we use OpenNLP? I think this is enough though.)\n\n        import org.apache.spark.ml._\n        import org.apache.spark.ml.feature._\n        val inquiryTokenizer = new Tokenizer().setInputCol(\"inquirymain\").setOutputCol(\"inquirytokens\")\n        val replyTokenizer = new Tokenizer().setInputCol(\"replymain\").setOutputCol(\"replytokens\")\n        val tokendf9 = replyTokenizer.transform(inquiryTokenizer.transform(tokendf8))\n        tokendf9.select(\"inquirytokens\", \"replytokens\").head(10)\n\n10. Synonym replacement (regex based first e.g. `pagi+` -> `pagi`), then static based e.g. `bila` -> `jika`)\n    and spell correction (u/ -> untuk, etc.). For spell correction, maybe you can have Indonesian\n    dictionary + Levenshtein corrector.\n11. Stop words: min, admin, pagi, sore, siang, malam, kak, yang, kok, koq, kan, deh\n    Remove ,+./()#!?\n\n        val stopWords = Array(\"\", \",\", \".\", \"!\", \"?\", \"??\", \"???\", \":(\", \":)\", \"*\", \"#\", \"@\", \"/\", \"-\", \"(\", \")\", \"min\", \"admin\", \"pagi\", \"sore\", \"siang\", \"malam\", \"kak\", \"yang\", \"kok\", \"koq\", \"kan\", \"deh\", \"kah\", \"dan\", \"atau\", \"atas\", \"silakan\", \"yang\", \"yg\", \"menjadi\", \"sebagai\", \"kami\", \"bantu\", \"dahulu\", \"hai\", \"di\", \"nya\", \"sangat\", \"sgt\", \"amat\")\n        val inquiryRemover = (new StopWordsRemover()\n            .setStopWords(stopWords)\n            .setInputCol(\"inquirytokens\")\n            .setOutputCol(\"inquiryfiltereds\"))\n        val replyRemover = (new StopWordsRemover()\n            .setStopWords(stopWords)\n            .setInputCol(\"replytokens\")\n            .setOutputCol(\"replyfiltereds\"))\n        val tokendf11 = replyRemover.transform(inquiryRemover.transform(tokendf9))\n        tokendf11.select(\"inquiryfiltereds\", \"replyfiltereds\").head(10)\n\n    Let's count the filtered tokens, both inquiry and reply:\n    \n        val inquiryExploded = tokendf11.select(explode($\"inquiryfiltereds\").alias(\"inquirytoken\"))\n        inquiryExploded.show()\n        val inquiryIndexer = new StringIndexer().setInputCol(\"inquirytoken\").setOutputCol(\"inquirytokenid\").fit(inquiryExploded)\n        // make this < 1000\n        inquiryIndexer.labels.size\n        inquiryIndexer.labels\n        println(inquiryIndexer.labels.mkString(\" \"))\n    \n12. OpenNLP-tokenize then re-join using \" \". This effectively does trim also. (both)\n13. Save it:\n\n        import java.io._\n        import org.apache.commons.io._\n        FileUtils.deleteDirectory(new File(\"telkomsel_tokenized.parquet\"))\n        tokendf11.coalesce(1).write.save(\"telkomsel_tokenized.parquet\")\n\n    Please copy to `Dropbox/Lumen/Reasoning/helpdesk`.\n\nReferences:\n\n* http://spark.apache.org/docs/latest/ml-guide.html\n* https://spark.apache.org/docs/latest/ml-features.html\n\n### TODO: Try doing binary classification on each of the reply labels instead\n\nYes, this means you'll train 1000 NN models for 1000 labels (reply words).\nBut make a proof of concept with, say, 3 labels. So you have 2^3 = 8 possible combinations. If you can get 100% accurate\nfor 3 labels, then you're good :)\n\n### Extract Features/Vectorize the Dataset\n\nI try to mimic: Vinyals, O., Le, Q. (2015). [A neural conversational model](http://arxiv.org/pdf/1506.05869v1.pdf). arXiv preprint arXiv:1506.05869.\n\nNow we need to convert that into feature-extracted dataset (word2vec), so each word/token from `inquiry_clean` becomes a column:\n\n1. Load the `tokenized` dataset\n\n        val featuredf = sqlContext.read.load(\"C:/Users/ceefour/Dropbox/Lumen/Reasoning/helpdesk/telkomsel_tokenized.parquet\")\n\n2. Hashing TF (inquiry only)\n\n        import org.apache.spark.sql.functions._\n        import org.apache.spark.ml._\n        import org.apache.spark.ml.feature._\n        val inquiryHashingTF = new HashingTF().setNumFeatures(1000).setInputCol(\"inquiryfiltereds\").setOutputCol(\"inquiryfeatures\")\n        val featuredf2 = inquiryHashingTF.transform(featuredf)\n        featuredf2.select(\"inquiryfiltereds\", \"inquiryfeatures\").head()\n        \n    Previous attempt with hashingTF labels (not working):\n\n        val inquiryHashingTF = new HashingTF().setNumFeatures(1000).setInputCol(\"inquiryfiltereds\").setOutputCol(\"inquiryfeatures\")\n        val replyHashingTF = new HashingTF().setNumFeatures(1000).setInputCol(\"replyfiltereds\").setOutputCol(\"replylabels\")\n        val featuredf2 = replyHashingTF.transform(inquiryHashingTF.transform(featuredf))\n        featuredf2.select(\"inquiryfeatures\", \"replylabels\").head()\n        \n        val labelIndexer = new StringIndexer().setInputCol(\"replyfiltereds\").setOutputCol(\"replylabels\").fit(featuredf)\n        labelIndexer.transform(featuredf).show()\n\n3. Join reply tokens as label then index label (not working well)\n        \n        val joinUdf = udf((filtereds: Seq[String]) => filtereds.mkString(\" \"))\n        val featuredf3 = featuredf2.withColumn(\"replylabel\", joinUdf($\"replyfiltereds\"))\n        featuredf3.select(\"replylabel\").head(10)\n        val labelIndexer = new StringIndexer().setInputCol(\"replylabel\").setOutputCol(\"replyid\").fit(featuredf3)\n        val featuredf4 = labelIndexer.transform(featuredf3)\n        // compare the dataset row count with labels size? ideally labels should be much (< 10%) fewer\n        featuredf4.count()\n        labelIndexer.labels.size\n        featuredf4.select(\"replylabel\", \"replyid\").head(10)\n\n    Attempt just use binary classification for X top reply words/labels:\n\n        val labelExploded = featuredf2.select(explode($\"replyfiltereds\").alias(\"replylabel\"))\n        labelExploded.show()\n        val labelIndexer = new StringIndexer().setInputCol(\"replylabel\").setOutputCol(\"replyid\").fit(labelExploded)\n        labelIndexer.labels.size\n        labelIndexer.labels\n        println(labelIndexer.labels.mkString(\" \"))\n\n    Attempt just use binary classification. e.g. `terima` reply:\n\n        val joinUdf = udf((filtereds: Seq[String]) => filtereds.mkString(\" \"))\n        val featuredf3 = featuredf2.withColumn(\"replyfiltered\", joinUdf($\"replyfiltereds\"))\n        featuredf3.select(\"replyfiltered\").head(10)\n        val featuredf4 = (featuredf3.withColumn(\"terimaLabel\", $\"replyfiltered\".like(\"%terima%\").cast(\"double\"))\n            .withColumn(\"baikLabel\", $\"replyfiltered\".like(\"%baik%\").cast(\"double\"))\n            .withColumn(\"cekLabel\", $\"replyfiltered\".like(\"%cek%\").cast(\"double\")))\n        featuredf4.select($\"inquiryfiltereds\", $\"terimaLabel\", $\"baikLabel\", $\"cekLabel\").show()\n        \n        // Split Training & Test Dataset\n        val test = featuredf4.limit(20)\n        val train = featuredf4.except(test)\n        \n        // Training\n        import org.apache.spark.ml.classification._\n        val layers = Array[Int](1000, 20, 2)\n        val terimaTrainer = (new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setMaxIter(50)\n            .setFeaturesCol(\"inquiryfeatures\").setLabelCol(\"terimaLabel\"))\n        val terimaModel = terimaTrainer.fit(train).setPredictionCol(\"terimaPrediction\")\n        val baikTrainer = (new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setMaxIter(50)\n            .setFeaturesCol(\"inquiryfeatures\").setLabelCol(\"baikLabel\"))\n        val baikModel = baikTrainer.fit(train).setPredictionCol(\"baikPrediction\")\n        val cekTrainer = (new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setMaxIter(50)\n            .setFeaturesCol(\"inquiryfeatures\").setLabelCol(\"cekLabel\"))\n        val cekModel = cekTrainer.fit(train).setPredictionCol(\"cekPrediction\")\n        // Test!\n        val predicted = cekModel.transform(baikModel.transform(terimaModel.transform(test)))\n        predicted.select(\"inquiryfiltereds\", \"replyfiltered\", \"terimaLabel\", \"terimaPrediction\", \"baikLabel\", \"baikPrediction\", \"cekLabel\", \"cekPrediction\").show()\n\n## Experiment: Training, Reply HashingTF are reduced using PCA first\n\n*TODO*: Fixing these may give you better performance:\n\n    // PCA\n    16/04/02 18:16:54 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n    16/04/02 18:16:54 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n    // Linear Regression\n    16/04/02 18:17:03 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n    16/04/02 18:17:03 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n\nLM doesn't work at all! :(\n\n    val inquiryHashingTF = new HashingTF().setNumFeatures(1000).setInputCol(\"inquiryfiltereds\").setOutputCol(\"inquiryfeatures\")\n    val replyHashingTF = new HashingTF().setNumFeatures(1000).setInputCol(\"replyfiltereds\").setOutputCol(\"replylabels\")\n    val featuredf2 = replyHashingTF.transform(inquiryHashingTF.transform(featuredf))\n    featuredf2.select(\"replyfiltereds\", \"replylabels\").show()\n    \n    // PCA the replylabels\n    featuredf2.select(\"replylabels\").map(x => x.getAs[SparseVector](0)).first()\n    featuredf2.select(\"replylabels\").map(x => x(0))\n    val pca = new org.apache.spark.ml.feature.PCA().setK(3).setInputCol(\"replylabels\").setOutputCol(\"replypca\")\n    val pcaModel = pca.fit(featuredf2)\n    import org.apache.spark.mllib.linalg._\n    val v0udf = udf((x: Vector) => x(0))\n    val v1udf = udf((x: Vector) => x(1))\n    val v2udf = udf((x: Vector) => x(2))\n    val featuredf3 = pcaModel.transform(featuredf2).withColumn(\"replypca0\", v0udf($\"replypca\") ).withColumn(\"replypca1\", v1udf($\"replypca\") ).withColumn(\"replypca2\", v2udf($\"replypca\") )\n    featuredf3.select(\"replyfiltereds\", \"replylabels\", \"replypca\", \"replypca0\", \"replypca1\", \"replypca2\").show()\n    // Split Training & Test Dataset\n    val test = featuredf3.limit(20)\n    val train = featuredf3.except(test)\n    // Train\n    import org.apache.spark.ml.classification._\n    import org.apache.spark.ml.regression._\n    val pca0Trainer = (new LinearRegression().setMaxIter(100).setRegParam(0.3).setElasticNetParam(0.8)\n        .setFeaturesCol(\"inquiryfeatures\").setLabelCol(\"replypca0\"))\n    val pca0Model = pca0Trainer.fit(train).setPredictionCol(\"prediction0\")\n    val pca1Trainer = (new LinearRegression().setMaxIter(100).setRegParam(0.3).setElasticNetParam(0.8)\n        .setFeaturesCol(\"inquiryfeatures\").setLabelCol(\"replypca1\"))\n    val pca1Model = pca1Trainer.fit(train).setPredictionCol(\"prediction1\")\n    val pca2Trainer = (new LinearRegression().setMaxIter(100).setRegParam(0.3).setElasticNetParam(0.8)\n        .setFeaturesCol(\"inquiryfeatures\").setLabelCol(\"replypca2\"))\n    val pca2Model = pca2Trainer.fit(train).setPredictionCol(\"prediction2\")\n    // Validate on train\n    val validated = pca2Model.transform(pca1Model.transform(pca0Model.transform(train)))\n    validated.select(\"inquiryfiltereds\", \"replyfiltereds\", \"replypca0\", \"prediction0\", \"replypca1\", \"prediction1\", \"replypca2\", \"prediction2\").show()\n    // Test!\n    val predicted = pca2Model.transform(pca1Model.transform(pca0Model.transform(test)))\n    predicted.select(\"inquiryfiltereds\", \"replyfiltereds\", \"replypca0\", \"prediction0\", \"replypca1\", \"prediction1\", \"replypca2\", \"prediction2\").show()\n\nResult:\n\n    +--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n    |    inquiryfiltereds|      replyfiltereds|           replypca0|         prediction0|           replypca1|         prediction1|           replypca2|       prediction2|\n    +--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n    |[pagiii, ada, lag...|[selamat, ada, bi...| -0.3314020219168389| -0.9912411564400341| 0.10456535127247156|  0.1516935557851563| 0.18097083199219668|0.4340939616977683|\n    |[filmnasionalfavo...|[terima, kasih, p...|  0.5069582705150794|-0.22704097315143223| -0.9607766570659154|-0.28251291428610076|   0.941528669847476|0.4340939616977683|\n    |[filmnasionalfavo...|[terima, kasih, p...|  0.5504657038728358|-0.22704097315143223| -0.9975449049444007|-0.28251291428610076|  0.9868268279466268|0.4340939616977683|\n\n### Try RandomForest Regressor:\n\n    val inquiryHashingTF = new HashingTF().setNumFeatures(1000).setInputCol(\"inquiryfiltereds\").setOutputCol(\"inquiryfeatures\")\n    val replyHashingTF = new HashingTF().setNumFeatures(1000).setInputCol(\"replyfiltereds\").setOutputCol(\"replylabels\")\n    val featuredf2 = replyHashingTF.transform(inquiryHashingTF.transform(featuredf))\n    featuredf2.select(\"replyfiltereds\", \"replylabels\").show()\n    \n    // PCA the replylabels\n    featuredf2.select(\"replylabels\").map(x => x.getAs[SparseVector](0)).first()\n    featuredf2.select(\"replylabels\").map(x => x(0))\n    val pca = new org.apache.spark.ml.feature.PCA().setK(3).setInputCol(\"replylabels\").setOutputCol(\"replypca\")\n    val pcaModel = pca.fit(featuredf2)\n    import org.apache.spark.mllib.linalg._\n    val vudf = udf((x: Vector, idx: Integer) => x(idx))\n    val vlimitudf = udf((x: Vector) => new DenseVector(Array(x(0), x(1), x(2))))\n    val featuredf3 = pcaModel.transform(featuredf2).withColumn(\"replypca0\", vudf($\"replypca\", lit(0)) ).withColumn(\"replypca1\", vudf($\"replypca\", lit(1)) ).withColumn(\"replypca2\", vudf($\"replypca\", lit(2)) ).withColumn(\"replypca\", vlimitudf($\"replypca\"))\n    featuredf3.select(\"replyfiltereds\", \"replylabels\", \"replypca\", \"replypca0\", \"replypca1\", \"replypca2\").show()\n    // Split Training & Test Dataset\n    val test = featuredf3.limit(20)\n    val train = featuredf3.except(test)\n    // Train\n    import org.apache.spark.ml.classification._\n    import org.apache.spark.ml.regression._\n    val layers = Array[Int](1000, 20, 1)\n    val maxDepth = 10\n    val numTrees = 20\n    val pca0Trainer = (new RandomForestRegressor().setMaxDepth(maxDepth).setNumTrees(numTrees)\n        .setFeaturesCol(\"inquiryfeatures\").setLabelCol(\"replypca0\"))\n    val pca0Model = pca0Trainer.fit(train).setPredictionCol(\"prediction0\")\n    val pca1Trainer = (new RandomForestRegressor().setMaxDepth(maxDepth).setNumTrees(numTrees)\n        .setFeaturesCol(\"inquiryfeatures\").setLabelCol(\"replypca1\"))\n    val pca1Model = pca1Trainer.fit(train).setPredictionCol(\"prediction1\")\n    val pca2Trainer = (new RandomForestRegressor().setMaxDepth(maxDepth).setNumTrees(numTrees)\n        .setFeaturesCol(\"inquiryfeatures\").setLabelCol(\"replypca2\"))\n    val pca2Model = pca2Trainer.fit(train).setPredictionCol(\"prediction2\")\n    \n    // Validate PCA0 only on train\n    val validated = pca0Model.transform(train))\n    validated.select(\"inquiryfiltereds\", \"inquiryfeatures\", \"replyfiltereds\", \"replypca0\", \"prediction0\").show()\n    // Validate on train\n    var pivot = train.filter($\"inquirymain\".startsWith(\" min knp sinyal\")).select(\"inquirymain\", \"replypca\").head()\n    val trainV = pivot.getAs[Vector](1) \n    val assembler = new VectorAssembler().setInputCols(Array(\"prediction0\", \"prediction1\", \"prediction2\")).setOutputCol(\"prediction\")\n    val distUdf = udf( (a: Vector, b: Vector) => Vectors.sqdist(a, b) )\n    val distpivotUdf = udf( (a: Vector) => Vectors.sqdist(a, trainV) )\n    val validated = assembler.transform(pca2Model.transform(pca1Model.transform(pca0Model.transform(train)))).withColumn(\"dist\", distUdf($\"replypca\", $\"prediction\")).withColumn(\"distpivot\", distpivotUdf($\"prediction\"))\n    validated.select(\"inquiryfiltereds\", \"replyfiltereds\", \"replypca\", \"prediction\", \"dist\", \"distpivot\").show()\n    \n    validated.select(\"inquiryfiltereds\", \"replyfiltereds\", \"replypca0\", \"prediction0\", \"replypca1\", \"prediction1\", \"replypca2\", \"prediction2\").show()\n    // Test!\n    val predicted = pca2Model.transform(pca1Model.transform(pca0Model.transform(test)))\n    predicted.select(\"inquiryfiltereds\", \"replyfiltereds\", \"replypca0\", \"prediction0\", \"replypca1\", \"prediction1\", \"replypca2\", \"prediction2\").show()\n\n## Training the Model\n\nSee: https://databricks.com/blog/2015/12/02/databricks-and-h2o-make-it-rain-with-sparkling-water.html\n\n1.\n\n        import org.apache.spark.ml.classification._\n        val layers = Array[Int](1000, 1, labelIndexer.labels.size)\n        val trainer = (new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setMaxIter(100)\n            .setFeaturesCol(\"inquiryfeatures\").setLabelCol(\"replyid\"))\n        val model = trainer.fit(featuredf4)\n\n## Testing the Trained Model\n\n1.\n        val test = featuredf4.limit(10)\n        model.transform(test).select(\"inquiryfiltereds\", \"replylabel\", \"replyid\", \"prediction\").show()\n        \n## Predicting\n\nTODO\n\n", 
  "id": 54453120
}