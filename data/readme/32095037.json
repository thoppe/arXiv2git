{
  "read_at": 1462543844, 
  "description": "The Deep Convolutional Inverse Graphics Network", 
  "readme.md": "# Deep Convolutional Inverse Graphics Network\n\nThis repository contains the code for the network described in http://arxiv.org/abs/1503.03167.\n\n<!-- [![A DC-IGN lighting demo](http://i.imgur.com/ukoMSxt.gif)](http://www.youtube.com/watch?v=FpuhUaugAP0) -->\n\n#### Use Cases: \n- Unsupervised Feature Learning\n- Neural 3D graphics engine: Given a static face image, our model can re-render (hallucinate) the face with arbitrary light and viewpoint transformations. Below is a sample movie generated by our model from a single face photograph -- this is achieved by varying the light neuron and obtaining the image frame prediction at each time step. Same can be done for pose variations (see paper or project website)\n\n![A DC-IGN lighting demo](http://i.imgur.com/ukoMSxt.gif)\n\n<!-- Click for the full video. -->\nProject Website: http://willwhitney.github.io/dc-ign/www/\n\n## Citation \n```\n@article{kulkarni2015deep,\n  title={Deep Convolutional Inverse Graphics Network},\n  author={Kulkarni, Tejas D and Whitney, Will and Kohli, Pushmeet and Tenenbaum, Joshua B},\n  journal={arXiv preprint arXiv:1503.03167},\n  year={2015}\n}\n```\n\n## Running\n\n### Requirements\n- A CUDA-capable GPU\n- [Torch7](http://torch.ch/)\n- The [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)\n- [cuDNN](https://developer.nvidia.com/cuDNN): NVidia's NN library\n- [cudnn.torch](https://github.com/soumith/cudnn.torch): Torch bindings to cuDNN.\n\nFacebook has some great instructions for installing these over at https://github.com/facebook/fbcunn/blob/master/INSTALL.md\n\n### Instructions\n\n**Dataset and pre-trained network**: The train/test dataset can be downloaded here:\nhttps://www.dropbox.com/sh/zuyiuyehs6j5tin/AAALfTtR2Zlu5oFm2i63Rp3za?dl=0\n\nA pretrained network is also available if you just want to see the results: https://www.dropbox.com/s/brv92isfvd7o67k/pretrained_network.zip?dl=0\n\nLet us know if you run into trouble with anything!\n\n#### Training a network with separated pose/light/shape etc (disentangled representations)\n\n1. `git clone` this repo\n1. [Download the dataset](https://www.dropbox.com/sh/zuyiuyehs6j5tin/AAALfTtR2Zlu5oFm2i63Rp3za?dl=0) and unzip it.\n1. Grab a coffee while you wait for that to happen. It's pretty big.\n1. Run `th monovariant_main.lua --help` to see the available options.\n1. To train from scratch: \n    1. run something like `th monovariant_main.lua --no_load --name my_first_dcign --datasetdir <path_to_dataset>`\n    1. [The network will save itself to `networks/<name>` after each epoch]\n    1. After a couple of epochs, open up `visualize_networks.lua` and set `network_search_str` to your network's name. Then you can run `th visualize_networks.lua` and it will create a folder called `renderings` with some visualizations of the kinds of faces your network generates.\n1. To use a pretrained network:\n    1. [Download the pretrained network](https://www.dropbox.com/s/brv92isfvd7o67k/pretrained_network.zip?dl=0) and unzip it.\n    2. More coffee while you wait.\n    3. Run a command like `th monovariant_main.lua --import <path/to/unzipped/network/dir> --name my_first_dcign --datasetdir <path_to_dataset>` that imports the directory of that pretrained net.\n    4. Or, just do the `visualize_networks` thing from above with the pretrained network to see what it makes.\n\n#### Training a network with undifferentiated latents\n\nInstructions coming soon, but if you're not afraid of code that hasn't been cleaned up yet, check out `main.lua`.\n\n<!-- - main.lua can be used to train the network in a fully unsupervised way and monovariant_main.lua can be used to train the network with separated pose/light/shape etc (disentangled representations). We found that pre-training the network with main.lua followed by monovariant_main.lua gives better results. For details about the different training schemes, please refer to the paper. -->\n\n\n## Paper abstract\nThis paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose, light, texture, shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model's efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.\n\n#### Acknowledgements\nA big shout-out to all the Torch developers. Torch is simply awesome. We thank Thomas Vetter for giving us access to the Basel face model. T. Kulkarni was graciously supported by the Leventhal Fellowship. This research was supported by ONR award N000141310333, ARO MURI W911NF-13-1-2012 and CBMM. We would also like to thank (y0ast) https://github.com/y0ast for making the variational autoencoder code available online. \n", 
  "id": 32095037
}