{
  "read_at": 1462550384, 
  "description": "A spark package to approximate the diameter of large graphs", 
  "README.md": "Graph diameter approximation on Spark\n=====================================\n\n[![Build Status](https://travis-ci.org/Cecca/graphx-diameter.svg)](https://travis-ci.org/Cecca/graphx-diameter)\n\n`graphx-diameter` is a Spark package that allows to approximate the\ndiameter of (weighted) graphs, that is the longest shortest path.\n\nThe algorithm implemented here is derived from the ones described in\nthe following papers\n\n - _Space and Time Efficient Parallel Graph Decomposition, Clustering,\n   and Diameter Approximation_ <br />\n   Matteo Ceccarello, Andrea Pietracaprina, Geppino Pucci, and Eli\n   Upfal <br />\n   [SPAA 2015](http://dx.doi.org/10.1145/2755573.2755591)\n   &middot;\n   [bibtex file](https://gist.githubusercontent.com/Cecca/0c1cabb1ed9e2bed396b/raw/d84c4177c73d11c110add5f47a57281b8b8259a0/CeccarelloPPU15-unweighted.bib)\n\n - _A Practical Parallel Algorithm for Diameter Approximation of\n   Massive Weighted Graphs_ <br />\n   Matteo Ceccarello, Andrea Pietracaprina, Geppino Pucci, and Eli\n   Upfal <br />\n   [Arxiv preprint](http://arxiv.org/abs/1506.03265)\n   &middot;\n   [bibtex file](https://gist.githubusercontent.com/Cecca/1dee801b2ac968cc4809/raw/e99b75bda9b945ef3840524e15af4a3eca3b9f4a/CeccarelloPPU15-weighted.bib)\n\nIf you use this software in your research, please cite the aforementioned\npapers.\n\n**NOTE**: the implementation contained in this package is *not* the\n  one used to perform the experiments described in the aforementioned\n  papers. That implementation is available under the GPL license\n  [here](http://crono.dei.unipd.it/gradias/), and was developed on\n  plain Spark. `graphx-diameter`, instead, provides an equivalent\n  implementation compatible with `graphx`.\n  \n\nMotivation\n----------\n\nThe diameter of a graph can be obtained by computing all pairs\nshortest paths. However, computing APSP is impractical on large\ngraphs, due to the excessive space and time requirements.\n\nTo compute an approximation to the diameter using only linear space,\none can resort to a simple Single Source Shortest Path (SSSP)\ncomputation, that approximates the diameter within a factor of\ntwo. The drawback of this approach is that it requires a number of\nrounds linear in the diameter itself: on a platform such as Spark,\nwhere for efficiency we seek to minimize the number of rounds, this\nis undesirable.\n\nA popular approach to approximate the diameter (and some centrality\nmeasures) are algorithms based on probabilistic counters, like\n[HyperANF](http://dl.acm.org/citation.cfm?doid=1963405.1963493)\n[Boldi, Rosa, Vigna - WWW11]. These\nalgorithms are able to attain a tight estimate of the\ndiameter. However, in a distributed computing setting like Spark, the\nrunning time linear in the diameter and the superlinear space\nrequirements limit the applicability of this approach.\n\nTherefore, we developed the algorithm implemented in this library with\ntwo goals:\n\n - performing a number of rounds sublinear in the diameter\n - using space linear in the size of the graph\n\nThe algorithm returns an approximation of the diameter in the form of\nan upper bound, with a provable polylogarithmic bound. In practice,\nthe approximation factor is usually a small constant.\n\nFurther details on the algorithm, its efficiency, and the\napproximation factor are given in the aforementioned papers:\n\n - For the unweighted case: [_\"Space and Time Efficient Parallel Graph\n   Decomposition, Clustering, and Diameter\n   Approximation\"_](http://dx.doi.org/10.1145/2755573.2755591);\n\n - For the weighted case: [_\"A Practical Parallel Algorithm for\n   Diameter Approximation of Massive Weighted\n   Graphs\"_](http://arxiv.org/abs/1506.03265).\n\t \n\nLinking\n-------\n\n`graphx-diameter` is cross compiled for both Scala 2.10 and 2.11. You\ncan include it in your project in several ways.\n\n### spark-shell, pyspark, or spark-submit\n\nThe suffix `_2.xx` appended to the package name must match the Scala\nversion that is run by the `spark-shell` command.\n\n```\n$ $SPARK_HOME/bin/spark-shell --packages it.unipd.dei:graphx-diameter_2.10:0.1.0\n```\n\n### sbt\n\nIf you use the\n[sbt-spark-package](http://github.com/databricks/sbt-spark-package)\nplugin then add the following line to your `build.sbt`\n\n```scala\nspDependencies += \"Cecca/graphx-diameter:0.1.0\"\n```\n\nOtherwise, you can add `graphx-diameter` as a normal dependency\n\n```scala\nlibraryDependencies += \"it.unipd.dei\" %% \"graphx-diameter\" % \"0.1.0\"\n```\n\n### Maven\n\nAgain, be sure that the `_2.xx` suffix matches the Scala version you\nalready use in your project.\n\n```xml\n<dependencies>\n  <!-- list of dependencies -->\n  <dependency>\n    <groupId>it.unipd.dei</groupId>\n    <artifactId>graphx-diameter_2.11</artifactId>\n    <version>0.1.0</version>\n  </dependency>\n</dependencies>\n```\n\nUsage\n-----\n\nThe library works on graphs with `Double` edge weights assigned to\nedges. The package `it.unipd.dei.graphx.diameter` defines a type\n`Distance` like the following\n\n```scala\ntype Distance = Double\n```\n\nThe algorithm takes two parameters, namely\n\n - `target`: this is the size of the quotient graph that will be built\n   by the underlying clustering algorithm. It depends on the size of\n   the local memory of the machines. The last step of the algorithm\n   computes the diameter of a graph of size `target`. Higher values of\n   `target` can result is shorter running times, whereas smaller ones\n   require less memory. Usually `target == 4000` provides a good\n   compromise, and this is the default.\n\n - `delta`: this parameter, representing a distance,\n   controls the number of nodes and edges that can be active in each\n   step of the algorithm. Intuitively, higher values will result in\n   fewer but slower rounds; smaller values will perform more shorter\n   rounds. In any case, this parameter is taken as a hint by the\n   algorithm, that then auto-tunes itself. A good initial guess is\n   (empirically) the average edge weight, which is the default.\n\nFor more details on these two parameters, we refer to the companion\npapers.\n\nGiven a `org.apache.spark.graphx.Graph[V, Distance]` object, you can\nget an approximation to its diameter as follows, using implicit\nconversions\n\n```scala\n// import implicit conversions\nimport it.unipd.dei.graphx.diameter.DiameterApproximation._\n\nval g = // ... build the graph object ...\n\n// Compute the approximation using the default parameters\ng.diameterApprox()\n\n// Specify the target size for the underlying clustering algorithm\ng.diameterApprox(target=5000)\n\n// Control the number of active nodes/edges in each step\ng.diameterApprox(delta=0.5)\n\n// Both parameters can be specified simultaneously\ng.diameterApprox(target=5000, delta=0.5)\n```\n\nIf you prefer to avoid implicit conversions, you can explicitly invoke\n`DiameterApproximation.run`, as follows\n\n```scala\nimport it.unipd.dei.graphx.diameter.DiameterApproximation\n\nval g = // ... build the graph object ...\n\n// Compute the approximation using the default parameters\nDiameterApproximation.run(g)\n\n// Specify the target size for the underlying clustering algorithm\nDiameterApproximation.run(g, target=5000)\n\n// Control the number of active nodes/edges in each step\nDiameterApproximation.run(g, delta=0.5)\n\n// Both parameters can be specified simultaneously\nDiameterApproximation.run(g, target=5000, delta=0.5)\n```\n", 
  "id": 47253163
}