{
  "read_at": 1462546830, 
  "description": "The FastPFOR C++ library: Fast integer compression", 
  "README.md": "# The FastPFOR C++ library : Fast integer compression\n[![Build Status](https://travis-ci.org/lemire/FastPFor.png)](https://travis-ci.org/lemire/FastPFor)\n\nby Daniel Lemire, Leonid Boytsov, Owen Kaser, Maxime Caron, Louis Dionne, Michel Lemay, Erik Kruus, Andrea Bedini, Matthias Petri, Robson Braga Araujo \n\n## What is this?\n\nA research library with integer compression schemes.\nIt is broadly applicable to the compression of arrays of\n32-bit integers where most integers are small.\nThe library seeks to exploit SIMD instructions (SSE)\nwhenever possible.\n\nThis library can decode at least 4 billions of compressed integers per second on most\ndesktop or laptop processors. That is, it can decompress data at a rate of 15 GB/s.\nThis is significantly faster than generic codecs like gzip, LZO, Snappy or LZ4.\n\nIt is used by the zsearch engine (http://victorparmar.github.com/zsearch/)\nas well as in GMAP and GSNAP (http://research-pub.gene.com/gmap/). It\nhas been ported to Java (https://github.com/lemire/JavaFastPFOR),\nC# (https://github.com/Genbox/CSharpFastPFOR)  and \nGo (https://github.com/reducedb/encoding). The Java port is used by\nClueWeb Tools (https://github.com/lintool/clueweb).\n\nApache Lucene version 4.6.x uses a compression format derived from our FastPFOR\nscheme (see http://lucene.apache.org/core/4_6_1/core/org/apache/lucene/util/PForDeltaDocIdSet.html).\n\n## Myths\n\nMyth: SIMD compression requires very large blocks of integers (1024 or more).\n\nFact: This is not true. Our fastest scheme (SIMDBinaryPacking) works over blocks of 128 integers.\nAnother very fast scheme (StreamVByte) works over blocks of four integers.\n\nMyth: SIMD compression means high speed but less compression.\n\nFact: This is wrong. Some schemes cannot easily be accelerated\nwith SIMD instructions, but many that do compress very well.\n\n## Working with sorted lists of integers\n\nIf you are working primarily with sorted lists of integers, then \nyou might want to use differential coding. That is you may want to\ncompress the deltas instead of the integers themselves. The current \nlibrary (fastpfor) is generic and was not optimized for this purpose.\nHowever, we have another library designed to compress sorted integer\nlists: \n\nhttps://github.com/lemire/SIMDCompressionAndIntersection\n\nThis other library (SIMDCompressionAndIntersection) also comes complete\nwith new SIMD-based intersection algorithms.\n\nThere is also a C library for differential coding (fast computation of\ndeltas, and recovery from deltas): \n\nhttps://github.com/lemire/FastDifferentialCoding\n\n# For a simple C library\n\nFastPFOR is a C++ research library. For something simpler,\nwritten in C, see:\n\nhttps://github.com/lemire/simdcomp\n\nor \n\nhttps://github.com/lemire/streamvbyte\n\nor \n\nhttps://github.com/lemire/LittleIntPacker\n\nor\n\nhttps://github.com/lemire/MaskedVByte\n\nWhich library is best depends on your needs.\n\n## Other recommended libraries\n\n* libvbyte: A fast implementation for varbyte 32bit/64bit integer compression https://github.com/cruppstahl/libvbyte\n* TurboPFor is a C library that offers lots of interesting optimizations. Well worth checking! (GPL license) https://github.com/powturbo/TurboPFor\n* Oroch is a C++ library that offers a usable API (MIT license) https://github.com/ademakov/Oroch\n\n## Reference and documentation\n\nFor a simple example, please see \n\nexample.cpp \n\nin the root directory of this project.\n\nPlease see:\n\n* Daniel Lemire and Leonid Boytsov, Decoding billions of integers per second through vectorization, Software Practice & Experience 45 (1), 2015.  http://arxiv.org/abs/1209.2137 http://onlinelibrary.wiley.com/doi/10.1002/spe.2203/abstract\n* Daniel Lemire, Leonid Boytsov, Nathan Kurz, SIMD Compression and the Intersection of Sorted Integers, Software Practice & Experience 46 (6), 2016 http://arxiv.org/abs/1401.6399\n* Jeff Plaisance, Nathan Kurz, Daniel Lemire, Vectorized VByte Decoding, International Symposium on Web Algorithms 2015, 2015. http://arxiv.org/abs/1503.07387\n* Wayne Xin Zhao, Xudong Zhang, Daniel Lemire, Dongdong Shan, Jian-Yun Nie, Hongfei Yan, Ji-Rong Wen, A General SIMD-based Approach to Accelerating Compression Algorithms, ACM Transactions on Information Systems 33 (3), 2015. http://arxiv.org/abs/1502.01916\n\n\nThis library was used by several papers including the following:\n\n* G. Ottaviano, R. Venturini, Partitioned Elias-Fano Indexes, ACM SIGIR 2014 http://www.di.unipi.it/~ottavian/files/elias_fano_sigir14.pdf\n* M. Petri, A. Moffat, J. S. Culpepper, Score-Safe Term Dependency Processing With Hybrid Indexes, ACM SIGIR 2014 http://www.culpepper.io/publications/sp074-petri.pdf\n\nIt has also inspired related work such as...\n\n* T. D. Wu, Bitpacking techniques for indexing genomes: I. Hash tables, Algorithms for Molecular Biology 11 (5), 2016. http://almob.biomedcentral.com/articles/10.1186/s13015-016-0069-5\n\n## License\n\nThis code is licensed under Apache License, Version 2.0 (ASL2.0).\n\n## Software Requirements\n\nThis code requires a compiler supporting C++11. This was\na design decision.\n\nIt builds under \n\n*  clang++ 3.2 (LLVM 3.2) or better,\n*  Intel icpc (ICC) 13.0.1 or better,\n*  MinGW32 (x64-4.8.1-posix-seh-rev5)\n*  Microsoft VS 2012 or better,\n* and GNU GCC 4.7 or better.\n\nThe code was tested under Windows, Linux and MacOS.\n\nThe build system expects an x64 operating system. Under Linux and MacOS, typing `uname -a` in the console should print the `x86_64` string. If you have a 32-bit system, you may need to do extra work to build and run this code. Please use a 64-bit system instead.\n\n\n## Hardware Requirements\n\nWe require an x64 platform.\n\nTo fully use the library, your processor should support SSSE3. This includes almost every Intel or AMD processor\nsold after 2006. (Note: the key schemes require merely SSE2.) \n\nSome specific binaries will only run if your processor supports SSE4.1. They have been purely used for specific tests however.\n\n## Building with CMake\n\nYou need cmake. On most linux distributions, you can simply do the following:\n\n      cmake .\n      make\n\nIt may be necessary to set the CXX variable.\n\nTo create project files for Microsoft Visual Studio, it might be useful to target 64-bit Windows (e.g., see http://www.cmake.org/cmake/help/v3.0/generator/Visual%20Studio%2012%202013.html).\n\n### Multithreaded context\n\nYou should not assume that our objects are thread safe.\nIf you have several threads, each thread should have its own IntegerCODEC\nobjects to ensure that there is no concurrency problems.\n\n\n### Installing GCC 4.8 under Linux\n\nWe support clang, Visual Studio and the Intel compiler, but a common default is GCC 4.8 or better.\n\nUnder a recent version of Ubuntu (12.14), you can install\nGCC 4.8 by typing:\n\n    sudo apt-get install gcc-4.8 g++-4.8\n\n### Installing GCC 4.8 under Mac\n\nMac Ports supports GCC 4.8. You can install it by typing:\n\n    sudo port install gcc48\n\nSee : https://www.macports.org/\n\n## Why C++11?\n\nWith minor changes, all schemes will compile fine under\ncompilers that do not support C++11. And porting the code\nto C should not be a challenge.\n\nIn any case, we already support 3 major C++ compilers so portability\nis not a major issue.\n\n## What if I prefer Java?\n\nMany schemes cannot be efficiently ported to Java. However\nsome have been. Please see:\n\nhttps://github.com/lemire/JavaFastPFOR\n\n## What if I prefer C#?\n\nSee CSharpFastPFOR: A C#  integer compression library  https://github.com/Genbox/CSharpFastPFOR\n\n## What if I prefer Go?\n\nSee  Encoding: Integer Compression Libraries for Go https://github.com/zhenjl/encoding\n\n## Testing\n\nIf you used CMake to generate the build files, the `check` target will\nrun the unit tests. For example , if you generated Unix Makefiles\n\n    make check\n\nwill do it. \n\n## Simple benchmark\n\n    make codecs\n    ./codecs --clusterdynamic\n    ./codecs --uniformdynamic\n\n## Optional : Snappy\n\nTyping \"make allallall\" will install some testing binaries that depend\non Google Snappy. If you want to build these, you need to install\nGoogle snappy. You can do so on a recent ubuntu machine as:\n\n    sudo apt-get install libsnappy-dev\n\n## Processing data files\n\nTyping \"make\" will generate an \"inmemorybenchmark\"\nexecutable that can process data files.\n\nYou can use it to process arrays on (sorted!) integers\non disk using the following 32-bit format: 1 unsigned 32-bit\ninteger  indicating array length followed by the corresponding\nnumber of 32-bit integer. Repeat.\n\n ( It is assumed that the integers are sorted.)\n\n\nOnce you have such a binary file somefilename you can\nprocess it with our inmemorybenchmark:\n\n    ./inmemorybenchmark --minlength 10000 somefilename\n\nThe \"minlength\" flag skips short arrays. (Warning: timings over\nshort arrays are unreliable.)\n\n\n## Testing with the Gov2 and ClueWeb09 data sets\n\nAs of April 2014, we recommend getting our archive at\n\nhttp://lemire.me/data/integercompression2014.html\n\nIt is the data was used for the following paper:\n\nDaniel Lemire, Leonid Boytsov, Nathan Kurz, SIMD Compression and the Intersection of Sorted Integers, arXiv: 1401.6399, 2014\nhttp://arxiv.org/abs/1401.6399\n\n## Testing with the ClueWeb09 data set (legacy)\n\n(Please consider grabbing our new archive at \nhttp://lemire.me/data/integercompression2014.html\ninstead.)\n\n\nGrab the data set from:\n\nhttp://boytsov.info/datasets/clueweb09gap/\n\nUsing the provided software, uncompress it and\nrun the \"toflat\" executable to create a \"clueweb09.bin\" file\nthen run:\n\n    ./inmemorybenchmark --minlength 10000 clueweb09.bin\n\nNote: processing can take over an hour.\n\n## Testing with the Gov2 data set  (legacy)\n\n(Please consider grabbing our new archive at \nhttp://lemire.me/data/integercompression2014.html\ninstead.)\n\nYou can test the library over d-gaps data\nfrom the TREC GOV2 data set that was made graciously\navailable by   Fabrizio Silvestri,  Takeshi Yamamuro\nand Rossano Venturini.\n\nGo to:\n\nhttp://integerencoding.isti.cnr.it/?page_id=8\n\nDownload both the software and the gov.sort.tar file.\nYou might find useful to grow their stable-0.2.0 version from\nhttps://github.com/maropu/integer_encoding_library/releases/tag/stable-0.2.0\n\nUntar the tar file:\n\n    tar xvf gov2.sort.tar\n\nYou may want to make the newly generated files non-writeable\n(I'm paranoid):\n\n    chmod -w gov2.sort.Delta gov2.sort.Delta.TOC\n\nBuild the software (you need the decoders executable)\nand\n\nYou need to run this command\n\n    ./decoders 3 gov2.sort somefilename\n\nwhere \"3\" is for delta.\n\nThen you should be able to test with out inmemorybenchmark:\n\n    ./inmemorybenchmark --minlength 10000 somefilename.DEC\n\nNote: processing can take over an hour.\n\n\n## I used your code and I get segmentation faults\n\nOur code is thoroughly tested.\n\nOne common issue is that people do not provide large enough buffers.\nSome schemes can have such small compression rates that the compressed data\ngenerated will be much larger than the input data.\n\nAlso, make sure that all provided buffers are 16-byte aligned or else,\nsome SSE instructions may fail.\n\n## Is any of this code subject to patents?\n\nI (D. Lemire) did not patent anything.\n\nHowever, we implemented varint-G8UI which was patented by its authors. \nDO NOT use varint-G8UI if you want to avoid patents.\n\nThe rest of the library *should be* patent-free.\n\n## Funding \n\nThis work was supported by NSERC grant number 26143.\n", 
  "id": 4797147
}