{
  "read_at": 1462548658, 
  "description": "Question answering dataset featured in \"Teaching Machines to Read and Comprehend", 
  "README.md": "# Question Answering Corpus\n\nThis repository contains a script to generate question/answer pairs using\nCNN and Daily Mail articles downloaded from the Wayback Machine.\n\nFor a detailed description of this corpus please read:\n[Teaching Machines to Read and Comprehend][arxiv], Hermann et al., NIPS 2015.\nPlease cite the paper if you use this corpus in your work.\n\n### Bibtex\n\n```\n@inproceedings{nips15_hermann,\nauthor = {Karl Moritz Hermann and Tom\\'a\\v{s} Ko\\v{c}isk\\'y and Edward Grefenstette and Lasse Espeholt and Will Kay and Mustafa Suleyman and Phil Blunsom},\ntitle = {Teaching Machines to Read and Comprehend},\nurl = {http://arxiv.org/abs/1506.03340},\nbooktitle = \"Advances in Neural Information Processing Systems (NIPS)\",\nyear = \"2015\",\n}\n```\n\n## Download Processed Version\n\nIn case the script does not work you can also download the processed data sets from [http://cs.nyu.edu/~kcho/DMQA/]. This should help in situations where the underlying data is not accessible (Wayback Machine partially down).\n\n## Running the Script\n\n### Prerequisites\n\nPython 2.7, `wget`, `libxml2`, `libxslt`, `python-dev` and `virtualenv`. `libxml2` must be version 2.9.1. \nYou can install `libxslt` from here: [http://xmlsoft.org/libxslt/downloads.html](http://xmlsoft.org/libxslt/downloads.html)\n\n```\nsudo pip install virtualenv\nsudo apt-get install python-dev\n```\n\n### Download Script\n\n```\nmkdir rc-data\ncd rc-data\nwget https://github.com/deepmind/rc-data/raw/master/generate_questions.py\n```\n\n### Download and Extract Metadata\n\n```\nwget https://storage.googleapis.com/deepmind-data/20150824/data.tar.gz -O - | tar -xz --strip-components=1\n```\n\nThe news article metadata is ~1 GB.\n\n### Enter Virtual Environment and Install Packages\n\n```\nvirtualenv venv\nsource venv/bin/activate\nwget https://github.com/deepmind/rc-data/raw/master/requirements.txt\npip install -r requirements.txt\n```\n\nYou may need to install `libxml2` development packages to install `lxml`:\n\n```\nsudo apt-get install libxml2-dev libxslt-dev\n```\n\n### Download URLs\n\n```\npython generate_questions.py --corpus=[cnn/dailymail] --mode=download\n```\n\nThis will download news articles from the Wayback Machine. Some URLs may be\nunavailable. The script can be run again and will cache\nURLs that already have been downloaded. Generation of questions can run\nwithout all URLs downloaded successfully.\n\n### Generate Questions\n\n```\npython generate_questions.py --corpus=[cnn/dailymail] --mode=generate\n```\n\nNote, this will generate ~1,000,000 small files for the Daily Mail so an SSD is\npreferred.\n\nQuestions are stored in [cnn/dailymail]/questions/ in the following format:\n\n```\n[URL]\n\n[Context]\n\n[Question]\n\n[Answer]\n\n[Entity mapping]\n```\n\n### Deactivate Virtual Environment\n\n```\ndeactivate\n```\n\n### Verifying Test Sets\n\n```\nwget https://github.com/deepmind/rc-data/raw/master/expected_[cnn/dailymail]_test.txt\ncomm -3 <(cat expected_[cnn/dailymail]_test.txt) <(ls [cnn/dailymail]/questions/test/)\n```\n\nThe filenames of the questions in the first column are missing generated questions. No output means everything is downloaded and generated correctly.\n\n[arxiv]: http://arxiv.org/abs/1506.03340\n", 
  "id": 43331133
}