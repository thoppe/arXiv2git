{
  "read_at": 1462543285, 
  "description": "Compressing next-generation sequencing data with extreme prejudice.", 
  "README.md": "![Quip](http://cs.washington.edu/homes/dcjones/quip/logo.png)\n\nQuip compresses next-generation sequencing data with extreme prejudice. It\nsupports input and output in the [FASTQ](http://en.wikipedia.org/wiki/Fastq)\nand [SAM/BAM](http://samtools.sourceforge.net/) formats, compressing large\ndatasets to as little as 15% of their original size.\n\nInstallation\n============\n\nBinaries\n--------\n\nIn the future will will provide binaries for a number of operating systems. In\nthe mean time, you must install from source.\n\n\nSource from GitHub\n------------------\n\nTo install from github, you will need a C compiler, as well as a relatively recent\nversion of [automake](http://www.gnu.org/software/automake/) and\n[autoconf](http://www.gnu.org/software/autoconf/).\n\nFirst clone the git repository with,\n\n    git clone git://github.com/dcjones/quip.git\n\nEnter the quip directory\n\n    cd quip\n\nGenerate the configure script using\n\n    autoreconf -i\n\nThen configure, compile, and install\n\n    ./configure && make install\n\n\nSource from a tarball\n---------------------\n\nYou will need only a C compiler to install from a source tarball.\n\nExtract the source tarball\n\n    tar xzf quip-1.1.0.tar.gz\n\nEnter the quip directory\n\n    cd quip-1.1.0\n\nThen configure, compile, and install\n\n    ./configure && make install\n\nBy default with install to `/usr/local/`, choose another prefix (e.g., your home\ndirectory) with `./configure --prefix=$HOME`.\n\n\nUsage\n=====\n\nQuip mostly works the same as `gzip` or `bzip2`. For example, if you do something like,\n\n    quip reads.fastq\n\nYou will get a file called `reads.fastq.qp` that is significantly smaller than\nthe original. Note that unlike `gzip` or `bzip2`, this will not delete the\noriginal file.\n\nFor more details, see `man quip` after installing.\n\n\nAlgorithms\n==========\n\nQuip implements an ensemble of compression techniques specially built to\ncompress sequencing reads as much as possible. The basis of the algorithm is\nstatistical compression of read identifiers, quality scores, and nucleotide\nsequences using arithmetic coding. In addition, we implement reference-based\ncompression in which aligned reads are stored a positions within a genome.\nAnd, when no reference genome is available, an extremely efficient de novo\nassembly algorithm can transparently construct one.\n\nIn the following sections we give specific details. \n\n\nRead IDs\n--------\n\nThe only requirement of read identifiers is that they uniquely identify the\nread. A single integer would do, but typically each read comes with a complex\nstring containing the instrument name, run identifier, flow cell identifier,\nand tile coordinates. Much of this information is the same for every read and\nis simply repeated, inflating the file size.\n\nTo remove this redundancy, we use a form of delta encoding. A  parser\ntokenizes the ID into separate fields which are then compared to the previous\nID. Tokens that remain the same from read to read (e.g. instrument name)\ncan be compressed to a negligible amount of space --- arithmetic coding produces\ncodes of less than 1 bit in such cases. Numerical tokens are recognized and\nstored efficiently, either directly or as an offset from the token in the same\nposition in previous read. Otherwise non-identical tokens are encoded by\nmatching as much of the prefix as possible to the previous read's token before\ndirectly encoding the non-matching suffix.\n\nThe end result is that read IDs, which are often 50 bytes or longer, are\ntypically stored in 2-4 bytes. Notably, in reads produced from Illumina\ninstruments, most parts of the ID can be compressed to consume almost no\nspace; the remaining few bytes are accounted for by tile coordinates. These\ncoordinates are almost never needed in downstream analysis, so removing them\nas a preprocessing step would shrink file sizes even further. The parser used\nis suitably general so that no change to the compression algorithm would be\nneeded.\n\nNucleotide Sequences\n--------------------\n\nTo compress nucleotide sequences, we adopt a very simple model based on high-order\nMarkov chains. The nucleotide at a given position in a read is predicted\nusing the preceding twelve positions. This model uses more memory than\ntraditional general-purpose compression algorithms (4^13 = 67,108,864\nparameters are needed, each represented in 32 bits)  but it is simple and\nextremely efficient (very little computation is required and run time is\nlimited primarily by memory latency, as lookups in such a large table result\nin frequent cache misses).\n\nAn order-12 Markov chain also requires a very large amount of\ndata to train, but there is no shortage with the datasets we wish to\ncompress. Though less adept at compressing extremely short files,\nafter compressing several million reads the parameters are tightly\nfit to the nucleotide composition of the dataset so that the remaining reads\nwill be highly compressed. Compressing larger files only results in a tighter\nfit and higher compression.\n\n\nQuality Scores\n--------------\n\nIt has been previously noted that the quality score at a given position is\nhighly correlated with the score at the preceding position (Kozanitis 2011).\nThis makes a Markov chain a natural model, but unlike\nnucleotides, quality scores are over a much larger alphabet (typically 41--46\ndistinct scores). This limits the order of the Markov chain: long chains will\nrequire a great deal of space and take a unrealistic amount of data to train.\n\nTo reduce the number of parameters, we use an order-3 Markov chain, but\ncoarsely bin the distal two positions. In addition to the preceding three\npositions, we condition on the position within the read and a running count of\nthe number large jumps in quality scores between adjacent positions (where\na ``large jump'' is defined as `|q_{i} - q_{i-1}| > 1`), which allows reads\nwith highly variable quality scores to be encoded using separate models. Both\nof these variables are binned to control the number of parameters.\n\n\nReference-based Compression\n---------------------------\n\nWe have also implemented lossless reference-based compression. Given aligned\nreads in SAM or BAM format, and the reference sequence to which they are\naligned (in FASTA format), the reads are compressed preserving all information\nin the SAM/BAM file, including the header, read IDs, alignment information,\nand all optional fields allowed by the SAM format. Unaligned reads are\nretained and compressed using the Markov chain model.\n\nAssembly-based Compression\n--------------------------\n\nTo complement the reference-based approach, we developed an assembly-based\napproach which offers some of the advantages of reference-based compression,\nbut requires no external sequence database and produces files which are\nentirely self-contained. We use the first (by default) 2.5 million reads to\nassemble contigs which are then used in place of a reference sequence\ndatabase to encode aligned reads compactly as positions.\n\nOnce contigs are assembled, read sequences are aligned using a simple ``seed\nand extend'' method: 12-mer seeds are matched using a hash table, and\ncandidate alignments are evaluated using Hamming distance. The best\n(lowest Hamming distance) alignment is chosen, assuming it falls below a given\ncutoff, and the read is encoded as a position within the contig set. Roughly,\nthis can be thought of as a variation on the Lempel-Ziv algorithm: as\nsequences are read, they are matched to previously observed data, or in this\ncase, contigs assembled from previously observed data. These contigs are\nnot explicitly stored, but rather reassembled during decompression.\n\n\nTraditionally, de novo assembly is extremely computationally intensive. The\nmost commonly used technique involves constructing a de Bruijn graph, a\ndirected graph in which each vertex represents a nucleotide k-mer present in\nthe data for some fixed k (e.g., k = 25 is a common choice). A directed\nedge from a k-mer u to v occurs if and only if the (k - 1)-mer suffix\nof u is also the prefix of v. In principle, given such a graph, an\nassembly can be produced by finding an Eulerian path, i.e., a path that\nfollows each edge in the graph exactly once (Pevzner 2001). In practice,\nsince NGS data has a non-negligible error rate, assemblers augment each vertex\nwith the number of observed occurrences of the k-mer and leverage these\ncounts using a variety of heuristics to filter out spurious paths.\n\nA significant bottleneck of the de Bruijn graph approach is building an\nimplicit representation of the graph by counting and storing k-mer\noccurrences in a hash table. The assembler implemented in Quip \novercomes this bottleneck to a large extent by using a data structure based on\nthe Bloom filter to count k-mers. The Bloom filter (Bloom 1970) is a\nprobabilistic data structure that represents a set of elements extremely\ncompactly, at the cost of elements occasionally colliding and incorrectly\nbeing reported as present in the set. It is probabilistic in the sense that\nthese collisions occur pseudo-randomly, determined by the size of the table\nand the hash functions chosen, but generally with low probability.\n\nThe Bloom filter is generalized in the counting Bloom filter, in which an\narbitrary count can be associated with each element (Fan 2000), and\nfurther refined in the d-left counting Bloom filter (dlCBF)\n(Bonomi 2006), which requires significantly less space than the already\nquite space efficient counting Bloom filter. We base our assembler on a\nrealization of the dlCBF. Because we use a probabilistic data structure,\nk-mers are occasionally reported to have incorrect (inflated) counts. The\nassembly can be made less accurate by these incorrect counts, however a poor\nassembly only results in slightly increasing the compression ratio.\nCompression remains lossless regardless of the assembly quality, and in\npractice collisions in the dlCBF occur at a very low rate (this is explored in\nthe results section).\n\nGiven a probabilistic de Bruijn graph, we assemble contigs using a very simple\ngreedy approach. A read sequence is used as a seed and extended on both ends\none nucleotide at a time by repeatedly finding the most abundant k-mer that\noverlaps the end of the contig by k-1 bases. More sophisticated heuristics\nhave been developed, but we choose to focus on efficiency, sacrificing a\ndegree of accuracy.\n\nCounting k-mers efficiently with the help of Bloom filters was previously\nexplored by (Melsted 2011), who use it in addition, rather than\nin place, of a hash table. The Bloom filter is used as a ``staging area'' to\nstore k-mers occurring only once, reducing the memory required by the hash\ntable. Concurrently with our work, (Pell 2011) have also developed a\nprobabilistic de Bruijn graph assembler, but using a non-counting Bloom\nfilter. While they demonstrate a significant reduction in memory use, unlike\nother de Bruijn graph assemblers, only the presence or absence of a k-mer is\nstored, not its abundance, which is essential information when the goal is\nproducing accurate contigs.\n\n\n\nAdditional Features\n-------------------\n\nIn designing the file format used by Quip, we included several useful features\nto protect data integrity. First, output is divided into blocks of several\nmagabytes each. In each block a separate 64 bit checksum is computed for read\nidentifiers, nucleotide sequences, and quality scores. When the archive is\ndecompressed, these checksums are recomputed on the decompressed data and\ncompared to the stored checksums, verifying the correctness of the output. The\nintegrity of an archived dataset can also be checked with the `quip --test`\ncommand.\n\nApart from data corruption, reference-based compression creates the\npossibility of data loss if the reference used for compression is lost, or an\nincorrect reference is used. To protect against this, Quip files store a 64\nbit hash of the reference sequence, ensuring that the same sequence is used\nfor decompression. To assist in locating the correct reference, the file name,\nand the lengths and names of the sequences used in compression are also stored\nand accessible without decompression.\n\nAdditionally, block headers store the number of reads and bases compressed in\nthe block, allowing summary statistics of a dataset to be listed without\ndecompression using the `quip --list` command.\n\n\nReferences\n==========\n\nChristos Kozanitis, Chris Saunders, Semyon Kruglyak, Vineet Bafna, and George\nVarghese. Compressing genomic sequence fragments using SlimGene. Journal of\nComputational Biology : a Journal of Computational Molecular Cell Biology,\n18(3):401-13, March 2011. ISSN 1557-8666. doi: 10.1089/cmb.2010.0253.\n\nP.A.Pevzner, H.Tang, and M.S.Waterman. An Eulerian path approach to DNA fragment\nassembly. Proceedings of the National Academy of Sciences of the United States\nof America, 98(17):9748-53, August 2001. ISSN 0027-8424. doi:\n10.1073/pnas.171285098.\n\nLi Fan, Pei Cao, J. Almeida, and A.Z. Broder. Summary cache: a scalable wide-\narea Web cache sharing protocol. IEEE/ACM Transactions on Networking,\n8(3):281-293, June 2000. ISSN 10636692. doi: 10.1109/90.851975.\n\nFlavio Bonomi, Michael Mitzenmacher, and Rina Panigrahy. An improved\nconstruction for counting Bloom filters. 14th Annual European Symposium on\nAlgorithms, LNCS 4168, pages 684-695, 2006.\n\nPall Melsted and Jonathan K Pritchard. Efficient counting of k-mers in DNA\nsequences using a Bloom Filter. BMC Bioinformatics, 12(1):333, 2011. ISSN\n1471-2105. doi: 10.1186/1471-2105-12-333.\n\nJason Pell, Arend Hintze, R Canino-Koning, and Adina Howe. Scaling metagenome\nsequence assembly with probabilistic de Bruijn graphs. Arxiv preprint arXiv:,\nI(1):1-11, 2011.\n", 
  "id": 2689834
}