{
  "read_at": 1462550354, 
  "description": "", 
  "README.md": "# AI\n\n## Lectures\n### Machine Learning\n#### Andrew Ng, Stanford University, Coursera\n##### Week 1\n###### Introduction\n* Welcome\n  * Welcome to Machine Learning!\n    * https://www.coursera.org/learn/machine-learning/lecture/zcAuT/welcome-to-machine-learning\n* Introduction\n  * Welcome\n    * https://www.coursera.org/learn/machine-learning/lecture/RKFpn/welcome\n  * Supervised Learning\n    * https://www.coursera.org/learn/machine-learning/lecture/1VkCb/supervised-learning\n  * Unsupervised Learning\n    * https://www.coursera.org/learn/machine-learning/lecture/olRZo/unsupervised-learning\n\n###### Linear Regression with One Variable\n* Model and Cost Function\n  * Model Representation\n    * https://www.coursera.org/learn/machine-learning/lecture/db3jS/model-representation\n  * Cost Function\n    * https://www.coursera.org/learn/machine-learning/lecture/rkTp3/cost-function\n  * Cost Function - Intuition I\n    * https://www.coursera.org/learn/machine-learning/lecture/N09c6/cost-function-intuition-i\n  * Cost Function - Intuition II\n    * https://www.coursera.org/learn/machine-learning/lecture/nwpe2/cost-function-intuition-ii\n* Parameter Learning\n  * Gradient Descent\n    * https://www.coursera.org/learn/machine-learning/lecture/8SpIM/gradient-descent\n  * Gradient Descent Intuition\n    * https://www.coursera.org/learn/machine-learning/lecture/GFFPB/gradient-descent-intuition\n  * Gradient Descent For Linear Regression\n    * https://www.coursera.org/learn/machine-learning/lecture/kCvQc/gradient-descent-for-linear-regression\n\n###### Linear Algebra Review\n* Linear Algebra Review\n  * Matrices and Vectors\n    * https://www.coursera.org/learn/machine-learning/lecture/38jIT/matrices-and-vectors\n  * Addition and Scalar Multiplication\n    * https://www.coursera.org/learn/machine-learning/lecture/R4hiJ/addition-and-scalar-multiplication\n  * Matrix Vector Multiplication\n    * https://www.coursera.org/learn/machine-learning/lecture/aQDta/matrix-vector-multiplication\n  * Matrix Matrix Multiplication\n    * https://www.coursera.org/learn/machine-learning/lecture/dpF1j/matrix-matrix-multiplication\n  * Matrix Multiplication Properties\n    * https://www.coursera.org/learn/machine-learning/lecture/W1LNU/matrix-multiplication-properties\n  * Inverse and Transpose\n    * https://www.coursera.org/learn/machine-learning/lecture/FuSWY/inverse-and-transpose\n\n##### Week 2\n###### Linear Regression with Multiple Variables\n* Multivariate Linear Regression\n  * Multiple Features\n    * https://www.coursera.org/learn/machine-learning/lecture/6Nj1q/multiple-features\n  * Gradient Descent for Multiple Variables\n    * https://www.coursera.org/learn/machine-learning/lecture/Z9DKX/gradient-descent-for-multiple-variables\n  * Gradient Descent in Practice I - Feature Scaling\n    * https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling\n  * Gradient Descent in Practice II - Learning Rate\n    * https://www.coursera.org/learn/machine-learning/lecture/3iawu/gradient-descent-in-practice-ii-learning-rate\n  * Features and Polynomial Regression\n    * https://www.coursera.org/learn/machine-learning/lecture/Rqgfz/features-and-polynomial-regression\n* Computing Parameters Analytically\n  * Normal Equation\n    * https://www.coursera.org/learn/machine-learning/lecture/2DKxQ/normal-equation\n\n###### Octave Tutorial\n* Octave Tutorial\n  * Basic Operations\n    * https://www.coursera.org/learn/machine-learning/lecture/9fHfl/basic-operations\n  * Moving Data Around\n    * https://www.coursera.org/learn/machine-learning/lecture/SZJIc/moving-data-around\n  * Computing on Data\n    * https://www.coursera.org/learn/machine-learning/lecture/Y6uuC/computing-on-data\n  * Plotting Data\n    * https://www.coursera.org/learn/machine-learning/lecture/I7gx3/plotting-data\n  * Control Statements: for, while, if statement\n    * https://www.coursera.org/learn/machine-learning/lecture/LRQnl/control-statements-for-while-if-statement\n  * Vectorization\n    * https://www.coursera.org/learn/machine-learning/lecture/WnQWH/vectorization\n  * Normal Equation Noninvertibility\n    * https://www.coursera.org/learn/machine-learning/lecture/zSiE6/normal-equation-noninvertibility\n* Submitting Programming Assignments\n  * Working on and Submitting Programming Assignments\n    * https://www.coursera.org/learn/machine-learning/lecture/Lt2Mx/working-on-and-submitting-programming-assignments\n\n##### Week 3\n###### Logistic Regression\n* Classification and Representation\n  * Classification\n    * https://www.coursera.org/learn/machine-learning/lecture/wlPeP/classification\n  * Hypothesis Representation\n    * https://www.coursera.org/learn/machine-learning/lecture/RJXfB/hypothesis-representation\n  * Decision Boundary\n    * https://www.coursera.org/learn/machine-learning/lecture/WuL1H/decision-boundary\n* Logistic Regression Model\n  * Cost Function\n    * https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function\n  * Simplified Cost Function and Gradient Descent\n    * https://www.coursera.org/learn/machine-learning/lecture/MtEaZ/simplified-cost-function-and-gradient-descent\n  * Advanced Optimization\n    * https://www.coursera.org/learn/machine-learning/lecture/licwf/advanced-optimization\n* Multiclass Classification\n  * Multiclass Classification: One-vs-all\n    * https://www.coursera.org/learn/machine-learning/lecture/68Pol/multiclass-classification-one-vs-all\n\n###### Regularization\n* Solving the Problem of Overfitting\n  * The Problem of Overfitting\n    * https://www.coursera.org/learn/machine-learning/lecture/ACpTQ/the-problem-of-overfitting\n  * Cost Function\n    * https://www.coursera.org/learn/machine-learning/lecture/B1MnL/cost-function\n  * Regularized Linear Regression\n    * https://www.coursera.org/learn/machine-learning/lecture/QrMXd/regularized-linear-regression\n  * Regularized Logistic Regression\n    * https://www.coursera.org/learn/machine-learning/lecture/4BHEy/regularized-logistic-regression\n\n##### Week 4\n###### Neural Networks: Representation\n* Motivations\n  * Non-linear Hypotheses\n    * https://www.coursera.org/learn/machine-learning/lecture/OAOhO/non-linear-hypotheses\n  * Neurons and the Brain\n    * https://www.coursera.org/learn/machine-learning/lecture/IPmzw/neurons-and-the-brain\n* Neural Networks\n  * Model Representation I\n    * https://www.coursera.org/learn/machine-learning/lecture/ka3jK/model-representation-i\n  * Model Representation II\n    * https://www.coursera.org/learn/machine-learning/lecture/Hw3VK/model-representation-ii\n* Applications\n  * Examples and Intuitions I\n    * https://www.coursera.org/learn/machine-learning/lecture/rBZmG/examples-and-intuitions-i\n  * Examples and Intuitions II\n    * https://www.coursera.org/learn/machine-learning/lecture/solUx/examples-and-intuitions-ii\n  * Multiclass Classification\n    * https://www.coursera.org/learn/machine-learning/lecture/gFpiW/multiclass-classification\n\n##### Week 5\n###### Neural Networks: Learning\n* Cost Function and Backpropagation\n  * Cost Function\n    * https://www.coursera.org/learn/machine-learning/lecture/na28E/cost-function\n  * Backpropagation Algorithm\n    * https://www.coursera.org/learn/machine-learning/lecture/1z9WW/backpropagation-algorithm\n  * Backpropagation Intuition\n    * https://www.coursera.org/learn/machine-learning/lecture/du981/backpropagation-intuition\n* Backpropagation in Practice\n  * Implementation Note: Unrolling Parameters\n    * https://www.coursera.org/learn/machine-learning/lecture/60Uxp/implementation-note-unrolling-parameters\n  * Gradient Checking\n    * https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking\n  * Random Initialization\n    * https://www.coursera.org/learn/machine-learning/lecture/ND5G5/random-initialization\n  * Putting It Together\n    * https://www.coursera.org/learn/machine-learning/lecture/Wh6s3/putting-it-together\n* Application of Neural Networks\n  * Autonomous Driving\n    * https://www.coursera.org/learn/machine-learning/lecture/zYS8T/autonomous-driving\n\n##### Week 6\n###### Advice for Applying Machine Learning\n* Evaluating a Learning Algorithm\n  * Deciding What to Try Next\n    * https://www.coursera.org/learn/machine-learning/lecture/OVM4M/deciding-what-to-try-next\n  * Evaluating a Hypothesis\n    * https://www.coursera.org/learn/machine-learning/lecture/yfbJY/evaluating-a-hypothesis\n  * Model Selection and Train/Validation/Test Sets\n    * https://www.coursera.org/learn/machine-learning/lecture/QGKbr/model-selection-and-train-validation-test-sets\n* Bias vs. Variance\n  * Diagnosing Bias vs. Variance\n    * https://www.coursera.org/learn/machine-learning/lecture/yCAup/diagnosing-bias-vs-variance\n  * Regularization and Bias/Variance\n    * https://www.coursera.org/learn/machine-learning/lecture/4VDlf/regularization-and-bias-variance\n  * Learning Curves\n    * https://www.coursera.org/learn/machine-learning/lecture/Kont7/learning-curves\n  * Deciding What to Do Next Revisited\n    * https://www.coursera.org/learn/machine-learning/lecture/zJTzp/deciding-what-to-do-next-revisited\n\n###### Machine Learning System Design\n* Building a Spam Classifier\n  * Prioritizing What to Work On\n    * https://www.coursera.org/learn/machine-learning/lecture/4h5X4/prioritizing-what-to-work-on\n  * Error Analysis\n    * https://www.coursera.org/learn/machine-learning/lecture/x62iE/error-analysis\n* Handling Skewed Data\n  * Error Metrics for Skewed Classes\n    * https://www.coursera.org/learn/machine-learning/lecture/tKMWX/error-metrics-for-skewed-classes\n  * Trading Off Precision and Recall\n    * https://www.coursera.org/learn/machine-learning/lecture/CuONQ/trading-off-precision-and-recall\n* Using Large Data Sets\n  * Data For Machine Learning\n    * https://www.coursera.org/learn/machine-learning/lecture/XcNcz/data-for-machine-learning\n\n##### Week 7\n###### Support Vector Machines\n* Large Margin Classification\n  * Optimization Objective\n    * https://www.coursera.org/learn/machine-learning/lecture/sHfVT/optimization-objective\n  * Large Margin Intuition\n    * https://www.coursera.org/learn/machine-learning/lecture/wrjaS/large-margin-intuition\n  * Mathematics Behind Large Margin Classification\n    * https://www.coursera.org/learn/machine-learning/lecture/3eNnh/mathematics-behind-large-margin-classification\n* Kernels\n  * Kernels I\n    * https://www.coursera.org/learn/machine-learning/lecture/YOMHn/kernels-i\n  * Kernels II\n    * https://www.coursera.org/learn/machine-learning/lecture/hxdcH/kernels-ii\n* SVMs in Practice\n  * Using An SVM\n    * https://www.coursera.org/learn/machine-learning/lecture/sKQoJ/using-an-svm\n\n##### Week 8\n###### Unsupervised Learning\n* Clustering\n  * Unsupervised Learning: Introduction\n    * https://www.coursera.org/learn/machine-learning/lecture/czmip/unsupervised-learning-introduction\n  * K-Means Algorithm\n    * https://www.coursera.org/learn/machine-learning/lecture/93VPG/k-means-algorithm\n  * Optimization Objective\n    * https://www.coursera.org/learn/machine-learning/lecture/G6QWt/optimization-objective\n  * Random Initialization\n    * https://www.coursera.org/learn/machine-learning/lecture/drcBh/random-initialization\n  * Choosing the Number of Clusters\n    * https://www.coursera.org/learn/machine-learning/lecture/Ks0E9/choosing-the-number-of-clusters\n\n#### Uncategorized\n* Linear regression (2): Gradient descent\n  * Alexander Ihler\n  * https://www.youtube.com/watch?v=WnqQrPNYz5Q\n\n### Linear Algebra\n#### Linear Algebra (Instructor(s): Prof. Gilbert Strang, MIT Course Number: 18.06, As Taught In: Spring 2010, Level: Undergraduate)\n* Lecture 1: The geometry of linear equations\n  * http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-1-the-geometry-of-linear-equations/\n* Lecture 2: Elimination with matrices\n  * http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-2-elimination-with-matrices/\n* Lecture 3: Multiplication and inverse matrices\n  * http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-3-multiplication-and-inverse-matrices/\n* Lecture 4: Factorization into A = LU\n  * http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-4-factorization-into-a-lu/\n* Lecture 5: Transposes, permutations, spaces R^n\n  * http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-5-transposes-permutations-spaces-r-n/\n* Lecture 6: Column space and nullspace\n  * http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-6-column-space-and-nullspace/\n* Lecture 7: Solving Ax = 0: pivot variables, special solutions\n  * http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-7-solving-ax-0-pivot-variables-special-solutions/\n\n## Tutorials\n### Oxford\n#### VGG\n* VGG Computer Vision Practicals\n  * http://www.robots.ox.ac.uk/~vgg/practicals/overview/index.html\n* Recognition of object instances practical\n  * http://www.robots.ox.ac.uk/~vgg/practicals/instance-recognition/index.html\n* Recognition of object categories practical\n  * http://www.robots.ox.ac.uk/~vgg/practicals/category-recognition/index.html\n* Object category detection practical\n  * http://www.robots.ox.ac.uk/~vgg/practicals/category-detection/index.html\n* VGG Convolutional Neural Networks Practical\n  * http://www.robots.ox.ac.uk/~vgg/practicals/cnn/index.html\n\n### Stanford\n#### UFLDL Tutorial\n##### Supervised Learning and Optimization\n* Linear Regression\n  * http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/\n* Logistic Regression\n  * http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/\n* Vectorization\n  * http://ufldl.stanford.edu/tutorial/supervised/Vectorization/\n* Debugging: Gradient Checking\n  * http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/\n* Softmax Regression\n  * http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/\n* Debugging: Bias and Variance\n  * http://ufldl.stanford.edu/tutorial/supervised/DebuggingBiasAndVariance/\n* Multi-Layer Neural Network\n  * http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/\n* Exercise: Supervised Neural Networks\n  * http://ufldl.stanford.edu/tutorial/supervised/ExerciseSupervisedNeuralNetwork/\n\n## Papers\n### CNN\n* Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books\n  * http://arxiv.org/pdf/1506.06724v1.pdf\n* Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks\n  * https://www.researchgate.net/publication/277334140_Texture_synthesis_and_the_controlled_generation_of_natural_stimuli_using_convolutional_neural_networks\n\n### RNN\n#### GRU\n* Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\n  * http://arxiv.org/pdf/1412.3555.pdf\n\n### NLP\n#### Word2vec\n* Efficient Estimation of Word Representations in Vector Space\n  * http://arxiv.org/pdf/1301.3781v3.pdf\n\n## Blogs\n### General\n* 10 Deep Learning Trends at NIPS 2015\n  * http://codinginparadise.org/ebooks/html/blog/ten_deep_learning_trends_at_nips_2015.html\n\n### LSTM\n* Understanding LSTM Networks (Christopher Olah)\n  * http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n\n### Math\n* l0-Norm, l1-Norm, l2-Norm, ... , l-infinity Norm\n  * https://rorasa.wordpress.com/2012/05/13/l0-norm-l1-norm-l2-norm-l-infinity-norm/\n* Why Minimize Negative Log Likelihood?\n  * https://quantivity.wordpress.com/2011/05/23/why-minimize-negative-log-likelihood/\n\n## Libraries\n### Python\n#### TensorFlow (https://www.tensorflow.org/)\n##### GET STARTED\n* Introduction\n  * https://www.tensorflow.org/versions/0.6.0/get_started/index.html\n* Download and Setup\n  * https://www.tensorflow.org/versions/0.6.0/get_started/os_setup.html\n* Basic Usage\n  * https://www.tensorflow.org/versions/0.6.0/get_started/basic_usage.html\n\n##### TUTORIALS\n* MNIST For ML Beginners\n  * https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/beginners/index.html\n* Deep MNIST for Experts\n  * https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/pros/index.html\n* TensorFlow Mechanics 101\n  * https://www.tensorflow.org/versions/r0.7/tutorials/mnist/tf/index.html\n* Convolutional Neural Networks\n  * https://www.tensorflow.org/versions/master/tutorials/deep_cnn/index.html\n\n##### HOW TO\n* Variables: Creation, Initialization, Saving, and Loading\n  * https://www.tensorflow.org/versions/r0.7/how_tos/variables/index.html\n* TensorBoard: Visualizing Learning\n  * https://www.tensorflow.org/versions/r0.7/how_tos/summaries_and_tensorboard/index.html\n* TensorBoard: Graph Visualization\n  * https://www.tensorflow.org/versions/r0.7/how_tos/graph_viz/index.html\n\n##### RESOURCES\n* Tensor Ranks, Shapes, and Types\n  * https://www.tensorflow.org/versions/0.6.0/resources/dims_types.html\n\n### Java\n#### DL4J (Deep Learning for Java)\n* A Beginner's Guide to Eigenvectors, PCA, Covariance and Entropy\n  * http://deeplearning4j.org/eigenvector.html\n* A Beginner's Tutorial for Restricted Boltzmann Machines\n  * http://deeplearning4j.org/restrictedboltzmannmachine.html\n* Convolutional Networks\n  * http://deeplearning4j.org/convolutionalnets.html\n* Deep-Belief Networks\n  * http://deeplearning4j.org/deepbeliefnetwork.html\n* Introduction to Deep Neural Networks\n  * http://deeplearning4j.org/neuralnet-overview.html\n* Using Neural Networks With Regression\n  * http://deeplearning4j.org/linear-regression.html\n* Word2Vec\n  * http://deeplearning4j.org/word2vec\n\n## Implementations\n### CNN\n* LeNet-5\n  * http://yann.lecun.com/exdb/lenet/\n\n## Demos\n* Neural Network, Matt Mazur\n  * http://www.emergentmind.com/neural-network\n\n## GPU\n### NVIDIA\n* KEPLER - THE WORLD'S FASTEST, MOST EFFICIENT HPC ARCHITECTURE\n  * http://www.nvidia.com/object/nvidia-kepler.html\n* THE CUDA PARALLEL COMPUTING PLATFORM\n  * http://www.nvidia.com/object/cuda-parallel-computing-platform.html\n* WHAT IS GPU ACCELERATED COMPUTING?\n  * http://www.nvidia.com/object/what-is-gpu-computing.html\n\n## Scholarpedia\n* Neural net language models\n  * http://www.scholarpedia.org/article/Neural_net_language_models\n\n## Wolfram\n* Diagonal Matrix\n  * http://mathworld.wolfram.com/DiagonalMatrix.html\n* Euclidean Plane\n  * http://mathworld.wolfram.com/EuclideanPlane.html\n* Euclidean Space\n  * http://mathworld.wolfram.com/EuclideanSpace.html\n* Identity Matrix\n  * http://mathworld.wolfram.com/IdentityMatrix.html\n* k-Matrix\n  * http://mathworld.wolfram.com/k-Matrix.html\n* L^1-Norm\n  * http://mathworld.wolfram.com/L1-Norm.html\n* L^2-Norm\n  * http://mathworld.wolfram.com/L2-Norm.html\n* Matrix Power\n  * http://mathworld.wolfram.com/MatrixPower.html\n* Orthogonal Vectors\n  * http://mathworld.wolfram.com/OrthogonalVectors.html\n* Transpose\n  * http://mathworld.wolfram.com/Transpose.html\n* Vector Norm\n  * http://mathworld.wolfram.com/VectorNorm.html\n", 
  "id": 48826513
}