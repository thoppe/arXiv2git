{
  "read_at": 1462558295, 
  "description": "LowRankModels.jl is a julia package for modeling and fitting generalized low rank models. ", 
  "README.md": "# LowRankModels.jl\n\n<!--[![Build Status](https://travis-ci.org/madeleineudell/LowRankModels.jl.svg?branch=master)](https://travis-ci.org/madeleineudell/LowRankModels.jl)-->\n\nLowRankModels.jl is a julia package for modeling and fitting generalized low rank models (GLRMs).\nGLRMs model a data array by a low rank matrix, and\ninclude many well known models in data analysis, such as \nprincipal components analysis (PCA), matrix completion, robust PCA,\nnonnegative matrix factorization, k-means, and many more.\n\nFor more information on GLRMs, see [our paper][glrmpaper].\n\nLowRankModels.jl makes it easy to mix and match loss functions and regularizers\nto construct a model suitable for a particular data set.\nIn particular, it supports \n\n* using different loss functions for different columns of the data array, \n  which is useful when data types are heterogeneous \n  (eg, real, boolean, and ordinal columns);\n* fitting the model to only *some* of the entries in the table, which is useful for data tables with many missing (unobserved) entries; and\n* adding offsets and scalings to the model without destroying sparsity,\n  which is useful when the data is poorly scaled.\n\n## Installation\n\nTo install, just call\n```\nPkg.add(\"LowRankModels\")\n```\nat the julia prompt.\n\n# Generalized Low Rank Models\n\nGLRMs form a low rank model for tabular data `A` with `m` rows and `n` columns, \nwhich can be input as an array or any array-like object (for example, a data frame).\nIt is fine if only some of the entries have been observed \n(i.e., the others are missing or `NA`); the GLRM will only be fit on the observed entries `obs`.\nThe desired model is specified by choosing a rank `k` for the model,\nan array of loss functions `losses`, and two regularizers, `rx` and `ry`.\nThe data is modeled as `X'*Y`, where `X` is a `k`x`m` matrix and `Y` is a `k`x`n` matrix.\n`X` and `Y` are found by solving the optimization problem\n<!--``\\mbox{minimize} \\quad \\sum_{(i,j) \\in \\Omega} L_{ij}(x_i y_j, A_{ij}) + \\sum_{i=1}^m r_i(x_i) + \\sum_{j=1}^n \\tilde r_j(y_j)``-->\n\n    minimize sum_{(i,j) in obs} losses[j]((X'*Y)[i,j], A[i,j]) + sum_i rx(X[:,i]) + sum_j ry(Y[:,j])\n\nThe basic type used by LowRankModels.jl is the GLRM. To form a GLRM,\nthe user specifies\n\n* the data `A` (any `AbstractArray`, such as an array, a sparse matrix, or a data frame)\n* the array of loss functions `losses`\n* the regularizers `rx` and `ry`\n* the rank `k`\n\nThe user may also specify\n\n* the observed entries `obs`\n* starting matrices X0 and Y0\n\n`obs` is a list of tuples of the indices of the observed entries in the matrix,\nand may be omitted if all the entries in the matrix have been observed.\nIf `A` is a sparse matrix, implicit zeros are interpreted \nas missing entries by default; \nsee the discussion of [sparse matrices](https://github.com/madeleineudell/LowRankModels.jl#fitting-sparse-matrices) below for more details.\n`X0` and `Y0` are initialization\nmatrices that represent a starting guess for the optimization.\n\nLosses and regularizers must be of type `Loss` and `Regularizer`, respectively,\nand may be chosen from a list of supported losses and regularizers, which include\n\nLosses:\n\n* quadratic loss `QuadLoss`\n* hinge loss `HingeLoss`\n* logistic loss `LogisticLoss`\n* poisson loss `PoissonLoss`\n* weighted hinge loss `WeightedHingeLoss`\n* l1 loss `L1Loss`\n* ordinal hinge loss `OrdinalHingeLoss`\n* periodic loss `PeriodicLoss`\n* multinomial categorical loss `MultinomialLoss`\n* multinomial ordinal (aka ordered logit) loss `OrderedMultinomialLoss`\n\nRegularizers:\n\n* quadratic regularization `QuadReg`\n* constrained squared euclidean norm `QuadConstraint`\n* l1 regularization `OneReg`\n* no regularization `ZeroReg`\n* nonnegative constraint `NonNegConstraint` (eg, for nonnegative matrix factorization)\n* 1-sparse constraint `OneSparseConstraint` (eg, for orthogonal NNMF)\n* unit 1-sparse constraint `UnitOneSparseConstraint` (eg, for k-means)\n* simplex constraint `SimplexConstraint`\n* l1 regularization, combined with nonnegative constraint `NonNegOneReg`\n* fix features at values `y0` FixedLatentFeaturesConstraint(y0)`\n\nEach of these losses and regularizers can be scaled \n(for example, to increase the importance of the loss relative to the regularizer) \nby calling `scale!(loss, newscale)`.\nUsers may also implement their own losses and regularizers,\nor adjust internal parameters of the losses and regularizers; \nsee [losses.jl](https://github.com/madeleineudell/LowRankModels.jl/blob/src/losses.jl) and [regularizers.jl](https://github.com/madeleineudell/LowRankModels.jl/blob/master/src/regularizers.jl) for more details.\n\n## Example\n\nFor example, the following code forms a k-means model with `k=5` on the `100`x`100` matrix `A`:\n\n    using LowRankModels\n    m,n,k = 100,100,5\n    losses = QuadLoss() # minimize squared distance to cluster centroids\n    rx = UnitOneSparseConstraint() # each row is assigned to exactly one cluster\n    ry = ZeroReg() # no regularization on the cluster centroids\n    glrm = GLRM(A,losses,rx,ry,k)\n\nTo fit the model, call\n\n\tX,Y,ch = fit!(glrm)\n\nwhich runs an alternating directions proximal gradient method on `glrm` to find the \n`X` and `Y` minimizing the objective function.\n(`ch` gives the convergence history; see \n[Technical details](https://github.com/madeleineudell/LowRankModels.jl#technical-details) \nbelow for more information.)\n\nThe `losses` argument can also be an array of loss functions, \nwith one for each column (in order). For example, \nfor a data set with 3 columns, you could use \n\n    losses = Loss[QuadLoss(), LogisticLoss(), HingeLoss()]\n\nSimiliarly, the `ry` argument can be an array of regularizers, \nwith one for each column (in order). For example, \nfor a data set with 3 columns, you could use \n\n    ry = Regularizer[QuadReg(1), QuadReg(10), FixedLatentFeatureConstraint([1,2,3])]\n\nThis regularizes the first to columns of `Y` with `||Y[:,1]||^2 + 10||Y[:,2]||^2`\nand constrains the third (and last) column of `Y` to be equal to `[1,2,3]`.\n\n[More examples here.](https://github.com/madeleineudell/LowRankModels.jl/blob/master/examples/simple_glrms.jl)\n\n# Missing data\n\nIf not all entries are present in your data table, just tell the GLRM\nwhich observations to fit the model to by listing tuples of their indices in `obs`.\nThen initialize the model using\n\n    GLRM(A,losses,rx,ry,k, obs=obs)\n\nIf `A` is a DataFrame and you just want the model to ignore \nany entry that is of type `NA`, you can use\n\n    obs = observations(A)\n\n# Standard low rank models\n\nLow rank models can easily be used to fit standard models such as PCA, k-means, and nonnegative matrix factorization. \nThe following functions are available:\n\n* `pca`: principal components analysis\n* `qpca`: quadratically regularized principal components analysis\n* `rpca`: robust principal components analysis\n* `nnmf`: nonnegative matrix factorization\n* `k-means`: k-means\n\nSee [the code](https://github.com/madeleineudell/LowRankModels.jl/blob/master/src/simple_glrms.jl) for usage.\nAny keyword argument valid for a `GLRM` object, \nsuch as an initial value for `X` or `Y`\nor a list of observations, \ncan also be used with these standard low rank models.\n\n# Scaling and offsets <a name=\"scaling\"></a>\n\nIf you choose, LowRankModels.jl can add an offset to your model and scale the loss \nfunctions and regularizers so all columns have the same pull in the model.\nSimply call \n\n    glrm = GLRM(A,losses,rx,ry,k, offset=true, scale=true)\n\nThis transformation generalizes standardization, a common proprocessing technique applied before PCA.\n(For more about offsets and scaling, see the code or the paper.)\n\nYou can also add offsets and scalings to previously unscaled models:\n\n* Add an offset to the model (by applying no regularization to the last row \n  of the matrix `Y`, and enforcing that the last column of `X` be all 1s) using\n\n      add_offset!(glrm)\n\n* Scale the loss functions and regularizers by calling\n\n      equilibrate_variance!(glrm)\n\n* Scale only the columns using `QuadLoss` or `HuberLoss`\n\n      prob_scale!(glrm)\n\n# Fitting DataFrames\n\nPerhaps all this sounds like too much work. Perhaps you happen to have a \n[DataFrame](https://github.com/JuliaStats/DataFrames.jl) `df` lying around \nthat you'd like a low rank (eg, `k=2`) model for. For example,\n\n    import RDatasets\n    df = RDatasets.dataset(\"psych\", \"msq\")\n\nNever fear! Just call\n\n\tglrm, labels = GLRM(df, k)\n\tX, Y, ch = fit!(glrm)\n\nThis will fit a GLRM with rank `k` to your data, \nusing a QuadLoss loss for real valued columns,\nHingeLoss loss for boolean columns, \nand ordinal HingeLoss loss for integer columns,\na small amount of QuadLoss regularization,\nand scaling and adding an offset to the model as described [here](#scaling).\nIt returns the column labels for the columns it fit, along with the model.\nRight now, all other data types are ignored.\n`NaN` values are treated as missing values (`NA`s) and ignored in the fit.\n\nThe full call signature is\n```\nGLRM(df::DataFrame, k::Int;\n    losses = Loss[], rx = QuadReg(.01), ry = QuadReg(.01),\n    offset = true, scale = false, \n    prob_scale = true, NaNs_to_NAs = true)\n```\nYou can modify the losses or regularizers, or turn off offsets or scaling,\nusing these keyword arguments.\n\nTo fit a data frame with categorical values, you can use the function\n`expand_categoricals!` to turn categorical columns into a Boolean column for each \nlevel of the categorical variable. \nFor example, `expand_categoricals!(df, [:gender])` will replace the gender \ncolumn with a column corresponding to `gender=male`, \na column corresponding to `gender=female`, and other columns corresponding to \nlabels outside the gender binary, if they appear in the data set.\n\nYou can use the model to get some intuition for the data set. For example,\ntry plotting the columns of `Y` with the labels; you might see\nthat similar features are close to each other!\n\n# Fitting Sparse Matrices\n\nIf you have a very large, sparsely observed dataset, then you may want to\nencode your data as a\n[sparse matrix](http://julia-demo.readthedocs.org/en/latest/stdlib/sparse.html).\nBy default, `LowRankModels` interprets the sparse entries of a sparse\nmatrix as missing entries (i.e. `NA` values). There is no need to\npass the indices of observed entries (`obs`) -- this is done\nautomatically when `GLRM(A::SparseMatrixCSC,...)` is called.\nIn addition, calling `fit!(glrm)` when `glrm.A` is a sparse matrix\nwill use the sparse variant of the proximal gradient descent algorithm,\n`fit!(glrm, SparseProxGradParams(); kwargs...)`.\n\nIf, instead, you'd like to interpret the sparse entries as zeros, rather\nthan missing or `NA` entries, use:\n```julia\nglrm = GLRM(...;sparse_na=false)\n```\nIn this case, the dataset is dense in terms of observations, but sparse\nin terms of nonzero values. Thus, it may make more sense to fit the\nmodel with the vanilla proximal gradient descent algorithm,\n`fit!(glrm, ProxGradParams(); kwargs...)`.\n\n# Technical details\n\n## Optimization\n\nThe function `fit!` uses an alternating directions proximal gradient method\nto minimize the objective. This method is *not* guaranteed to converge to \nthe optimum, or even to a local minimum. If your code is not converging\nor is converging to a model you dislike, there are a number of parameters you can tweak.\n\n### Warm start\n\nThe algorithm starts with `glrm.X` and `glrm.Y` as the initial estimates\nfor `X` and `Y`. If these are not given explicitly, they will be initialized randomly.\nIf you have a good guess for a model, try setting them explicitly.\nIf you think that you're getting stuck in a local minimum, try reinitializing your\nGLRM (so as to construct a new initial random point) and see if the model you obtain improves.\n\nThe function `fit!` sets the fields `glrm.X` and `glrm.Y`\nafter fitting the model. This is particularly useful if you want to use \nthe model you generate as a warm start for further iterations.\nIf you prefer to preserve the original `glrm.X` and `glrm.Y` (eg, for cross validation),\nyou should call the function `fit`, which does not mutate its arguments.\n\nYou can even start with an easy-to-optimize loss function, run `fit!`,\nchange the loss function (`glrm.losses = newlosses`), \nand keep going from your warm start by calling `fit!` again to fit \nthe new loss functions.\n\n### Initialization\n\nIf you don't have a good guess at a warm start for your model, you might try\none of the initializations provided in `LowRankModels`.\n\n* `init_svd!` initializes the model as the truncated SVD of the matrix of observed entries, with unobserved entries filled in with zeros. This initialization is known to result in provably good solutions for a number of \"PCA-like\" problems. See [our paper][glrmpaper] for details.\n* `init_kmeanspp!` initializes the model using a modification of the [kmeans++](https://en.wikipedia.org/wiki/K-means_clustering) algorithm for data sets with missing entries; see [our paper][glrmpaper] for details. This works well for fitting clustering models, and may help in achieving better fits for nonnegative matrix factorization problems as well.\n* `init_nndsvd!` initializes the model using a modification of the [NNDSVD](https://github.com/JuliaStats/NMF.jl/blob/master/src/initialization.jl#L18) algorithm as implemented by the [NMF](https://github.com/JuliaStats/NMF.jl) package. This modification handles data sets with missing entries by replacing missing entries with zeros. Optionally, by setting the argument `max_iters=n` with `n>0`, it will iteratively replace missing entries by their values as imputed by the NNDSVD, and call NNDSVD again on the new matrix. (This procedure is similar to the [soft impute](http://dl.acm.org/citation.cfm?id=1859931) method of Mazumder, Hastie and Tibshirani for matrix completion.)\n\n### Parameters\n\nAs mentioned earlier, `LowRankModels` uses alternating proximal\ngradient descent to derive estimates of `X` and `Y`. This can be done\nby two slightly different procedures: (A) compute the full \nreconstruction, `X' * Y`, to compute the gradient and objective function;\n(B) only compute the model estimate for entries of `A` that are observed.\nThe first method is likely preferred when there are few missing entries\nfor `A` because of hardware level optimizations\n(e.g. chucking the operations so they just fit in various caches). The\nsecond method is likely preferred when there are many missing entries of\n`A`.\n\nTo fit with the first (dense) method:\n```julia\nfit!(glrm, ProxGradParams(); kwargs...)\n```\n\nTo fit with the second (sparse) method:\n```julia\nfit!(glrm, SparseProxGradParams(); kwargs...)\n```\n\nThe first method is used by default if `glrm.A` is a standard\nmatrix/array. The second method is used by default if `glrm.A` is a\n`SparseMatrixCSC`.\n\n`ProxGradParams()` and `SparseProxGradParams()` run these respective\nmethods with the default parameters:\n\n* `stepsize`: The step size controls the speed of convergence.\nSmall step sizes will slow convergence, while large ones will cause \ndivergence. `stepsize` should be of order 1.\n* `abs_tol`: The algorithm stops when the decrease in the\nobjective per iteration is less than `abs_tol*length(obs)`.\n* `rel_tol`: The algorithm stops when the decrease in the\nobjective per iteration is less than `rel_tol`. \n* `max_iter`: The algorithm also stops if maximum number of rounds\n`max_iter` has been reached.\n* `min_stepsize`: The algorithm also stops if `stepsize` decreases below \nthis limit.\n* `inner_iter`: specifies how many proximal gradient steps to take on `X`\nbefore moving on to `Y` (and vice versa).\n\nThe default parameters are: `ProxGradParams(stepsize=1.0;max_iter=100,inner_iter=1,abs_tol=0.00001,rel_tol=0.0001,min_stepsize=0.01*stepsize)` \n\n### Convergence\n`ch` gives the convergence history so that the success of the optimization can be monitored;\n`ch.objective` stores the objective values, and `ch.times` captures the times these objective values were achieved.\nTry plotting this to see if you just need to increase `max_iter` to converge to a better model.\n\n# Cross validation\n\nA number of useful functions are available to help you check whether a given low rank model overfits to the test data set. \nThese functions should help you choose adequate regularization for your model.\n\n## Cross validation\n\n* `cross_validate(glrm::GLRM, nfolds=5, params=Params(); verbose=false, use_folds=None, error_fn=objective, init=None)`: performs n-fold cross validation and returns average loss among all folds. More specifically, splits observations in `glrm` into `nfolds` groups, and builds new GLRMs, each with one group of observations left out. Fits each GLRM to the training set (the observations revealed to each GLRM) and returns the average loss on the test sets (the observations left out of each GLRM).\n\n    **Optional arguments:**\n    * `use_folds`: build `use_folds` new GLRMs instead of `n_folds` new GLRMs, each with `1/nfolds` of the entries left out. (`use_folds` defaults to `nfolds`.)\n    * `error_fn`: use a custom error function to evaluate the fit, rather than the objective. For example, one might use the imputation error by setting `error_fn = error_metric`.\n    * `init`: initialize the fit using a particular procedure. For example, consider `init=init_svd!`. See [Initialization](https://github.com/madeleineudell/LowRankModels.jl#initialization) for more options.\n\n* `cv_by_iter(glrm::GLRM, holdout_proportion=.1, params=Params(1,1,.01,.01), niters=30; verbose=true)`: computes the test error and train error of the GLRM as it is trained. Splits the observations into a training set (`1-holdout_proportion` of the original observations) and a test set (`holdout_proportion` of the original observations). Performs `params.maxiter` iterations of the fitting algorithm on the training set `niters` times, and returns the test and train error as a function of iteration. \n\n## Regularization paths\n\n* `regularization_path(glrm::GLRM; params=Params(), reg_params=logspace(2,-2,5), holdout_proportion=.1, verbose=true, ch::ConvergenceHistory=ConvergenceHistory(\"reg_path\"))`: computes the train and test error for GLRMs varying the scaling of the regularization through any scaling factor in the array `reg_params`.\n\n## Utilities\n\n* `get_train_and_test(obs, m, n, holdout_proportion=.1)`: splits observations `obs` into a train and test set. `m` and `n` must be at least as large as the maximal value of the first or second elements of the tuples in `observations`, respectively. Returns `observed_features` and `observed_examples` for both train and test sets.\n\n# Citing this package\n\nIf you use LowRankModels for published work, \nwe encourage you to cite the software.\n\nUse the following BibTeX citation:\n\n    @article{udell2014,\n        title = {Generalized Low Rank Models},\n        author ={Udell, Madeleine and Horn, Corinne and Zadeh, Reza and Boyd, Stephen},\n        year = {2014},\n        archivePrefix = \"arXiv\",\n        eprint = {1410.0342},\n        primaryClass = \"stat-ml\",\n        journal={arXiv preprint arXiv:1410.0342},\n    }\n\n[glrmpaper]: http://arxiv.org/abs/1410.0342\n", 
  "id": 24545458
}