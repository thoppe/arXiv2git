{
  "read_at": 1462547684, 
  "description": "http://arxiv.org/abs/1511.02583", 
  "README.md": "# Batch_Normalized_Maxout_NIN\nPaper on arxiv: http://arxiv.org/abs/1511.02583\n\nThis is my implmentation of Batch Normalized Maxout NIN.\n\nOriginal Matconvnet: https://github.com/vlfeat/matconvnet\n\nThis respository is my modification of Matconvnet.\n\nYou can install this modification as same as original installation:\nhttp://www.vlfeat.org/matconvnet/install/\n<blockquote>\nvl_compilenn('enableGpu', true, 'cudaMethod', 'nvcc', ...\n'cudaRoot', your-cuda-toolkit\\CUDA\\v6.5', ...\n'enableCudnn', true, 'cudnnRoot', 'your-cudnn-root\\cuda') ;\n</blockquote>\n\nI used VS2013, CUDA-6.5 and cudnn-v4.\n\nI added followwing functions:\n<blockquote>\n<ul>Maxout units (GPU supported only) </ul>\n<ul>Data augmentations (horizontal flipping / pad zeros and random cropping)</ul>\n</blockquote>\n\n<h1>Usage:</h1>\n##To run BN Maxout NIN\n<blockquote>\nAfter installation, run \"\\example\\cifar\\cnn_cifar.m\"\n</blockquote>\n\n<h2>Use maxout units as pooling layers</h2>\nfor example: a batch normalized maxout layer consist of a convolutional layer, a BN layer, and a maxout layer\n\n\"unit1\"  is the number of maxout units\n\n\"piece1\" is the number of maxout pieces\n<blockquote>\n<p>net.layers{end+1} = struct('type', 'conv', ...\n                           'name', 'maxoutconv1', ...\n                           'weights', {{single(orthonorm(1,1,unit0,unit1*piece1)), b*ones(1,unit1*piece1,'single')}}, ...\n                           'stride', 1, ...\n                           'learningRate', [.1 1], ...\n                           'weightDecay', [1 0], ...\n                           'pad', 0) ;</p>\n\n<p>net.layers{end+1} = struct('type', 'bnorm', 'name', 'bn2', ...\n                           'weights', {{ones(unit1*piece1, 1, 'single'), zeros(unit1*piece1, 1, 'single')}},'learningRate', [1 1 .5],'weightDecay', [0 0]) ;</p>   \n\n<p>net.layers{end+1} = struct('type', 'maxout','numunit',unit1,'numpiece',piece1) ; </p>\n</blockquote>\n\n<h2>Data augmentations:</h2>\n<blockquote>\n<p>add following to your net opts</p>\n<p>-> net.meta.trainOpts.augmentation= true;</p>\n</blockquote>\n####Using this implmentation, I achieved 8.13+-0.19% test error without augmentation in CIFAR-10 datasets.\n####DATA preprocessing: GCN and Whitening.\n\n", 
  "id": 55235259
}