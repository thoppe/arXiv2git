{
  "README": "A longer description of this library, including a detailed overview of all the \nfunctions that are part of its API, has been published as \nA. Deuzeman, S.F. Reker and C. Urbach, for the ETM Collaboration,\n\"Lemon: an MPI parallel I/O library for data encapsulation using LIME\"\nand is currently available from http://arxiv.org/abs/1106.4177\n\nThis README is intended to help new users get Lemon up and running quickly. Upon\ndownloading and extracting the Lemon tarball, one should have a obtained a \ndirectory containing, amongst this README file, a configure script and several \nsubdirectories. These subdirectories include all the headers associated with the\nlibrary (in the directory include), the source code itself (in the directory\nsrc) and some binaries that do not themselves form part of Lemon (in the\ndirectory check).\n\nThe GNU build system is used for Lemon and the usual configure script is\nprovided. Configuring Lemon should be straightforward, as only the common\narguments are taken into consideration. Of relevance is mainly the --prefix\nargument, that will set the install directory. It is crucial, however, to either\nuse an MPI wrapper around the compiler, or set the include and linker paths\nsuch that MPI can be found. This can be done by setting environment variables,\ne.g. CC=mpicc\n\nOnce the library has been configured, make will compile both the library itself\nand two binaries that can be found in the test subdirectory (The system will\ncall aclocal-1.9, automake-1.9 and autoconf. This will result in an error if\nonly newer versions are available, but one can simply invoke aclocal, automake\nand autoconf manually and call make again to continue). These two binaries not\nonly provide short, self-contained samples of Lemon usage, but are in fact\npotentially useful in a production environment. The first, lemon_contents, is a\ndirect port of the C-LIME lime_contents program. It displays a short overview of\nall the records available within a particular LIME file. If the contents of a\nrecord data block are both non-binary and short enough, lemon_contents will send\nit to standard output. Potential uses for lemon_contents include checking if a\nLIME file is well-formed and displaying the metadata associated with a\nparticular file. The second program that is compiled by default is\nlemon_benchmark. This executable will generate an artificial lattice of random\ndata, the topology of which will be generated automatically taking into account\nthe number of MPI processes. This lattice is written out and subsequently\nread in, both using Lemon's parallel I/O routines. The I/O speed is calculated\nfrom the timing of both operations and reported on standard output. Two arguments \nshould be provided. The first is the length L of the spatial dimensions of the \nlattice, which determines the global size of the lattice as L^3 x 2 L. The second\nis the number of iterations of the write-read cycle. To check the correct operation \nof the library, an MD5  hash is calculated for the data before and after the I/O \noperations and any discrepancies will be reported. The MD5 implementation included \nhere was written by L. Peter Deutsch and is available freely from \nhttp://sourceforge.net/projects/libmd5-rfc/files/. This code is intended to allow \nthe user to obtain basic information on the I/O performance on his or her particular\nsystem and to give some indication of the scaling. Of course, it also functions as\na basic tool for detecting major problems in the writing or retrieving of data. \nAfter running make, the user can run make install to install the library, its headers \nand the two binaries described above to the directory specified by the --prefix argument \nto configure (or the default location if that argument has been omitted). \n\nAdditional binaries can still be compiled by calling make check. These will\ninclude the writing and reading of an artifical metadata record, both using\nblocking and non-blocking I/O (xlf and xlf_non_blocking}), that can be displayed\nusing lemon_contents. Also provided are two programs that write a small amount\nof data using the mapped parallel I/O routines, again in a blocking and\nnon-blocking version (parallel and parallel_non_blocking). The data written will\nbe a set of characters identifying a particular MPI process, such that the\noutput file demonstrates the linearisation of the data. The last sample program\nthat has been included is canonical, a skeleton implementation of the library.\nNone of these binaries will be installed, as they serve no particular function\nother than being examples.\n\nAs an example, an MPI run of lemon_benchmark on 4 MPI processes, performing 3\nmeasurements of the IO timings for an 8 x 8 x 8 x 16 lattice should provide the\nfollowing result (your reading and writing speeds and times may vary):\n\nmpirun -np 4 lemon_benchmark 8 3\nBenchmark on a block of data  4.72 MB in size,\nrepresenting a 8 x 8 x 8 x 16 lattice,\ndistributed over 4 MPI processes\nfor a local 4 x 4 x 8 x 16 lattice.\n\nMeasurement 1 of 3.\nTime spent writing was 0.15 s.\nTime spent reading was 0.016 s.\nAll nodes report that MD5 hash matches.\n\nMeasurement 2 of 3.\nTime spent writing was  0.2 s.\nTime spent reading was 0.017 s.\nAll nodes report that MD5 hash matches.\n\nMeasurement 3 of 3.\nTime spent writing was 0.21 s.\nTime spent reading was 0.018 s.\nAll nodes report that MD5 hash matches.\n\nAverage time spent writing was 1.88e-01 s, with a standard deviation of 2.50e-02 s.\nAverage time spent reading was 1.68e-02 s, with a standard deviation of 1.05e-03 s.\n\nAverage writing speed was  25.1 MB/s\nAverage reading speed was   281 MB/s\nAll data hashed correctly.\n", 
  "read_at": 1462543274, 
  "description": "Lemon is an MPI parallel I/O library that is intended to allow for efficient parallel I/O of both binary and metadata on massively parallel architectures. Data is stored in the SciDAC Lattice QCD Interchange Message Encapsulation format, that allows for storing large blocks of binary data and corresponding metadata in the same file.", 
  "id": 3307401
}