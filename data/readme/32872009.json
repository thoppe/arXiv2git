{
  "read_at": 1462544694, 
  "description": "Convert XML/SVG/PDF into normalised, sectioned, scholarly HTML", 
  "README.md": "# Norma\n\n_edited 20150326_\n_edited 20150412_\n_ediedt 201504016_\n\n================================\nNOTE: Norma does not do all transformations. The current position is:\n- main branch does XML to scholarly HTML only\n- petermr / dev fork/branch does PDF 2 TXT based on PDFBox.\n\n\nThe PDF2HTML code is in there but it's messy. I'd guess a week or two to get the logic working, but the results will be very variable depending on the type of PDF.\n================================\n\nA tool to convert a variety of inputs into normalized, tagged, XHTML (with embedded/linked SVG and PNG where\nappropriate). The initial emphasis is on scholarly publications but much of the technology is general.  \n\n## Overview\n\nSee https://github.com/petermr/norma .\n\n## History\n\nOriginally AMI (http://bitbucket.org/petermr/ami-core) involved the conversion of legacy inputs (PDF, XML, HTML, etc.) into well-formed documents which were then searched by AMI-plugins. This required a Visitor pattern\nwith _n_ inputs and _m_ visitors. The inputs could be PDF, SVG, HTML, XML, etc. and AMI often had to convert. This became unmanageable.\nAMI has therefore been refactored into `ami2`.\n\n## Current architecture\n\nWe have now split this into two components:\n\n  * norma. This takes legacy files (more later) and converts to normalized, tagged, XHTML (`scholarly.html`)\n  * ami2. This reads `scholarly.html` and applies one or more plugins.\n  \nThe workflow is then roughly\n\n```\n                   quickscrape              \n                       ||\n                       CM\n                       ||\n                       V\nsingle files => {PDF, HTML, ...} => Norma => NHTML => AMI => results (XML or JSON)\n                                      ^                ^\n                                      |                |\n                                    tagger           plugin\n                    \n```\n\n## Input\n\nThere are two modes of input to `norma`:\n * single legacy files (e.g. `-i my/dir/foo.html`). A new CM directory is created from the `-o` argument, e.g `-o /some/where` creates a new CM directory `/some/where/`.  `foo.html` is then copied to `/some/where/fulltext.html` (`fulltext.html` is a reserved name triggered by the input `.html` suffix.)\n * pre-existing CM directories with one or more reserved files. These are normally generated by a `quickscrape` operation and the user does not need to copy or rename anything. Note that `quickscrape`-generated CMs should always have a `results.json` file, but some or all of the `fulltext.*` files may be missing since they didn't exist or weren't downloaded.\n \n\n\n## Taggers and Normalization\n\n *Taggers are not yet standard*\n \nEach document type has a tagger, based on its format, structure and vocabulary. For example an HTML file from\nPLoSONE is not well-formed and must be converted, largely automatically. Some documents are \"flat\" and must be grouped into sections (e.g. Elsevier and Hindawi HTML have no structuring ``div``s. After normalization the tagger will identify sections\nbased on XPaths in pubstyle; and add tags which should be standardized across a wide range of input sources.\n\n### Format\n\nTaggers will be created in XML, and use XSLT2 where possible. The tagger carries out the following:\n\n * normalize to well-formed XHTML. This may be automatic but may also require some specific editing for some of the\n worst inputs (lack of quotes, etc.) We use Jsoup and HtmlUnit as appropriate and these do a good job most of the time.\n * structure the XHTML. Some publications are \"flat\" (e.g h2 and h3 are siblings) and need structuring (XSLT2)\n * tag the XHTML. We use XPath (with XSLT2) to add ``tag`` attributes to sections. These can be then recognized by AMI using the ``-x`` xpath-like option.\n \n### Development of taggers\n\nIn general each publication type requires a tagger. This is made much easier where a publisher uses the same toolchain for all its publications  (e.g. BMC, Hindawi or Elsevier). We believe that taggers can be developed by most people, given a supportive community. The main requirements are:\n\n * understand the structure and format of the input.\n * work out how this maps onto similar, solved, publications.\n * write XPath expressions for the tagging. If you understand filesystems and the commandline you shouldn't have much problem and we believe it should take about 15-30 minutes.\n * use established tag names for the sections.\n \n## Transformation and Legacy formats\n\nMost input files will need transformation. \"html\" files are often not W3C-compliant and in any case need normalization and tagging. All  `xml` and `pdf` files need significant transformation. In most cases the default output format is `scholarly.html`. In some cases the transformation mechanism must be made specific (e.g. to transform `xml` to `html` requires an XSL stylesheet and the argument `--xsl foo.xsl` will trigger the transformation of the `-i` input file to the `-o` output file.\n\n### HTML\n\nThere are several normalization processes and often all are needed:\n * _encoding_. Some HTML files do not use UTF-8 and it may be neceesary to convert the output encoding. Ones from MS tools are often a problem. \n * _character normalization to Unicode_. This applies to all input formats and we try to convert where possible.\n * _well-formedness_. HTML is not required to be compliant `XML` (`XHTML`) and it is necessary to make it so. This is not guarantted to be possible - some HTML is so awful that it is impossible to parse. HTML5 need not be well formed, but if compliant it must conform to the W3C spec. There are a number of tools that try their best (Jtidy, JSoup, Tagsoup, HtmlUnit). Most will output well-formed HTML but some will refuse with some documents. We therefore offer a default choice based on experience but allow the user to choose others (experimental).\n * _structuring_. Many HTML documents are \"flat\" (e.g. Hindawi press), i.e. there are no container elements such as `div`. We thne have to try to group all the elements in a section together. This may be done by an `XSLT2` spreadsheet or we may have to use custom code.\n \n\n### PDF\n\nThe main legacy format is PDF. We provide two Java-based converters (PDF2TXT, and PDF2XML). PDF2TXT can be called automatically from Norma with the `--xsl pdf2txt` flag. PDF2XML is more sophisticated and deals with graphics and layout. \n\nAll PDF converters are lossy and likely to fail slightly or seriously on boxes, tables, etc. PDF2XML concentrates on science and particularly mathematical symbols, styling/italics and sub/superscripts. You may have other converters which do a better job of some of the parts - and should configure them to output HTML (which we can normalize later.\n\nPDF2TXT creates a single file `fulltext.pdf.txt`. This loses all formatting and style and  often has words or blocks in the wrong order. That's not PDFBox's fault, it's the fundamental problem of PDF.\n\nPDF2XML convert to a single file, but in most cases it will be a collection. So there will be a special subdirectory `pdf`.\n\n### SVG\n\nSome graphics is published as vectors within PDF and these can be converted into SVG. The SVG is wrapped in NHTML and can, in certain cases such as chemistry, be converted into science. It's possible that formats such as EPS or WMF/EMF can be converted into SVG.\n\nWhere the `SVG` is created by processing `PDF` it will be in a subdirectory of `pdf`. \n\n### DOC/X\n\nSome years ago I wrote a semi-complete parser for DOCX but it's lost... Probably could resurrect if required. Most likely use would be theses or possibly arXiv. Output would probably be `html` and `svg` in a `docx` subdirectory.\n\n### LaTeX\n\nNo specific tools yet. I'd like to know if there is a parsable intermediate output from LaTeX (e.g. before the `*.dvi`). Otherwise we have to recreate a LaTeX parser which I'd prefer not to do.\n\n## Output\n\nThe preferred output method is to offer a QuickscrapeNorma (CM) (`-q`) option, to which output files can be added. A CM is essentially a directory \nwith conventional names for files and subdirectories. The minimal output is likely to be a single `scholarly.html` \n The `scholarly.html` should always be well-formed, Unicode, structured XHTML. That makes it a significant output in its \nown right.\n\nThus \n```\nnorma -q foo/bar/\n```\nwill expect a variety of files in ```foo/bar``` created either by `quickscrape` or repeated applications of other CM software (e.g. `norma`\nor `ami2`). Norma will then output the normalised result as \n```\nscholarly.html\n```\n\nWhere Norma is not given a FileContainer then the output directory or file must be specified.\n\n## Commandline arguments\n\nsee ARGS.md\n\nThe current arguments  are:\n\n```\n-i or --input  file(s)_and/or_url(s)\n```\n\t\t\t\n\t\t\tInput stream (Files, directories, URLs), \n```\n-q or --cmdir  director(ies)\n```\n\t\t\t\n\t\t\tcreate or use CM directory \n\n```\n-o or --output  file_or_directory\n```\n\n\t\t\tOutput is to local filestore \n\t\t\n```\n-r or --recursive \n```\n\n\t\t\trecurse through input directories\n\t\t\n```\n-e or --extensions  ext1 [ext2...]\n```\n\n\t\t\tinput file extensions (may be obsolete)\n```\n-h or --help \n```\n\n\t\t\t\toutputs help for all options, including superclass DefaultArgProcessor\n\n```\t\t\t\t\n--makedocs \n```\n\n\t      Create HTML version of all args.xml file for inclusion in documentation    \n\t      Iterates over src/main/resources directory to find all args.xml files    \n\t      and creates a sister args.html file.    \n\t\t\n```\n-c or --chars  [pairs of characters, as unicode points; i.e. char00, char01, char10, char11, char20, char21\n```\n\n\t\t\tReplaces one character by another. (Not tested) \n\t\t\n```\n-d or --divs  expression [expression] ...\n```\n\t\t\t\n\t\t\tList of expressions identifying XML element to wrap as divs/sections\n\t\t\n```\n-n or --name   tag1,tag2 [tag1,tag2 ....]\n```\n\t\t\t\n\t\t\tList of comma-separated pairs of HTML tags to change\n\t\t\n```\n-p or --pubstyle  pub_code\n```\n\n\t\t\tCode or mnemomic to identifier the publisher or journal style. \n\n```\n-z or --standalone  boolean\n```\n\t\t\t\n\t\t\tTreats XML document as standalone. \n\t\t\n```\n-s or --strip  [options to strip]\n```\n\t\t\tList of XML components to strip from the raw well-formed HTML (not yet tested)\n\n\t\t\n```\n-t or --tidy  [HTML tidy option]\n```\n\t\t\t\n\t\t\tChoose an HTML tidy tool. OBSOLETE - use --html\n\t\t\t\n```\n--html  [HTML tidy option, jsoup, jtidy, htmlunit]\n```\n\n\t\t\tClean raw HTML and produce XHTML.   \n\t\t\tNote: This is NOT scholarly.html and will need a publisher-specific    \n\t\t\t\tstylesheet for further processing.    \n\t\t\n```\n-x or --xsl  stylesheet\n```\n\t\t\t\n\t\t\tTransform XML or HTML input with stylesheet.   \n\t\t\tThis is DEPRECATED. Use --transform instead.\n\t\t\t\n```\n--transform  stylesheet\n```\n\n\t\t\n\t\t\tTransform XML or HTML or PDF or other input.    \n\t\t\tRelacement for --xsl. TRANSFORMATION OPTIONS:   \n\t\t\thocr2svg, pdf2html, pdf2svg, pdf2txt, pdf2images, txt2html, tex2html\n", 
  "id": 32872009
}