{
  "read_at": 1462511560, 
  "description": ":crown: Supervised Semantics-preserving Deep Hashing (SSDH)", 
  "README.md": "# Caffe-DeepBinaryCode\n\nSupervised Learning of Semantics-Preserving Deep Hashing (SSDH)\n\nCreated by Kevin Lin, Huei-Fang Yang, and Chu-Song Chen at Academia Sinica, Taipei, Taiwan.\n\n\n## Introduction\n\nWe present a simple yet effective supervised deep hash approach that constructs binary hash codes from labeled data for large-scale image search. SSDH constructs hash functions as a latent layer in a deep network and the binary codes are learned by minimizing an objective function defined over classification error and other desirable hash codes properties. Compared to state-of-the-art results, SSDH achieves 26.30% (89.68% vs. 63.38%), 17.11% (89.00% vs. 71.89%) and 19.56% (31.28% vs. 11.72%) higher precisions averaged over a different number of top returned images for the CIFAR-10, NUS-WIDE, and SUN397 datasets, respectively.\n\n<img src=\"https://www.csie.ntu.edu.tw/~r01944012/ssdh_intro.png\" width=\"800\">\n\nThe details can be found in the following [arXiv preprint.](http://arxiv.org/abs/1507.00101)\nPresentation slide can be found [here](http://www.csie.ntu.edu.tw/~r01944012/deepworkshop-slide.pdf)\n\n\n## Citing the deep hashing work\n\nIf you find our work useful in your research, please consider citing:\n\n    Supervised Learning of Semantics-Preserving Hashing via Deep Neural Networks for Large-Scale Image Search\n    Huei-Fang Yang, Kevin Lin, Chu-Song Chen\n    arXiv preprint arXiv:1507.00101\n\n\n\n## Prerequisites\n\n  0. MATLAB (tested with 2012b on 64-bit Linux)\n  0. Caffe's [prerequisites](http://caffe.berkeleyvision.org/installation.html#prequequisites)\n\n\n## Install Caffe-DeepBinaryCode\n\nAdjust Makefile.config and simply run the following commands:\n\n    $ make all -j8\n    $ make test -j8\n    $ make matcaffe\n    $ ./prepare.sh\n\nFor a faster build, compile in parallel by doing `make all -j8` where 8 is the number of parallel threads for compilation (a good choice for the number of threads is the number of cores in your machine).\n\n## Demo\n \nLaunch matlab and run `demo.m`. This demo will generate 48-bits binary codes for each image using the proposed SSDH.\n    \n    >> demo\n\n\n<img src=\"https://www.csie.ntu.edu.tw/~r01944012/ssdh_demo.png\" width=\"350\">\n\n\n## Retrieval evaluation on CIFAR10\n\nLaunch matalb and run `run_cifar10.m` to perform the evaluation of `precision at k` and `mean average precision at k`. We set `k=1000` in the experiments. The bit length of binary codes is `48`. This process takes around 12 minutes.\n    \n    >> run_cifar10\n\n\nThen, you will get the `mAP` result as follows. \n\n    >> MAP = 0.899731\n\nMoreover, simply run the following commands to generate the `precision at k` curves:\n\n    $ cd analysis\n    $ gnuplot plot-p-at-k.gnuplot \n\nYou will reproduce the precision curves with respect to different number of top retrieved samples when the 48-bit hash codes are\nused in the evaluation.\n \n## Train SSDH on CIFAR10\n\nSimply run the following command to train SSDH:\n\n\n    $ ./examples/SSDH/train.sh\n\n\nAfter 50,000 iterations, the top-1 error is 9.7% on the test set of CIFAR10 dataset:\n```\nI0107 19:24:32.258903 23945 solver.cpp:326] Iteration 50000, loss = 0.0274982\nI0107 19:24:32.259012 23945 solver.cpp:346] Iteration 50000, Testing net (#0)\nI0107 19:24:36.696506 23945 solver.cpp:414]     Test net output #0: accuracy = 0.903125\nI0107 19:24:36.696543 23945 solver.cpp:414]     Test net output #1: loss: 50%-fire-rate = 1.47562e-06 (* 1 = 1.47562e-06 loss)\nI0107 19:24:36.696552 23945 solver.cpp:414]     Test net output #2: loss: classfication-error = 0.332657 (* 1 = 0.332657 loss)\nI0107 19:24:36.696559 23945 solver.cpp:414]     Test net output #3: loss: forcing-binary = -0.00317774 (* 1 = -0.00317774 loss)\nI0107 19:24:36.696565 23945 solver.cpp:331] Optimization Done.\nI0107 19:24:36.696570 23945 caffe.cpp:214] Optimization Done.\n```\n\nThe training process takes roughly 2~3 hours on a desktop with Titian X GPU. You will finally get your model named `SSDH48_iter_xxxxxx.caffemodel` under folder `/examples/SSDH/`\n\nTo use the model, modify the `model_file` in `demo.m` to link to your model:\n\n```\n    model_file = './YOUR/MODEL/PATH/filename.caffemodel';\n```\n\nLaunch matlab, run `demo.m` and enjoy!\n    \n    >> demo\n\n## Train SSDH on another dataset\n\nIt should be easy to train the model using another dataset as long as that dataset has label annotations.\n \n  0. Convert your training/test set into leveldb/lmdb format using `create_imagenet.sh`.\n  0. Modify the `source` in `/example/SSDH/train_val.prototxt` to link to your training/test set.\n  0. Run `./examples/SSDH/train.sh`, and start training on your dataset.\n\n\n## Resources\n\n**Note**: This documentation may contain links to third party websites, which are provided for your convenience only. Third party websites may be subject to the third party's terms, conditions, and privacy statements.\n\nIf `./prepare.sh` fails to download data, you may manually download the resouces from:\n\n0. 48-bit SSDH model: [MEGA](https://mega.nz/#!kJ1jwDpJ!X4dVUeWJ7Eqg9L8bhJaGbr9l5-HS3ccudbjIjIbYNpk), [DropBox](https://www.dropbox.com/s/6iqyz1mdhadhzbu/SSDH48_iter_50000.caffemodel?dl=0), [BaiduYun](http://pan.baidu.com/s/1nurCaJR)\n\n0. CIFAR10 dataset (jpg format): [MEGA](https://mega.nz/#!RENV1bhZ!x0uFnAkqUSTJzKr6HzeeNV9mtDjlgQ0x6ZaXfpxbJkw), [DropBox](https://www.dropbox.com/s/f7q3bbgvat2q1u2/cifar10-dataset.zip?dl=0), [BaiduYun](http://pan.baidu.com/s/1pKsSK7h)\n\n0. AlexNet pretrained networks: [MEGA](https://mega.nz/#!UZ0VGIYB!y2crhbo89S9hYLv5TyHLXXB5Sus8ZkpUzTNkeUPkfU4), [DropBox](https://www.dropbox.com/s/nlggnj47xxdmwkb/bvlc_reference_caffenet.caffemodel?dl=0), [BaiduYun](http://pan.baidu.com/s/1qWRMy4G)\n\n\n## Contact\n\nPlease feel free to leave suggestions or comments to Kevin Lin (kevinlin311.tw@iis.sinica.edu.tw), Huei-Fang Yang (hfyang@citi.sinica.edu.tw) or Chu-Song Chen (song@iis.sinica.edu.tw)\n\n\n\n", 
  "id": 34248410
}