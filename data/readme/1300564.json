{
  "README": "Release Notes for QUDA v0.8.0                         1st February 2016\n-----------------------------\n\nOverview:\n\nQUDA is a library for performing calculations in lattice QCD on\ngraphics processing units (GPUs), leveraging NVIDIA's CUDA platform.\nThe current release includes optimized Dirac operators and solvers for\nthe following fermion actions:\n\n* Wilson\n* Clover-improved Wilson\n* Twisted mass (including non-degenerate pairs)\n* Twisted mass with a clover term\n* Staggered fermions\n* Improved staggered (asqtad or HISQ)\n* Domain wall (4-d or 5-d preconditioned)\n* Mobius fermion\n\nImplementations of CG, multi-shift CG, BiCGstab, and DD-preconditioned\nGCR are provided, including robust mixed-precision variants supporting\ncombinations of double, single, and half (16-bit \"block floating\npoint\") precision.  The library also includes auxilliary routines\nnecessary for Hybrid Monte Carlo, such as HISQ link fattening, force\nterms and clover-field construction.  Use of many GPUs in parallel is\nsupported throughout, with communication handled by QMP or MPI.\n\n\nSoftware Compatibility:\n\nThe library has been tested under Linux (CentOS 5.8 and Ubuntu 14.04)\nusing releases 6.5, 7.0 and 7.5 of the CUDA toolkit.  CUDA 6.0 and\nearlier are not supported (though they may continue to work fine).\nThe library also works on recent 64-bit Intel-based Macs.  Due to\nissues with compilation using LLVM, under Mac OS X 10.9.x it is\nrequired to install and use GCC instead of the default clang compiler,\nthough this is unnecesary with Mac OS X 10.10.x.\n\nSee also \"Known Issues\" below.\n\n\nHardware Compatibility:\n\nFor a list of supported devices, see\n\nhttp://developer.nvidia.com/cuda-gpus\n\nBefore building the library, you should determine the \"compute\ncapability\" of your card, either from NVIDIA's documentation or by\nrunning the deviceQuery example in the CUDA SDK, and pass the\nappropriate value to QUDA's configure script.  For example, the Tesla\nC2075 is listed on the above website as having compute capability 2.0,\nand so to configure the library for this card, you'd run \"configure\n--enable-gpu-arch=sm_20 [other options]\" before typing \"make\".\n\nAs of QUDA 0.8.0, only devices of compute capability 2.0 or greater\nare supported.  See also \"Known Issues\" below.\n\n\nInstallation:\n\nThe recommended method for compiling QUDA is to use cmake, and build\nQUDA in a separate directory from the source directory.  For\ninstructions on how to build QUDA using cmake see this page\nhttps://github.com/lattice/quda/wiki/Building-QUDA-with-cmake.\n\nAlternatively, QUDA can also be built using \"configure\" and \"make\",\nthough this build approach is considered deprecated and will be\nremoved in a subsequent QUDA release.  See \"./configure --help\" for a\nlist of configure options.  At a minimum, you'll probably want to set\nthe GPU architecture; see \"Hardware Compatibility\" above.\n\nEnabling multi-GPU support requires passing the --enable-multi-gpu\nflag to configure, as well as --with-mpi=<PATH> and optionally\n--with-qmp=<PATH>.  If the latter is given, QUDA will use QMP for\ncommunications; otherwise, MPI will be called directly.  By default,\nit is assumed that the MPI compiler wrappers are <MPI_PATH>/bin/mpicc\nand <MPI_PATH>/bin/mpicxx for C and C++, respectively.  These choices\nmay be overriden by setting the CC and CXX variables on the command\nline as follows:\n\n./configure --enable-multi-gpu --with-mpi=<MPI_PATH> \\\n[--with-qmp=<QMP_PATH>] [OTHER_OPTIONS] CC=my_mpicc CXX=my_mpicxx\n\nFinally, with some MPI implementations, executables compiled against\nMPI will not run without \"mpirun\".  This has the side effect of\ncausing the configure script to believe that the compiler is failing\nto produce a valid executable.  To skip these checks, one can trick\nconfigure into thinking that it's cross-compiling by setting the\n--build=none and --host=<HOST> flags.  For the latter,\n\"--host=x86_64-linux-gnu\" should work on a 64-bit linux system.\n\nBy default only the QDP and MILC interfaces are enabled.  For\ninterfacing support with QDPJIT, BQCD or CPS; this should be enabled\nat configure time with the appropriate flag, e.g.,\n--enable-bqcd-interface.  To keep compilation time to a minimum it is\nrecommended to only enable those interfaces that are used by a given\napplication.  The QDP and MILC interfaces can be disabled with the,\ne.g., --disable-milc-interface flag.\n\nThe eigen-vector solvers (eigCG and incremental eigCG) require the\ninstallation of the MAGMA dense linear algebra package.  It is\nrecommended that MAGMA 1.7.x is used, though versions are 1.5.x and\n1.6.x should work.  MAGMA is available from\nhttp://icl.cs.utk.edu/magma/index.html.  MAGMA is enabled using the\nconfigure option --with-magma=MAGMA_PATH.\n\nIf Fortran interface support is desired, the F90 environment variable\nshould be set when configure is invoked, and \"make fortran\" must be\nrun explicitly, since the Fortran interface modules are not built by\ndefault.\n\nAs examples, the scripts \"configure.milc.titan\" and\n\"configure.chroma.titan\" are provided.  These configure QUDA for\nexpected use with MILC and Chroma, respectively, on Titan (the Tesla\nK20X-powered Cray XK7 supercomputer at the Oak Ridge Leadership\nComputing Facility).\n\nThroughout the library, auto-tuning is used to select optimal launch\nparameters for most performance-critical kernels.  This tuning\nprocess takes some time and will generally slow things down the first\ntime a given kernel is called during a run.  To avoid this one-time\noverhead in subsequent runs (using the same action, solver, lattice\nvolume, etc.), the optimal parameters are cached to disk.  For this\nto work, the QUDA_RESOURCE_PATH environment variable must be set,\npointing to a writeable directory.  Note that since the tuned parameters\nare hardware-specific, this \"resource directory\" should not be shared\nbetween jobs running on different systems (e.g., two clusters\nwith different GPUs installed).  Attempting to use parameters tuned\nfor one card on a different card may lead to unexpected errors.\n\nThis autotuning information can also be used to build up a first-order\nkernel profile: since the autotuner measures how long a kernel takes\nto run, if we simply keep track of the number of kernel calls, from\nthe product of these two quantities we have a time profile of a given\njob run.  If QUDA_RESOURCE_PATH is set, then this profiling\ninformation is output to the file \"profile.tsv\" in this specified\ndirectory.  Optionally, the output filename can be specified using the\nQUDA_PROFILE_OUTPUT environment variable, to avoid overwriting\npreviously generated profile outputs.\n\nUsing the Library:\n\nInclude the header file include/quda.h in your application, link\nagainst lib/libquda.a, and study tests/invert_test.cpp (for Wilson,\nclover, twisted-mass, or domain wall fermions) or\ntests/staggered_invert_test.cpp (for asqtad/HISQ fermions) for\nexamples of the solver interface.  The various solver options are\nenumerated in include/enum_quda.h.\n\n\nKnown Issues:\n\n* For compatibility with CUDA, on 32-bit platforms the library is\n  compiled with the GCC option -malign-double.  This differs from the\n  GCC default and may affect the alignment of various structures,\n  notably those of type QudaGaugeParam and QudaInvertParam, defined in\n  quda.h.  Therefore, any code to be linked against QUDA should also\n  be compiled with this option.\n\n* When the auto-tuner is active in a multi-GPU run it may cause issues\n  with binary reproducibility of this run. This is caused by the\n  possibility of different launch configurations being used on\n  different GPUs in the tuning run. If binary reproducibility is\n  strictly required make sure that a run with active tuning has\n  completed. This will ensure that the same launch configurations for\n  a given Kernel is used on all GPUs and binary reproducibility.\n\nGetting Help:\n\nPlease visit http://lattice.github.com/quda for contact information.\nBug reports are especially welcome.\n\n\nAcknowledging QUDA:\n\nIf you find this software useful in your work, please cite:\n\nM. A. Clark, R. Babich, K. Barros, R. Brower, and C. Rebbi, \"Solving\nLattice QCD systems of equations using mixed precision solvers on GPUs,\"\nComput. Phys. Commun. 181, 1517 (2010) [arXiv:0911.3191 [hep-lat]].\n\nWhen taking advantage of multi-GPU support, please also cite:\n\nR. Babich, M. A. Clark, B. Joo, G. Shi, R. C. Brower, and S. Gottlieb,\n\"Scaling lattice QCD beyond 100 GPUs,\" International Conference for\nHigh Performance Computing, Networking, Storage and Analysis (SC),\n2011 [arXiv:1109.2935 [hep-lat]].\n\nSeveral other papers that might be of interest are listed at\nhttp://lattice.github.com/quda .\n\n\nAuthors:\n\nRonald Babich (NVIDIA)\nKipton Barros (Los Alamos National Laboratory)\nRichard Brower (Boston University)\nNuno Cardoso (NCSA)\nMike Clark (NVIDIA)\nJustin Foley (University of Utah)\nJoel Giedt (Rensselaer Polytechnic Institute)\nSteven Gottlieb (Indiana University)\nDean Howarth (Rensselaer Polytechnic Institute)\nBalint Joo (Jefferson Laboratory)\nHyung-Jin Kim (Samsung Advanced Institute of Technology)\nClaudio Rebbi (Boston University)\nGuochun Shi (NCSA)\nAlexei Strelchenko (Fermi National Accelerator Laboratory)\nAlejandro Vaquero (INFN Sezione Milano Bicocca)\nMathias Wagner (NVIDIA)\n\n\nPortions of this software were developed at the Innovative Systems Lab,\nNational Center for Supercomputing Applications\nhttp://www.ncsa.uiuc.edu/AboutUs/Directorates/ISL.html\n\nDevelopment was supported in part by the U.S. Department of Energy\nunder grants DE-FC02-06ER41440, DE-FC02-06ER41449, and\nDE-AC05-06OR23177; the National Science Foundation under grants\nDGE-0221680, PHY-0427646, PHY-0835713, OCI-0946441, and OCI-1060067;\nas well as the PRACE project funded in part by the EUs 7th Framework\nProgramme (FP7/2007-2013) under grants RI-211528 and FP7-261557.  Any\nopinions, findings, and conclusions or recommendations expressed in\nthis material are those of the authors and do not necessarily reflect\nthe views of the Department of Energy, the National Science\nFoundation, or the PRACE project.\n", 
  "read_at": 1462546888, 
  "description": "QUDA is a library for performing calculations in lattice QCD on GPUs.", 
  "id": 1300564
}