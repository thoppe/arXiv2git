{
  "read_at": 1462556662, 
  "description": "An attempt to reproduce the results of \"Asynchronous Methods for Deep Reinforcement Learning\" (http://arxiv.org/abs/1602.01783)", 
  "README.md": "# Async-RL\n\nThis is a repository where I attempt to reproduce the results of [Asynchronous Methods for Deep Reinforcement Learning](http://arxiv.org/abs/1602.01783). It's still work-in-progress and not so successfull compared to the original results.\n\nAny feedback is welcome :)\n\n## Current Status\n\nI trained A3C for ALE's Breakout with 8 processes for about 2 days and 5 hours. The scores of test runs along training are plotted below. One test run for every 100000 training steps (counted by the global shared counter).\n\n![A3C scores on Breakout](https://raw.githubusercontent.com/muupan/async-rl/master/trained_model/breakout_scores.png)\n\nYou can make the trained model to play Breakout by the following command:\n\n```\npython demo_a3c_ale.py <path-to-breakout-rom> trained_model/breakout_48100000.h5\n```\n\n### Some Hyperparameters\n\n- RMSprop\n - learning rate: initialize with 3.5e-4 (policy) and 7e-4 (value function) and linearly decrease to zero\n - epsilon: 0.1 (epsilon is inside sqrt)\n - alpha: 0.99\n\n## Requirements\n\n- Python 3.5.1\n- chainer 1.8.1\n- cached-property 1.3.0\n- h5py 2.5.0\n- Arcade-Learning-Environment\n\n## Train\n\n```\npython a3c_ale.py <number-of-processes> <path-to-atari-rom>\n```\n\n`a3c_ale.py` will save best-so-far models and test scores into the output directory.\n\n## Evaluation\n\n```\npython demo_a3c_ale.py <path-to-atari-rom> <trained-model>\n```\n", 
  "id": 58062739
}