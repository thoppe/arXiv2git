{
  "read_at": 1462556567, 
  "description": "Less is More: Nystr\u00f6m Computational Regularization", 
  "README.md": "\n\n\n![LIM2](http://s22.postimg.org/x4jihvp0x/lim2.png)\nThe NystromCoRe Matlab Package\n========================\n***Less is More: Nystrom Computational Regularization***\n\n![LIM1](http://s4.postimg.org/e8cnt4sst/LIS1.png)\n\n\n\n\nCopyright (C) 2015, [Laboratory for Computational and Statistical Learning](http://lcsl.mit.edu/#/home) (IIT@MIT).\nAll rights reserved.\n\n*By Alessandro Rudi, Raffaello Camoriano and Lorenzo Rosasco*\n\n*Contact: raffaello.camoriano@iit.it*\n\nPlease check the attached license file.\n\nIntroduction\n============\n\nThis Matlab package provides an implementation of the Nystrom Computational Regularization algorithm presented in the following work:\n\n> *Alessandro Rudi, Raffaello Camoriano, Lorenzo Rosasco*, ***Less is More: Nystrom Computational Regularization***, 16 Jul 2015, http://arxiv.org/abs/1507.04717\n\n> We study Nystrom type subsampling approaches to large scale kernel methods, and prove learning bounds in the statistical learning setting, where random sampling and high probability estimates are considered. In particular, we prove that these approaches can achieve optimal learning bounds, provided the subsampling level is suitably chosen. These results suggest a simple incremental variant of Nystrom Kernel Regularized Least Squares, where the subsampling level implements a form of computational regularization, in the sense that it controls at the same time regularization and computations. Extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets. \n\nThis software package provides a simple and extendible interface to Nystrom Computational Regularization. It has been tested on MATLAB r2014b, but should work on newer and older versions too. If it does not, please contact us and/or open an issue. Examples are available  in the \"examples\" folder.\n\nExamples\n====\n\nAutomatic training with default options\n----\n\n```matlab\n\nload breastcancer\n\n% Perform default cross validation\n[ training_output ] = nystromCoRe_train( Xtr , Ytr );\n\n% Perform predictions on the test set and evaluate results\n[ prediction_output ] = nystromCoRe_test( Xte , Yte , training_output);\n```\n\nSpecifying a custom kernel parameter\n----\n```matlab\n\n\nload breastcancer\n\n% Customize configuration\nconfig = config_set('kernel.kernelParameter' , 0.9 , ...           % Change gaussian kernel parameter (sigma)\n                    'kernel.kernelFunction' , @gaussianKernel);     % Change kernel function\n\n% Perform default cross validation\n[ training_output ] = nystromCoRe_train( Xtr , Ytr , config);\n\n% Perform predictions on the test set and evaluate results\n[ prediction_output ] = nystromCoRe_test( Xte , Yte , training_output);\n```\n\nSpecifying a custom subsampling level range\n----\n```matlab\n\nload breastcancer\n\n% Customize configuration\nconfig = config_set('kernel.minM' , 10 , ...         % Minimum subsampling level\n                    'kernel.maxM' , 200 , ...        % Maximum subsampling level\n                    'kernel.numStepsM' , 191 , ...   % Set m steps (in this version, it must be set to maxM - minM + 1)\n                    'kernel.kernelParameter' , 0.9 , ...           % Change gaussian kernel parameter (sigma)\n                    'kernel.kernelFunction' , @gaussianKernel);     % Change kernel function\n\n% Perform default cross validation\n[ training_output ] = nystromCoRe_train( Xtr , Ytr , config);\n\n% Perform predictions on the test set and evaluate results\n[ prediction_output ] = nystromCoRe_test( Xte , Yte , training_output);\n```\n\nSome more customizations\n----\n```matlab\n\nload breastcancer\n\n% Customize configuration\nconfig = config_set('crossValidation.recompute' , 1 , ...           % Recompute the solution after cross validation\n                    'crossValidation.codingFunction' , @zeroOneBin , ...   % Change coding function\n                    'crossValidation.errorFunction' , @classificationError , ...   % Change error function\n                    'kernel.kernelParameter' , 0.9 , ...           % Change gaussian kernel parameter (sigma)\n                    'kernel.kernelFunction' , @gaussianKernel);     % Change kernel function\n\n% Perform default cross validation\n[ training_output ] = nystromCoRe_train( Xtr , Ytr , config);\n\n% Perform predictions on the test set and evaluate results\n[ prediction_output ] = nystromCoRe_test( Xte , Yte , training_output);\n```\n**For a complete list of customizable configuration options, see the next section.**\n\n\nConfiguration Parameters\n====\nAll the configurable parameters of the algorithm can be set by means of the provided *config_set* function, which returns a custom configuration structure that can be passed to the *nystromCoRe_train* function. If no configuration structure is passed, *nystromCoRe_train* uses the default configuration parameters listed below. *nystromCoRe_train* performs the training by running the NYTRO algorithm. It returns a structure with the trained model, which can then be passed to *nystromCoRe_test* for performing predictions and test error assessment.\n\nThis is an example of how the configuration parameters can be customized by means of the *config_set* function. See the code in \"examples/customCrossValidation.m\" for more details.\n\n```matlab\n% Customize configuration\nconfig = config_set('crossValidation.recompute' , 1 , ...           % Recompute the solution after cross validation\n                    'crossValidation.codingFunction' , @zeroOneBin , ...   % Change coding function\n                    'crossValidation.errorFunction' , @classificationError , ...   % Change error function\n                    'kernel.kernelParameter' , 0.9 , ...           % Change kernel parameter (sigma)\n                    'kernel.kernelFunction' , @gaussianKernel);     % Change kernel function\n```\n\n**The default configuration parametrs are reported below:**\n* **Data**\n    * data.shuffle = 1\n\n* **Cross Validation**\n    * crossValidation.storeTrainingError = 0\n    * crossValidation.validationPart = 0.2\n    * crossValidation.recompute = 0\n    * crossValidation.errorFunction = @rmse\n        * Provided functions (*errorFunctions* folder):\n            * @rmse: root mean squared error\n            * @classificationError : Relative classification error (error rate)\n        * Custom functions can be implemented by the user, simply following the input-output structure of any of the provided functions.\n    * crossValidation.codingFunction = [ ]\n        * Provided functions (*codingFunctions* folder):\n            * @plusMinusOneBin: Class 1: +1, class 2: -1\n            * @zeroOneBin : Class 1: +1, class 2: 0\n        * Custom functions can be implemented by the user, simply following the input-output structure of any of the provided functions.\n    * crossValidation.stoppingRule = @windowLinearFitting\n        * Provided functions (*stoppingRules* folder):\n            * @windowSimple: Stops if the ratio e1/e0 >= (1-threshold). e1 is the error of the most recent iteration. e0 is the error of the oldest iteration in the window\n            * @windowAveraged : Works like @windowSimple, but taking e1 and e0 as the mean over the oldest and newest 10% of the points contained in the window (to increase stability).\n            * @windowMedian : Works like @windowAveraged, but computes the median rather than the mean.\n            * @windowLinearFitting : Works like @windowSimple, but uses a linear fitting of all the points in the window to obtain a more stable estimate of e0 and e1.\n        * Custom functions can be implemented by the user, simply following the input-output structure of any of the provided functions.\n    * crossValidation.windowSize = 10\n    * crossValidation.threshold = 0\n\n* **Filter**\n    * filter.numLambdaGuesses  = 11\n    * filter.lambdaGuesses  = logspace(0,-10,config.filter.numLambdaGuesses)\n\n* **Kernel**\n    * kernel.kernelFunction  = @gaussianKernel\n        * Provided functions:\n            * @gaussianKernel : Gaussian kernel function. In this case, the kernel parameter is the bandwidth sigma.\n        * Custom functions can be implemented by the user, simply following the input-output structure of any of the provided functions.\n    * kernel.kernelParameter = 1\n    * kernel.fixedM = [];\n    * kernel.minM = 10;\n    * kernel.maxM = 100;\n    * kernel.numStepsM = 91;    \n    \nOutput structures\n======\n\n*nystromCoRe_train*\n----\n\n* best.\n    * validationError : Best validation error found in cross validation\n    * m : Best subsampling level\n    * alpha : Best coefficients vector\n    * lambda : Best lambda\n    * lambdaIdx : Best lambda index\n    * sampledPoints : Sampled training points associated to the lowest validation error\n\n* time.\n    * kernelComputation : Time for kernel computation\n    * crossValidationTrain : Time for filter iterations during cross validation\n    * crossValidationEval : Time for validation error evaluation during cross validation\n    * crossValidationTotal : Cumulative cross validation time\n    * retrainint : Time for retraining after cross validation\n    * fullTraining : Training time in the just-train case (no cross validation)\n\n* errorPath.\n    * training : Training error path for each of the computed iterations\n    * validation : Validation error path for each of the computed iterations\n\n*nystromCoRe_test*\n----\n\n* YtePred : Predicted output\n* testError : Test error\n* time.\n    * kernelComputation : Kernel computation time\n    * prediction : Prediction computation time\n    * errorComputation : Error computation time\n", 
  "id": 45035117
}