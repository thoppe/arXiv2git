{
  "README": "JNN (Java Neural Network Toolkit) - 2015-09-10\n\nOriginal writer: Wang Ling\n\nThis package contains a Java Neural Network Toolkit with implementations of:\n-An Word Representation model allowing vectors to be generated as a word lookup table,a set of features, or/and the C2W model (words are representated by their sequence of characters)\n-An LSTM-based Language Model\n-An LSTM-based Part-Of-Speech-Tagger Model\n\nThe system requires Java 1.8+ to be installed, and approximately 8-16 GB of memory depending on the size and complexity of the network.\n\n0.1 Quick Start \n-----------------------------------------------\nExamples for training a Part-of-speech tagger and language models can be found in scripts/run_pos.sh and scripts/run_lm.sh, respectively.\nThese can be run with the following commands:\n\nsh scripts/run_pos.sh\nsh scripts/run_lm.sh\n\nThese scripts download currently available data for both tasks, and serve as examples of how the code is to be run. \nThe POS tagger is trained on the Ark POS dataset found in https://code.google.com/p/ark-tweet-nlp/downloads/list . The language models are trained on subsets of wikipedia, which we make available at https://www.l2f.inesc-id.pt/~wlin/wiki.gz .\n\n1.1 Language Modeling\n-----------------------------------------------\n\nSample datasets can be downloaded by running:\nsh scripts/download_wikidata.sh\n\nThe LSTM-based language model can be trained by calling:\n\njava -Xmx10g -cp jnn.jar:libs/* jnn.functions.nlp.app.lm.LSTMLanguageModel -batch_size 10 -iterations 1000000 -lr 0.1 -output_dir sample_lm_model -softmax_function word-5000 -test_file wiki/wiki.test.en -threads 8 -train_file wiki/wiki.train.en -validation_file wiki/wiki.dev.en -validation_interval 10000 -word_dim 50 -char_dim 50 -char_state_dim 150 -lm_state_dim 150 -word_features characters -nd4j_resource_dir nd4j_resources -update momentum\n\nThis command will train a neural language model using the training file wiki/wiki.train.en, validating on wiki/wiki.val.en, and testing on wiki/wiki.test.en. \n\nArguments are described below:\n\nbatch_size - number of sentences (lines) processed in each each mini-batch\niterations - number of iterations the model is to be trained (each iterations processes one mini-batch)\nlr - learning rate\noutput_dir - directory to save the model, write the statistics (perplexities), and scores for the test data\nword_features - type of word representation used (options described in 3)\nsoftmax_function - type of softmax unit used for predicting words (options described in 1.2)\ntrain_file - training text file \nvalidation_file - validation text file \ntest_file - test text file\nvalidation_interval - number of mini-batches to be run before computing perplexities on the validation set\nword_dim - word vector dimension (In a lookup table, this will generate a vocab*word_dim table, while in the C2W model, the character LSTM states will be projected into a vector of size word_dim)\nchar_dim - character vector dimension (Always uses a lookup table)\nchar_state_dim - LSTM state and cell dimensions used to build lstm states\nlm_state - LSTM state and cell dimensions for the language model\nnd4j_resource_dir - ND4J configuration directories (simply point to nd4j_resources)\nthreads - number of threads to be used (sentences in each mini-batch will be divided among threads)\nupdate - sgd method (regular, momentum or adagrad)\n\nThe following files will be created in the directory specified by -output_dir:\n\nmodel.gz - The model is stored in this file every time the validation perplexity improves over the previous best value. If this file exists when the command is called, this model will be loaded and training will be carried out from this point. This way if something goes wrong during training (e.g. server crashes), training will resume at the last saved point.\nmodel.tmp.gz - A backup copy of hte model.gz file, this is kept so that if the script fails when model.gz is being written, it is not lost. Thus, if model.gz is incomplete, simply copy model.tmp.gz over it.\nrep.gz - The word representation model, this can be used in order to reuse the word representations trained on this task as initilization for other tasks.\ntest.scores.gz - Once the model finishes training, the file specified by -test_file is trained and sentence level perplexities are computed and stored in this file. (to simply run a model on the test set, make sure the model.gz is created and set -iterations to 0)\nstats - Reports statistics during training. In this task, perplexities on the development set are reported.\n\n1.2 Softmax functions\n-----------------------------------------------\nThe most straight-forward way to predict each word as a softmax over the whole training vocabulary (set -softmax_function to word). However, the normalization over the whole vocabulary is expensive. One way around this problem is to prune the vocabulary by replacing less frequent words by an unknown token. This can be done by setting -softmax_function to word-*, where * is the number of words to consider. Thus, word-5000, will perform a softmax over the top 5000 words and replaces the rest of the words by an unknown token. \n\nIt is also possible to use Noise Constrastive Estimation by setting the -softmax_function parameter to word-nce. This allows parameters to be estimated for the whole vocabulary, while avoiding the normalization over the whole vocabulary at training time.\n\n2.1 Part-of-Speech Tagging\n-----------------------------------------------\n\nSample datasets can be downloaded by running:\nsh scripts/download_posdata.sh\n\nThe LSTM-based Part-of-Speech Tagger can be trained by calling:\n\njava -Xmx10g -cp jnn.jar:libs/* jnn.functions.nlp.app.pos.PosTagger -lr 0.3 -batch_size 100 -validation_interval 10 -threads 8 -train_file twpos-data-v0.3/oct27.splits/oct27.train -validation_file twpos-data-v0.3/oct27.splits/oct27.dev -test_file twpos-data-v0.3/oct27.splits/oct27.test -input_format conll-0-1 -word_features characters -context_model blstm -iterations 1000 -output_dir /models/pos_model -sequence_activation 2 -word_dim 50 -char_dim 50 -char_state_dim 150  -context_state_dim 150 -update momentum -nd4j_resource_dir nd4j_resources/\n\nThis command will train a POS tagger using the training file twpos-data-v0.3/oct27.splits/oct27.train, validating on twpos-data-v0.3/oct27.splits/oct27.dev, and testing on twpos-data-v0.3/oct27.splits/oct27.test. \n\nArguments are described below:\nbatch_size - number of sentences (lines) processed in each each mini-batch\niterations - number of iterations the model is to be trained (each iterations processes one mini-batch)\nlr - learning rate\noutput_dir - directory to save the model, write the statistics (accuracies), and scores for the test data\nword_features - type of word representation used (options described in 3)\ntrain_file - training file\nvalidation_file - validation file\ntest_file - test file\ninput_format - file format (options described in 2.2)\ncontext_model - model that encodes contextual information (options described in 2.3)\nword_dim - word vector dimension (In a lookup table, this will generate a vocab*word_dim table, while in the C2W model, the character LSTM states will be projected into a vector of size word_dim)\nchar_dim - character vector dimension (Always uses a lookup table)\nchar_state_dim - LSTM state and cell dimensions used to build lstm states\nlm_state - LSTM state and cell dimensions for the language model\nsequence_activation - Activation function applied to the word vector after the composition (0 = none, 1 = logistic, 2 = tanh)\nnd4j_resource_dir - ND4J configuration directories (simply point to nd4j_resources)\nthreads - number of threads to be used (sentences in each mini-batch will be divided among threads)\nupdate - sgd method (regular, momentum or adagrad)\n\nThe following files will be created in the directory specified by -output_dir:\n\nmodel.gz - The model is stored in this file every time the validation perplexity exceeds the previous highest value. If this file exists when the command is called, this model will be loaded and training will be carried out from this point. This way if something goes wrong during training (e.g. server crashes), training will resume at the last saved point.\nmodel.tmp.gz - A backup copy of hte model.gz file, this is kept so that if the script fails when model.gz is being written, it is not lost. Thus, if model.gz is incomplete, simply copy model.tmp.gz over it.\nrep.gz - The word representation model, this can be used in order to reuse the word representations trained on this task as initilization for other tasks.\nvalidation.output - Automatically tagged validation set using the tagger.\ntest.output - Automatically tagged test set using the tagger.\nvalidation.output - Reports statistics during training on the validation set. In this task, tagging accuracies are reported.\ntest.output - Reports statistics during training on the test set. In this task, tagging accuracies are reported.\nvalidation.correct - Lists correctly labelled words in the validation set.\ntest.correct - Lists correctly labelled words in the test set.\nvalidation.incorrect - Lists incorrectly labelled words in the validation set.\ntest.incorrect - Lists incorrectly labelled words in the test set.\n\n2.2 File Formats\n-----------------------------------------------\nWe allow 3 different formats. \n1 - The Conll column format is displayed as follows:\n\n1       In      _       IN      IN      _       43      ADV     _       _\n2       an      _       DT      DT      _       5       NMOD    _       _\n3       Oct.    _       NN      NNP     _       5       TMP     _       _\n4       19      _       CD      CD      _       3       NMOD    _       _\n5       review  _       NN      NN      _       1       PMOD    _       _\n6       of      _       IN      IN      _       5       NMOD    _       _\n7       ``      _       ``      ``      _       9       P       _       _\n8       The     _       DT      DT      _       9       NMOD    _       _\n9       Misanthrope     _       NN      NN      _       6       PMOD    _       _\n10      ''      _       ''      ''      _       9       P       _       _\n11      at      _       IN      IN      _       9       NMOD    _       _\n12      Chicago _       NN      NNP     _       15      NMOD    _       _\n13      's      _       PO      POS     _       12      NMOD    _       _\n14      Goodman _       NN      NNP     _       15      NMOD    _       _\n15      Theatre _       NN      NNP     _       11      PMOD    _       _\n\nThis format can be specified by setting -input_format as conll-*-+, where * is the column of the token (starting from 0) and + is the column of the POS tag. In the example above you probably wish to set -input_format as conll-1-4.\n\n2 - The parallel data format, where the tokens and tags are separated by \" ||| \":\nIn an Oct. 19 review of `` The Misanthrope '' at Chicago 's Goodman Theatre ||| IN DT NNP CD NN IN `` DT NN '' IN NNP POS NNP NNP\n\nThis format can be specfied by setting -input_format as parallel.\n\n3 - The Stanford original format with chunking information is displayed as follows:\n\nIf/IN\n[ you/PRP ]\n'd/MD really/RB rather/RB have/VB\n[ a/DT Buick/NNP ]\n,/, do/VB n't/RB leave/VB\n[ home/NN ]\nwithout/IN\n[ the/DT American/NNP Express/NNP card/NN ]\n./.\n\nThis format can be specified by defining the -input_format as stanford.\n\n2-3 Context Models\n-----------------------------------------------\nOur tagger uses a Bidirectional Long Short Term Memory RNN to encode contextual information before tagging each word, which can be specified by setting -context_model to BLSTM. This leads to better results in general as compared to a window-based model, which can be specified by setting -context_model to window.\n\n3-1 Word Representations Lookup Tables\n-----------------------------------------------\nIn general, words are converted into K-dimensional vectors though a lookup table, where each individual word type is matched with an independent vector. This can be specified by setting the -word_features argument as \"words\". This can be generalized any discrete feature, for instance, if we wish to model words as their 3 letter prefix, we simply build a lookup table of all observed prefixes with size 3 and all words sharing the same 3 letter prefix will be associated with the same vector. In this view, the \"words\" feature can be seen as an identity feature, where no word types share the same vector. Finally, multiple features can be used simultanously by enumerating the desired features. For instance, setting -word_features to \"words,prefix-3\", will simultaneuously use the identity feature and the prefix feature with size 3. Below are the features that are available:\n\nword : lowercased word\nprefix-* : prefix of size * (e.g. setting * to 3 means prefix with size 3)\nsuffix-* : suffix of size * (e.g. setting * to 3 means suffix with size 3)\ncapitalization : binary feature that is 1 or 0 depending on whether the first letter is uppercased\ncasing : ternary feature that is 2 if all letters are uppercased, 1 if the first letter is uppercased and 0 otherwise\nshape : replaces uppercased letters with X, lowercased letters with x and digits with d (e.g. John12 -> Xxxxdd)\nshape-no-repeat : same as shape, but removes repeated letters (e.g. John12 -> Xxd)\n\nFinally, it is common for unseen feature activations in the training set to be found in the development and test sets, such as out-of-vocabulary words or prefixes. If this happens an unknown feature is activated instead. During training, we stochastically replace singleton feature activations by this token with probability 0.5.\n\n3-2 C2W Model\n-----------------------------------------------\n\nAn option to lookup tables is to compose the word representation from its characters. In our work, we use LSTMs to compose the K-dimensional vector for each word. We call this model C2W (character to word), and it can be used by setting -word_features to one of the following options:\n\ncharacters : LSTM composition for characters\ncharacters-lowercase : LSTM composition for lowercased characters\n\nOnce again, it is possible to use the C2W model with lookup tables, for instance, setting the -word_features to word,suffix-3,character will use the word lookup table, a suffix lookup table and the C2W model. \n\nIt is also worth mentioning that if the same word occurs multiple times in the same mini-batch, it will only be composed once to save computation. Thus, it is generally advised to user larger mini-batches (more than 10 sentences) for faster computation. \n\nFor more information regarding the C2W model, check out our work at:\nhttp://arxiv.org/abs/1508.02096\n\nCONTENTS\n-----------------------------------------------\nREADME\n\n  This file.\n\nCOPYING\n\n  License file\n\njnn.jar\n\n  This is a JAR file containing all classes in JNN and necessary to run the language models and taggers.\n\nsrc\n\n  A directory containing the Java 1.8 source code for JNN.\n\nlibs\n\n  These are java libraries needed to run JNN\n\nscripts\n\n  This directory contains examples scripts for different tasks\n\nnd4j_resources\n\n  Configuration files for ND4J (http://nd4j.org)\n\nLICENSE\n-----------------------------------------------\n\nThe code provided by this package free and is distributed under the following license\n\n                         Wang Ling\n                         wanglin1122@gmail.com\n                         Copyright (c) 2015\n                         All Rights Reserved.\n\nPermission is hereby granted, free of charge, to use and distribute these items wiwithout restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of this work, and to permit persons to whom this work is furnished to do so, subject to the following conditions:\n1. The contents of this package must retain the above copyright notice, this list of conditions and the following disclaimer.\n2. Any modifications must be clearly marked as such.\n3. Original authors' names are not deleted.\n4. The authors' names are not used to endorse or promote products derived from this software without specific prior written permission.\n\nTHE AUTHORS AND THE CONTRIBUTORS TO THIS WORK DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT SHALL CARNEGIE MELLON UNIVERSITY NOR THE CONTRIBUTORS BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nCONTACT\n-----------------------------------------------\nFor more information, bug reports, fixes, contact:\n    \t\t\t\n\t\t\tWang Ling\n\t\t\twanglin1122@gmail.com\n\t\t\tINESC-ID & Carnegie Mellon University\n\t\t\tLisbon, Portugal\n", 
  "read_at": 1462544660, 
  "description": "Java Neural Network Library", 
  "id": 31564630
}