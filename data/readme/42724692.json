{
  "read_at": 1462549061, 
  "description": "A set of programs to compute luminosity functions.", 
  "README.md": "LFtools\n=======\n\nLFtools is a set of four programmes to compute luminosity functions\n(LF). One programme corrects the catalogue for absorption, photometric\nredshift uncertainties, and redshift incompleteness; all corrections\nare optional. The other three programmes compute LFs as binned\nestimates, maximum-likelihood fit, or Bayesian posterior\nsamples. Companion software includes plotting routines.\n\n\nInstallation\n------------\n\n### Binary installation ###\n\nBinary files are provided for Linux and Mac at the address http://www.astro.lu.se/~piero/LFtools/index.html .\n\nThe maximum-likelihood programme (`lf-ml`) also needs\n[CERNLIB](http://cernlib.web.cern.ch/cernlib/). Ubuntu users can\ninstall it from the standard repositories; all other users should get\nthem from CERN (link above).\n\nThe following libraries are also needed for `lf-mn`:\n\n  * [BLAS](http://en.wikipedia.org/wiki/Blas) (any version);\n  * openmpi\n\nThey are available, for Mageia and Ubuntu, from the standard\nrepositories and can be therefore installed from there. For CentOS,\nwhich does not provide up-to-date compilers and libraries, we offer\na tar (`centos6-libextra.tgz`) containing precompiled versions of\nlibopenmpi and libgfortran.\n\n\n### Source installation ###\n\nLFtools has the following requirements:\n\n  * gfortran, at least version 4.7;\n  * CERNLIB (for maximum-likelihood fits);\n  * MultiNest (for Bayesian inference).\n\nMultiNest itself requires: cmake, openmpi, BLAS. Please refer to the\nMultiNest documentation.\n\nYou can get the LFtools sources as a zip file from the\nrepository. Then unzip and cd into the LFtools directory:\n\n    unzip LFtools.zip\n    cd LFtools\n\nStart by compiling the FSON and m_option_parser libraries:\n    cd libextra/fson\n    make\n    cd ../m_option_parser\n    gfortran -c m_option_parser.F90\n    cd ../..\n\nNext, compile `lf-catcorrect` and `lf-binned`:\n\n    cd src1\n    make\n\nIf the programmes compile successfully, the binaries will be in\nLFtools/bin. Next, compile `lf-ml` and `lf-mn`. These are compiled\nseparately. Move to the second source directory:\n\n    cd ../src2\n\nand edit the Makefile, making sure to set up correctly the LDFLAGS\nline at its beginning: it should points to the location where the\nrequired libraries are. There are a few examples included as\ncomments. Compile:\n\n    make\n\nFinally, compile the Bayesian inference tools:\n\n    cd Multinest\n    make\n\nIf the programmes compile successfully, all the binaries will be in\nLFtools/bin.\n\nWe acknowledge the use of the following Fortran modules, whose\nsource we redistribute together with LFtools, and which whose copyrights\nare of the respective authors:\n\n  * m_option_parser  (BSD license, http://users.telenet.be/tuinbels/fortran_cmd_line_parser.html )\n  * FSON             (MIT license, https://github.com/josephalevin/fson )\n\n\nPreliminary steps\n-----------------\n\n### Prepare catalogue, area, and photoz files ###\n\nThe first step is to prepare the input catalogue and area files.\n\nThe input catalogue should be an ASCII table with the following columns:\n\n 1. ID number\n 2. 2-10 keV flux\n 3. specflag  (1 if spectroscopic redshift; 0 if photometric)\n 4. redshift\n 5. match probability (use 1 if you are uncertain)\n 6. 0.5-2 / 2-10 keV flux ratio\n 7. optical ID number\n\nComment lines are recognised if they start with a hash symbol (#).\nOptical ID numbers will be used to locate the file containing the\nphotometric redshift probability distribution (one file per source in\nthe catalogue).\n\nAn example of how to prepare the catalogue can be found in\n`src1/convert-cosmos.f90`; this programme reads the XMM-COSMOS catalogue,\nextracts the relevant information, and writes it in the LFtools input\nformat. To compile and run it:\n\n    make convert\n    ./convert-cosmos\n\n\n\nThe area file should be an ASCII table with the following columns:\n\n 1. log10( flux )\n 2. coverage (in deg$^{-2}$)\n\nwhere the coverage is the area over which a source brighter than\n*flux* could be observed. Comment lines are recognised if they start\nwith a hash symbol (#).\n\nIf you want to correct for photometric redshift uncertainties, you\nneed a file for each source in the catalogue. These files should be\nnamed \"Id number .spec\" where number is the optical ID, padded with\nzeroes to 9 digits. Example: source with optical ID = 123 has its\nphotoz distribution in the file \"Id000000123.spec\".  These files\nshould contain a section, starting at line LSTART and with a length of\nLLEN, with two columns:\n\n 1. redshift\n 2. probability\n\nThe programme `lf-catcorrect` will need to know LSTART and LLEN when\nrun. In this way, you can use photoz files directly as they come from\nthe photoz software (e.g.,\n[LePHARE](http://www.cfht.hawaii.edu/~arnouts/LEPHARE/lephare.html)).\nYou can put the photoz files in a separate directory from the\ncatalogue.\n\n\n### Correct and format the catalogue for downstream tools ###\n\nHaving prepared the input catalogue, the area files, and the photoz\nfiles, you can now run `lf-catcorrect` to do the corrections. Using\nagain XMM-COSMOS as an example:\n\n    ./lf-catcorrect --infile=xmmcosmos-formatted-cat.dat\n                   --outfile=xmmcosmos-corrected-cat.dat\n                   --nhcorr=T  --savenhcorr\n                   --photozpdf=T\n                   --pdfstart=63 --pdfstop=713\n                   --pdfpath=/home/piero/Dati/Teoria/XMMLSS/SPEC_files_cosmos/\n                   --kcorrgamma=1.7\n\t\t   --complcorr=T\n\t\t   --corrfile=xmmcosmos-fluxhisto.dat\n\nIn this example we have specified all possible options:\n\n  * input and output files (infile, outfile);\n  * corrections for absorption (nhprob);\n  * save the applied absorption corrections as a further column in outfile;\n  * corrections for photoz uncertainties (photozpdf);\n  * photoz files are in .../SPEC_files_cosmos;\n  * photoz information starts at line 63 and goes on to line 713;\n  * K-corrections assume a power-law spectrum with Gamma=1.7;\n  * completeness corrections (complcorr, corrfile).\n\nTo turn off corrections, use --nhprob=F and/or --photozpdf=F and/or --complcorr=F\n(but corrections are anyway off by default).\n\nThe only mandatory options are infile and outfile.\n\nThe output catalogue file will have many more records and a different\nstructure with respect to the input catalogue, so be aware that the\ntwo are not interchangeable. The output catalogue is what is needed by\nthe downstream tools. Its format is, for documentation's sake, the\nfollowing:\n\n  1. ID number\n  2. absorption-corrected flux\n  3. weight\n  4. redshift\n  5. luminosity\n [6. optional: applied correction for absorption, in Log-scale]\n\nIn fact, what `lf-catcorrect` does is to split each source into all\nthe possible values of luminosity and redshift that it could have,\ngiven the probability distributions of the photoz and of the\nabsorption.\n\n\nBinned luminosity functions\n---------------------------\n\nThe programme `lf-binned` computes non-parametric, binned estimates of\nthe LF, using the method described in [Ranalli et\nal. 2015](http://arxiv.org/abs/1512.05563) which is a variant of the\nmethod by Page & Carrera (2000, MNRAS 311, 433). Both methods have\ntheir root in the 1/V<sub>max</sub> method (Schmidt 1968, ApJ 151,\n393).\n\nAs the input for this programme, you need at least a corrected\ncatalogue produced by `lf-catcorrect`, and an area file. This should\nbe specified in a configuration file, which allows multiple catalogues\nto be read together. Following the previous example:\n\n    ./lf-binned  xmmcosmos.json\n\nThe configuration file is in the [JSON](http://en.wikipedia.org/wiki/JSON) format.\nSee below for [how to write a configuration file](#Configuration files).\n\nThe programme will then ask for the bin boundaries: log Lmin, log\nLmax, zmin, zmax. The output, marked by a per cent sign (%) at the end\nof the line, will be the LF estimate in Mpc$^{-3}$, followed by the\nlower and upper ends of the 68.3% confidence interval. The estimate of\nthe confidence interval is done assuming Gaussianity if there are at\nleast 50 sources in the bin; or otherwise using the Gehrels\napproximation.\n\nAfter computation, the programme will ask for a new bin. So, a useful\nidiom to calculate an entire LF is:\n\n    ./lf-binned xmmcosmos.json < inputbinned.dat | grep %\n\nWhere the file inputbinned.dat contains the definitions of the\ninteresting bins, and the grep command will remove all the inessential\ninformation and only catch the LF result.\n\n\nParametric luminosity functions\n-------------------------------\n\nParametric LFs allow to estimate model features such as the *knee*\nluminosity Lstar, critical redshifts, etc., by means of fitting a\nmodel to the data.  The likelihood is a function of the data and of\nthe model, and measures how well the model describes the\ndata. (Formally, it is the probability of observing the data, given\nthe model).\n\nThe most common model for LFs is a double powerlaw, which has the\nfollowing parameters:\n\n  * gamma1 (slope at L < $\\sim$ Lstar);\n  * gamma2 (slope at L > $\\sim$ Lstar);\n  * Lstar;\n  * normalisation.\n\nLFs evolve, i.e. they change their parameters with the\nredshift. Several models have been proposed in the literature;\ncurrently the most favoured ones are:\n\n  * LDDE (luminosity-dependent density evolution), requiring 5\n    parameters to describe the evolution;\n  * LADE (luminosity and density evolution), requiring 4 parameters.\n\nAn in-depth description of these models, their parameters, and their\nastrophysical meaning is beyond the scope of this document. More\ninformation can be found in [Ranalli et\nal. 2015](http://arxiv.org/abs/1512.05563) or in other literature\npapers (e.g. Aird et al. 2010, ...).\n\n\nLFtools currently implements the following models:\n\n  * double powerlaw + LDDE;\n  * double powerlaw + LADE;\n  * double powerlaw + LADEBPL.\n\nWe note that LADE here uses a different normalisation than what used\nby Aird et al. (2010). Our normalisation is such that density\nevolution is zero at z=0, as in Fotopoulou et al. (submitted). For\nmore information, see [Ranalli et al. 2015](http://arxiv.org/abs/1512.05563).\n\nLADEBPL is a variant on LADE: instead of using the double powerlaw as\nin Aird et al. (2010), it uses a broken poweraw as in Ueda et\nal. (2003); everything else is the same.\n\n\nMaximum-likelihood estimates\n----------------------------\n\nThe programme `lf-ml` finds the best-fit model parameters by\nmaximising the likelihood. The same model can be fit to several data\nsets (currently, up to 10) at the same timing, giving the parameters\nthat fit best all the data.\n\nTo pass a variable number of catalogue and area files to `lf-ml`, a\n*configuration file* is used. Also, the user has the possibility of\nputting constraints on the parameters and/or fixing them at any value;\nthis is done through a *command file*.\n\nThe programme can be run as:\n\n    ./lf-ml xmmcosmos.json  minuit.ldde\n\nThe configuration file (xmmcosmos.json in the example) is a text file\ncontaining the choice of evolution, and the list of catalogue and area\nfiles. The [JSON](http://en.wikipedia.org/wiki/JSON) format is used.\nSee below for [how to write a configuration file](#Configuration files).\n\nThe [MINUIT](http://wwwasdoc.web.cern.ch/wwwasdoc/minuit/minmain.html)\nlibrary is used to maximise the likelihood (technically, the opposite\nof the likelihood is minimised). The command file (minuit.ldde) is\nused by MINUIT to know the range of validity of each parameter, and to\nstart the minimisation. Two command files, `minuit.ldde` and\n`minuit.lade`, are provided with LFtools and can be used as they\nare. They contain reasonable defaults and commands, and can be\nmodified by the user if needed (please refer to the MINUIT\ndocumentation).  MINUIT is a part of the\n[CERNLIB](http://cernlib.web.cern.ch/cernlib/) libraries, which must\nbe installed in order to use `lf-ml`.\n\nThe output of `lf-ml` is directly provided by MINUIT; it is usually\nself-explanatory.\n\nThe running time of `lf-ml` depends on the catalogue size, on\nwhether absorption and photoz corrections have been applied, and on\nthe model; in worst cases it may require several tens of minutes.\n\n\nBayesian estimates\n------------------\n\nWhile maximum-likelihood gives a *point estimate* of the parameters,\nwith errors computed assuming Gaussianity in the neighbourhood of the\nbest-fit values, Bayesian estimates provide a more accurate\ndescription of the best-fitting parameter space. It requires, however,\nmuch longer computational times than maximum-likelihood.\n\nThe starting point for Bayesian estimation is the same likelihood\nfunction used for maximum-likelihood. *Prior distributions* are also\nrequired for the parameters under analysis. The priors and the\nlikelihood are combined to produce *posterior distributions* for the\nparameters. The posterior measures the probability of a given set of\nparameter values, after having extracted all information from the\ndata. For more information, we refer the user to any textbook on\nBayesian statistics (e.g. Gregory, \"Bayesian logical data analysis for\nthe physical sciences\", Cambridge, 2005), or to astrophysics-oriented\nintroductions such as Andreon 2011 (arXiv:1112.3652).\n\nAn important aspect of Bayesian computation is the method used to get\nthe posterior distributions. Most times, these distributions cannot be\nobtained in closed form. Instead, they are usually obtained in the\nform of *posterior samples*, i.e. large sets of parameter values whose\nstatistical distribution follows the posterior. The posterior\ndistribution is then reconstructed empirically from the posterior\nsamples.\n\nSeveral methods have been devised to obtain posterior samples, the\nbest-known one probably being MonteCarlo Markov Chains\n(MCMC). Recently, *nested sampling* has been proposed as a more\nefficient alternative (Skilling 2006, Bayesian Analysis, 1, 833) and\nused for LFs by Aird et al. (2010). A growingly-used implementation of\nnested sampling is offered by the *MultiNest* libraries (Feroz et\nal. 2008, arXiv:0809.3437; Feroz et al. 2013, arXiv:1306.2144).\n\n\n### lf-mn ###\n\nThe programme `lf-mn` performs Bayesian estimation of LFs, using the\nMultiNest libraries for posterior sampling. It can be run as a\nsingle-processor job as:\n\n    ./lf-mn xmmcosmos.json\n\nor as a parallel job as (for example, on 4 processor cores):\n\n    mpirun -n 4 ./lf-mn xmmcosmos.json\n\nThe configuration file (xmmcosmos.json in the example) is a text file\ncontaining the choice of evolution, the prior distributions, and the\nlist of catalogue and area files. The same configuration file may be\nused for maximum-likelihood and for Bayesian analysis (see below for\ndetails). The catalogue and area files are specified in the the\nconfiguration; they should be in the same format used for `lf-binned`\nand `lf-ml`.\n\nThe prior distributions can be either flat within a range, or a\ncombination of Cauchy and Gamma functions. See [below](#Prior\ndistributions).\n\nThe output of `lf-mn` is governed by the MultiNest libraries. A series\nof files will be created. Below we only describe two of them, and refer\nto the MultiNest documentation for further information.\n\nA nice feature of the MultiNest libraries is that they allow resuming\nan interrupted run. As long as the output files are not tampered with,\nand the [root]resume.dat file is present, you can restart the\ncomputation using the same command given when starting.\n\n\n### Prior distributions ###\n\nMultiNest assumes that all parameters follow a uniform distribution\nbounded in the [0,1] interval. These parameters must be mapped to\nphysical-world parameters (e.g. normalisation, Lstar, ...) by the\nlikelihood function.\n\nLFtools can use either flat priors, where the parameters are uniformly\ndistributed in a physically-relevant interval, or a combination of\nCauchy and Gamma function.\n\nAn example of flat priors is the following:\n\n    parameter              min      max\n    -----------------------------------\n    A                       -1        3\n    gamma1                  .3        3\n    gamma2                  .3        5\n    log Lstar             41         47\n    zc                      .01       5\n    p1                   -10         10\n    p2                   -10         10\n    alpha (LDDE only)     -1          3\n    La (LDDE only)        41         47\n    d (LADE only)         -1          5\n\nThey should be specified in the configuration file by assigning the\nkeyword:\n\n        \"priors\"    : \"flat\"\n\nand listing the min-maxes of the parameters as follows:\n\n    \"ladelimits\" : {\n\t\"A\"     : [ -1., 5. ],\n\t\"gamma1\": [ -7, 7. ],\n\t\"gamma2\": [ -7, 7. ],\n\t\"Lstar\" : [ 41, 47 ],\n\t\"zc\"    : [ 0.01, 5 ],\n\t\"p1\"    : [-10, 10 ],\n\t\"p2\"    : [-10, 10 ],\n\t\"d\"     : [ -1, 5  ]\n    },\n    \"lddelimits\" : {\n\t\"A\"     : [ -1., 5. ],\n\t\"gamma1\": [ -7, 7. ],\n\t\"gamma2\": [ -7, 7. ],\n\t\"Lstar\" : [ 41, 47 ],\n\t\"zc\"    : [ 0.01, 5 ],\n\t\"p1\"    : [-10, 10 ],\n\t\"p2\"    : [-10, 10 ],\n\t\"alpha\" : [ -1, 3  ],\n\t\"La\"    : [ 41, 47 ]\n    }\n\n\nCauchy/Gamma priors do not have hard bounds as the flat\nprios. Instead, they have a location (Cauchy only) and scale. Cauchy\npriors have the following probability density distribution:\n\n    P_Cauchy(x) = 1 / { PI * [1 + ((x-m)/s)^2] }\n\nwhere PI is 3.141593..., m is the location, and s is the scale. The\nCauchy distribution is also known as the Lorentz distribution.\nGamma priors follow a Gamma(k=1,theta=1) distribution:\n\n    P_Gamma(x) = exp(-x/s)\n\nThe main difference is that the Cauchy distribution allows negative\nvalues, while the Gamma does not. Therefore the Gamma is especially\nuseful for quantities such as redshift. This kind of prior should be\nspecified as follows:\n\n    \"ladecauchygamma\" : {\n\t\"A\"     : [ 1.5,  2.5 , 1],\n\t\"gamma1\": [ 0.6,  2.5 , 1],\n\t\"gamma2\": [ 3,    2.5 , 1],\n\t\"Lstar\" : [ 45,   2.0 , 1],\n\t\"zc\"    : [  2,     0 , 0],\n\t\"p1\"    : [ 6.4,    5 , 1],\n\t\"p2\"    : [ -0.24,  5 , 1],\n\t\"d\"     : [ -0.2, 2.5 , 1]\n    }\n\nEach triplet of numbers specifies location, scale, and switch.  The\nswitch is the last element in the triplet, and commands whether the\nCauchy is to be used (switch=1) or the Gamma (switch=0).\nFor Cauchy, the two first and second elements in the\ntriplet are location and scale, respectively. For Gamma, the first\nelement is scale and the second is not used.\n\nLocation can be thought as the mean of the distribution, and scale has\nthe same role that the sigma has for Gaussians. Location should be set\nto some \"best-guess\" value, e.g. results from a previous paper. Scale\nis a bit trickier because Cauchy distributions have long tails;\ntreating them as they were Gaussian sigmas is a reasonable guess.\n\nThe LADEBPL model uses the same priors of LADE, so they shold be\nspecified with \"ladelimits\" or \"ladecauchygamma\".\n    \n\n### Output: posterior samples file ###\n\nThe file `[root]post_equal_weights.dat` contains the posterior\nsamples. Each line is a draw from the posterior distribution of the LF\nparameters. The structure of the file is as follows:\n\n 1. parameter no. 1 (A = LF normalisation)g\n 2. parameter no. 2 (gamma1)\n 3. parameter no. 3 (gamma2)\n 4. parameter no. 4 (log Lstar)\n 5. parameter no. 5 (zc)\n 6. parameter no. 6 (p1)\n 7. parameter no. 7 (p2)\n 8. parameter no. 8 (if LDDE: alfa; if LADE: d)\n 9. if LDDE: parameter no. 9 (La); if LADE: log likelihood\n10. if LDDE: log likelihood;       if LADE: not used\n\nThe parameters are in MultiNest's scale, i.e. in the 0,1 interval. The\nPerl scripts `readchains.pl` and `readchains-lade.pl` (see the section\n[Companion programmes](#Companion programmes)) can be used to scale\nthe parameters to their phyisical-world scales:\n\n    ./readchains.pl my_post_equal_weights.dat > my_rescaled_posteriors.dat\n         (or)\n    ./readchains-lade.pl my_post_equal_weights.dat > my_rescaled_posteriors.dat\n\nThe rescaled posteriors can be inspected for example with\n[TOPCAT](http://www.star.bris.ac.uk/~mbt/topcat/), which can also plot\nthe marginal densities.  To plot the LF, the `hpdchains3.pl` and\n`hplplot5.pl` programmes can be use (see [Companion\nprogrammes](#Companion programmes)).\n\nAs further possiblities, we note that the MultiNest documentation\nmentions the following options for visualising its output:\n\n  * the getdist package which is part of\n    [CosmoMC](http://cosmologist.info/cosmomc/readme.html#Analysing);\n  * [PyMultiNest](https://github.com/JohannesBuchner/PyMultiNest).\n\n\n### Livepoints file ###\n\nThe file `[root]physlive.dat` (where *[root]* is that specified in the\nconfiguration file) contains the *live points* used by MultiNest for\nits sampling. The live points are continuously updated during\ncomputation, and represent the current best estimate of the LF. This\nfile is useful for monitoring MultiNest's progress. The columns in the\nfile contain first the parameters, than the log-likelihood and the\nmode number. The format is therfore the same as for\npost_equal_weights.dat, just with a further column. The same set of\ncompanion programmes (`readchains.pl`, `hpdchains3.pl`, `hpdplot5.pl`)\ncan be used.\n\n\n### MultiNest ###\n\nThe basic (\"vanilla\") MultiNest algorithm is used, with multi-modal\nsearch turned on. Switching to Importance Nested Sampling, or to\nconstant-efficiency mode, requires editing the file params.f90 and\nrecompiling `lf-mn`. Other hard-coded parameters of interest are:\ntol=0.5, efr=0.5. See the MultiNest documentation.\n\n\nConfiguration files\n-------------------\n\nThe following is an example of a configuration file:\n\n    {\n        \"catalogues\" : [\n\t    {\n\t\t\"cat\"  : \"catalogue-cdfs.dat\",\n    \t    \t\"area\" : \"xmmcdfs-completeness210.dat\"\n    \t    },\n\t    {\n\t\t\"cat\"  : \"catalogue-cosmos.dat\",\n    \t    \t\"area\" : \"xmmcosmos-area.dat\"\n    \t    }\n        ],\n\t\"cosmology\" : {\n\t\t    \"H0\" : 70.,\n\t\t    \"OL\" : 0.7,\n\t\t    \"OM\" : 0.3\n\t\t    },\n        \"nhcorr\" : 1,\n\t\"evolution\" : \"ldde\",\n        \"root\" : \"chains/test-\",\n        \"seed\" : 8471\n    }\n\nNot all keywords need to be specified, since this depends on which\nprogramme in LFtools is being run. The requirements are:\n\n  * `lf-binned` just needs catalogues and cosmology;\n  * `lf-ml` needs catalogues, cosmology, and evolution;\n  * `lf-mn` needs everything.\n\nThe example above includes:\n\n  * the seed for the number generator;\n  * a root name for the output files (in this case, files will be put\n    in a `chains` subdirectory, and their name will start with\n    `test-`. The `chains` directory should be created before running\n    `lf-mn`);\n  * the choice of evolution (ldde or lade);\n  * a flag (nhcorr) to specify that absorption corrections should be\n    considered when computing the coverage (see [Ranalli et al. 2015](http://arxiv.org/abs/1512.05563)\n    nhcorr=0 means using Eq.(6), nhcorr=1 means using Eq.(9);\n  * a list of catalogues and area files. Note the use of square\n    brackets to contain the list, and of braces to contain the\n    {cat,area} pairs.\n\nIf there is any error in the format of the configuration file, the\nprogram will exit with an error referring to \"object creation\" or\n\"unexpected character\".\n\nNote that the JSON standard specifies that number start with a number,\nthat is, a quantity like 0.1 should be written as \"0.1\" and not as\n\".1\". A common error may therefore be the following:\n\n    ERROR: Unexpected character while parsing value. '.' ASCII=          46\n\nwhich is usually due to some number written with a leading dot (e.g.: .1).\n\nFor more information on the JSON format, see\ne.g. [the wikipedia article](http://en.wikipedia.org/wiki/JSON) or the\n[official website](http://json.org/).\n\n\n\nHow to add more evolution models\n--------------------------------\n\nEditing the code is required.\n\n1. In src2/lumf_funcs.f90, add a new evolution type by subclassing\n'evolution' (see how ldde, lade etc. work).  Non-power-law models can\nbe added by subclassing z0function (see the doublepowerlaw\nclass). Models that break the assumption that evolution can be broken\ninto a luminosity-evolution part and a density-evolution part\n(e.g. the flexible double power-law by Aird et al. 2015) my require\na complete rework of the class hierarchy.\n\n2. In src2/startup.f90, subroutine allocateLF, add a proper case to\nthe if/elseif/endif sequence to allocate the newly defined class.\n\n3. In src2/ml-fit.f90, subroutine FCN, add a proper case to the select\ntype statement. You need to pass to ev%set() the proper amount of\nparameters: start with xval(5) and increment up to what you need.\n\n4. In src2/Multinest/lfmnconfig.f90, subroutine configure, add a\nproper case to the if/elseif/endif sequence right after the 'set prior\nparameters' comment. The variable numparams is the total number of\nparameters (4 for the double powerlaw, plus what you need for\nevolution). End the block with a call to a new subroutine\n('set_yourevolution_params'). Write the latter subroutine, looking at\nthe others as examples. The variable 'keyword' is the one which you\nwill need to specify in the configuration .json file.\n\n5. Recompile, calling make both in src2/ and in src2/Multinest/.\n\n\n\nCompanion programmes\n--------------------\n\n### Convert MultiNest chains to physical-world scale ###\n\n\n### Plot all chains ###\n\n\n### Compute and plot Highest Posterior Densities (HPD) ###\n\n\n\nCaveats\n-------\n\n  * Coverage curves are currently limited to <=150 data points\n\n\n\n\n", 
  "id": 42724692
}