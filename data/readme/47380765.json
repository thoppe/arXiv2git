{
  "read_at": 1462550386, 
  "description": "a simple attention model for a simple function on sequence", 
  "README.md": "# Simple Construction of Attention Model in TF\n\n## TLDR:\n\n\n## Motivation\n\nIn this paper:\n\nhttp://arxiv.org/abs/1409.0473\n\nThe authors constructed an attention model, with a bi-directional RNN as the\nattention context, which is capable of doing a certain sequence manipulation\ntasks, namely, translation from English to French with very good results.\n\n\nWe want to duplicate the model presented in this paper in its simplist form\n(i.e. use it for a simpler sequence manipulation task than machine translation)\nfor clarity and understanding. \n\n# Goal of this exercise:\n\n1. Use a bi-directional RNN consisting with LSTM units to proccess the input sequence into attention context\n2. Construct a soft-max layer modeling attention to select choice contexts during decoding\n3. Use this attention context in conjunction of a decoding RNN, generate the output sequence\n4. Training and Evaluation of the model\n\nEverything is bare-bone minimum as I'm just trying to learn the TF library\n\n## The Sequence Manipulation Task\n\nWe solve a contrived task of taking in a binary input sequence of even length,\nreverse the sequence, then perform xor on every pairs of its elements\n\ni.e. on input [0,1,0,0,1,0,1,1]\n\nwe first reverse it [1,1,0,1,0,0,1,0]\n\nthen for every 2 consequtive elements, perform an xor\n\n[xor(1,1), xor(0,1), xor(0,0), xor(1,0)]\n\nthe output is this:\n\n[0, 1, 0, 1]\n\nWe'll train a neural network to perform this function with the TensorFlow library.\n\n## Data Generation\n\nlook in data_gen.py for data generation, in particular, we use a 1-hot encoding\nto represent ( and ), the sequence is also 0 padded, because TF does not\nsupport dynamic unrolling, to take into account of various different lengths. \n\nFor example, if we have an unrolling of size 6, and our sequence is ( ) ( ), we'll get the following encoding \n    \n    [[0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]]\n\nHere, [0, 1, 0] denotes (, [0, 0, 1] denotes ), and [1, 0, 0] denotes the 0 padding\n\nThe output is also 1-hot encoded, [1, 0] denotes true, and [0, 1] denotes false\n\nBest way is to load data_gen into the interactive python shell via\nexecfile('data_gen.py') and execute some of the functions, they're very self\nexplainatory.\n\n## RNN Model\n\nThe RNN model is a simple one, it is unrolled a fixed number of times, say 6, and conceptually perform the following program:\n\n    def matching_paren(sequence):\n      state = init_state\n      for i in range(0, 6):\n        output, state = lstm(sequence[i], state)\n      return post_proccess(state)\n\nHere, init_state is the initial state for the lstm unit, which is a vector that\ncan be learned, lstm is a rnn unit from the tf.models.rnn module, and\npost_proccess is a small neural network with a layer of relu and a soft-max\nlayer for outputing true/false. \n\nThus, all the tunable parameters are:\n\n1. The initial state\n2. All the weights in the LSTM unit\n3. All the weights in the post_process units\n\nSee matching.py for details.\n\n## RNN training and evaluation\n\nFor training, I compute the softmax as the predicted label. The error is the\ncross entropy between the prediction and the true label. For training I'm using\ngradient clipping as the gradient can become NaN otherwise, and I'm using\nAdaptiveGradient because why the f not (maybe other is better but I have not\ngotten around to learn to use those).\n\nFor evaluation, I evaluate the performance once every 100 epochs.\n\n## Results\n\n1. In matching.py, we use a single layer lstm and the result isn't as good, can only go to length of 8\n\n2. In matching2.py, we use a stacked lstm and the result is better, can go up to length of 30\n\n## Remarks\nHope this is helpful! I'm still new to TF so I probably can't answer any questions on TF reliably. Direct all questions to the TF discussion group on google.\n\n", 
  "id": 47380765
}