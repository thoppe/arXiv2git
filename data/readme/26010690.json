{
  "read_at": 1462511414, 
  "description": "Visualization of NMF training process", 
  "README.md": "NMFViz\n======\n\nThis simple program proposes a visualization of what happens while training a NMF as in :\n\n* *Algorithms for non-negative matrix factorization* from Lee & Seung\n[(NIPS 2011)](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf)\n* *Non-negative sparse coding* from Hoyer [(arXiv)](http://arxiv.org/pdf/cs/0202009.pdf)\n* An adaption of Hoyer's NMF using L2 regularization on W\n\nIt supports the following datasets:\n\n* [Extended Yale B dataset](http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html)\n* [MNIST](http://yann.lecun.com/exdb/mnist/)\n* [CBCL](http://cbcl.mit.edu/cbcl/software-datasets/FaceData2.html)\n* [CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html)\n\nIt works for me (tm) on OSX with Python 3.4.3 and these libraries:\n\n* matplotlib==1.4.3\n* numpy==1.9.2\n* simplejson==3.8.0\n\nPlease do contact me if you have any trouble using this tool.\n\nHow-to\n------\n\n1. Fetch some data to play with:\n\t1. Use the get_yale.sh bash script to get the data, put them all in the same folder (and remove *Ambient.pgm files*)\n\t1. Use the get_mnist.sh bash script to get the MNSIT data (gunzip binary file)\n\t1. Use the get_cbcl.sh bash script to get the CBCL data\n\t1. Use the get_cifar10.sh bash script to get the CIFAR-10 and convert it so python 3 can read it\n1. Use the nmf_viz.py python script to visualize the training procedure.\n\n### Parameters of viz.py\nThe python script viz.py reads its configuration from the conf.json file. Parameters are :\n\n1. data = Information on the data\n\t\t1. type: type of dataset. For now: 'Cropped Yale' or 'MNSIT'\n    1. path: The path to the directory storing all images (CroppedYale/ if you successfully used the get_yale.sh script)\n    1. number: Number of images to use (they are 2409 that are successfully loaded by the script from cropped Yale)\n1. nmf = Describes the NMF with:\n    1. type: the name of the loss to use (\"euclidean\" or \"divergence\" or \"sparse\" or \"sparse-l2\")\n    1. components: the number of components to use\n    1. iterations: the number of training iterations\n    1. sparseness: the weight of the sparseness constraint, if any in the model\n    1. learning rate: the learning rate, if any in the model\n    1. learning rate decay: the learning rate decay, if any in the model\n    1. l2: the weight of the L2 regularization, if any in the model\n\n### cifar10_p3.py\nCareful, this script is written in Python 2. It converts the 'cPickled' batches to '.npy' files that numpy can load.\nIt also converts RGB images to grayscale images.\n\nScreenshots\n------\n\nHere is a screenshot of the 'divergence' NMF with 40 components and 100 iterations on 100 'Cropped Yale' images.\n\n![screenshot using divergence](imgs/screenshot_divergence.png)\n\nHere is another screenshot of the 'sparse-L2' NMF with 40 components, 150 iterations on 10000 'MNSIT' images.\n\n![screenshot using sparse L2 on MNIST](imgs/screenshot_sparse_l2.png)\n\nHere is another screenshot of the 'sparse-L2' NMF with 40 components, 150 iterations on 6000 'CBCL' images.\n\n![screenshot using sparse L2 on CBCL](imgs/screenshot_cbcl.png)\n\nHere is a last example on CIFAR-10. The shapes are not really well defined... Well, maybe using a non-linear method\nto extract representations, like a neural network, on this dataset is better. It is trained using the 'euclidean' NMF\nwith 100 components and 150 iterations on 2000 CIFAR images.\n\n![screenshot using euclidean on CIFAR-10](imgs/screenshot_cifar10.png)\n", 
  "id": 26010690
}