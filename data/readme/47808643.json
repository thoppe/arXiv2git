{
  "read_at": 1462550341, 
  "description": "contains the code associated with SceneNet", 
  "README.md": "#SceneNet\n\nAll the necessary source code for SceneNet  [SceneNet: Understanding Real World Indoor Scenes with Synthetic Data](http://arxiv.org/abs/1511.07041) will be available here soon.\n\n#Updates\n\nThis code enables depth and annotation rendering given a 3D model and trajectory. We provide a sample 3D model in the **data** folder and a trajectory in **data/room_89_simple_data** folder. More details will be updated soon. Things to do \n\n- [ ] Simulated Annealing\n- [ ] [RGB Rendering](https://github.com/ankurhanda/SceneGraphRendering) to be merged with the code\n- [ ] Releasing Trajectories\n- [ ] Converting Depth to DHA format\n- [ ] Emphasise on Negative Focal Length\n\n#Dependencies\n\n* [Pangolin] (https://github.com/ankurhanda/Pangolin-local) Local copy of https://github.com/stevenlovegrove/Pangolin \n* [CVD](https://github.com/ankurhanda/libcvd) (It will not be a dependency any more in future!)\n* [TooN](https://github.com/ankurhanda/TooN) (It will be replaced by Eigen3 in future!)\n* [ImageUtilities](https://github.com/ankurhanda/imageutilities)\n* [SceneGraphRendering](https://github.com/ankurhanda/SceneGraphRendering) (will be merged within soon!)\n* OpenCV/OpenCV2\n* libnoise (from synaptic)\n\n#Build\n\n```\nmkdir build\ncd build\ncmake .. -DCUDA_PROPAGATE_HOST_FLAGS=0\nmake -j8\n```\n\n#Demo\nin your build, run\n\n```\n./opengl_depth_rendering ../data/room_89_simple.obj\n```\nYou should have annotated images in the folder **data/room_89_simple_data** that should look like these\n\n![Montage-0](Resources/out.png)\n\n#Adding noise to the depth maps \n\n[Video](https://youtu.be/3nmQ3SiZKuk?t=42s)\n\n\n#SceneNet Basis Models (Work In Progress) \n\n- [ ] [Bedrooms](https://bitbucket.org/robotvault/bedroomscenenet/)\n- [Bedroom Layouts](https://bitbucket.org/robotvault/bedroomlayoutscenenet)\n- [ ] [Livingrooms](https://bitbucket.org/robotvault/livingroomsscenenet/)\n- [Livingroom Layouts](https://bitbucket.org/robotvault/livingroomslayoutsscenenet/)\n- [ ] [Offices](https://bitbucket.org/robotvault/officesscenenet)\n- [Office Layouts](https://bitbucket.org/robotvault/officeslayoutsscenenet)\n- [ ] [Kitchens](https://bitbucket.org/robotvault/kitchensscenenet/)\n- [Kitchen Layouts](https://bitbucket.org/robotvault/kitchenslayoutscenenet)\n- [ ] [Bathrooms](https://bitbucket.org/robotvault/bathroomscenenet/)\n- [Bathroom Layouts](https://bitbucket.org/robotvault/bathroomslayoutscenenet)\n\n#Sample Texture Library\n\nDownload the sample texture library to texture the models from [here](http://tinyurl.com/zpc9ppb). Textured bedrooms are shown below. \n\n![Montage-0](Resources/bedroom_textured.png)\n\n\n#Website \n\nMore information is available here at our website [robotvault.bitbucket.org](http://robotvault.bitbucket.org)\n\n#Labels\n\nLabel name and number mapping is from Eigen et al. arXiv 2015, ICCV 2015\n\n\n| Label Number  | Label Name    |\n|:-------------:|:-------------:|\n| 1  | Bed         | \n| 2  | Books       |  \n| 3  | Ceiling     |  \n| 4  | Chair       |  \n| 5  | Floor       |  \n| 6  | Furniture   |  \n| 7  | Objects     |  \n| 8  | Picture     |  \n| 9  | Sofa        |  \n| 10 | Table       |  \n| 11 | TV          |  \n| 12 | Wall        |  \n| 13 | Window      |  \n\n#Conversion from SUN RGB-D/NYUv2 37/40 Labels \n\nGet the 40 class mapping for NYUv2 from [http://www.cs.berkeley.edu/~sgupta/cvpr13/](http://www.cs.berkeley.edu/~sgupta/cvpr13/). SUN RGB-D already provide the 37 class mapping in their meta-data files.\n\nLabel name and number are from SUN RGB-D/ NYUv2. Their corresponding SceneNet/Eigen et al. mapping is in the last column.\n\n| SUN RGB-D/NYUv2Label Number  | Label Name    | Eigen et al./SceneNet Mapping | \n|:-------------:|:-------------:|:-------------:|\n| 1  | Wall         | 12 | \n| 2  | Floor       | 5 |\n| 3  | Cabinet     | 6 | \n| 4  | Bed       | 1 | \n| 5  | Chair   | 4 | \n| 6  | Sofa     | 9 | \n| 7  | Table     | 10 | \n| 8  | Door        | 12 | \n| 9 | Window       | 13 | \n| 10 | BookShelf         | 6 | \n| 11 | Picture        | 8 | \n| 12 | Counter      | 6 | \n| 13 | Blinds      | 13 | \n| 14 | Desks      | 10 \n| 15 | Shelves      | 6 | \n| 16 | Curtain      | 13 | \n| 17 | Dresser      | 6 | \n| 18 | Pillow     | 7 | \n| 19 | Mirror      | 7 | \n| 20 | Floor-mat      | 5 | \n| 21 | Clothes      | 7 | \n| 22  | Ceiling         | 3 | \n| 23  | Books       | 2 | \n| 24  | Refrigerator     | 6 | \n| 25  | Television       | 11  | \n| 26  | Paper       | 7 | \n| 27  | Towel   | 7 | \n| 28  | Shower-curtain | 7   |\n| 29  | Box     | 7 | \n| 30  | Whiteboard    | 7    |\n| 31 | Person       | 7 |\n| 32 | NightStand       | 6 |\n| 33 | Toilet        | 7 | \n| 34 | Sink      | 7 | \n| 35 | Lamp      | 7 | \n| 36 | Bathtub      | 7 | \n| 37 | Bag      | 7 | \n| 38 | Other-structure | 7  |\n| 39 | Other-furniture   | 6   |\n| 40 | Other-prop      | 7 | \n\n#Accuracy Script\n\nCompute the global/per-class accuracy with [getAccuracyNYU.m](https://github.com/ankurhanda/SceneNetv1.0/blob/master/getAccuracyNYU.m) script provided in the repository.\n\n#Latex Code \n\n```\n\\usepackage[table]{xcolor}\n\\definecolor{bedColor}{rgb}{0, 0, 1}\n\\definecolor{booksColor}{rgb}{0.9137,0.3490,0.1882}\n\\definecolor{ceilColor}{rgb}{0, 0.8549, 0}\n\\definecolor{chairColor}{rgb}{0.5843,0,0.9412}\n\\definecolor{floorColor}{rgb}{0.8706,0.9451,0.0941}\n\\definecolor{furnColor}{rgb}{1.0000,0.8078,0.8078}\n\\definecolor{objsColor}{rgb}{0,0.8784,0.8980}\n\\definecolor{paintColor}{rgb}{0.4157,0.5333,0.8000}\n\\definecolor{sofaColor}{rgb}{0.4588,0.1137,0.1608}\n\\definecolor{tableColor}{rgb}{0.9412,0.1373,0.9216}\n\\definecolor{tvColor}{rgb}{0,0.6549,0.6118}\n\\definecolor{wallColor}{rgb}{0.9765,0.5451,0}\n\\definecolor{windColor}{rgb}{0.8824,0.8980,0.7608}\n```\n\n```\n\\begin{table*}\n\\begin{tabular}{l}\n\\textbf{13 class semantic segmentation: NYUv2} \\\\\n\\end{tabular}\n\\centering\n\\begin{tabular}{|l|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|}\n\\hline\nTraining  & \\cellcolor{bedColor}\\rotatebox{90}{bed} & \\cellcolor{booksColor}\\rotatebox{90}{books}  & \\cellcolor{ceilColor}\\rotatebox{90}{ceil.} & \\cellcolor{chairColor}\\rotatebox{90}{chair}  & \\cellcolor{floorColor}\\rotatebox{90}{floor}  & \\cellcolor{furnColor}\\rotatebox{90}{furn}   & \\cellcolor{objsColor}\\rotatebox{90}{objs.} & \\cellcolor{paintColor}\\rotatebox{90}{paint.} & \\cellcolor{sofaColor}\\rotatebox{90}{sofa}   & \\cellcolor{tableColor}\\rotatebox{90}{table}  & \\cellcolor{tvColor}\\rotatebox{90}{tv}     & \\cellcolor{wallColor}\\rotatebox{90}{wall}   & \\cellcolor{windColor}\\rotatebox{90}{window} \\\\ \\hline\nNYU-DHA & 67.7 & 6.5 & 69.9 & 47.9 & \\textbf{96.2} & 53.8 & 46.5 & 11.3 & 50.7 & 41.6 & 10.8 & 85.0 & 25.8 \\\\ \\hline\nSceneNet-DHA & 60.8 & 2.0 & 44.2 & 68.3 & 90.2 & 26.4 & 27.6 &  6.3 & 21.1 & 42.2 & 0 & \\textbf{92.0} & 0.0 \\\\ \\hline\nSceneNet-FT-NYU-DHA & 70.8 & 5.3 & 75.0 & 58.9 & 95.9 & 63.3 & 48.4 & 15.2 & 58.0 & 43.6 & 22.3 &  85.1  & 29.9 \\\\ \\hline\nNYU-DO-DHA & 69.6 & 3.1 & 69.3 & 53.2 & 95.9 & 60.0 & 49.0 & 11.6 & 52.7 & 40.2 & 17.3 & 85.0 & 27.1 \\\\ \\hline\nSceneNet-DO-DHA & 67.9 & 4.7 & 41.2 & 67.7 & 87.9 & 38.4 & 25.6 &  6.3 & 16.3 & 43.8 & 0 & 88.6 & 1.0 \\\\ \\hline\nSceneNet-FT-NYU-DO-DHA & \\textbf{70.8} & 5.5 & 76.2 & 59.6 & 95.9 & \\textbf{62.3} & \\textbf{50.0} & 18.0 & \\textbf{61.3} & 42.2 & 22.2 & 86.1 & 32.1 \\\\ \\hline\nEigen \\textit{et al.} (rgbd+normals) \\cite{Eigen:etal:ICCV2015} & 61.1 & \\textbf{49.7} & 78.3 & \\textbf{72.1} & 96.0 & 55.1 & 40.7 &\\textbf{58.7} & 45.8 &\\textbf{44.9}& \\textbf{41.9} & 88.7 & \\textbf{57.7}  \\\\ \\hline\nHermans \\textit{et al.}(rgbd+crf)\\cite{Hermans:etal:ICRA2014} & 68.4 & N/A & \\textbf{83.4} & 41.9 & 91.5 & 37.1 & 8.6 & N/A & 28.5 & 27.7 & 38.4 & 71.8 & 46.1 \\\\ \\hline\n\\end{tabular}\n\\vspace{0.5mm} \\vspace{0.5mm}\n\\caption{Results on NYUv2 test data for 13 semantic classes. We see a similar pattern here --- adding synthetic data helps immensely in improving the performance of nearly all functional categories of objects using DHA as input channels. As expected, accuracy on \\textit{books}, \\textit{painting}, \\textit{tv}, and \\textit{windows}, is compromised highlighting that the role of depth as a modality to segment these objects is limited. Note that we recomputed the accuracies of \\cite{Eigen:etal:ICCV2015} using their publicly available annotations of 320$\\times$240 and resizing them to 224$\\times$224. Hermans \\textit{et al.} \\cite{Hermans:etal:ICRA2014} use ``\\textit{Decoration}\" and ``\\textit{Bookshelf}\" instead of \\textit{painting} and \\textit{books} as the other two classes. Therefore, they are not directly comparable. Also, their annotations are not publicly available but we have still added their results in the table. Note that they use 640$\\times$480. Poor performance of SceneNet-DHA and SceneNet-DO-DHA on \\textit{tv} and \\textit{windows} is mainly due to limited training data for these classes in SceneNet.}\n\\label{table: CA breakdown for 13 classes}\n\\end{table*}\n```\n\n# Relevant Documents\n\nThe whole process of data generation and experiments are in \n\n[SceneNet: an Annotated Model Generator for Indoor Scene Understanding](http://www.saistent.com/assets/pdfs/handa16icra.pdf), _ICRA 2016_\n<br>\nAnkur Handa, Viorica Patraucean, Simon Stent, Roberto Cipolla\n\n[Understanding Real World Indoor Scenes With Synthetic Data](https://www.repository.cam.ac.uk/handle/1810/254859), _CVPR 2016_\n<br>\nAnkur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, Roberto Cipolla\n\n\nAn up-to-date version is maintained on the arXiv \n\nSceneNet: Understanding Real World Indoor Scenes with Synthetic Data [arXiv link](http://arxiv.org/pdf/1511.07041v2.pdf)\n<br>\nAnkur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, Roberto Cipolla\n\n#License\n\nAll the code and data are released under a creative commons license which is purely for research purposes only. Please view the summary here [http://creativecommons.org/licenses/by-nc/4.0/]([http://creativecommons.org/licenses/by-nc/4.0/)\n\n", 
  "id": 47808643
}