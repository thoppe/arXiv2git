{
  "read_at": 1462544999, 
  "description": "RNNLIB is a recurrent neural network library for sequence learning problems. Forked from Alex Graves work http://sourceforge.net/projects/rnnl/", 
  "README.md": "# Origin\n\nThe original RNNLIB is hosted at http://sourceforge.net/projects/rnnl\nwhile this \"fork\" is created to repeat results for the \nonline handwriting prediction and synthesis reported in \nhttp://arxiv.org/abs/1308.0850. The later by now is Alex Graves's classic \npaper on LSTM networks showing of what RNN can learn about the\nstructure present in the sequential input.\n\nThe copy of this software runs on http://www.inkposter.com\n\n\n# Building\n\nBuilding rnnlib requires the following:\n\n* C++11 compiler\n* fortran for building OpenBLAS\n* cmake\n* libcurl\n* automake\n* libtool\n* texinfo\n\nIn addition, the following python packages are needed for the auxiliary scripts in the 'utils' directory:\n\n* SciPy\n* PyLab\n* PIL\n\nAnd this package is needed to create and manipulate netcdf data files with python, and to run the experiments in the 'examples' directory:\n\n* ScientificPython (NOT Scipy)\n\nTo build RNNLIB do\n\n$ cmake -DCMAKE_BUILD_TYPE=Release .\n$ cmake --build .\n\nCmake run creates the binary files 'rnnlib', 'rnnsynth' and 'gradient_check' in the current directory. \n\nIt is recommended that you add the directory containing the 'rnnlib' binary to your path,\nas otherwise the tools in the 'utilities' directory will not work.\n\nProject files for the integrated development environments can be generated by cmake. Run cmake --help\nto get list of supported IDEs.\n\n \n# Handwriting synthesis\n\nStep in to examples/online_prediction and go through few steps below to prepare the \ntraining data, train the model and eventually plot the results of the synthesis\n\n## Downloading online handwriting dataset\n\nStart by registering and downloading pen strokes data from \nhttp://www.iam.unibe.ch/~fkiwww/iamondb/data/lineStrokes-all.tar.gz\nText lables for strokes can be found here\nhttp://www.iam.unibe.ch/~fkiwww/iamondb/data/ascii-all.tar.gz\nThen unzip ./lineStrokes and ./ascii under examples/online_prediction.\nData format in the downloaded files can not be used as is \nand requires further preprocessing to convert pen coordinates to offsets from\nprevious point and merge them into the single file of netcdf format.\n\n## Preparing the training data\n\nRun ./build_netcdf.sh to split dataset to training and validation sets. \nThe same script does all necessary preprocessing including normalisation\nof the input and makes corresponding online.nc and online_validation.nc \nfiles for use with rnnlib .\n\nEach point in the input sequences from online.nc consists of three numbers: \nthe x and y offset from the previous point, and the binary end-of-stroke feature.\n\n## Gradient check\n\nTo gain some confidence that the build is fine run the gradient check:\n\ngradient_check --autosave=false check_synth2.config\n\n## Training\n\nThe training goes in two steps. First it is done without weights regularization\nand then repeated again with adaptive weight noise (MDL in rnnlib terms) from the\nbest network recorded by step one. Training with MDL from the beginning will have\ntoo slow convergence rate.\n\n### Step 1\n\nrnnlib --verbose=false  synth1d.config\n\nWhere synth1d.config is 1st step configuration file that defines network topology:\n3 LSTM hidden layers of 400 cells, 20 gaussian mixtures as output layer, 10 mixtures\nfor character warping window layer\nSomewhere between training epoch 10-15 it will find optimal solution and will do\n\"early stopping\" w/o improvement for 20 epoch. \"Early\" here takes 3 days on Intel\nSandybridge CPU. Normally training can be stopped as long as loss starts rising up\nfor 2-3 consequent epochs.\nThe best solution found is stored in synth1d@<time step>.best_loss.save file\n\n### Step 2\n\nBest loss error from step 1 is expected to be around -1080 nats and it can be further\nimproved (ca. 10%) by using weights regularisation. Loss error goes up and down during the\ntraining unlike in Step 1. Therefore one must be more patient to declare early stopping and \nwait for 20 epochs with loss worse then the best result so far. Rnnlib has implementation\nof MDL regulariser which is used in this step. The command line is as following:\n\nrnnlib --mdl=true --mdlOptimiser=rmsprop from_step1.best_loss.save\n\n### Synthesis\n\nHandwriting synthesis is done by rnnsynth binary using network parameters obtained by\nstep 2:\n\nrnnsynth from_step2.best_loss.save\n\nThe character sequence is given to stdin and output is written to stdout. The output sequence\nis the same as input where each data point has x,y offsets and end-of-stroke flag.\n\n### Plotting the results\n\nRnnsynth output is the sequence of x,y offsets and end-of-stroke flags. To visualise it one\ncan use show_pen.m Octave script:\n\noctave:>show_pen('/tmp/trace1')\n\nWhere /tmp/trace1 contains stdout from rnnsynth.\n\n### Rnnlib configuration file\n\nConfiguration options are exlained in http://sourceforge.net/p/rnnl/wiki/Home/. Since then\nthere are few things added:\n* lstm1d as hiddenType layer type - optimised LSTM layer when input dimension is 1d\n* rmsprop optimizer type\n* mixtures=N where N is number of gaussians in the output layer\n* charWindowSize=N where N is the number of gaussians in the character window layer\n* skipConnections=true|false - whether to add skip connections; default is true\n\n# Contact\n\nPlease create github issues to discuss the problems\n\n", 
  "id": 40478136
}