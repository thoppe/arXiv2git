{
  "read_at": 1462557626, 
  "description": "DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing Hyperparameters of Deep Neural Networks", 
  "README.md": "# DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing Hyperparameters of Deep Neural Networks\n\n![](https://github.com/bigaidream-projects/drmad/blob/master/shortcut.jpg)\n\n[![Gitter](https://badges.gitter.im/Join Chat.svg)](https://gitter.im/bigaidream/drmad?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![License](http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat)](LICENSE.md)\n[![ZenHub] (https://raw.githubusercontent.com/ZenHubIO/support/master/zenhub-badge.png)](https://zenhub.io)\n\n> Source code for http://arxiv.org/abs/1601.00917\n\n## Abstract\n\nThe performance of deep neural networks is well-known to be sensitive to the setting of their hyperparameters. Recent advances in reverse-mode automatic differentiation allow for optimizing hyperparameters with gradients. The standard way of computing these gradients involves a forward and backward pass of computations. However, the backward pass usually needs to consume unaffordable memory to store all the intermediate variables to exactly reverse the forward training procedure. In this work we propose a simple but effective method, DrMAD, to distill the knowledge of the forward pass into a shortcut path, through which we approximately reverse the training trajectory. Experiments on several image benchmark datasets show that DrMAD is at least 45 times faster and consumes 100 times less memory compared to state-of-the-art methods for optimizing hyperparameters with minimal compromise to its effectiveness. To the best of our knowledge, DrMAD is the first research attempt to make it practical to automatically tune thousands of hyperparameters of deep neural networks.\n\n## Citation\n```\n@inproceedings{drmad2016,\n  title={DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing Hyperparameters of Deep Neural Networks},\n  author={Fu, Jie and Luo, Hongyin and Feng, Jiashi and Low, Kian Hsiang and Chua, Tat-Seng},\n  booktitle={Proceedings of the 25th International Joint Conference on Artificial Intelligence},\n  year={2016}\n}\n\n```\n\n## GPU Version (Lua/Torch)\n\nI'm working with [Nicholas Leonard](https://github.com/nicholas-leonard) from Element Research (NYC) to provide a toolbox for efficiently tuning Lua/Torch based deep learning systems. It is of course under the MIT license. The code have been moved to a new repo:\n\n**Check it out at** https://github.com/nicholas-leonard/drmad\n\n## CPU Version (Python)\n\nThe CPU code is used in the original paper. The code is mainly modified from [Gradient-based Optimization of Hyperparameters through Reversible Learning](https://github.com/HIPS/hypergrad/). \n\n### How to run these experiments (following the instruction of hypergrad)\n\nTo reproduce our experiments, use the code in [/cpu_py/experiments](https://github.com/bigaidream-projects/drmad/tree/master/cpu_py/experiments) folder, e.g. [./exp1/safe/safe.py](https://github.com/bigaidream-projects/drmad/blob/master/cpu_py/experiments/exp1/safe/safe.py). \n\n> We strongly recommend that you take a look at the code of [autograd](https://github.com/HIPS/autograd) first. \n\nYou'll need to install [autograd](https://github.com/HIPS/autograd), our automatic differentiation package.\nHowever, autograd (aka funkyYak) has changed a lot since we wrote the hypergrad code, and it would take a little bit of work to make them compatible again.\n\nHowever, the hypergrad code should work with the version of FunkyYak as of Feb 2, at this revision:\nhttps://github.com/HIPS/autograd/tree/be470d5b8d6c84bfa74074b238d43755f6f2c55c\n\nSo if you clone autograd, then type\ngit checkout be470d5b8d6c84bfa74074b238d43755f6f2c55c,\nyou should be at the same version we used to run the experiments.\n\nThat version also predates the setup.py file, so to get your code to use the old version, you'll either have to copy setup.py into the old revision and reinstall, or add FunkyYak to your PYTHONPATH.\n\n\n## Acknowledgements\nJie Fu would like to thank Microsoft Azure for Research for providing the computational resources. This work is also supported by NUS-Tsinghua Extreme Search (NExT) project through the National Research Foundation, Singapore.", 
  "id": 51800865
}