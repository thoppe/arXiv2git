{
  "read_at": 1462549188, 
  "description": "Torch implementation of DRAW: A Recurrent Neural Network For Image Generation", 
  "README.md": "Torch implementation of DRAW: A Recurrent Neural Network For Image Generation http://arxiv.org/pdf/1502.04623.pdf. Watch Deep Learning Lecture 14: Karol Gregor on Variational Autoencoders and Image Generation https://www.youtube.com/watch?v=P78QYjWh5sM&list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu&index=3\n\nRun \n```th draw_attention.lua```\nin Terminal.app, it generates `x_prediction`, which you can plot by running `plot_results*.lua` in zbs-torch (https://github.com/soumith/zbs-torch) with QLua-LuaJit interpreter selected from 'Project' tab. Adjust the running time of the script by changing:\n```\n1. n_data (the number of MNIST examples to train on)\n2. number of iterations\n3. n_z, dimension of the hidden layer z\n4. rnn_size, dimension of h_dec and h_enc\n```\n\ndraw_attention.lua works with 28x28 MNIST dataset. You can adjust it to other datasets by changing A, N and replacing number '28' everywhere in the script. I haven't done it but it is possible.\n\ndraw_no_attention*.lua scripts implement DRAW without attention.\nIn draw_attention_read.lua only read is attentive, while write is without attention.\n\ndraw_no_attention*.lua scripts print arrays in the end, which helps to quickly estimate the quality of the results without plotting\n\nExample output by plot_results.lua\n![th visualize_word_vectors.lua](https://github.com/vivanov879/draw/blob/master/Plot_results_example.png)\n\nExample output by plot_results_no_binarization.lua\n![th visualize_word_vectors.lua](https://github.com/vivanov879/draw/blob/master/Plot_result_no_binarization_example.png)\n\n\n\n\n", 
  "id": 36968874
}