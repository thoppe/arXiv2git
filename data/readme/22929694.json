{
  "read_at": 1462553124, 
  "description": "Port of RBM_toolbox to lua", 
  "README.md": "RBM Toolbox for Torch\n===============\n\nRBM toolbox is a Torch7 toolbox for online training of RBM's. A MATLAB version exists at \n[LINK](https://github.com/skaae/rbm_toolbox).\n\nThe following is supported:\n * Support for training RBM's with class labels including:\n    * Generative training objective [2,7]\n    * Discriminative training objective [2,7]\n    * Hybrid training objective [2,7]\n    * Semi-supervised learning [2,7]   untested\n * CD - k (contrastive divergence k) [5]\n * PCD (persistent contrastive divergence) [6]\n * RBM Classification support [2,7]\n * Regularization: L1, L2, sparsity, early-stopping, dropout [1], momentum [3] \n\n# Installation\n\n 1. Install torch7: Follow [these](https://github.com/torch/torch7/wiki/Cheatsheet#installing-and-running-torch) instructions\n 2. download this repository: `git clone https://github.com/skaae/rbm_toolbox_lua.git`\n 4. To run the examples install wget with homebrew\n 3. Run example rbms with examples/runrbm.lua \n\n# Examples\nRun from /example folder\n\n\n  1) th runrbm.lua -eta 0.05 -alpha 0 -nhidden 500 -folder test_discriminative\n  \n  2) th runrbm.lua -eta 0.05 -alpha 0 -nhidden 500 -folder test_discriminative_dropout -dropout 0.5\n  \n   3) th runrb,.lua -eta 0.05 -alpha 1 -nhidden 500 -folder test_generative_pcd -traintype PCD\n   4) th runrbm.lua -eta 0.05 -alpha 0.01 -nhidden 1500 -folder test_hybrid\n\n   5) th runrbm.lua -eta 0.05 -alpha 0.01 -nhidden 1500 -folder test_hybrid_dropout -dropout 0.5\n\n   6) th runrbm.lua -eta 0.05 -alpha 0.01 -nhidden 3000 -folder test_hybrid_sparsity -sparsity 0.0001\n\n   7) th runrbm.lua -eta 0.05 -alpha 0.01 -nhidden 3000 -folder test_hybrid_sparsity_dropout -sparsity 0.0001 -dropout 0.5\n\n   8) th runrbm.lua -eta 0.05 -alpha 1 -nhidden 1000 -folder test_generative\n\n   9) th runrbm.lua -eta 0.05 -alpha 1 -nhidden 2000 -folder test_generative -dropout -0.5\n\n# Using your own data\nYou can create our own datasets with the functions in\ncode/dataset-from-tensor.lua\n\n```LUA\ncodeFolder = '../code/'\nrequire('torch')\nrequire(codeFolder..'rbm')\nrequire(codeFolder..'dataset-from-tensor')\nrequire 'paths'\ngeometry = {1,100}   -- dimensions of your training data\nnclasses = 3\nnSamples = 5 \ntrainTensor = torch.rand(nSamples,geometry[1],geometry[2])\ntrainLabels = torch.Tensor({1,2,3,1,2})\nclasses = {'ClassA','ClassB','ClassC'}\ntrainData = datatensor.createDataset(trainTensor,\n                                     oneOfK(nclasses,trainLabels),\n                                     classes,\n                                     geometry)\nprint(trainData:next())\nprint(trainData[2])\nprint(trainData:classnames())\n\n```\n# TODO\n\n 1. DO DROPOUT DISCRIMINATIVE WITH SPARSITY?   \n 2. Use momentum to smooth gradients? + Decrease learning rate    \n 3. Generative training example + samples drawn from model   \n 4. Hybrid training exampe\n 5. Semisupervised example\n 6. Implement stacking of RBM's\n\n# References\n\n[1] Srivastava Nitish, G. Hinton, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting,\" J. Mach. Learn. Res., vol. 5(Jun), no. 2, p. 1929-1958, 2014.    \n[2] H. Larochelle and Y. Bengio, \"Classification using discriminative restricted Boltzmann machines,\" in Proceedings of the 25th international conference on Machine learning. ACM,, 2008.     \n[3] G. Hinton, \"A practical guide to training restricted Boltzmann machines,\" Momentum, vol. 9, no. 1, p. 926, 2010.    \n[4] G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, \"Improving neural networks by preventing co-adaptation of feature detectors,\" arXiv Prepr., vol. 1207.0580, no. Hinton, Geoffrey E., et al. \"Improving neural networks by preventing co-adaptation of feature detectors.\" arXiv preprint arXiv:1207.0580 (2012)., Jul. 2012.    \n[5] G. Hinton, \"Training products of experts by minimizing contrastive divergence,\" Neural Comput., vol. 14, no. 8, pp. 1771-1800, 2002.     \n[6] T. Tieleman, \"Training restricted Boltzmann machines using approximations to the likelihood gradient,\" in Proceedings of the 25th international conference on Machine learning. ACM, 2008.    \n[7] H. Larochelle, M. Mandel, R. Pascanu, and Y. Bengio, \"Learning algorithms for the classification restricted boltzmann machine,\" J. Mach. Learn. Res., vol. 13, no. 1, pp. 643-669, 2012.    \n[8] R. Salakhutdinov and I. Murray, \"On the quantitative analysis of deep belief networks,\" in Proceedings of the 25th international conference on Machine learning. ACM,, 2008.    \n[9] Y. Tang and I. Sutskever, \"Data normalization in the learning of restricted Boltzmann machines,\" Dep. Comput. Sci. Toronto Univ., vol. UTML-TR-11, 2011.     \n[10] L. Wan, M. Zeiler, S. Zhang, Y. Le Cun, and R. Fergus, \"Regularization of Neural Networks using DropConnect,\" in Proceedings of The 30th International Conference on Machine Learning, 2013, pp. 1058-1066. \n\nCopyright (c) 2014, Soren Kaae Sonderby (skaaesonderby@gmail.com) All rights reserved.\n", 
  "id": 22929694
}