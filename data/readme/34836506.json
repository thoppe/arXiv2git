{
  "read_at": 1462511946, 
  "description": "Easy, expressive, trainable graph computation for torch", 
  "README.md": "# symtorch\n\nI prefer [torch7](https://github.com/torch/torch7) over similar libraries for almost everything, but sometimes miss [theano](https://github.com/theano/theano)'s expressiveness, especially when constructing complex models like RNN's with LSTM units. symtorch brings expressive and trainable graph computation to torch. symtorch doesn't copy theano's symbolic tensors/computations because I'm not a huge fan of that.\n\nYes, I know [torch/nngraph](https://github.com/torch/nngraph) exists, but I think comparing LSTM examples in nngraph and symtorch will illustrate why I like symtorch.\n\nLSTM's with nngraph, taken from [wojzaremba/lstm](https://github.com/wojzaremba/lstm/blob/master/main.lua#L65):\n\n```lua\nlocal function lstm(i, prev_c, prev_h)\n  local function new_input_sum()\n    local i2h            = nn.Linear(params.rnn_size, params.rnn_size)\n    local h2h            = nn.Linear(params.rnn_size, params.rnn_size)\n    return nn.CAddTable()({i2h(i), h2h(prev_h)})\n  end\n  local in_gate          = nn.Sigmoid()(new_input_sum())\n  local forget_gate      = nn.Sigmoid()(new_input_sum())\n  local in_gate2         = nn.Tanh()(new_input_sum())\n  local next_c           = nn.CAddTable()({\n    nn.CMulTable()({forget_gate, prev_c}),\n    nn.CMulTable()({in_gate,     in_gate2})\n  })\n  local out_gate         = nn.Sigmoid()(new_input_sum())\n  local next_h           = nn.CMulTable()({out_gate, nn.Tanh()(next_c)})\n  return next_c, next_h\nend\n```\n\nLikewise, LSTM's with symtorch are written exactly like the equations tell us:\n\n```lua\nlocal symtorch = require 'symtorch'\nlocal sigmoid = symtorch.sigmoid\nlocal tanh = symtorch.tanh\n\nlocal LSTM = Class {\n   __init__ = function(self, input, hidden, output)\n      self.input_size  = input\n      self.hidden_size = hidden\n      self.output_size = output\n\n      self.W_xi = symtorch.Tensor(hidden, input):rand(0, 0.08)\n      self.W_hi = symtorch.Tensor(hidden, hidden):rand(0, 0.08)\n      self.b_i  = symtorch.Tensor(hidden, 1)\n      self.W_xf = symtorch.Tensor(hidden, input):rand(0, 0.08)\n      self.W_hf = symtorch.Tensor(hidden, hidden):rand(0, 0.08)\n      self.b_f  = symtorch.Tensor(hidden, 1)\n      self.W_xc = symtorch.Tensor(hidden, input):rand(0, 0.08)\n      self.W_hc = symtorch.Tensor(hidden, hidden):rand(0, 0.08)\n      self.b_c  = symtorch.Tensor(hidden, 1)\n      self.W_xo = symtorch.Tensor(hidden, input):rand(0, 0.08)\n      self.W_ho = symtorch.Tensor(hidden, hidden):rand(0, 0.08)\n      self.b_o  = symtorch.Tensor(hidden, 1)\n      self.W_od = symtorch.Tensor(output, hidden):rand(0, 0.08) -- output decoder\n      self.b_od = symtorch.Tensor(output, 1)\n\n      self.prev_h = {}\n      self.prev_c = {}\n      for i = 1, input do\n         table.insert(self.prev_h, symtorch.Tensor(hidden, 1))\n         table.insert(self.prev_c, symtorch.Tensor(hidden, 1))\n      end\n   end,\n\n   forward = function(self, input)\n      -- Modeled after http://arxiv.org/pdf/1411.4555v1.pdf\n      -- Equations 4-8\n\n      local function step(i, x_t, prev_h, prev_c)\n         local i_t = sigmoid(self.W_xi:dot(x_t) + self.W_hi:dot(prev_h) + self.b_i)                   -- (4)\n         local f_t = sigmoid(self.W_xf:dot(x_t) + self.W_hf:dot(prev_h) + self.b_f)                   -- (5)\n         local c_t = f_t * prev_c + i_t * tanh(self.W_xc:dot(x_t) + self.W_hc:dot(prev_h) + self.b_c) -- (6)\n         local o_t = sigmoid(self.W_xo:dot(x_t) + self.W_ho:dot(prev_h) + self.b_o)                   -- (7)\n         local h_t = o_t * tanh(c_t)                                                                  -- (8)\n         self.prev_h[i] = h_t\n         self.prev_c[i] = c_t\n         return h_t, c_t\n      end\n\n      local res = symtorch.scan{\n         fn = step,\n         sequences = {input, self.prev_h, self.prev_c}\n      }\n      local final = res[#res][1]\n      return self.W_od:dot(final) + self.b_od\n   end\n}\n```\n\nThis is still a work in progress, thus thoughts, feedback, and contributions are very welcome! I think the code should be more self-explanatory than the giant and impressive compiler theano. Convolutions, max pooling, and some other operations are written in C and called via LuaJIT FFI. I am currently writing a neural network library on top of symtorch, [symnn](https://github.com/benglard/symnn).\n\n## Installation\n\n```\n> (sudo) luarocks install https://raw.githubusercontent.com/benglard/luaclass/master/luaclass-scm-1.rockspec\n> (sudo) luarocks install https://raw.githubusercontent.com/benglard/luaimport/master/luaimport-scm-1.rockspec\n> (sudo) luarocks install https://raw.githubusercontent.com/benglard/symtorch/master/symtorch-scm-1.rockspec\n```", 
  "id": 34836506
}