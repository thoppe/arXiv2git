{
  "read_at": 1462511789, 
  "description": "first", 
  "README.md": "Long Short Term Memory Units (original README)\n============================\nThis is self-contained package to train a language model on word level Penn Tree Bank dataset. \nIt achieves 115 perplexity for a small model in 1h, and 81 perplexity for a big model in \na day. Model ensemble of 38 big models gives 69 perplexity.\nThis code is derived from https://github.com/wojciechz/learning_to_execute (the same author, but \na different company).\n\n\nMore information: http://arxiv.org/pdf/1409.2329v4.pdf\n\nFor the Deep Learning NYU spring 2015 course\n==========================\nModifications to the original code:\n\n+ Made functions global and put the main part outside of a function, for easier interactive sessions.\n+ Added a4\\_commununication\\_loop.lua for an example of stdin/stdout communication.\n+ Added character-preprocessed train and validation ptb set in data/.\n+ Modified data.lua so we can all easily load the data in the same way and agree on the dictionary. \n+ Added a simple script a4\\_vocab.lua that loads the data and prints the character-level vocabulary (which is the vocabulary that will also be used in grading).\n+ Added a4\\_grading.py so you can test how your program performance will be automatically evaluated.\n\nFor more information, see the assignment instructions pdf.\n", 
  "id": 34696372
}