{
  "read_at": 1462556751, 
  "description": "Reading one paper each day. ", 
  "README.md": "## May 2016\n\n**Friday, the 6th, Singapore**\n\n**Scalable Bayesian Optimization Using Deep Neural Networks**\nSnoek et al. ([arXiv:1502.05700](http://arxiv.org/abs/1502.05700))\n\nThis work presents a global Bayesian optimization method that uses deep neural networks, i.e. Deep Networks for Global Optimization (DNGO). \nThe motivation for using neural networks for this task is the computational inefficiency of Gaussian processes (GP) based methods, which are most commonly used for optimization of expensive black-box functions. \nThe computational inefficiency comes from the calculation of the inverse of the covariance matrix, which grows cubically in the number of observations.\nThe neural network approach scales linearly in the number of observations, but still cubically in the number of parameters being optimised. \n\nThe paper offers an interesting alternative to GP based optimisation method. \nIt successfully combines Bayesian optimization with neural networks.\nIt shows comparable performance with GP based method, while using a lot less computational time. \n\nHowever, the difference between GP based methods and DNGO is only evident after 500 number of observations.\nThus, the efficiency advantage is diminished in the problem of hyperparameter optimization of deep neural networks, where is practical to make only around 200 number of observations. \nOne observation requires one full training of the network, which for interesting problems takes at least one whole day (26.6 hours in their case). \nEven, if the difference became evident after for example 10 observations, it would still not make difference since the computation time needed by GP and DNGO is in order of several minutes, while the computation time for one observation is in order of several hours. \nThey partially address the issue by offering a parallel evaluation, however it's rare to have enough computing resources to really take advantage of the parallelism DNGO offers. \n\nAll in all, it's an interesting paper, one that offers an alternative to the standard hyperparameter optimisation methods, and a view in the current state of the area. \nAlthough, in practice, it does not have an advantage over the well-established methods.\n\n---\n\n\n\n", 
  "id": 58175386
}