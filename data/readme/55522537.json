{
  "read_at": 1462548462, 
  "description": "", 
  "README.txt": "The NN architecture was inspired by and adapted from Yoon Kim's use of CNN\nfor sentence classification.\n  Code https://github.com/yoonkim/CNN_sentence\n  and paper http://arxiv.org/abs/1408.5882\n\n----------------------------------------------------------------------------------------\nTo run document retrieval on a file containing queries, cd into code and type\n       python retrieve_cnn.py path/to/file path/to/cnn_model\nThe first argument must be provided. It is a file containing all the queries\nin the format of the CLEF 2016 teaser1 test set. If the second model is not\nprovided, the code will automatically look for a pretrained model in data/\nnamed golden_model.npz. The output file is results/rank_predictions.txt.\n\ngolden_model.npz is trained on a dataset of roughly 300,000 examples. Each\nexample contains 5 words, and its corresponding 200 dimension vector. Refer\nto trainCNN.py to see what settings the network was trained on.\n\n----------------------------------------------------------------------------------------\nThe files that must be present for the code to run are data/visfeat_test_reduced_{x}.txt,\nand data/glove.6B/glove.6B.{y}d.txt, where x and y are dimension numbers.\n\nThe first file contains the list of documents we are choosing from, with each document\nbeing represented with a vector of x dimension. This file was obtained by running PCA\non a sample of 100000 other feature representation vectors of 4096 dimensions, then\napplying the resulting PCA to our dataset. Furthermore, the file contains only document\nIDs for which we don't know what their corresponding words are, as this is the test set.\nFor evaluation I am using 200 dimension vectors, this makes retrieval top 100 documents\nfairly quick.\n\nThe second file contains word embeddings. For evaluation I use 200 dimension word vectors,\nas the results are equally bad for other ones. To use different dimension word vectors,\none would need to train the cnn on 200 dimensions.\n\n----------------------------------------------------------------------------------------\nSummary of what each file does:\ncreateTestingData: creates dummy testing data to evaluate our CNN ranking predictions\nagainst training examples\ncreateTrainingData: creates training data to train our CNN\nevaluate_cnn: takes dummy testing data and outputs top_k predictions\nhelper_fxns: contains commonly used functions\nretrieve_cnn: outputs rankings based on queries\nsolveAnalogy: solves word analogies using word embedding based on different models\ntrainCNN: trains the convolution neural network\nPCA.m: finds k principal components from the original 4096 dimensioned visual feature vectors\n", 
  "id": 55522537
}