{
  "read_at": 1462550430, 
  "description": "", 
  "README.md": "bd_news_vis collaboration\n\n### Team Division\n- Saquib (analysis - distribution fitting, PCA, DTW)\n- Khalid (analysis - distribution fitting + tagging)\n- Mehrab (analysis, data collection from other sources (weather shocks, climate, policies in heath and education))\n- Mishuk (back-end, analysis)\n- Salman (web front end and back end, ngram viewer)\n- Tamzid (facebook graph api -- internet reach in different districts, interest topics of different people)\n\n### Todo list\n- ~~daily star crawled text (mishuk, salman)~~\n  - images to be downloaded\n  - tags to be machine learned\n- ~~dhaka tribune crawled text (mishuk)~~\n  - images to be downloaded\n- prothom alo english version crawled text (mishuk)\n- ~~NER tagging (mishuk + khalid)~~\n  - ~~interface with MongoDB~~\n- d3plus visualization (salman, saquib)\n  - improve on existing code from zeeshan (kolpokoushol student)\n  - ~~ngram viewer type interface~~\n  - network diagrams (show relationship between entities)\n- server space (mishuk + saquib)\n  - ~~AWS account for server space~~\n  - for now, mit.edu server. later AWS if needed.\n- Marketing (everyone)\n  - prothom alo news\n  - facebook page\n  - after product launch\n- Brand name\n  - a group of data volunteers?\n\n### Issues (add more to keep track)\n- ~~xpath parsing not returning 0 or null results (mishuk)~~\n- ~~scrapy crawling not working with xpath (salman)~~\n- d3plus map not showing full region (saquib)\n\n### Data analysis methods (ideas)\n- PCA on entity-location matrix (locations are columns/dimensions)\n  - who is going where, are people following each other around (for elections)\n  - do MPs talk about their area in newspaper?\n- DTW (dynamic time warping) on time series of popularity/attention of entities.\n  - can we track \n- Devise a metric for popularity decay that takes account of\n  - width of waves (w_1, ..., w_n)\n  - period between waves (T_1, ..., T_(n-1))\n  - amplitude and/or area under the curve of a particular pulse (A_1, ..., A_n)\n  - Reward function for frequent show-ups of entities.\n  - We may need to sub-sample the time series to make the above quantities meaningful.\n- Gaussian Distribution (a standard one for khaleda or hasina). \n- What is the functional shape that describes a newspaper memory. (sum of a few functions that can generalize to any entity news).\n  - we can try to fit a power law by binning time. See if newspapers have similar exponents\n  - say, a weibull distribution describes rare events. then we can express it as a sum of the same distribution for popular entities.\n- rank distribution (sigmoid function) (bursty-ness)\n- Come up with a metric for comparing newspapers (giving a formal structure to previous newspaper area coverage work)\n  - This metric can take account of mutual information overlap between two newspapers based on an ensemble of entities and their relative coverage.\n  - entity in this case may mean location or person.\n\n### Further ideas:\n- decay rate of public memory (characteristic rate for every country?)\n- does climate change correlate with news trend?\n- policy making data available? any correlation with public memory decay rate?\n- network analysis between entities\n- Fulfillment of promises by polittical leaders (maybe far fetched but still worth exploring)\n- comparing newspaper biases\n  - sentiment analysis can be augmented with frequency of bias terms\n- automatic crawler (mishuk and salman)\n- how to find a metric of transparency? control dataset?\n- can we use grammar analysis or regx parser or similar tools to understand road accident locations\n- investment opportunities based on location based news\n- news trend and correlation with economic growth\n- sentiment analysis and news mention can be analyzed to see if they have correlation with economic metrics.\n\n### further further ideas\n- surface temperature, rainfall viz for district wise.\n- blog topics\n- facebook feeds (graph api)\n\n### Possible Journals: \n- AEP\n- APS\n- Arxiv\n\n### Result Dump\nURL for write up: https://www.sharelatex.com/project/548c9da8f76b211010f38142\n\n### To run the crawler\n\n- Install the required python modules\n```\npip install newspaper pymongo elasticsearch\n```\nYou may get an error in Mac while installing `newspaper`. You can avoid it by install these modules `brew install libtiff libjpeg webp little-cms2`.\n\n- Go to python and download the nltk corpora using following ocmmands:\n```\nimport nltk\nnltk.download('punkt')\nnltk.download('all')\n```\n- Now install the Stanford NER tagger:\n```\ncd $HOME\n\n# Update / Install NLTK\npip install -U nltk\n\n# Download the Stanford NLP tools\nwget http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip\n# Extract the zip file.\nunzip stanford-ner-2015-04-20.zip \nunzip stanford-parser-full-2015-04-20.zip \nunzip stanford-postagger-full-2015-04-20.zip\n```\n- Copy and add the follwing paths to `.bashrc` file using this command `nano ~/.bashrc`:\n```\nexport STANFORDTOOLSDIR=$HOME\n\nexport CLASSPATH=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/stanford-postagger.jar:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/stanford-ner.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar\n\nexport STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/models:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/classifiers\n```\n[Source: Stackoverflow](http://stackoverflow.com/questions/13883277/stanford-parser-and-nltk/34112695#34112695)\n\n- Install Java 8 (oracle-jdk-8)\n```\njava -version\nsudo apt-get install python-software-properties\nsudo add-apt-repository ppa:webupd8team/java\nsudo apt-get update\nsudo apt-get install oracle-java8-installer\n```\n[Source: Digital Ocean](https://www.digitalocean.com/community/tutorials/how-to-install-java-on-ubuntu-with-apt-get)\n- Install mongodb:\n```\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927\necho \"deb http://repo.mongodb.com/apt/ubuntu trusty/mongodb-enterprise/stable multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-enterprise.list\nsudo apt-get update\nsudo apt-get install -y mongodb-enterprise\n```\n[Source: MongoDB](https://docs.mongodb.org/manual/tutorial/install-mongodb-enterprise-on-ubuntu/)\n#### Points to be noted for Stanford NER tagger:\n- Sometimes it confuses one single entity into multiple entities, e.g. \"Lt Colonel Shahidur Rahman\", \"Shahidur Rahman\", \"Shahidur\" are recognized as different entities\n- It mixes up the organizations and persons frequently\n- So far I haven't seen to it to identify the Taka amounts as money in any article\n\n### Mongo Data structure for the news:\n\n- _id : `ObjectID`\n- newspaper_name: `String` e.g. \"Dhaka Tribune\", \"The Daily Star\"\n- newspaper_url: `URL` e.g. \"http://www.thedailystar.net\"\n- news_headline : `String` e.g. \"They all care about democracy\"\n- news_publish_date: `ISODate`\n- news_url: `URL` e.g. \"http://www.thedailystar.net/frontpage/they-all-care-about-democracy-197176\"\n- news_original_tags : `list of Lowercase String` e.g. \"bangladesh\"\n- news_naive_tags: `list of Lowercase Strings` e.g. \\[\"crime\"\\]\n- news_ml_tags : `list of Strings` e.g. \\[\"violence\", \"domestic\", \"crime\"\\]\n- news_reporters : `List of Strings` e.g. [\"Captain Bangladesh\", \"Captain America\"]\n- news_location: `String` e.g. \"Narail\" (This is a district name. To keep the district name same we can use the same `districts` list provided below. For dhaka tribune, all the dhaka news and national news are marked as \"national\", other wise tried to find the location whule crawling using thier tag. Still we have to find and verifiy the locations using NER tagging)\n- news_text: `String` (I am keeping it as an utf text with all the newlines and quotation marks)\n- is_negative : `boolean`\n- news_image_urls : `list of URLs`\n- news_crawled_date: `ISODate`\n- news_keywords: `list of String`\n- news_ner_tags: {locations:\\[list of string\\], persons:\\[list of string\\], organizations:\\[list of string\\], moneys:\\[list of amounts\\], percents:\\[list of percents\\], dates:\\[list of dates\\], times:\\[list of times\\],locations_unique:\\[list of string\\], persons_unique:\\[list of string\\], organizations_unique:\\[list of string\\], moneys_unique:\\[list of amounts\\], percents_unique:\\[list of percents\\], dates_unique:\\[list of dates\\], times_unique:\\[list of times\\]\n\n}\n\nA python list of districts:\n\n```\ndistricts = [\"Barisal\",\"Bagerhat\",\"Bandarban\",\"Barguna\",\"Bhola\",\"Brahmanbaria\",\"Bogra\",\"Chandpur\",\"Chapainawabganj\",\"Chittagong\",\"Chuadanga\",\"Comilla\",\"Coxs Bazar\",\"Dhaka\",\"Dinajpur\",\"Feni\",\"Faridpur\",\"Gaibandha\",\"Gazipur\",\"Gopalganj\",\"Habiganj\",\"Jessore\",\"Jhalokati\",\"Jamalpur\",\"Joypurhat\",\"Jhenaidah\",\"Kurigram\",\"Khulna\",\"Khagrachhari\",\"Kushtia\",\"Kishoreganj\",\"Lakshmipur\",\"Lalmonirhat\",\"Madaripur\",\"Magura\",\"Meherpur\",\"Moulvibazar\",\"Mymensingh\",\"Manikganj\",\"Munshiganj\",\"Narail\",\"Narayanganj\",\"Noakhali\",\"Naogaon\",\"Narsingdi\",\"Natore\",\"Netrokona\",\"Nilphamari\",\"Pabna\",\"Panchagarh\",\"Patuakhali\",\"Pirojpur\",\"Rajshahi\",\"Rajbari\",\"Rangamati\",\"Rangpur\",\"Sylhet\",\"Shariatpur\",\"Satkhira\",\"Sherpur\",\"Sirajganj\",\"Sunamgonj\",\"Tangail\",\"Thakurgaon\"]\n```\n### Installing Elasticsearch and Kibana\n\n#### Mac:\nUse this [tutorial](https://gist.github.com/squarism/8fa9cdd7d6b36c9fcb45) to install Elasticsearch and Kibana and Nginx in Mac. You do not need to install logstash.\n\n####Ubuntu:\nUse this [tutorial](https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elk-stack-on-ubuntu-14-04) to install Elasticearch, Kibana and Nginx. Again, we do not need logstash.\n\n#### To check if the database is up and running:\n```\ncurl -X GET 'http://localhost:9200'\n```\n\n#### To check the available databases and their size:\n```\ncurl 'localhost:9200/_cat/indices?v'\n```\n\n#### Running Kibana\n```\ncd /path/to/kibana\ncd bin\n./kibana\n```\n[Source](http://codingexplained.com/operating-systems/mac/installing-kibana-for-elasticsearch-on-os-x)\n", 
  "id": 48772746
}