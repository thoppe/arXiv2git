{
  "read_at": 1462548121, 
  "description": "Hyper-parameter Optimization with DrMAD and Hypero", 
  "README.md": "# DrMAD\n\nTo provide an efficnet and easy-to-use hyperparameter tuning toolbox for Torch deep learning ecosystems. \n\nIt combines Bayesian optimization (BO) and automatic differentiation (AD). For the Bayesian optimization module, we will extend on [hypero](https://github.com/Element-Research/hypero); the automatic differentiation part is based on DrMAD method, https://arxiv.org/abs/1601.00917. \n\nIt will be the only tool that can tune thousands of continuous hyperparameters (e.g. L2 penalties for each neuron or learning rates for each layer) with a reasonable time/computational budget -- reads: outside Google. \n\n## Current Status\nOnly skechy code for L2 penalties on MNIST dataset. \n\n## TODO\n1. Experiments on CIFAR-10 and ImageNet\n2. Support for learning rates\n3. Refactoring\n\n\n## Dependencies\n* Twitter Torch Autograd: https://github.com/twitter/torch-autograd\n\n## Tricks\n\n### Rally with ([Net2Net](https://github.com/soumith/net2net.torch))\nImageNet dataset usually needs ~450,000 iterations. DrMAD may not approxiate this long trajectory well. \n\nOne approach would be to repeatedly initialize the weights using Net2Net, from small subsets to larget subsets and finally to the full dataset. \n\n### BO and AD\nBO is a global optimization method, whereas AD can only find local solutions. We first use BO to get some initial average hyperparameters (10 at most). Then we use AD method to further search for diverse local hyperparameters. \n\n", 
  "id": 56114362
}