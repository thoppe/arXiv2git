{
  "read_at": 1462546627, 
  "description": "BiCVM Code", 
  "README.md": "BiCVM\n======\n\nBiCVM code for learning distributed representations in a variety of settings. In\nparticular, this code can be used to reproduce the results of the paper\n\"Multilingual Models for Compositional Distributional Semantics\" (Hermann and\nBlunsom, ACL 2104).\n\nSince publishing this paper, the code-base has been rewritten quite\nsignificantly to allow training on significantly larger data than was previously\npossible.\n\nDependencies\n====\n\nThis code require several libraries in order to run:\n\n* PugiXML (included)\n* libLBFGS (http://www.chokkan.org/software/liblbfgs/)\n* Boost\n* Eigen (version >3.2.0)\n\nInstallation\n====\n\nInstall via CMake:\n```\n   mkdir Release\n   cd Release\n   cmake ../src\n   make\n```\n\nUsage\n====\n\n* **dbltrain** - Learns embeddings given two files of parallel, sentence aligned\ntext\n* **doctrain** - Learns embeddings given two folders of sentence aligned\ndocuments (matching names in folders)\n\nMore documentation to come - please contribute!\n\nOne simple example\n```\n./dbltrain --input1 sample/english \\\n    --input2 sample/german \\\n    --tree plain \\\n    --type additive \\\n    --method adagrad \\\n    --word-width 128 \\\n    --hinge_loss_margin 128 \\\n    --model1-out modelA.en.128 \\\n    --model2-out modelB.de.128 \\\n    --noise 10 \\\n    --batches 10 \\\n    --eta 0.05 \\\n    --lambdaD 1 \\\n    --calc_bi_error1 true \\\n    --calc_bi_error2 true \\\n    --iterations 5\n```\nshould result in the following output:\n```\nBiCVM Distributed Representation Learner: Copyright 2013-2014 Karl Moritz Hermann\n################################\n# Config Summary\n# alpha = 0.2\n# batches = 10\n# calc_bi_error1 = 1\n# calc_bi_error2 = 1\n# calc_lbl_error1 = 0\n# calc_lbl_error2 = 0\n# calc_rae_error1 = 0\n# calc_rae_error2 = 0\n# calc_thr_error1 = 0\n# calc_thr_error2 = 0\n# calc_uae_error1 = 0\n# calc_uae_error2 = 0\n# cv-split = -1\n# dump-frequency = 10\n# dynamic-mode = 0\n# embeddings = -1\n# epsilon = 1e-06\n# eta = 0.05\n# ftcbatches = 100\n# ftceta = 0.01\n# ftiterations = 1000\n# gamma = 0.1\n# hinge_loss_margin = 128\n# initI = 1\n# input1 = sample/english\n# input2 = sample/german\n# iterations = 9\n# l1 = 0\n# lambdaBd = 1\n# lambdaBl = 1\n# lambdaD = 1\n# lambdaWd = 1\n# lambdaWl = 1\n# linesearch = armijo\n# method = adagrad\n# model1-out = modelA.en.128\n# model2-out = modelB.de.128\n# noise = 10\n# norm = 0\n# num-sentences = 0\n# tree = plain\n# type = additive\n# updateD1 = 1\n# updateD2 = 1\n# updateF1 = 1\n# updateF2 = 1\n# updateWd1 = 1\n# updateWd2 = 1\n# updateWl1 = 1\n# updateWl2 = 1\n# word-width = 128\n# History\n#  | adagrad(armijo) it:9 lambdas: /1/\n################################\nRead in 0 words and 0 embeddings.\nRead in 0 words and 0 embeddings.\nL1 Size 10\nL2 Size 10\nReindexed dictionary from 19 entries down to 19.\nReindexed dictionary from 22 entries down to 22.\nDict size: 19 and 22\nTraining with AdaGrad\nBatch size: 2  eta 0.05\nIteration 0\nError\t524.989\nIteration 1\nError\t483.947\nIteration 2\nError\t434.471\nIteration 3\nError\t367.271\nIteration 4\nError\t281.668\nIteration 5\nError\t179.812\nIteration 6\nError\t86.0947\nIteration 7\nError\t27.2102\nIteration 8\nError\t5.8242\n```\n\nUntil there is more documentation, please refer to the code and the \"--help\"\ncommands for more information on usage.\n\nReferences\n====\n\nIf you use this software package in your experiments and publish related work,\n   please cite one of the following papers as appropriately:\n\nFor most work, the following paper should be cited, in which this code was\nintroduced, as well ash the recursive document-level model.\n```\n@InProceedings{Hermann:2014:ACLphil,\n  author    = {Hermann, Karl Moritz and Blunsom, Phil},\n  title     = {{Multilingual Models for Compositional Distributional Semantics}},\n  booktitle = {Proceedings of ACL},\n  year      = {2014},\n  month     = jun,\n  url       = {http://arxiv.org/abs/1404.4641},\n}\n```\n\nFor the basic noise-contrastive/large margin objective function over parallel\ndata:\n\n```\n@InProceedings{Hermann:2014:ICLR,\n  author    = {Hermann, Karl Moritz and Blunsom, Phil},\n  title     = {{Multilingual Distributed Representations without Word Alignment}},\n  booktitle = {Proceedings of ICLR},\n  year      = {2014},\n  month     = apr,\n  url       = {http://arxiv.org/abs/1312.6173},\n}\n```\n\nFor anything related to syntax-based composition models:\n```\n@InProceedings{Hermann:2013:ACL,\n  author    = {Hermann, Karl Moritz and Blunsom, Phil},\n  title     = {{The Role of Syntax in Vector Space Models of Compositional Semantics}},\n  booktitle = {Proceedings of ACL},\n  year      = {2013},\n  month     = aug,\n  url       = {http://www.karlmoritz.com/_media/hermannblunsom_acl2013.pdf}\n}\n```\n\nNotes\n====\n\nThis is development code and may not be fully functional. That said, the\n**doctrain** and **dbltrain** programmes should work as expected and you should\nbe able to reproduce all the results from the papers mentioned above.\n\nThere are a number of auxiliary programmes to extract vectors and for file\nconversion, as well as a number of programmes that use the underlying model for\ndifferent tasks such as question answering.\n\nIn particular the QA related code is still under development and included here\nprimarily in order to demonstrate how the model can easily be extended with\ndifferent training algorithms and modalities.\n\n\nTo Do\n====\n\nDictionary/Model initialisation could be cleaned up!\n\nThere is some technical debt from the memory optimisation on the corpus side.\nPrincipally this concerns the push_back overload for the corpus function - this\nneeds to be completed given more complex models.\n\nMost models from the 2013 ACL paper have not yet been ported to the new\narchitecture (CCAE-A..D etc.). This should be done at some point. For now,\nplease refer to the [\"oxcsvm\" repository](https://github.com/karlmoritz/oxcvsm),\nwhich features some of these models.\n", 
  "id": 19541794
}