{
  "read_at": 1462544929, 
  "description": "Molding CNNs for text (http://arxiv.org/abs/1508.04112)", 
  "README.md": "\n#### News\nThe code has been re-factored and integrated into the new repo: https://github.com/taolei87/rcnn/tree/master/code/sentiment\n\n\n## CNNs with non-linear and non-consecutive feature maps\n\n\nThis repo contains an implementation of CNNs described in the paper [Molding CNNs for text: non-linear, non-consecutive convolutions](http://arxiv.org/abs/1508.04112) by Tao Lei, Regina Barzilay and Tommi Jaakkola.\n\n\n\n#### Dependencies\n\n * [Theano](http://deeplearning.net/software/theano/) >= 0.7\n * Python >= 2.7\n * Numpy\n\n\n\n#### Data\n \n * [Stanford Sentiment Treebank](http://nlp.stanford.edu/sentiment/index.html): <br>\n  The annotated constituency trees from the treebank are converted into plain text sequences. `data/stsa.fine.*` are annotated with 5-class fine-grained labels; `data/stsa.binary.*` are annotated with binary labels. <br>\n  In the training files `data/stsa.binary.phrases.train` and `data/stsa.fine.phrases.train` we also add phrase annotations. Each phrase (and its sentiment label) is a separate training instance.\n\n * [Glove](http://nlp.stanford.edu/projects/glove/) word embeddings: <br>\n  We use the 840B Common Crawl version. Note the original compressed file is 2GB. In the directory `word_vectors/` we provide a smaller version (~37MB) by pruning words not in the sentiment treebank.\n\n * [Sogou Chinese news corpora](http://www.sogou.com/labs/dl/c.html): <br>\n  Data redistribution is not allowed. Please contact Sogou Lab to obtain the news corpora.\n\n\n#### Results\n\nDirectory `trained_models` contains saved models of the sentiment classification task. We reproduced the results mentioned in our paper by setting the random seed explicitly. The performance of each trained models are listed below:  \n\nFine-grained models  |  Dev acc.  |  Test acc. \n:--- | --- | ---\nstsa.root.fine.pkl.gz  |  49.5  | 50.6 \nstsa.phrases.fine.pkl.gz  |  53.4  | 51.2  \nstsa.phrases.fine.2.pkl.gz **  |  53.5  |  52.7\n| |\n**Binary models**  |  **Dev acc.**  |  **Test acc.**\nstsa.root.binary.pkl.gz  |  87.0  |  87.0\nstsa.phrases.binary.pkl.gz  |  88.9  |  88.6\n| |\n** **Note**: more recent run (`stsa.phrases.fine.2.pkl.gz`) gets better results than those reported in our paper.\n\n<br>\n\n#### Usage\n\nOur model is implemented in `model.py`. The command `python model.py --help` will list all the parameters and corresponding descriptions.\n\nHere is an example command to train a model on the binary sentiment classification task:\n```\npython model.py --embedding word_vectors/stsa.glove.840B.d300.txt.gz  \\\n    --train data/stsa.binary.phrases.train  \\\n    --dev data/stsa.binary.dev  --test data/stsa.binary.test  \\\n    --model output_model\n```\n\nWe can optionally specify Theano configs via `THEANO_FLAGS`:\n```\nTHEANO_FLAGS='device=cpu,floatX=float32'; python model.py ...\n```\n\nAnother example with more hyperparamter settings:\n```\nexport OMP_NUM_THREADS=1;   #specify number of cores \n\nTHEANO_FLAGS='device=cpu,floatX=float64'; python model.py  \\\n    --embedding word_vectors/stsa.glove.840B.d300.txt.gz  \\\n    --train data/stsa.binary.phrases.train  \\\n    --dev data/stsa.binary.dev  --test data/stsa.binary.test  \\\n    --model output_model  \\\n    --depth 3  --order 3  --decay 0.5  --hidden_dim 200  \\\n    --dropout_rate 0.3  --l2_reg 0.00001  --act relu  \\\n    --learning adagrad  --learning_rate 0.01\n```\n", 
  "id": 40742366
}