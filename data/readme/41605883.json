{
  "read_at": 1462546015, 
  "description": "A repository for computational models of human memory", 
  "README.md": "# Human-memory\nA repository for computational models of human memory\n\nI'm a beginner! Anyone who is at the similar level who wants to learn about\nmodels for human memory is welcome to participate in this repo.\nThe goal is ultimately to just learn about the dynamics of connected neurons,\ntheir learning rules, and how our minds work!\n\nI'm also trying to focus on biologically feasible systems.\n\nProjects are implemented in Javascript and Python.\n\nTheory:\nMost models of human memory involve categorization and classification, and the equations that achieve this simple traverse an error or fitness surface to find the respective max and min.\n\n#####Running all the JS files:\nBasically just download the git repo, and run the respective index.html files.\n\n\n###1) Self organizing map/ Kohonen net (implemented in Javascript):\n\nI would like to explore how neurons could exhibit this kind of activity with\nbiological constraints. For example, the kohonen net is based on comparative\nerror, and I'm not sure if neurons would ever be able to do such precise\ncalculations by only summing dendrite inputs.\nI wonder if it would be possible to create a robust classifier using SDR's.\n\nBasically a SOM works by creating a field of several different classification populations through competive training. (Note: by populations I mean blobs of similar weight vectors). They're very interesting, and I'm going to try to think of ways in which you could chain a bunch of SOM's and feature extractors together to compute a complex classification problem.\nI've read in several places that these are the \"ideal model for human memory\", but I seriously doubt these nets are the entire story. They might be a PART of human memory, but I think the remarkable type diversity of memory circuits in the brain will surprise us. These nets at best probably just loosely model the classification and categorization modules of our minds.\n\nJust writing this one down... Not sure how this idea is connected to any of these ideas but are a bunch of simple graded \"yes/no\" responses enough to  match the accuracy of a single backpropagated ideal solution?\n\n###2) Hopfield net:\nBug's fixed! I wrote the original net under the assumption that online training was possible: which I now see is simply not true. So I'm a bit disappointed in the hopfield net as a model for neural function. It does illustrate how local connectivity and correlation would allow for biologically feasible pattern matching, which I guess is something. Hop.py should work if you run it like a standard python program, although not necessarily beautifully.\nTo run the hopfield server (JS client, and python Flask backend):  \n   Dependencies:  \n   * Flask\n   * Numpy\nDownload, move to server directory, and run the command 'python server.py'.  \nNext, open http://127.0.0.1:5000/  \nNext feel free to train and run all the mofocking patterns you want.\n\nTheory behind Hopfield Nets:\nMinimization of an energy function through activity-correlation to create content-adressable memory.\n\nRecurrent Systems:\n* http://www.scholarpedia.org/article/Hopfield_network\n* http://www.cs.toronto.edu/~mackay/itprnn/ps/506.522.pdf\n* https://www.youtube.com/watch?v=gfPUWwBkXZY\n\n###3)Python Neural net:\nObject oriented approach to implementing a neural net. (Python)\nShit is slooooow.\n\n\n###4) Deep Belief Networks: Stacked RBMS\nIncludes the RBM:\nMost of the credit for this code goes to Edwin Chen,\nhttps://github.com/echen/restricted-boltzmann-machines\nWhere I took a sizeable amount of the training code from.\n\n\nGenerative models\nProbabilistic inference networks: infer the relationship between the hidden units that could creates the visible effect.  \n\nRelated Concepts:\n\n * RBMS (restricted boltzmann machines)\n * Sigmoid Belief Networks\n * Bayesian Networks\n * Autoencoders\n\nhttps://deeplearning.net/tutorial/DBN.html\nhttps://www.youtube.com/watch?v=vkb6AWYXZ5I\nhttps://www.cs.toronto.edu/~hinton/nipstutorial/nipstut3.pdf\n\n###5) Convolutional Nets:\n\nA folder full of broken shit! What's more to love.\n\nConvolutional nets are crazy: they're closest thing we've gotten to a general solution for complex problems. Still, they're difficult to understand, slow to train, and aren't even close to perfect.\nI'll keep developing this folder over the next couple years and see where it goes. \n\nI use nolearn (built with lasagne and theano) and Caffe (bvlc vision).\n\nThere are scripts for nolearn networks and ipython notebooks for Caffe.\n\n\"Style project\": Is based on this paper.\n* http://arxiv.org/pdf/1508.06576v2.pdf\n\n\n####6) Other shit to learn:\n   Auto-encoder, (standard, variational), Recurrent neural nets, Recurrent-CNNs, Sparse distributed Representations, Deep-belief nets, Hopfield nets.\n\n   Projects:\n   * Cifar-10\n   * Deep-art, style and content mixing\n\n#### Considerations in creating a biologically accurate neural system:\nBasically, the question is how do you create models with learning rules and units that are constrained in the same ways as biological neurons, that can still compute information.\nTis a mystery.\nBut here are some constraints. (And I'm sure we'll find more constraints the better biological imaging and activity reading technology gets)\n\n   * Spiking\n   * Different neurotransmitters corresponding to neuron types\n   * Dales law (Ratio of inhibitory to excitatory cells [20/80])\n   * Sparse coding\n   * Refractory periods (Leaky integrate and fire)\n   * Connectome structures: biological make up of the brain (backwards feeding, layers, ect.)\n   * Synaptic modification (Neuronal activity shapes circuits by strengthening synapses, stabilizing dendritic spines, and stimulating dendritic growth and branching. These effects are mediated through increases in intracellular calcium levels, activation of kinases, and downstream changes in gene transcription.)\n   * LTP, LTD, Spike-timing dependent plasticity.\n   * Limited connectivity/receptive fields\n   * Neuron structures and types\n   * Local Learning rules vs larger scale fine-tuning learning (check machine learning vs reasoning)\n\n\nHow we can cheat to find our answers:\n   * Look at neuron differentiation during development, see what does what\n   * Repeating structures in brain? Is the brain broken up into functional units? (Other organ systems do it, does the brain as well?)\n   * Raphe nucleas, islets of langerhans? Small isolated systems that perform important functions\n   * Connectome (all the connections between brain regions), Dynome (The way they interact)\n   * Larger scale recordings of ensembles of neurons (There's nothing like observation when it comes to creating behavioral rules)\n   * Find proof of the existence of a universal code/learning rule? If there was one, it would simplify the problem of discovering the computational rules of each brain region.\n   * Analyze species with simpler brains, (some rules we would find MUST be conserved over the course of evolution)\n\n\n###Links to models of biological systems:\n\nMIT initial object recognition in the visual cortex: (full model and implementation details)\nhttp://cbcl.mit.edu/projects/cbcl/publications/ai-publications/2005/AIM-2005-036.pdf\n\nMachine Learning vs Machine reasoning:\nhttp://research.microsoft.com/pubs/192773/tr-2011-02-08.pdf\n\nMIT Texture + object recognition:\nhttp://cbcl.mit.edu/cbcl/publications/ps/unifiedC2.pdf\n\nOrientation maps of the visual cortex:\nhttp://www.scholarpedia.org/article/Visual_map#Orientation_Maps\n\nLateral occipital cortex and its role in object recognition:\nhttp://math.bu.edu/people/horacio/tutorials/jascha_2.pdf\n\nVisual perception of texture: (Something that I think will be incredibly important in the future of effective object segregation and recognition)\nhttp://www.cns.nyu.edu/~msl/papers/landygraham02.pdf\n\nLarge scale model of the functioning brain (only the abstract)\nhttp://www.sciencemag.org/content/338/6111/1202.short\n", 
  "id": 41605883
}