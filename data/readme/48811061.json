{
  "id": 48811061, 
  "read_at": 1462550403, 
  "README.rst": "==========\nBitshuffle\n==========\n\nFilter for improving compression of typed binary data.\n\nBitshuffle is an algorithm that rearranges typed, binary data for improving\ncompression, as well as a python/C package that implements this algorithm\nwithin the Numpy framework.\n\nThe library can be used along side HDF5 to compress and decompress datasets and\nis integrated through the `dynamically loaded filters`_ framework. Bitshuffle\nis HDF5 filter number ``32008``.\n\nAlgorithmically, Bitshuffle is closely related to HDF5's `Shuffle filter`_\nexcept it operates at the bit level instead of the byte level. Arranging a\ntyped data array in to a matrix with the elements as the rows and the bits\nwithin the elements as the columns, Bitshuffle \"transposes\" the matrix,\nsuch that all the least-significant-bits are in a row, etc.  This transpose\nis performed within blocks of data roughly 8kB long [1]_.\n\nThis does not in itself compress data, only rearranges it for more efficient\ncompression. To perform the actual compression you will need a compression\nlibrary.  Bitshuffle has been designed to be well matched Marc Lehmann's\nLZF_ as well as LZ4_. Note that because Bitshuffle modifies the data at the bit\nlevel, sophisticated entropy reducing compression libraries such as GZIP and\nBZIP are unlikely to achieve significantly better compression than simpler and\nfaster duplicate-string-elimination algorithms such as LZF and LZ4. Bitshuffle\nthus includes routines (and HDF5 filter options) to apply LZ4 compression to\neach block after shuffling [2]_.\n\nThe Bitshuffle algorithm relies on neighbouring elements of a dataset being\nhighly correlated to improve data compression. Any correlations that span at\nleast 24 elements of the dataset may be exploited to improve compression.\n\nBitshuffle was designed with performance in mind. On most machines the\ntime required for Bitshuffle+LZ4 is insignificant compared to the time required\nto read or write the compressed data to disk. Because it is able to exploit the\nSSE and AVX instruction sets present on modern Intel and AMD processors, on\nthese machines compression is only marginally slower than an out-of-cache\nmemory copy.  On modern x86 processors you can expect Bitshuffle to have a\nthroughput of roughly 1 byte per clock cycle, and on the Haswell generation of\nIntel processors (2013) and later, you can expect up to 2 bytes per clock\ncycle. In addition, Bitshuffle is parallelized using OpenMP.\n\nAs a bonus, Bitshuffle ships with a dynamically loaded version of\n`h5py`'s LZF compression filter, such that the filter can be transparently\nused outside of python and in command line utilities such as ``h5dump``.\n\n.. [1] Chosen to fit comfortably within L1 cache as well as be well matched window of the LZF compression library.\n\n.. [2] Over applying bitshuffle to the full dataset then applying LZ4 compression, this has the tremendous advantage that the block is already in the L1 cache.\n\n.. _`dynamically loaded filters`: http://www.hdfgroup.org/HDF5/doc/Advanced/DynamicallyLoadedFilters/HDF5DynamicallyLoadedFilters.pdf\n\n.. _`Shuffle filter`: http://www.hdfgroup.org/HDF5/doc_resource/H5Shuffle_Perf.pdf\n\n.. _LZF: http://oldhome.schmorp.de/marc/liblzf.html\n\n.. _LZ4: https://code.google.com/p/lz4/\n\n\nApplications\n------------\n\nBitshuffle might be right for your application if:\n\n- You need to compress typed binary data.\n- Your data is arranged such that adjacent elements over the fastest varying\n  index of your dataset are similar (highly correlated).\n- A special case of the previous point is if you are only exercising a subset\n  of the bits in your data-type, as is often true of integer data.\n- You need both high compression ratios and high performance.\n\n\nComparing Bitshuffle to other compression algorithms and HDF5 filters:\n\n- Bitshuffle is less general than many other compression algorithms.\n  To achieve good compression ratios, consecutive elements of your data must\n  be highly correlated.\n- For the right datasets, Bitshuffle is one of the few compression\n  algorithms that promises both high throughput and high compression ratios.\n- Bitshuffle should have roughly the same throughput as Shuffle, but\n  may obtain higher compression ratios.\n- The MAFISC_ filter actually includes something similar to Bitshuffle as one of\n  its prefilters,  However, MAFICS's emphasis is on obtaining high compression\n  ratios at all costs, sacrificing throughput.\n\n.. _MAFISC: http://wr.informatik.uni-hamburg.de/research/projects/icomex/mafisc\n\n\nInstallation for Python\n-----------------------\n\nInstallation requires python 2.7+ or 3.3+, HDF5 1.8 or later, HDF5 for python,\nNumpy and Cython.  To use the dynamically loaded HDF5 filter requires HDF5\n1.8.11 or later.\n\nTo install::\n\n    python setup.py install [--h5plugin [--h5plugin-dir=spam]]\n\nIf using the dynamically loaded HDF5 filter (which gives you access to the\nBitshuffle and LZF filters outside of python), set the environment variable\n``HDF5_PLUGIN_PATH`` to the value of ``--h5plugin-dir`` or use HDF5's default\nsearch location of ``/usr/local/hdf5/lib/plugin``.\n\nIf you get an error about missing source files when building the extensions,\ntry upgrading setuptools.  There is a weird bug where setuptools prior to 0.7\ndoesn't work properly with Cython in some cases.\n\n\nUsage from Python\n-----------------\n\nThe `bitshuffle` module contains routines for shuffling and unshuffling\nNumpy arrays.\n\nIf installed with the dynamically loaded filter plugins, Bitshuffle can be used\nin conjunction with HDF5 both inside and outside of python, in the same way as\nany other filter; simply by specifying the filter number ``32008``. Otherwise\nthe filter will be available only within python and only after importing\n`bitshuffle.h5`. Reading Bitshuffle encoded datasets will be transparent.\nThe filter can be added to new datasets either through the `h5py` low level\ninterface or through the convenience functions provided in\n`bitshuffle.h5`. See the docstrings and unit tests for examples.\n\n\nUsage from C\n------------\n\nIf you wish to use Bitshuffle in your C program and would prefer not to use the\nHDF5 dynamically loaded filter, the C library in the ``src/`` directory is\nself-contained and complete.\n\n\nFor Best Results\n----------------\n\nHere are a few tips to help you get the most out of Bitshuffle:\n\n- For multi-dimensional datasets, order your data such that the fastest varying\n  dimension is the one over which your data is most correlated (have\n  values that change the least), or fake this using chunks.\n- To achieve the highest throughput, use a data type that is 64 *bytes* or\n  smaller. If you have a very large compound data type, consider adding a\n  dimension to your datasets instead.\n- To make full use of the SSE2 instruction set, use a data type whose size \n  is a multiple of 2 bytes. For the AVX2 instruction set, use a data type whose\n  size is a multiple of 4 bytes.\n\n\nCiting Bitshuffle\n-----------------\n\nBitshuffle was initially described in\nhttp://dx.doi.org/10.1016/j.ascom.2015.07.002, pre-print available at\nhttp://arxiv.org/abs/1503.00638.\n", 
  "description": "Clone of kiyo-masui/bitshuffle for nexusformat, created with gracious permission of Kiyoshu Masui"
}