{
  "read_at": 1462548454, 
  "description": "", 
  "README.md": "# Adaptive Neural Network Representations for Parallel and Scalable Bayesian Optimization\n\n**Neural Network Bayesian Optimization** is function optimization technique inpsired by the work of:\n> Jasper Snoek, et al <br>\n> Scalable Bayesian Optimization Using Deep Neural Networks <br>\n> http://arxiv.org/abs/1502.05700\n\nThis repository contains the python code written by James Brofos and Rui Shu of a modified approach that continually retrains the neural network underlying the optimization technique, and implements the technique within a parallelized setting for improved speed performance. \n\nMotivation\n----------\nThe success of most machine learning algorithms is dependent the proper tuning of the hyperparameters. A popular technique for hyperparameter tuning is Bayesian optimization, which canonically uses a Gaussian process to interpolate the hyperparameter space. The computation time for GP-based Bayesian optimization, however, grows rapidly with respect to sample size (the number of tested hyperparameters) and quickly becomes very time consuming, if not all together intractable. Fortunately, a neural network is capable of mimicking the behavior of a Guassian process whilst providing a significant reduction in computation time. \n\nDependencies\n------------\nThis code requires:\n\n* Python 2.7\n* MPI (and [MPI4Py](http://mpi4py.scipy.org/))\n* [Numpy](http://www.numpy.org/) \n* [Scipy](http://www.scipy.org/)\n* [Theano](http://deeplearning.net/software/theano/)\n* [Theanets](http://theanets.readthedocs.org/en/stable/)\n* [Statsmodels](http://statsmodels.sourceforge.net/devel/)\n* [Matplotlib](http://matplotlib.org/)\n* [pyGPs](http://www-ai.cs.uni-dortmund.de/weblab/static/api_docs/pyGPs/)\n\nCode Execution\n--------------\nTo run the code from the home directory in parallel with 4 cores, simply call mpiexec:\n```       \nmpiexec -np 4 python -m mpi.mpi_optimizer\n```\n\nTo run a sequential version of the code:\n```       \npython -m sequential.seq_optimizer\n```\n\nTo run the gaussian process version of Bayesian optimization:\n```\npython -m sequential.seq_gaussian_process\n```\n\n**Sample output**:\n```\nRandomly query a set of initial points...  Complete initial dataset acquired\nPerforming optimization... \n0.100 completion...\n0.200 completion...\n0.300 completion...\n0.400 completion...\n0.500 completion...\n0.600 completion...\n0.700 completion...\n0.800 completion...\n0.900 completion...\n1.000 completion...\nSequential gp optimization task complete.\nBest evaluated point is:\n[-0.31226245  3.80792522]\nPredicted best point is:\n[-0.31226245  3.7755048 ]\n```\n\n**Note:** The code, as written, focuses the use of the algorithm on any black-box function. A few common functions are available in `learning_objective`. The chosen function is set in `hidden_function.py`. To really appreciate the time-savings gained by the parallelized code, it is important to realize that evaluating a real-world black-box function (i.e. computing the test performance for an ML algorithm with a given set of hyperparameters) takes time.\n\nThis can be simulated by uncommenting the line: `# time.sleep(2)` in `hidden_function.py`.\n\n", 
  "id": 55241566
}