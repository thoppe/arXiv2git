{
  "read_at": 1462553698, 
  "description": "QCD miniapp", 
  "README.md": "# CCS QCD Miniapp \n\nSee doc/README_original for the original README and doc/README_FS for \nFS-specific topics, including how to evaluate the results.\n\n## Original Application and Authors\n\nThis miniapp is derived from a QCD benchmark code, which was developed\nby the following authors:\n\n* Ken-Ichi Ishikawa (Hiroshima University) \n* Yoshinobu Kuramashi (University of Tsukuba) \n* Akira Ukawa (University of Tsukuba) \n* Taisuke Boku (University of Tsukuba) \n\nSee the README_original file for the original README. See below for\nsome of the major changes from the original version.\n\n## Problem Classes\n\nThis miniapp has five problem classes, for which the first three are\nrelatively small problems just for testing this miniapp itself. The\nremaining two are the target problem sizes for the HPCI FS\nevaluation. \n\n* Class 1: 8x8x8x32 (default MPI config: 1x1x1)\n* Class 2: 32x32x32x32 (default MPI config: 4x4x4)\n* Class 3: 64x64x64x32 (default MPI config: 8x8x8)\n* Class 4 (FS target): 160x160x160x160 (default MPI config: 20x20x20)\n* Class 5 (FS target): 256x256x256x256 (default MPI config: 32x32x32)\n* Class 6 (FS target): 192x192x192x192 (default MPI config: 24x24x24)\n\nThe default MPI configuration indicates how the lattice is decomposed\nover MPI processes. It can be configured with make command\noptions.\n\n## Parallelization\n\nThe 3-D space of the overall lattice field is decomposed over MPI\nprocesses. The time dimension is not decomposed. Each MPI process\ntakes an evenly decomposed sub domain and runs the BiCGStab solver and\nClover routine with OpenMP-based multithreading.\n\nThe MPI process dimension can be configured by overriding _NDIMX,\n_NDIMY, and _NDIMZ macros. Alternatively, these macros can be set by \nmake command-line options. See the compilation instruction below.\n\nNote that no specific optimization is implemented for NUMA\nmemory affinity. Thus, for example, an MPI-OpenMP configuration with\none MPI process with multiple OpenMP threads on a multi-socket node\nmight cause performance problems due to inefficient memory\naccesses. Use one MPI process per NUMA node and OpenMP threading\nwithin the associated socket would be a simple workaround.\n\n## Compilation\n\nMove to the src directory first. The standard method of compilation is\nto use make as follows: \n\n    make MAKE_INC=<make-sysdep-file> CLASS=<problem-class>\n    \nOnce the compilation finishes, an executable file named\n`ccs_qcd_solver_bench_classN`, where `N` is the problem class number,\nshould be generated.\n\nSome predefined sysdep files are included in this package:\n\n* make.fx10.inc: For K and FX10-based systems\n* make.gfortran.inc: For using the GNU compiler\n* make.ifort.inc: For using the Intel compiler\n* make.pgi.inc: For using the pgi compiler\n* make.pgiacc.inc: For using the pgi compiler with OpenACC (required the PGI compiler version 14.6 or higher)\n\n\nTo change the process decomposition, use PX, PY, PZ options as\nfollows:\n\n    make MAKE_INC=<make-sysdep-file> CLASS=<problem-class> PX=<#x-dim-processes> PY=<#y-dim-processes> PX=<#z-dim-processes>\n\nNote that each lattice dimension must be divisible by the process\ndimension.\n\nThe parallelization of this version is limited to the spatial\ndimensions. The time dimension is sequntially processed by the same\nprocess (or thread).\n \n\n## Running Miniapp\n\nJust run the executable with the standard `mpirun` command. The\nprogram accepts three options that set variables `kappa` `csw`, and\n`tol`. There is no need to explicitly set these variable values.\nIn addition, batch job scripts for certain known platforms can be\nfound in the run directory. \n\nNote that the stack size limit may need to be increased since the\nprogram may crash if the stack size limit is too small.\n\n### K Computer\n\nMove to directory run/k to find pjsub script files. For example, to\nrun the class 4 problem, execute the following command:\n\n    pjsub pjsub-class4.sh\n\n\n## Validating Results\n\nTo be written.\n\n\n## Performance Notes\n\nSome sample results can be found in the results directory.\n\nThe bottleneck is the matrix-vector multiply in the BiCGStab\nSolver, which is implemented in subroutine mult_eo_tzyx. The clover\nroutine spends relatively minor execution time, but it should at least\nachieve 20% of the BiCGStab performance (FLOPS).\n\nSome potential optimizations include:\n\n* Overlapping of computation and communication\n* NUMA-aware memory usage\n* Intra-socket cache optimization\n* Parallelization of the time dimension for more scalable performance \n\n\n## Major Changes from the Original Version\n\nThe code base is mostly kept as the original one with the following\nexceptions:\n\n* The build system is extended with the problem class configuration.\n* The command line arguments are made optional.\n* The performance measurement component is integrated (This is still\n  preliminary and will be enhanced in near future).\n* Display a message on the performance requirement.\n\n\n## References\n* Boku et al., \"Multi-block/multi-core SSOR preconditioner for the QCD\n  quark solver for K computer,\" arXiv:1210.7398.\n* Terai et al., \"Performance Tuning of a Lattice QCD code on a node of\n  the K computer,\" IPSJ High Performance Computing Symposium, 2013.\n\n## Version History\n\n* [v1.0 (March 31,\n  2014)](http://github.com/fiber-miniapp/ccs-qcd/tree/1.0)\n    - Initial release\n\n* [v1.1 (July 5,\n  2014)](http://github.com/fiber-miniapp/ccs-qcd/tree/1.1)\n    - Update clover.h90 and make.fx10.inc\n\n* [v1.1.1 (August 28,\n  2014)](http://github.com/fiber-miniapp/ccs-qcd/tree/1.1.1)\n    - Fix file permission\n\n* [v1.2 (October 20,\n  2014)](http://github.com/fiber-miniapp/ccs-qcd/tree/1.2)\n    - Add the OpenACC implementation\n\n* [v1.2.1 (November 6,\n  2014)](http://github.com/fiber-miniapp/ccs-qcd/tree/1.2.1)\n    - Add the OpenACC implementation of clover term\n\n  \n", 
  "id": 18286307
}