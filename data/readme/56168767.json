{
  "read_at": 1462548414, 
  "description": "Source code for Deep Saliency with Encoded Low Level Distance Map and High Level Features, CVPR 2016.", 
  "README.md": "## SaliencyELD\n\nSource code for our CVPR 2016 paper \"Deep Saliency with Encoded Low level Distance Map and High Level Features\" by [Gayoung Lee](https://sites.google.com/site/gylee1103/), [Yu-Wing Tai](http://www.gdriv.es/yuwing) and [Junmo Kim](https://sites.google.com/site/siitkaist/professor). **([ArXiv paper link] (http://arxiv.org/abs/1604.05495))**\n\n![Image of our model](./figs/model_pic.png)\n\nAcknowledgement : Our code uses various libraries: [Caffe](http://github.com/BVLC/caffe), [VLfeat](http://www.vlfeat.org), [OpenCV](http://www.opencv.org) and [Boost](http://www.boost.org).\n\n## Usage\n1. **Dependencies**\n    0. OS : Our code is tested on Ubuntu 14.04\n    0. CMake : Tested on CMake 2.8.12\n    0. Caffe : Caffe that we used is contained in this repository.\n    0. VLFeat : Tested on VLFeat 0.9.20\n    0. OpenCV 3.0 : We used OpenCV 3.0, but the code may work with OpenCV 2.4.X version.\n    0. g++ : Our code uses openmp and C++11 and was tested with g++ 4.9.2.\n    0. Boost : Tested on Boost 1.46\n\n2. **Installation**\n    0. Get our pretrained model and VGG16 model. NOTE: Some paths for caffe models and prototxts are hard-coded in **main.cpp**. Check them if you download models in the other folder.\n\n        ```shell\n        cd $(PROJECT_ROOT)/models/\n        sh get_models.sh\n        ```\n\n    0. Build Caffe in the project folder using CMake:\n\n        ```shell\n        cd $(PROJECT_ROOT)/caffe/\n        mkdir build\n        cd build/\n        cmake ..\n        make -j4\n        ```\n\n    0. Change library paths in $(PROJECT_ROOT)/CMakeLists.txt for your custom environment and build our code:\n\n        ```shell\n        cd $(PROJECT_ROOT)\n        edit CMakeList.txt\n        mkdir build\n        cd build/\n        cmake ..\n        make\n        ```\n\n    0. Run the executable file which takes one argument for the path of the directory containing test images:\n\n        ```shell\n        ./SaliencyELD ../test_images\n        ```\n\n    0. The results will be generated in the test directory.\n\n## Results of datasets used in the paper\n\n![visualization](./figs/visualization.png)\n\nWe provide our results of benchmark datasets used in the paper for convenience.\n\n[ASD results](https://www.dropbox.com/s/mpkxuuok5h4sp7b/ASD_ELD.tar.gz?dl=1) (ASD dataset [site](http://ivrlwww.epfl.ch/supplementary_material/RK_CVPR09/index))\n\n[ECSSD results](https://www.dropbox.com/s/j5xooaqkxpduh51/ECSSD_ELD.tar.gz?dl=1) (ECSSD dataset [site](http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html))\n\n[PASCAL-S results](https://www.dropbox.com/s/o2g9ykynkpqjgdw/pascal_ELD.tar.gz?dl=1) (PASCAL-S dataset [site](http://cbi.gatech.edu/salobj/))\n\n[DUT-OMRON results](https://www.dropbox.com/s/kyomr7lnn42og4q/DUTOMRON_ELD.tar.gz?dl=1) (DUT-OMRON dataset [site](http://202.118.75.4/lu/DUT-OMRON/index.htm))\n\n[THUR15K results](https://www.dropbox.com/s/zu004jx5hc0kabn/THUR15000_ELD.tar.gz?dl=1) (THUR15K dataset [site](http://mmcheng.net/gsal/))\n\n\n\n## Citing our work\nPlease kindly cite our work if it helps your research:\n\n    @inproceedings{lee2016saliency,\n        title = {Deep Saliency with Encoded Low level Distance Map and High Level Features},\n        author={Gayoung, Lee and Yu-Wing, Tai and Junmo, Kim},\n        booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n        year = {2016}\n    }\n\n", 
  "id": 56168767
}