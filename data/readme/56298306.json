{
  "read_at": 1462548554, 
  "description": "", 
  "README.md": "#Learning Fair Classifiers\n\nThis repository provides a logistic regression implementation in python for our fair classification mechanism introduced in [(Zafar et al., 2016)](http://arxiv.org/abs/1507.05259v3). Please cite the paper when using the code.\n\n**Dependencies:** numpy, matplotlib, scipy\n\n##1. Fair classification demo\n\nFair classification corresponds to a scenario where we are learning classifiers from a dataset that is biased towards/against a specific demographic group, yet the classifier predictions are fair and do not show the biases contained in the data. For more details, have a look at Section 2 of our [paper](http://arxiv.org/pdf/1507.05259v3.pdf).\n\n###1.1. Generating a biased dataset\nLets start off by generating a sample dataset where class labels are biased towards a certain group.\n\n```shell\n$ cd synthetic_data_demo\n$ python decision_boundary_demo.py\n```\n\nThe code will generate a dataset with a multivariate normal distribution. The data consists of two non-sensitive features and one sensitive feature.\n\n<img src=\"synthetic_data_demo/img/data.png\" width=\"500px\" style=\"float: right;\">\n\nGreen color denotes the positive class while red denotes negative. Circles represent the non-protected group while crosses represent the protected group. It can be seen that class labels (green and red) are highly correlated with the sensitive feature value (protected and non-protected), that is, most of the green (positive class) points are in the non-protected class while most of red (negative class) points are in the protected class. **Close the figure** for the code to continue. Next, the code will output the following details about the dataset:\n\n```\nTotal data points: 2000\n# non-protected examples: 914\n# protected examples: 1086\nNon-protected in positive class: 642 (70%)\nProtected in positive class: 358 (33%)\nP-rule is: 47%\n```\n\nThe p-rule is essentially the ratio of (fractions of) protected and non-protected examples in the positive class. According to the [doctrine of disparate impact](https://en.wikipedia.org/wiki/Disparate_impact), fair systems should maintain a p-rule of at least 80%. More discussion on p-rule can be found in our [paper](http://arxiv.org/pdf/1507.05259v3.pdf) (Section 2).\n\n###1.2. Training an unconstrained classifier on the biased data\n\nNext, we will train a logistic regression classifier on the data to see the correlations between the classifier decisions and sensitive feature value:\n\n```python\n# all constraint flags are set to 0 since we want to train an unconstrained (original) classifier\napply_fairness_constraints = 0\napply_accuracy_constraint = 0\nsep_constraint = 0\nw_uncons, p_uncons, acc_uncons = train_test_classifier()\n```\n\nWe are training the classifier without any constraints (more on constraints to come later). \"train_test_classifier()\" will in turn call the function \"train_model(...)\":\n\n```python\nut.train_model(x_train, y_train, x_control_train, loss_function, apply_fairness_constraints, apply_accuracy_constraint, sep_constraint, sensitive_attrs, sensitive_attrs_to_cov_thresh, gamma)\n```\n\nFollowing output is generated by the program:\n\n```\n== Unconstrained (original) classifier ==\nAccuracy: 0.85\nProtected/non-protected in +ve class: 33% / 70%\nP-rule achieved: 48%\nCovariance between sensitive feature and decision from distance boundary : 0.809\n```\n\nWe can see that the classifier decisions reflect the biases contained in the original data, and the p-rule is 48%, showing the unfairness of classifier outcomes. The reason why the classifier shows similar biases as ones contained in the data is that the classifier model tries to minimize the loss (or maximize the accuracy) on the training data by learning the patterns in the data as best as possible. One of the patterns was the unfairness w.r.t. the sensitive feature, and the classifier ended up learning that as well.\n\n###1.3. Optimizing classifier accuracy subject to fairness constraints\n\nNext, we will try to make these outcomes fair by still **optimizing for classifier accuracy**, but **subject it to fairness constraints**. Refer to Section 3.2 of our [paper](http://arxiv.org/pdf/1507.05259v3.pdf) for more details.\n\n```python\napply_fairness_constraints = 1 # set this flag to one since we want to optimize accuracy subject to fairness constraints\napply_accuracy_constraint = 0\nsep_constraint = 0\nsensitive_attrs_to_cov_thresh = {\"s1\":0}\nw_f_cons, p_f_cons, acc_f_cons  = train_test_classifier()\n```\n\nNotice that setting _{\"s1\":0}_ means that the classifier should achieve 0 covariance between the sensitive feature (s1) value and distance to the decision boundary. A 0 covariance would mean no correlation between the two variables.\n\nThe results for the fair classifier look like this:\n\n```\n== Classifier with fairness constraint ==\nAccuracy: 0.71\nProtected/non-protected in +ve class: 51% / 53%\nP-rule achieved: 97%\nCovariance between sensitive feature and decision from distance boundary : 0.014\n```\n\nWe can see that the classifier sacrificed some accuracy to achieve similar fractions of protected/non-protected examples in the positive class. The code will also show how the classifier shifts its boundary to achieve fairness, while making sure that the smallest possible loss in accuracy is incurred.\n\n<img src=\"synthetic_data_demo/img/f_cons.png\" width=\"500px\" style=\"float: right;\">\n\nThe figure shows the original decision boundary (without any constraints) and the shifted decision boundary that was learnt by the fair classifier. Notice how the boundary shifts to push more non-protected points to the negative class (and vice-versa).\n\n\n###1.4. Optimizing fairness subject to accuracy constraints\n\nNow lets try to **optimize fairness** (that does not necessarily correspond to a 100% p-rule) **subject to a deterministic loss in accuracy**. The details can be found in Section 3.3 of our [paper](http://arxiv.org/pdf/1507.05259v3.pdf).\n\n```python\napply_fairness_constraints = 0 # flag for fairness constraint is set back to0 since we want to apply the accuracy constraint now\napply_accuracy_constraint = 1 # now, we want to optimize fairness subject to accuracy constraints\nsep_constraint = 0\ngamma = 0.5\nw_a_cons, p_a_cons, acc_a_cons = train_test_classifier()    \n```\n\nThe \"gamma\" variable controls how much loss in accuracy we are willing to take while optimizing for fairness. A larger value of gamma will result in more fair system, but we will be getting a more loss in accuracy.\n\nThe results and the decision boundary for this experiment are:\n\n```\n== Classifier with accuracy constraint ==\nAccuracy: 0.79\nProtected/non-protected in +ve class: 46% / 66%\nP-rule achieved: 70%\nCovariance between sensitive feature and decision from distance boundary : 0.155\n```\n\n<img src=\"synthetic_data_demo/img/a_cons.png\" width=\"500px\" style=\"float: right;\">\n\nYou can experiment with more values of gamma to see how allowing more loss in accuracy leads to more fair boundaries.\n\n\n###1.5. Constraints on misclassying positive examples\n\nNext, lets try to train a fair classifier, however, lets put an additional constraint: do not misclassify any non-protected points that were classified in positive class by the original (unconstrained) classifier! The idea here is that we only want to promote the examples from protected group to the positive class, without demoting any non-protected points from the positive class (this might be a business necessity in many scenarios). Details of this formulation can be found in Section 3.3 of our [paper](http://arxiv.org/pdf/1507.05259v3.pdf). The code works as follows:\n\n```python\napply_fairness_constraints = 0 # flag for fairness constraint is set back to0 since we want to apply the accuracy constraint now\napply_accuracy_constraint = 1 # now, we want to optimize accuracy subject to fairness constraints\nsep_constraint = 1 # set the separate constraint flag to one, since in addition to accuracy constrains, we also want no misclassifications for certain points (details in demo README.md)\ngamma = 2000.0\nw_a_cons_fine, p_a_cons_fine, acc_a_cons_fine  = train_test_classifier()\n```\n\nThe output looks like:\n\n```\n== Classifier with accuracy constraint (no +ve misclassification) ==\nAccuracy: 0.67\nProtected/non-protected in +ve class: 81% / 90%\nP-rule achieved: 90%\nCovariance between sensitive feature and decision from distance boundary : 0.075\n\n```\n\n<img src=\"synthetic_data_demo/img/a_cons_fine.png\" width=\"500px\" style=\"float: right;\">\n\nNotice the movement of decision boundary: we are only moving points to the positive class to achieve fairness (and not moving any non-protected point that was classified as positive by the original classifier to the negative class).\n\n###1.6. Understanding trade-offs between fairness and accuracy\nRemember, while optimizing for accuracy subject to fairness constraints, we forced the classifier to achieve perfect fairness by setting the covariance threshold to 0. This resulted in a perfectly fair classifier but we had to incur a rather big loss in accuracy (0.71 from 0.85). Lets see if we try a range of fairness values (not necessarily 100% p-rule), what kind of accuracy we will be achieving. We will do that by trying a range of values of covariance threshold (not only 0!) for this purpose. Execute the following command:\n\n```shell\n$ cd synthetic_data_demo\n$ python fairness_acc_tradeoff.py\n```\n\nThe code will generate the synthetic data as before, and call the following function:\n\n```python\nut.plot_cov_thresh_vs_acc_pos_ratio(X, y, x_control, NUM_FOLDS, loss_function, apply_fairness_constraints, apply_accuracy_constraint, sep_constraint, ['s1'])\n```\n\nThe following output is generated:\n\n<img src=\"synthetic_data_demo/img/tradeoff.png\" width=\"500px\" style=\"float: right;\">\n\nWe can see that decreasing the covariance threshold value gives a continuous trade-off between fairness and accuracy. Specifically, we see that the fractions of protected and non-protected examples in positive class starts to converge (resulting in a greater p-rule), however, we get an increasing drop in accuracy.\n\n###1.7. Adult data\n\nWe also provide a demo of our code on [Adult dataset](http://archive.ics.uci.edu/ml/datasets/Adult). For applying the fairness constraints on the adult dataset, execute the following commands:\n\n```shell\n$ cd adult_data_demo\n$ python demo_constraints.py\n```\n\n##2. Using the code\n\n###2.1. Training a(n) (un)fair classifier\n\nFor training a fair classifier, set the values for constraints that you want to apply, and call the following function:\n\n```python\nimport utils as ut\n\napply_fairness_constraints = 0\napply_accuracy_constraint = 0\nsep_constraint = 0\ngamma = 0\nw = ut.train_model(x_train, y_train, x_control_train, loss_function, apply_fairness_constraints, apply_accuracy_constraint, sep_constraint, sensitive_attrs, sensitive_attrs_to_cov_thresh, gamma)\n```\n\nThe function resides in file \"fair_classification/utils.py\". **Documentation about the type/format of the variables can be found at the beginning of the function**.\n\nSetting all the constraint values to 0 will train an unconstrained (original) logistic regression classifier. You can choose the constraint values selectively depending on which fairness formulation you want to use (**examples for each case provided in the demo above**).\n\n###2.2. Making predictions\n\nThe function will return the weight vector learned by the classifier. Given an _(n)_ x _(d+1)_ array _X_ consisting of data _n_ points (and _d_ features -- first column in the weight array is for the intercept, and should be set to 1 for all data points), you can make the classifier predictions using the weight vector as follows:\n\n```python\ndistance_boundary = numpy.dot(w, X) # will give the distance from the decision boundary\npredicted_labels = np.sign(class_probabilities) # sign of the class probability is the class label\n```\n\nFor using k-fold cross validation, call the function \"ut.compute_cross_validation_error(...)\". This function will automatically split the data into k (specified by user) train/test folds and return the accuracy and fairness numbers.\n\n###References\n * **Learning Fair Classifiers**. Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna P. Gummadi. http://arxiv.org/abs/1507.05259v3\n", 
  "id": 56298306
}