{
  "read_at": 1462556526, 
  "description": "A survey of reinforcement learning solutions to the inverted pendulum problem.", 
  "README.md": "# inverted-pendulum\nA survey of reinforcement learning solutions to the inverted pendulum problem.\n\n# Introduction\nThe inverted pendulum problem can be defined concisely as creating a system\nthat autonomously balances a rotating pendulum attached to a cart on a rail\nusing actuators to move the cart along the rail, and sensors to reveal the\nstate of the cart and pendulum. Given a specific inverted pendulum system,\nsolving the problem amounts to choosing which sensors to use, optionally\nformulating a deterministic or stochastic model that approximates the physics\ninvolved, and most importantly figuring out a control strategy that works.\nIt is a classic problem in control theory and dynamics and can serve as a good\ntesting ground for the development of real-time control algorithms. A good\nargument for the motivation of understanding the problem can be seen by noting\nits prevalence in nature and the man made world. For example, every person when\nupright needs to constantly make adjustments to prevent from falling over, and\nso all of us repeatedly solve a much more difficult version of this problem\nwhen sitting or moving about.\n\n# Problem Formulation\nA pendulum of length **l** with a mass **m** on one end and attached to a cart\nof mass **M** with a hinge, is able to rotate by the effect of the application of\nsome force **F** on the cart.\n\n\n![Alt text](/img/pendulum.png?raw=true \"Schematic of the problem setup. Graphic by Krishnavedala, Wikimedia commons (CC0 1.0).\")\n\nBy assumption, the friction of the cart on the ground as well as the friction\nof the pendulum on the cart are ignored.  The cart is limited to motion in the\nhorizontal x-axis, while the pendulum is able to rotate along the x, y plane\nfreely, making an angle of **th** with the y-axis. It is also assumed that the\nforce vector **F** has no component in the y direction. A controller attempts\nto balance the pendulum by applying a finite force to the cart, allowing it to\nmove left or right with some acceleration. After some amount of time **[?]T** has\npassed, the controller fails if either the angle of the pendulum deviates by\nmore than **+-[?]th** or the position of the cart reaches the bounds **+-L**. The\nproblem can now be stated explicitly: develop a controller that balances the\npendulum under these constraints.\n\n# References\nThe following list contains background research material.\n\n[1] Stimac, Andrew K. Standup and stabilization of the inverted pendulum. Diss.\nMassachusetts Institute of Technology, 1999.\n\n[2] Russell, Stuart, and Peter Norvig. \"Artificial intelligence: a modern\napproach.\" (1995).\n\n[3] Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An\nintroduction. Vol. 1. No. 1. Cambridge: MIT press, 1998.\n\n[4] Melo, Francisco S., Sean P. Meyn, and M. Isabel Ribeiro. \"An analysis of\nreinforcement learning with function approxi- mation.\" Proceedings of the 25th\ninternational conference on Machine learning. ACM, 2008.\n\n[5] Kober, J. & Peters, J. (2012). Reinforcement learning in robotics: A\nsurvey. In Reinforcement Learning, Vol. 12 of Adaptation, Learning, and\nOptimization, pp. 579-610. Springer Berlin Heidelberg.\n\n[6] Anderson, Charles W. \"Learning to control an inverted pendulum using neural\nnetworks.\" Control Systems Magazine, IEEE 9.3 (1989): 31-37.\n\n[7] Watkins, Christopher JCH, and Peter Dayan. \"Q-learning.\" Machine learning\n8.3-4 (1992): 279-292.\n\n[8] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet\nclassification with deep convolutional neural net-\nworks.\" Advances in neural information processing systems. 2012.\n\n[9] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\"\narXiv preprint arXiv:1312.5602 (2013).\n\n", 
  "id": 43706426
}