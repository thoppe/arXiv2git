{
  "read_at": 1462557020, 
  "description": "For the reproduction of research by Agostinelli et al. Learning Activation Functions to Improve Deep Neural Networks. http://arxiv.org/abs/1412.6830", 
  "README.md": "==========================================\nIntroduction\n==========================================\nThis repository if for the reproduction of research by Agostinelli et al. Learning Activation Functions to Improve Deep Neural Networks. http://arxiv.org/abs/1412.6830\n\nThis is a learnable activation function for neural networks. Experiments have shown that it outperforms rectified linear units (ReLU) and Leaky ReLU.\n\n==========================================\nMemory\n==========================================\nThe use of this layer will require more memory. There is a \"save_mem\" option that can be used. However, this will lead to slower performance and has not yet been thoroughly tested.\n\n==========================================\nIn place computation\n==========================================\nIn place computation can be done. However, due to implementation details, it does not conserve memory and tests show it will result in a slight decrease in speed.\n\n==========================================\nSolver Files\n==========================================\nWe made custom changes to the solver files. The changes are reflected in src/caffe/solver.cpp, include/caffe.solver.hpp, and src/caffe/proto/caffe.proto\n\n==========================================\nDefining the Learned Activation Functions\n==========================================\n```\nlayer {\n  name: \"apl1\"\n  type: \"APL\"\n  bottom: \"blob_name\"\n  top: \"blob_name\"\n  param {\n    decay_mult: 1 # We set so weight decay is 0.001\n  }\n  param {\n    decay_mult: 1 # We set so weight decay is 0.001\n  }\n  apl_param {\n    sums: 1\n    slope_filler {\n      type: \"uniform\"\n      min: -0.5\n      max: 0.5\n    }\n    offset_filler {\n      type: \"gaussian\"\n      std: 0.5\n    }\n  }\n}\n```\n", 
  "id": 28163969
}