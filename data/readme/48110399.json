{
  "read_at": 1462550033, 
  "description": "C++ implementation for Neural Network-based NLP, such as LSTM machine translation!", 
  "README.md": "# N3LP\nC++ implementation for Neural Network-based NLP, such as LSTM machine translation!<br>\nThis project ONLY requires a template library for linear algebra, Eigen (http://eigen.tuxfamily.org/index.php?title=Main_Page)\n\n## Long Short-Term Memory (LSTM)\nThe LSTM implemented in this project employs a variant of the major LSTM's gate computation where previous cell states are used to compute input/output gates.\nSee [1, 2] for the simplified version of the LSTM implemented here.\n\n[1] http://arxiv.org/abs/1410.4615<br>\n[2] http://nlp.stanford.edu/pubs/tai-socher-manning-acl2015.pdf\n\n## BlackOut sampling\nBlackOut [3, 4] is an approximation method to softmax classification learning with the large number of classes.\n\n[3] http://arxiv.org/abs/1511.06909<br>\n[4] https://github.com/IntelLabs/rnnlm\n\n## USAGE ##\n1) modify the line in Makefile to use Eigen<br>\nEIGEN_LOCATION=$$HOME/local/eigen_new #Change this line to use Eigen\n\n2) run the command \"make\"\n\n3) ./run the command \"n3lp\", and then the seq2seq model training starts (currently)\n\n## Projects using N3LP ##\nFeel free to tell me (hassy@logos.t.u-tokyo.ac.jp) if you are using N3LP or have any questions!\n* Tree-to-Sequence Attentional Neural Machine Translation  \nPaper: http://arxiv.org/abs/1603.06075<br>\nCode: https://github.com/tempra28/tree2seq\n\n## Contributors ##\n* <a href=\"http://www.logos.t.u-tokyo.ac.jp/~hassy/\">Kazuma Hashimoto</a> - Mainly developing this project\n* <a href=\"http://www.logos.t.u-tokyo.ac.jp/~eriguchi/\">Akiko Eriguchi</a> - Developing practical applications (e.g. <a href=\"https://github.com/tempra28/tree2seq\">tree-to-sequence neural machine translation</a>)\n", 
  "id": 48110399
}