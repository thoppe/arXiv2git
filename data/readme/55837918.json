{
  "read_at": 1462547822, 
  "description": "deeplearning paper notes", 
  "README.md": "# notes\n\n2016-4-9   \nCNN for NLP    \nConvolutional Neural Networks for Sentence Classification  http://arxiv.org/abs/1408.5882        \nhttp://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp  \n\nDeepLearning Resource   \nhttp://handong1587.github.io/deep_learning/2015/10/09/dl-resources.html#imagenet  \n  \nPath-SGD   \nPath-SGD: Path-Normalized Optimization in Deep Neural Networks,Behnam Neyshabur, Ruslan Salakhutdinov, Nathan Srebro.Neural Information Processing Systems (NIPS) 28, 2015    \npaper http://arxiv.org/abs/1506.02617   \ncode http://ttic.uchicago.edu/~bneyshabur/   \n\nlinks about DL   \nhttp://gitxiv.com/   \n\nUsing HMM to achieve a simple Pinyin input method   \nhttp://sobuhu.com/ml/2013/03/07/hmm-pinyin-input-method.html    \n\n2016-4-14   \nHighway Networks & Deep Residual Networks & lowrank-highwaynetwork    \npaper http://arxiv.org/abs/1507.06228    \nhttp://arxiv.org/abs/1512.03385    \nhttp://arxiv.org/abs/1603.03116     \ncode  https://github.com/zomux/deepy/tree/master/experiments/highway_networks    \nhttps://github.com/alrojo/lasagne_residual_network    \nhttps://github.com/Avmb/lowrank-highwaynetwork    \n\n2016-4-16   \npaper  Neural Networks with Few Multiplications http://arxiv.org/abs/1510.03009    \ncode  https://github.com/hantek/BinaryConnect   \n\npaper  SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size  http://arxiv.org/abs/1602.07360   \ncode  https://github.com/DeepScale/SqueezeNet   \n\nA detailed introduction to the neural network    \nhttps://mp.weixin.qq.com/s?__biz=MzA3MzE5MjM2Mw==&mid=2672246378&idx=1&sn=8c0863ea512fbcba024d4548bede6a21   \n\nA detailed introduction to face detection   \nhttps://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&mid=402840844&idx=1&sn=25cce8cdd0d6403943074bce18949b61&scene=1&srcid=0412gEhqheWw0hdggK665pGZ&pass_ticket=jAPclBu6ufBKD9QV9VJ00deYCG3dO5K3UJtDZGNXkFBl4C8kmklFrRWrwlHYI991#rd    \n\npaper  Deep Networks with Stochastic Depth  https://arxiv.org/abs/1603.09382     \ncode  https://github.com/yueatsprograms/Stochastic_Depth   \n      https://github.com/dblN/stochastic_depth_keras\n\npaper  learning compact recurrent neural networks http://arxiv.org/abs/1604.02594  \n\npaper  Unsupervised Learning on Neural Network Outputs http://arxiv.org/abs/1506.00990   \ncode  https://github.com/yaolubrain/ULNNO    \n\npaper  Compressing Neural Networks with the Hashing Trick  http://arxiv.org/abs/1504.04788   \n\npaper   Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding  http://arxiv.org/abs/1510.00149\n\npaper  DisturbLabel: Regularizing CNN on the Loss Layer  http://research.microsoft.com/en-us/um/people/jingdw/pubs%5CCVPR16-DisturbLabel.pdf\n\npaper  universum prescription: regularization using unlabeled data  http://arxiv.org/abs/1511.03719   \n\npaper SparkNet: Training Deep Networks in Spark http://arxiv.org/abs/1511.06051   \ncode  https://github.com/amplab/SparkNet\n\nLinks:RNN and LSTM http://handong1587.github.io/deep_learning/2015/10/09/rnn-and-lstm.html#types-of-rnn\n\nAwesome Python:A curated list of awesome Python frameworks, packages, software and resources https://python.libhunt.com/\n\n\npaper Net2Net: Accelerating Learning via Knowledge Transfer  http://arxiv.org/abs/1511.05641    \n\npaper All you need is a good init  http://arxiv.org/abs/1511.06422\n\n\nlinks: http://liweithu.me/    \nhttp://licstar.net/    \n\npaper Gated Graph Sequence Neural Networks  http://arxiv.org/abs/1511.05493v1   \ncode  https://github.com/yujiali/ggnn\n  \npaper Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) http://arxiv.org/abs/1511.07289    \ncode  https://github.com/iassael/torch-elu   \n\npaper  ACDC  A Structured Efficient Linear Layer  http://arxiv.org/abs/1511.05946    \ncode  https://github.com/mdenil/acdc-torch    \n\npaper  Importance Weighted Autoencoders  http://arxiv.org/abs/1509.00519    \ncode  https://github.com/yburda/iwae    \n\npaper  Reducing Overfitting in Deep Networks by Decorrelating Representations  http://arxiv.org/abs/1511.06068   \n\npaper  Convolutional neural networks with low-rank regularization  http://arxiv.org/abs/1511.06067    \ncode  https://github.com/chengtaipu/lowrankcnn    \n\nblog Deep Language Modeling for Question Answering using Keras http://benjaminbolte.com/blog/2016/keras-language-modeling.html    \n\n", 
  "id": 55837918
}