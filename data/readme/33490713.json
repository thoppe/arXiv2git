{
  "read_at": 1462511746, 
  "description": "deep extensions to nn", 
  "README.md": "# dpnn : deep extensions to nn\n\nThis package provides many useful features that aren't part of the main nn package. \nThese include [sharedClone](#nn.Module.sharedClone), which allows you to clone a module and share \nparameters or gradParameters with the original module, without incuring any memory overhead.\nWe also redefined [type](#nn.Module.type) such that the type-cast preserves Tensor sharing within a structure of modules. \n\nThe package provides the following Modules:\n\n * [Decorator](#nn.Decorator) : abstract class to change the behaviour of an encapsulated module ;\n * [DontCast](#nn.DontCast) : prevent encapsulated module from being casted by `Module:type()` ;\n * [Serial](#nn.Serial) : decorate a module makes its serialized output more compact ; \n * [NaN](#nn.NaN) : decorate a module to detect the source of NaN errors ;\n * [Inception](#nn.Inception) : implements the Inception module of the GoogleLeNet article ;\n * [Collapse](#nn.Collapse) : just like `nn.View(-1)`;\n * [Convert](#nn.Convert) : convert between different tensor types or shapes;\n * [ZipTable](#nn.ZipTable) : zip a table of tables into a table of tables;\n * [ZipTableOneToMany](#nn.ZipTableOneToMany) : zip a table of element `el` and table of elements into a table of pairs of element `el` and table elements;\n * [CAddTensorTable](#nn.CAddTensorTable) : adds a tensor to a table of tensors of the same size;\n * [ReverseTable](#nn.ReverseTable) : reverse the order of elements in a table;\n * [PrintSize](#nn.PrintSize) : prints the size of inputs and gradOutputs (useful for debugging);\n * [Clip](#nn.Clip) : clips the inputs to a min and max value;\n * [Constant](#nn.Constant) : outputs a constant value given an input (which is ignored);\n * [SpatialUniformCrop](#nn.SpatialUniformCrop) : uniformly crops patches from a input;\n * [SpatialGlimpse](#nn.SpatialGlimpse) : takes a fovead glimpse of an image at a given location;\n * [WhiteNoise](#nn.WhiteNoise) : adds isotropic Gaussian noise to the signal when in training mode;\n * [OneHot](#nn.OneHot) : transforms a tensor of indices into [one-hot](https://en.wikipedia.org/wiki/One-hot) encoding.\n * [Kmeans](#nn.Kmeans) : [Kmeans](https://en.wikipedia.org/wiki/K-means_clustering) clustering layer. Forward computes distances with respect to centroids and returns index of closest centroid. Centroids can be updated using gradient descent. Centroids could be initialized randomly or by using [kmeans++](https://en.wikipedia.org/wiki/K-means%2B%2B) algoirthm;\n * [SpatialRegionDropout](#nn.SpatialRegionDropout) : Randomly dropouts a region (top, bottom, leftmost, rightmost) of the input image. Works with batch and any number of channels.;\n * [FireModule](#nn.FireModule) : FireModule as mentioned in the [SqueezeNet] (http://arxiv.org/pdf/1602.07360v1.pdf).;\n * [SpatialFeatNormalization](#nn.SpatialFeatNormalization) : Module for widely used preprocessing step of mean zeroing and standardization for images.\n * [SpatialBinaryConvolution](#nn.SpatialBinaryConvolution) : Module for binary spatial convolution (Binary weights) as mentioned in [XNOR-Net](http://arxiv.org/pdf/1603.05279v2.pdf).\n\nThe following modules and criterions can be used to implement the REINFORCE algorithm :\n\n * [Reinforce](#nn.Reinforce) : abstract class for REINFORCE modules;\n * [ReinforceBernoulli](#nn.ReinforceBernoulli) : samples from Bernoulli distribution;\n * [ReinforceNormal](#nn.ReinforceNormal) : samples from Normal distribution;\n * [ReinforceGamma](#nn.ReinforceGamma) : samples from Gamma distribution;\n * [ReinforceCategorical](#nn.ReinforceCategorical) : samples from Categorical (Multinomial with one sample) distribution;\n * [VRClassReward](#nn.VRClassReward) : criterion for variance-reduced classification-based reward;\n\nAdditional differentiable criterions\n * [ModuleCriterion](#nn.ModuleCriterion) : adds an optional `inputModule` and `targetModule` before a decorated criterion;\n * [BinaryLogisticRegression](#nn.BLR) : criterion for binary logistic regression.\n * [SpatialBinaryLogisticRegression](#nn.SpatialBLR) : criterion for pixel wise binary logistic regression.\n\nA lot of the functionality implemented here was pulled from \n[dp](https://github.com/nicholas-leonard/dp), which makes heavy use of this package. \nHowever, dpnn can be used without dp (for e.g. you can use it with optim), \nwhich is one of the main reasons why we made it.\n\n## Tutorials \n\n[Sagar Waghmare](https://github.com/sagarwaghmare69) wrote a nice [tutorial](tutorials/ladder.md)\non how to use dpnn with nngraph to reproduce the \n[Lateral Connections in Denoising Autoencoders Support Supervised Learning](http://arxiv.org/pdf/1504.08215.pdf). \n\nA brief (1 hours) overview of Torch7, which includes some details about __dpnn__, \nis available via this [NVIDIA GTC Webinar video](http://on-demand.gputechconf.com/gtc/2015/webinar/torch7-applied-deep-learning-for-vision-natural-language.mp4). In any case, this presentation gives a nice overview of Logistic Regression, Multi-Layer Perceptrons, Convolutional Neural Networks and Recurrent Neural Networks using Torch7.\n\n\n\n<a name='nn.Module'></a>\n## Module ##\n\nThe Module interface has been further extended with methods that facilitate \nstochastic gradient descent like [updateGradParameters](#nn.Module.updageGradParameters) (i.e. momentum learning), \n[weightDecay](#nn.Module.weightDecay), [maxParamNorm](#nn.Module.maxParamNorm) (for regularization), and so on.\n\n<a name='nn.Module.dpnn_parameters'></a>\n### Module.dpnn_parameters ###\n\nA table that specifies the name of parameter attributes. \nDefaults to `{'weight', 'bias'}`, which is a static variable (i.e. table exists in class namespace). \nSub-classes can define their own table statically. \n\n<a name='nn.Module.dpnn_gradParameters'></a>\n### Module.dpnn_gradParameters ###\n\nA table that specifies the name of gradient w.r.t. parameter attributes. \nDefaults to `{'gradWeight', 'gradBias'}`, which is a static variable (i.e. table exists in class namespace). \nSub-classes can define their own table statically. \n\n<a name='nn.Module.type'></a>\n### [self] Module:type(type_str) ###\n\nThis function converts all the parameters of a module to the given `type_str`. \nThe `type_str` can be one of the types defined for [torch.Tensor](https://github.com/torch/torch7/blob/master/doc/tensor.md)\nlike `torch.DoubleTensor`, `torch.FloatTensor` and `torch.CudaTensor`. \nUnlike the [type method](https://github.com/torch/nn/blob/master/doc/module.md#nn.Module.type)\ndefined in [nn](https://github.com/torch/nn), this one was overriden to \nmaintain the sharing of [storage](https://github.com/torch/torch7/blob/master/doc/storage.md#storage)\namong Tensors. This is especially useful when cloning modules share `parameters` and `gradParameters`.\n\n<a name='nn.Module.sharedClone'></a>\n### [clone] Module:sharedClone([shareParams, shareGradParams]) ###\n\nSimilar to [clone](https://github.com/torch/nn/blob/master/doc/module.md#nn.Module.clone).\nYet when `shareParams = true` (the default), the cloned module will share the parameters \nwith the original module. \nFurthermore, when `shareGradParams = true` (the default), the clone module will share \nthe gradients w.r.t. parameters with the original module.\nThis is equivalent to :\n```lua\nclone = mlp:clone()\nclone:share(mlp, 'weight', 'bias', 'gradWeight', 'gradBias')\n```\nyet it is much more efficient, especially for modules with lots of parameters, as these \nTensors aren't needlessly copied during the `clone`.\nThis is particularly useful for [Recurrent neural networks](https://github.com/Element-Research/rnn/blob/master/README.md) \nwhich require efficient copies with shared parameters and gradient w.r.t. parameters for each time-step.\n\n<a name='nn.Module.maxParamNorm'></a>\n### Module:maxParamNorm([maxOutNorm, maxInNorm]) ###\n\nThis method implements a hard constraint on the upper bound of the norm of output and/or input neuron weights \n[(Hinton et al. 2012, p. 2)](http://arxiv.org/pdf/1207.0580.pdf) .\nIn a weight matrix, this is a contraint on rows (`maxOutNorm`) and/or columns (`maxInNorm`), respectively. \nHas a regularization effect analogous to [weightDecay](#nn.Module.weightDecay), but with easier to optimize hyper-parameters. \nAssumes that parameters are arranged (`output dim x ... x input dim`). \nOnly affects parameters with more than one dimension.\nThe method should normally be called after [updateParameters](https://github.com/torch/nn/blob/master/doc/module.md#nn.Module.updateParameters). \nIt uses the C/CUDA optimized [torch.renorm](https://github.com/torch/torch7/blob/master/doc/maths.md#torch.renorm) function.\nHint : `maxOutNorm = 2` usually does the trick. \n\n<a name='nn.Module.momentumGradParameters'></a>\n### [momGradParams] Module:momentumGradParameters() ###\n\nReturns a table of Tensors (`momGradParams`). For each element in the \ntable, a corresponding parameter (`params`) and gradient w.r.t. parameters \n(`gradParams`) is returned by a call to [parameters](https://github.com/torch/nn/blob/master/doc/module.md#nn.Module.parameters).\nThis method is used internally by [updateGradParameters](#nn.Module.updateGradParameters).\n\n<a name='nn.Module.updateGradParameters'></a>\n### Module:updateGradParameters(momFactor [, momDamp, momNesterov]) ###\n\nApplies classic momentum or Nesterov momentum [(Sutskever, Martens et al, 2013)](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf) to parameter gradients. \nEach parameter Tensor (`params`) has a corresponding Tensor of the same size for gradients w.r.t. parameters (`gradParams`).\nWhen using momentum learning, another Tensor is added for each parameter Tensor (`momGradParams`).\nThis method should be called before [updateParameters](https://github.com/torch/nn/blob/master/doc/module.md#nn.Module.updateParameters)\nas it affects the gradients w.r.t. parameters.\n\nClassic momentum is computed as follows :\n\n```lua\nmomGradParams = momFactor*momGradParams + (1-momDamp)*gradParams\ngradParams = momGradParams\n```\n\nwhere `momDamp` has a default value of `momFactor`.\n\nNesterov momentum (`momNesterov = true`) is computed as follows (the first line is the same as classic momentum):\n\n```lua\nmomGradParams = momFactor*momGradParams + (1-momDamp)*gradParams\ngradParams = gradParams + momFactor*momGradParams\n```\nThe default is to use classic momentum (`momNesterov = false`).\n\n<a name='nn.Module.weightDecay'></a>\n### Module:weightDecay(wdFactor [, wdMinDim]) ###\n\nDecays the weight of the parameterized models. \nImplements an L2 norm loss on parameters with dimensions greater or equal to `wdMinDim` (default is 2).\nThe resulting gradients are stored into the corresponding gradients w.r.t. parameters.\nSuch that this method should be called before [updateParameters](https://github.com/torch/nn/blob/master/doc/module.md#nn.Module.updateParameters).\n\n<a name='nn.Module.gradParamClip'></a>\n### Module:gradParamClip(cutoffNorm [, moduleLocal]) ###\n\nImplements a contrainst on the norm of gradients w.r.t. parameters [(Pascanu et al. 2012)](http://arxiv.org/pdf/1211.5063.pdf).\nWhen `moduleLocal = false` (the default), the norm is calculated globally to Module for which this is called.\nSo if you call it on an MLP, the norm is computed on the concatenation of all parameter Tensors.\nWhen `moduleLocal = true`, the norm constraint is applied \nto the norm of all parameters in each component (non-container) module.\nThis method is useful to prevent the exploding gradient in \n[Recurrent neural networks](https://github.com/Element-Research/rnn/blob/master/README.md).\n\n<a name='nn.Module.reinforce'></a>\n### Module:reinforce(reward) ###\n\nThis method is used by Criterions that implement the REINFORCE algorithm like [VRClassReward](#nn.VRClassReward). \nWhile vanilla backpropagation (gradient descent using the chain rule), \nREINFORCE Criterions broadcast a `reward` to all REINFORCE modules between the `forward` and the `backward`.\nIn this way, when the following call to `backward` reaches the REINFORCE modules, \nthese will compute a `gradInput` using the broadcasted `reward`.\nThe `reward` is broadcast to all REINFORCE modules contained \nwithin `model` by calling `model:reinforce(reward)`. \nNote that the `reward` should be a 1D tensor of size `batchSize`, \ni.e. each example in a batch has its own scalar reward.\n\nRefer to [this example](https://github.com/Element-Research/rnn/blob/master/examples/recurrent-visual-attention.lua)\nfor a complete training script making use of the REINFORCE interface.\n \n<a name='nn.Decorator'></a>\n## Decorator ##\n\n```lua\ndmodule = nn.Decorator(module)\n```\n\nThis module is an abstract class used to decorate a `module`. This means \nthat method calls to `dmodule` will call the same method on the encapsulated \n`module`, and return its results.\n\n<a name='nn.DontCast'></a>\n## DontCast ##\n\n```lua\ndmodule = nn.DontCast(module)\n```\n\nThis module is a decorator. Use it to decorate a module that you don't\nwant to be cast when the `type()` method is called.\n\n```lua\nmodule = nn.DontCast(nn.Linear(3,4):float())\nmodule:double()\nth> print(module:forward(torch.FloatTensor{1,2,3}))\n 1.0927\n-1.9380\n-1.8158\n-0.0805\n[torch.FloatTensor of size 4]\n``` \n\n<a name='nn.Serial'></a>\n## Serial ##\n```lua\ndmodule = nn.Serial(module, [tensortype])\ndmodule:[light,medium,heavy]Serial()\n``` \nThis module is a decorator that can be used to control the serialization/deserialization \nbehavior of the encapsulated module. Basically, making the resulting string or \nfile heavy (the default), medium or light in terms of size. \n\nFurthermore, when specified, the `tensortype` attribute (e.g *torch.FloatTensor*, *torch.DoubleTensor* and so on.),\ndetermines what type the module will be cast to during serialization. \nNote that this will also be the type of the deserialized object.\nThe default serialization `tensortype` is `nil`, i.e. the module is serialized as is. \n\nThe `heavySerial()` has the serialization process serialize every attribute in the module graph, \nwhich is the default behavior of nn. \n\nThe `mediumSerial()` has the serialization process serialize \neverything except the attributes specified in each module's `dpnn_mediumEmpty`\ntable, which has a default value of `{'output', 'gradInput', 'momGradParams', 'dpnn_input'}`.\nDuring serialization, whether they be tables or Tensors, these attributes are emptied (no storage).\nSome modules overwrite the default `Module.dpnn_mediumEmpty` static attribute with their own.\n\nThe `lightSerial()` has the serialization process empty  \neverything a call to `mediumSerial(type)` would (so it uses `dpnn_mediumEmpty`).\nBut also empties all the parameter gradients specified by the \nattribute `dpnn_gradParameters`, which defaults to `{gradWeight, gradBias}`.\n\nWe recomment using `mediumSerial()` for training, and `lightSerial()` for \nproduction (feed-forward-only models).\n\n<a name='nn.NaN'></a>\n## NaN ##\n\n```lua\ndmodule = nn.NaN(module, [id])\n``` \n\nThe `NaN` module asserts that the `output` and `gradInput` of the decorated `module` do not contain NaNs.\nThis is useful for locating the source of those pesky NaN errors. \nThe `id` defaults to automatically incremented values of `1,2,3,...`.\n\nFor example :\n\n```lua\nlinear = nn.Linear(3,4)\nmlp = nn.Sequential()\nmlp:add(nn.NaN(nn.Identity()))\nmlp:add(nn.NaN(linear))\nmlp:add(nn.NaN(nn.Linear(4,2)))\nprint(mlp)\n``` \n\nAs you can see the `NaN` layers are have unique ids :\n\n```lua\nnn.Sequential {\n  [input -> (1) -> (2) -> (3) -> output]\n  (1): nn.NaN(1) @ nn.Identity\n  (2): nn.NaN(2) @ nn.Linear(3 -> 4)\n  (3): nn.NaN(3) @ nn.Linear(4 -> 2)\n}\n``` \n\nAnd if we fill the `bias` of the linear module with NaNs and call `forward`:\n\n```lua\nnan = math.log(math.log(0)) -- this is a nan value\nlinear.bias:fill(nan)\nmlp:forward(torch.randn(2,3))\n```  \n\nWe get a nice error message:\n```lua\n/usr/local/share/lua/5.1/dpnn/NaN.lua:39: NaN found in parameters of module :\nnn.NaN(2) @ nn.Linear(3 -> 4)\n``` \n\n<a name='nn.Inception'></a>\n## Inception ##\nReferences :\n \n  * A. [Going Deeper with Convolutions](http://arxiv.org/abs/1409.4842)\n  * B. [GoogleLeNet](http://image-net.org/challenges/LSVRC/2014/slides/GoogLeNet.pptx)\n\n```lua\nmodule = nn.Inception(config)\n```\n\nThis module uses `n`+2 parallel \"columns\". \nThe original paper uses 2+2 where the first two are (but there could be more than two):\n  \n  * 1x1 conv (reduce) -> relu -> 5x5 conv -> relu\n  * 1x1 conv (reduce) -> relu -> 3x3 conv -> relu \n\nand where the other two are : \n  \n  * 3x3 maxpool -> 1x1 conv (reduce/project) -> relu \n  * 1x1 conv (reduce) -> relu. \n\nThis module allows the first group of columns to be of any \nnumber while the last group consist of exactly two columns.\nThe 1x1 convoluations are used to reduce the number of input channels \n(or filters) such that the capacity of the network doesn't explode. \nWe refer to these here has *reduce*. \nSince each column seems to have one and only one reduce, their initial \nconfiguration options are specified in lists of n+2 elements.\n\nThe sole argument `config` is a table taking the following key-values :\n\n  * Required Arguments :\n   * `inputSize` : number of input channels or colors, e.g. 3;\n   * `outputSize` : numbers of filters in the non-1x1 convolution kernel sizes, e.g. `{32,48}`\n   * `reduceSize` : numbers of filters in the 1x1 convolutions (reduction) used in each column, e.g. `{48,64,32,32}`. The last 2 are used respectively for the max pooling (projection) column (the last column in the paper) and the column that has nothing but a 1x1 conv (the first column in the paper). This table should have two elements more than the outputSize\n  * Optional Arguments :\n   * `reduceStride` : strides of the 1x1 (reduction) convolutions. Defaults to `{1,1,...}`.\n   * `transfer` : transfer function like `nn.Tanh`,`nn.Sigmoid`, `nn.ReLU`, `nn.Identity`, etc. It is used after each reduction (1x1 convolution) and convolution. Defaults to `nn.ReLU`.\n   * `batchNorm` : set this to `true` to use batch normalization. Defaults to `false`. Note that batch normalization can be awesome\n   * `padding` : set this to `true` to add padding to the input of the convolutions such that output width and height are same as that of the original non-padded `input`. Defaults to `true`.\n   * `kernelSize` : size (`height = width`) of the non-1x1 convolution kernels. Defaults to `{5,3}`.\n   * `kernelStride` : stride of the kernels (`height = width`) of the convolution. Defaults to `{1,1}`\n   * `poolSize`: size (`height = width`) of the spatial max pooling used in the next-to-last column. Defaults to 3.\n   * `poolStride` : stride (`height = width`) of the spatial max pooling. Defaults to 1.\n   \n\nFor a complete example using this module, refer to the following :\n * [deep inception training script](https://github.com/nicholas-leonard/dp/blob/master/examples/deepinception.lua) ;\n * [openface facial recognition](https://github.com/cmusatyalab/openface) (the model definition is [here](https://github.com/cmusatyalab/openface/blob/master/models/openface/nn4.def.lua)).\n\n<a name='nn.Collapse'></a>\n## Collapse ##\n\n```lua\nmodule = nn.Collapse(nInputDim)\n```\n\nThis module is the equivalent of:\n```\nview = nn.View(-1)\nview:setNumInputDim(nInputDim)\n```\nIt collapses all non-batch dimensions. This is useful for converting \na spatial feature map to the single dimension required by a dense \nhidden layer like Linear.\n\n<a name='nn.Convert'></a>\n## Convert ##\n\n```lua\nmodule = nn.Convert([inputShape, outputShape])\n```\nModule to convert between different data formats.\nFor example, we can flatten images by using :\n```lua\nmodule = nn.Convert('bchw', 'bf')\n``` \nor equivalently\n```lua\nmodule = nn.Convert('chw', 'f')\n```\nLets try it with an input:\n```lua\nprint(module:forward(torch.randn(3,2,3,1)))\n 0.5692 -0.0190  0.5243  0.7530  0.4230  1.2483\n-0.9142  0.6013  0.5608 -1.0417 -1.4014  1.0177\n-1.5207 -0.1641 -0.4166  1.4810 -1.1725 -1.0037\n[torch.DoubleTensor of size 3x6]\n```\nYou could also try:\n\n```lua\nmodule = nn.Convert('chw', 'hwc')\ninput = torch.randn(1,2,3,2)\ninput:select(2,1):fill(1)\ninput:select(2,2):fill(2)\nprint(input)\n(1,1,.,.) = \n  1  1\n  1  1\n  1  1\n(1,2,.,.) = \n  2  2\n  2  2\n  2  2\n[torch.DoubleTensor of size 1x2x3x2]\nprint(module:forward(input))\n(1,1,.,.) = \n  1  2\n  1  2\n\n(1,2,.,.) = \n  1  2\n  1  2\n\n(1,3,.,.) = \n  1  2\n  1  2\n[torch.DoubleTensor of size 1x3x2x2]\n```\n\n\nFurthermore, it automatically converts the `input` to have the same type as `self.output`\n(i.e. the type of the module).\nSo you can also just use is for automatic input type converions:\n```lua\nmodule = nn.Convert()\nprint(module.output) -- type of module\n[torch.DoubleTensor with no dimension]\ninput = torch.FloatTensor{1,2,3}\nprint(module:forward(input))\n 1\n 2\n 3\n[torch.DoubleTensor of size 3]\n```\n\n<a name='nn.ZipTable'></a>\n## ZipTable ##\n\n```lua\nmodule = nn.ZipTable()\n```\n\nZips a table of tables into a table of tables.\n\nExample:\n```lua\nprint(module:forward{ {'a1','a2'}, {'b1','b2'}, {'c1','c2'} })\n{ {'a1','b1','c1'}, {'a2','b2','c2'} }\n```\n\n<a name='nn.ZipTableOneToMany'></a>\n## ZipTableOneToMany ##\n\n```lua\nmodule = nn.ZipTableOneToMany()\n```\n\nZips a table of element `el` and table of elements `tab` into a table of tables, where the i-th table contains the element `el` and the i-th element in table `tab`\n\nExample:\n```lua\nprint(module:forward{ 'el', {'a','b','c'} })\n{ {'el','a'}, {'el','b'}, {'el','c'} }\n```\n\n<a name='nn.CAddTensorTable'></a>\n## CAddTensorTable ##\n\n```lua\nmodule = nn.CAddTensorTable()\n```\n\nAdds the first element `el` of the input table `tab` to each tensor contained in the second element of `tab`, which is itself a table\n\nExample:\n```lua\nprint(module:forward{ (0,1,1), {(0,0,0),(1,1,1)} })\n{ (0,1,1), (1,2,2) }\n```\n\n\n<a name='nn.ReverseTable'></a>\n## ReverseTable ##\n\n```lua\nmodule = nn.ReverseTable()\n```\n\nReverses the order of elements in a table.\n\nExample:\n\n```lua\nprint(module:forward{1,2,3,4})\n{4,3,2,1}\n```\n\n<a name='nn.PrintSize'></a>\n## PrintSize ##\n\n```lua\nmodule = nn.PrintSize(name)\n```\n\nThis module is useful for debugging complicated module composites. \nIt prints the size of the `input` and `gradOutput` during `forward`\nand `backward` propagation respectively.\nThe `name` is a string used to identify the module along side the printed size.\n\n<a name='nn.Clip'></a>\n## Clip ##\n\n```lua\nmodule = nn.Clip(minval, maxval)\n```\n\nThis module clips `input` values such that the output is between `minval` and `maxval`.\n\n<a name='nn.Constant'></a>\n## Constant ##\n\n```lua\nmodule = nn.Constant(value, nInputDim)\n```\n\nThis module outputs a constant value given an input.\nIf `nInputDim` is specified, it uses the input to determine the size of the batch. \nThe `value` is then replicated over the batch. \nOtherwise, the `value` Tensor is output as is.\nDuring `backward`, the returned `gradInput` is a zero Tensor of the same size as the `input`.\nThis module has no trainable parameters. \n\nYou can use this with nn.ConcatTable() to append constant inputs to an input : \n\n```lua\nnn.ConcatTable():add(nn.Constant(v)):add(nn.Identity())\n```\n\nThis is useful when you want to output a value that is independent of the \ninput to the neural network (see [this example](https://github.com/Element-Research/rnn/blob/master/examples/recurrent-visual-attention.lua)).\n\n<a name='nn.SpatialUniformCrop'></a>\n## SpatialUniformCrop ##\n\n```lua\nmodule = nn.SpatialUniformCrop(oheight, owidth)\n```\n\nDuring training, this module will output a cropped patch of size `oheight, owidth`\nwithin the boundaries of the `input` image.\nFor each example, a location is sampled from a uniform distribution \nsuch that each possible patch has an equal probability of being sampled.\n\nDuring evaluation, the center patch is cropped and output.\n\nThis module is commonly used at the input layer to artificially \naugment the size of the dataset to prevent overfitting.\n\n<a name='nn.SpatialGlimpse'></a>\n## SpatialGlimpse ##\nRef. A. [Recurrent Model for Visual Attention](http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf)\n\n```lua\nmodule = nn.SpatialGlimpse(size, depth, scale)\n```\n\nA glimpse is the concatenation of down-scaled cropped images of \nincreasing scale around a given location in a given image.\nThe input is a pair of Tensors: `{image, location}`\n`location` are `(y,x)` coordinates of the center of the different scales \nof patches to be cropped from image `image`. \nCoordinates are between `(-1,-1)` (top-left) and `(1,1)` (bottom-right).\nThe `output` is a batch of glimpses taken in image at location `(y,x)`.\n\n`size` can be either a scalar which specifies the `width = height` of glimpses, \nor a table of `{height, width}` to support a rectangular shape of glimpses.\n`depth` is number of patches to crop per glimpse (one patch per depth).\n`scale` determines the `size(t) = scale * size(t-1)` of successive cropped patches.\n\nSo basically, this module can be used to focus the attention of the model \non a region of the input `image`. \nIt is commonly used with the [RecurrentAttention](https://github.com/Element-Research/rnn#rnn.RecurrentAttention) \nmodule (see [this example](https://github.com/Element-Research/rnn/blob/master/examples/recurrent-visual-attention.lua)).\n\n<a name='nn.WhiteNoise'></a>\n## WhiteNoise ##\n\n```lua\nmodule = nn.WhiteNoise([mean, stdev])\n```\n\nUseful in training [Denoising Autoencoders] (http://arxiv.org/pdf/1507.02672v1.pdf). \nTakes `mean` and `stdev` of the normal distribution as input. \nDefault values for mean and standard deviation are 0 and 0.1 respectively. \nWith `module:training()`, noise is added during forward. \nDuring `backward` gradients are passed as it is. \nWith `module:evaluate()` the mean is added to the input.\n\n<a name='nn.SpatialRegionDropout'></a>\n## SpatialRegionDropout ##\n\n```lua\nmodule = nn.SpatialRegionDropout(p)\n```\nFollowing is an example of `SpatialRegionDropout` outputs on the famous lena image.\n\n**Input**\n\n![Lena](tutorials/lena.jpg)\n\n**Outputs**\n\n![Lena](tutorials/srd1.jpg)           ![Lena](tutorials/srd2.jpg)\n\n<a name='nn.FireModule'></a>\n## FireModule ##\nRef: http://arxiv.org/pdf/1602.07360v1.pdf\n```lua\nmodule = nn.FireModule(nInputPlane, s1x1, e1x1, e3x3, activation)\n```\nFireModule is comprised of two submodules 1) A *squeeze* convolution module comprised of `1x1` filters followed by 2) an *expand* module that is comprised of a mix of `1x1` and `3x3` convolution filters.\nArguments: `s1x1`: number of `1x1` filters in the squeeze submodule, `e1x1`: number of `1x1` filters in the expand submodule, `e3x3`: number of `3x3` filters in the expand submodule. It is recommended that `s1x1` be less than `(e1x1+e3x3)` if you want to limit the number of input channels to the `3x3` filters in the expand submodule.\nFireModule works only with batches, for single sample convert the sample to a batch of size 1.\n\n<a name='nn.SpatialFeatNormalization'></a>\n## SpatialFeatNormalization ##\n```lua\nmodule = nn.SpatialFeatNormalization(mean, std)\n```\nThis module normalizies each feature channel of input image based on its corresponding mean and standard deviation scalar values. This module does not learn the `mean` and `std`, they are provided as arguments.\n\n<a name='nn.SpatialBinaryConvolution'></a>\n## SpatialBinaryConvolution ##\n\n```lua\nmodule = nn.SpatialBinaryConvolution(nInputPlane, nOutputPlane, kW, kH)\n```\nInitialization is sample as nn/SpatialConvolution. Binary weights are used for forward and backward. Floating point weights are used for weight updates. Check **Binary-Weight-Network** section of [XNOR-net](http://arxiv.org/pdf/1603.05279v2.pdf).\n\n\n<a name = 'nn.OneHot'></a>\n## OneHot ##\n\n```lua\nmodule = nn.OneHot(outputSize)\n```\n\nTransforms a tensor of `input` indices having integer values between 1 and `outputSize` into\na tensor of one-hot vectors of size `outputSize`. \n\nForward an index to get a one-hot vector :\n\n```lua\n> module = nn.OneHot(5) -- 5 classes\n> module:forward(torch.LongTensor{3})\n 0  0  1  0  0\n[torch.DoubleTensor of size 1x5]\n``` \n\nForward a batch of 3 indices. Notice that these need not be stored as `torch.LongTensor` :\n\n```lua\n> module:forward(torch.Tensor{3,2,1})\n 0  0  1  0  0\n 0  1  0  0  0\n 1  0  0  0  0\n[torch.DoubleTensor of size 3x5]\n``` \n\nForward batch of `2 x 3` indices :\n\n```lua\noh:forward(torch.Tensor{{3,2,1},{1,2,3}})\n(1,.,.) = \n  0  0  1  0  0\n  0  1  0  0  0\n  1  0  0  0  0\n\n(2,.,.) = \n  1  0  0  0  0\n  0  1  0  0  0\n  0  0  1  0  0\n[torch.DoubleTensor of size 2x3x5]\n``` \n\n<a name='nn.Kmeans'></a>\n## Kmeans ##\n\n```lua\nkm = nn.Kmeans(k, dim)\n```\n\n`k` is the number of centroids and `dim` is the dimensionality of samples.\nYou can either initialize centroids randomly from input samples or by using *kmeans++* algorithm.\n\n```lua\nkm:initRandom(samples) -- Randomly initialize centroids from input samples.\nkm:initKmeansPlus(samples) -- Use Kmeans++ to initialize centroids.\n``` \n\nExample showing how to use Kmeans module to do standard Kmeans clustering.\n\n```lua\nattempts = 10\niter = 100 -- Number of iterations\nbestKm = nil\nbestLoss = math.huge\nlearningRate = 1\nfor j=1, attempts do\n   local km = nn.Kmeans(k, dim)\n   km:initKmeansPlus(samples)\n   for i=1, iter do\n      km:zeroGradParameters()\n      km:forward(samples) -- sets km.loss\n      km:backward(samples, gradOutput) -- gradOutput is ignored\n\n      -- Gradient Descent weight/centroids update\n      km:updateParameters(learningRate)\n   end\n   \n   if km.loss < bestLoss then\n      bestLoss = km.loss\n      bestKm = km:clone()\n   end\nend\n``` \n`nn.Kmeans()` module maintains loss only for the latest forward. If you want to maintain loss over the whole dataset then you who would need do it my adding the module loss for every forward.\n\nYou can also use `nn.Kmeans()` as an auxillary layer in your network. \nA call to `forward` will generate an `output` containing the index of the nearest cluster for each sample in the batch.\nThe `gradInput` generated by `updateGradInput` will be zero. \n\n<a name='nn.ModuleCriterion'></a>\n## ModuleCriterion ##\n\n```lua\ncriterion = nn.ModuleCriterion(criterion [, inputModule, targetModule, castTarget])\n``` \n\nThis criterion decorates a `criterion` by allowing the `input` and `target` to be \nfed through an optional `inputModule` and `targetModule` before being passed to the \n`criterion`. The `inputModule` must not contain parameters as these would not be updated. \n\nWhen `castTarget = true` (the default), the `targetModule` is cast along with the `inputModule` and \n`criterion`. Otherwise, the `targetModule` isn't.  \n\n<a name='nn.Reinforce'></a>\n## Reinforce ##\nRef A. [Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning](http://incompleteideas.net/sutton/williams-92.pdf)\n\nAbstract class for modules that implement the REINFORCE algorithm (ref. A).\n\n```lua\nmodule = nn.Reinforce([stochastic])\n```\n\nThe `reinforce(reward)` method is called by a special Reward Criterion (e.g. [VRClassReward](#nn.VRClassReward)).\nAfter which, when backward is called, the reward will be used to generate gradInputs. \nWhen `stochastic=true`, the module is stochastic (i.e. samples from a distribution) \nduring evaluation and training.\nWhen `stochastic=false` (the default), the module is only stochastic during training.\n\nThe REINFORCE rule for a module can be summarized as follows :\n```lua\n            d ln(f(output,input))\ngradInput = ---------------------  * reward\n                  d input\n```\nwhere the `reward` is what is provided by a Reward criterion like \n[VRClassReward](#nn.VRClassReward) via the [reinforce](#nn.Module.reinforce) method.\nThe criterion will normally be responsible for the following formula :\n```lua\nreward = a*(R - b)\n```\nwhere `a` is the alpha of the original paper, i.e. a reward scale,\n`R` is the raw reward (usually 0 or 1), and `b` is the baseline reward, \nwhich is often taken to be the expected raw reward `R`.\n\nThe `output` is usually sampled from a probability distribution `f()`\nparameterized by the `input`. \nSee [ReinforceBernoulli](#nn.ReinforceBernoulli) for a concrete derivation.\n\nAlso, as you can see, the gradOutput is ignored. So within a backpropagation graph,\nthe `Reinforce` modules will replace the backpropagated gradients (`gradOutput`) \nwith their own obtained from the broadcasted `reward`.\n\n<a name='nn.ReinforceBernoulli'></a>\n## ReinforceBernoulli ##\nRef A. [Simple Statistical Gradient-Following Algorithms for\nConnectionist Reinforcement Learning](http://incompleteideas.net/sutton/williams-92.pdf)\n\n```lua\nmodule = nn.ReinforceBernoulli([stochastic])\n```\n\nA [Reinforce](#nn.Reinforce) subclass that implements the REINFORCE algorithm \n(ref. A p.230-236) for the Bernoulli probability distribution.\nInputs are bernoulli probabilities `p`. \nDuring training, outputs are samples drawn from this distribution.\nDuring evaluation, when `stochastic=false`, outputs are the same as the inputs.\nUses the REINFORCE algorithm (ref. A p.230-236) which is \nimplemented through the [reinforce](#nn.Module.reinforce) interface (`gradOutputs` are ignored).\n\nGiven the following variables : \n\n * `f` : bernoulli probability mass function\n * `x` : the sampled values (0 or 1) (i.e. `self.output`)\n * `p` : probability of sampling a 1\n\nthe derivative of the log bernoulli w.r.t. probability `p` is :\n```\nd ln(f(output,input))   d ln(f(x,p))    (x - p)\n--------------------- = ------------ = ---------\n      d input               d p         p(1 - p)\n```\n\n<a name='nn.ReinforceNormal'></a>\n## ReinforceNormal ##\nRef A. [Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning](http://incompleteideas.net/sutton/williams-92.pdf)\n\n```lua\nmodule = nn.ReinforceNormal(stdev, [stochastic])\n```\n\nA [Reinforce](#nn.Reinforce) subclass that implements the REINFORCE algorithm \n(ref. A p.238-239) for a Normal (i.e. Gaussian) probability distribution.\nInputs are the means of the normal distribution.\nThe `stdev` argument specifies the standard deviation of the distribution. \nDuring training, outputs are samples drawn from this distribution.\nDuring evaluation, when `stochastic=false`, outputs are the same as the inputs, i.e. the means.\nUses the REINFORCE algorithm (ref. A p.238-239) which is \nimplemented through the [reinforce](#nn.Module.reinforce) interface (`gradOutputs` are ignored).\n\nGiven the following variables : \n   \n  * `f` : normal probability density function\n  * `x` : the sampled values (i.e. `self.output`)\n  * `u` : mean (`input`)\n  * `s` : standard deviation (`self.stdev`)\n\nthe derivative of log normal w.r.t. mean `u` is :\n```\nd ln(f(x,u,s))   (x - u)\n-------------- = -------\n     d u           s^2\n```\n\nAs an example, it is used to sample locations for the [RecurrentAttention](https://github.com/Element-Research/rnn#rnn.RecurrentAttention) \nmodule (see [this example](https://github.com/Element-Research/rnn/blob/master/examples/recurrent-visual-attention.lua)).\n\n<a name='nn.ReinforceGamma'></a>\n## ReinforceGamma ##\nRef A. [Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning](http://incompleteideas.net/sutton/williams-92.pdf)\n\n```lua\nmodule = nn.ReinforceGamma(scale, [stochastic])\n``` \n\nA [Reinforce](#nn.Reinforce) subclass that implements the REINFORCE algorithm \n(ref. A) for a [Gamma probability distribution](https://en.wikipedia.org/wiki/Gamma_distribution) \nparametrized by shape (k) and scale (theta) variables.\nInputs are the shapes of the gamma distribution.\nDuring training, outputs are samples drawn from this distribution.\nDuring evaluation, when `stochastic=false`, outputs are equal to the mean, defined as the product of\nshape and scale ie. `k*theta`.\nUses the REINFORCE algorithm (ref. A) which is \nimplemented through the [reinforce](#nn.Module.reinforce) interface (`gradOutputs` are ignored).\n\nGiven the following variables : \n   \n  * `f` : gamma probability density function\n  * `g` : digamma function\n  * `x` : the sampled values (i.e. `self.output`)\n  * `k` : shape (`input`)\n  * `t` : scale\n\nthe derivative of log gamma w.r.t. shape `k` is :\n```\nd ln(f(x,k,t))\n-------------- = ln(x) - g(k) - ln(t)\n      d k\n``` \n\n<a name='nn.ReinforceCategorical'></a>\n## ReinforceCategorical ##\nRef A. [Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning](http://incompleteideas.net/sutton/williams-92.pdf)\n\n```lua\nmodule = nn.ReinforceCategorical([stochastic])\n```\n\nA [Reinforce](#nn.Reinforce) subclass that implements the REINFORCE algorithm \n(ref. A) for a Categorical (i.e. Multinomial with one sample) probability distribution.\nInputs are the categorical probabilities of the distribution : `p[1], p[2], ..., p[k]`.\nThese are usually the output of a SoftMax.  \nFor `n` categories, both the `input` and `output` ares of size `batchSize x n`.\nDuring training, outputs are samples drawn from this distribution.\nThe outputs are returned in one-hot encoding i.e. \nthe output for each example has exactly one category having a 1, while the remainder are zero.\nDuring evaluation, when `stochastic=false`, outputs are the same as the inputs, i.e. the probabilities `p`.\nUses the REINFORCE algorithm (ref. A) which is \nimplemented through the [reinforce](#nn.Module.reinforce) interface (`gradOutputs` are ignored).\n\n\nGiven the following variables : \n\n  * `f` : categorical probability mass function\n  * `x` : the sampled indices (one per sample) (`self.output` is the one-hot encoding of these indices)\n  * `p` : probability vector (`p[1], p[2], ..., p[k]`) (`input`)\n\nthe derivative of log categorical w.r.t. probability vector `p` is :\n```\nd ln(f(x,p))     1/p[i]    if i = x  \n------------ =   \n    d p          0         otherwise\n```\n\n<a name='nn.VRClassReward'></a>\n## VRClassReward ##\nRef A. [Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning](http://incompleteideas.net/sutton/williams-92.pdf)\n\nThis Reward criterion implements the REINFORCE algoritm (ref. A) for classification models.\nSpecifically, it is a Variance Reduces (VR) classification reinforcement leanring (reward-based) criterion.\n\n```lua\nvcr = nn.VRClassReward(module [, scale, criterion])\n```\n \nWhile it conforms to the Criterion interface (which it inherits), \nit does not backpropagate gradients (except for the baseline `b`; see below).\nInstead, a `reward` is broadcast to the `module` via the [reinforce](#nn.Module.reinforce) method.\n\nThe criterion implements the following formula :\n```lua\nreward = a*(R - b)\n```\nwhere `a` is the alpha described in Ref. A, i.e. a reward `scale` (defaults to 1),\n`R` is the raw reward (0 for incorrect and 1 for correct classification), \nand `b` is the baseline reward, which is often taken to be the expected raw reward `R`.\n\nThe `target` of the criterion is a tensor of class indices.\nThe `input` to the criterion is a table `{y,b}` where `y` is the probability \n(or log-probability) of classes (usually the output of a SoftMax), \nand `b` is the baseline reward discussed above. \n\nFor each example, if `argmax(y)` is equal to the `target` class, the raw reward `R = 1`, otherwize `R = 0`.\n\nAs for `b`, its `gradInputs` are obtained from the `criterion`, which defaults to `MSECriterion`.\nThe `criterion`'s target is the commensurate raw reward `R`.\nUsing `a*(R-b)` instead of `a*R` to obtain a `reward` is what makes this class variance reduced (VR).\nBy reducing the variance, the training can converge faster (Ref. A).\nThe predicted `b` can be nothing more than the expectation `E(R)`.\n\nNote : for RNNs with R = 1 for last step in sequence, encapsulate it\nin `nn.ModuleCriterion(VRClassReward, nn.SelectTable(-1))`.\n\nFor an example, this criterion is used along with the [RecurrentAttention](https://github.com/Element-Research/rnn#rnn.RecurrentAttention) \nmodule to [train a recurrent model for visual attention](https://github.com/Element-Research/rnn/blob/master/examples/recurrent-visual-attention.lua).\n\n<a name='nn.BLR'></a>\n## BinaryLogisticRegression ##\nRef A. [Learning to Segment Object Candidates](http://arxiv.org/pdf/1506.06204v2.pdf)\nThis criterion implements the score criterion mentioned in (ref. A).\n\n```lua\ncriterion = nn.BinaryLogisticRegression()\n```\n\nBinaryLogisticRegression implements following cost function for binary classification.\n```\n\n log( 1 + exp( -y_k * score(x_k) ) )\n\n```\nwhere ```y_k``` is binary target ```score(x_k)``` is the corresponding prediction. ```y_k``` has value ```{-1, +1}``` and ```score(x_k)``` has value in ```[-1, +1]```.\n\n<a name='nn.SpatialBLR'></a>\n## SpatialBinaryLogisticRegression ##\nRef A. [Learning to Segment Object Candidates](http://arxiv.org/pdf/1506.06204v2.pdf)\n\nThis criterion implements the spatial component of the criterion mentioned in  (ref. A).\n\n```lua\ncriterion = nn.SpatialBinaryLogisticRegression()\n```\n\nSpatialBinaryLogisticRegression implements following cost function for binary pixel classification.\n```\n   1\n_______ sum_ij [ log( 1 + exp( -m_ij * f_ij ) ) ]\n 2*w*h\n```\nwhere ```m_ij``` is target binary image and ```f_ij``` is the corresponding prediction. ```m_ij``` has value ```{-1, +1}``` and ```f_ij``` has value in ```[-1, +1]```.\n", 
  "id": 33490713
}