{
  "read_at": 1462552000, 
  "description": "Computational statistics on the (key)word choices of code", 
  "README.md": "# Code Linguistics\n\nThe statistical frequency of words in a natural language follows a power-law distribution, known as [Zipfs Law](http://en.wikipedia.org/wiki/Zipf%27s_law#Related_laws).\nMore specifically, there are three distinct regimes that fit well to power-laws with exponents of `0.5, 1.0, 2.0`.\n\nThe goal of this project is to determine if the following observations holds for code.\nThe dataset will be a large sampling of code hosted on github.\nThe hypothesis is that the keywords (when restricted to a single language) will give an exponent of 0.5, variables will fit to 1.0 exponent and the comments and everything else will fit to a greater exponent (possibly not 2.0).\n\n#### Author: [Travis Hoppe](http://thoppe.github.io/)\n\n## Roadmap:\n\n+ Download a large sampling of code from github. [Completed](gitpull/).\n+ Process, filter and tokenize the dataset. [Completed](process_code/).\n+ Fit the proper power laws values to the data. [In-progress](fit_tokens/)\n+ Determine a list of keywords for all languages we are interested in.\n+ Plot results and interpret.\n+ Draft submission for arXiv.\n\n## Presentations\n\n+ ***[DC Hack and Tell Round 18: Ternary Bits](http://www.meetup.com/DC-Hack-and-Tell/events/220231708/)***, March 17, 2015, [presentation link](http://thoppe.github.io/code-linguistics/HnC_presentation.html).\n\n", 
  "id": 30661071
}