{
  "read_at": 1462557857, 
  "description": "", 
  "README.md": "# Translation by Recurrent Neural Network  \nTranslation model with multi-layered LSTMs in [Chainer](http://chainer.org/) based on the following papers  \n* [arXiv:1409.3215](http://arxiv.org/abs/1409.3215): Sequence to Sequence [Ilya Sutskever+ 2014]   \n* [arXiv:1406.1078](http://arxiv.org/abs/1406.1078): Encoder to Decoder [Kyunghyun Cho+ 2014]  \n\n## Translation Examples  \n* Word to word translation (no word order change) seems to work well.  \n\n| FR (input) | Qui est le president des Etats-Unis ? |\n|:---|---:|\n| 1L (ours) | who is the president of the united states ? <EOS> |\n| 2L (ours) | who is the president of the united states ? <EOS> |  \n\n* More accurate translation was achieved in the 2-layered model.  \n\n| FR (input) | Qui peut m'aider en anglais ? |\n|:---|---:|\n| 1L (ours) | who can help english ? <EOS> |\n| 2L (ours) | who can help me in english ? <EOS> |  \n\n* The auxiliary verb \"do\" in questions was not achieved in both models.  \n\n| FR (input) | Avez-vous des questions ? |\n|:---|---:|\n| 1L (ours) | have you questions ? <EOS> |\n| 2L (ours) | are you talking about the questions ? <EOS> |  \n| EN (expected) | do you have any questions ? <EOS> |  \n\n* Auxiliary verbs must be located before the subject in questions but are not located.  \n\n| FR (input) | Je peux vous poser une question ? |\n|:---|---:|\n| 1L (ours) | i can ask you to answer ? <EOS> |\n| 2L (ours) | i can ask you a question ? <EOS> |  \n| EN (expected) | can i ask you a question ? <EOS> |  \n\n## Translation at each learning stage  \n* The number of processed sentences and the results   \n\n| Training Iterations | Qui peut m'aider en anglais ? |\n|:---|---:|\n| 100K | who are <UNK> ? <EOS> |\n| 200K | who is going to be <UNK> ? <EOS> |  \n| 300K | who can do so ? <EOS> |\n| 400K | who can do so ? <EOS> |  \n| 500K | who can do that in the <UNK> ? <EOS> |\n| 600K | who can help the <UNK> ? <EOS> |  \n| 700K | who can help me in the english ? <EOS> |\n| 800K | who can help me ? <EOS> |  \n| 900K | who can help me in english ? <EOS> |  \n\n## Dependencies\nPython 3.5, Chainer, Jupyter  \n\n## Code \n* data preprocessing  \n[nbviewer.jupyter.org/github/masaki-y/translation-by-rnn/blob/master/data_util.ipynb](http://nbviewer.jupyter.org/github/masaki-y/translation-by-rnn/blob/master/data_util.ipynb)  \n* model training  \n[nbviewer.jupyter.org/github/masaki-y/translation-by-rnn/blob/master/train.ipynb](http://nbviewer.jupyter.org/github/masaki-y/translation-by-rnn/blob/master/train.ipynb)   \n* model test  \n[nbviewer.jupyter.org/github/masaki-y/translation-by-rnn/blob/master/test.ipynb](http://nbviewer.jupyter.org/github/masaki-y/translation-by-rnn/blob/master/test.ipynb)  \n\n## Usage\nYou can translate any sentence by the trained model from following URL.  \n* 1-layered model, 88MB: [fr2en.chainermodel](https://drive.google.com/file/d/0B4wYkVkE3gL5QlVubllaTHU2eU0/view?usp=sharing)  \n* 2-layered model, 103MB: [fr2en-2L.chainermodel](https://drive.google.com/file/d/0B4wYkVkE3gL5dWljWV83ZFlZLVk/view?usp=sharing)  \n\nIf you're going to train a model, you need to use GPU for accelerating computations.  \nWe trained the 2-layered model for about 5 days with a GPU NVIDIA Titan X.  \n(2M sentences, 512 LSTMs in each layer)  \n\nThen set the GPU ID to \"`gpuid`\" in the source code.\nChainer will switch automatically to GPU mode.\n```py\ngpuid = -1 # gpu device ID (cpu if negative)\nxp = cuda.cupy if gpu >= 0 else np  \n```\n", 
  "id": 52245696
}