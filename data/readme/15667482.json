{
  "read_at": 1462546927, 
  "description": "Repository of my thesis \"Understanding Random Forests\"", 
  "README.md": "Understanding Random Forests\n============================\n\nPhD dissertation, Gilles Louppe, July 2014. Defended on October 9, 2014. \n\n_arXiv:_ http://arxiv.org/abs/1407.7502\n\n_Mirrors:_ \n- http://hdl.handle.net/2268/170309\n- http://www.montefiore.ulg.ac.be/~glouppe/pdf/phd-thesis.pdf\n\n_License:_ BSD 3 clause\n\n_Contact:_ Gilles Louppe (@glouppe, <g.louppe@gmail.com>)\n\nPlease cite using the following BibTex entry:\n\n```\n@phdthesis{louppe2014understanding,\n  title={Understanding Random Forests: From Theory to Practice},\n  author={Louppe, Gilles},\n  school={University of Liege, Belgium},\n  year=2014,\n  month=10,\n  note={arXiv:1407.7502}\n}\n```\n\n---\n\nData analysis and machine learning have become an integrative part of the\nmodern scientific methodology, offering automated procedures for the prediction\nof a phenomenon based on past observations, unraveling underlying patterns in\ndata and providing insights about the problem. Yet, caution should\navoid using machine learning as a black-box tool, but rather consider it as a\nmethodology, with a rational thought process that is entirely dependent on the\nproblem under study. In particular, the use of algorithms\nshould ideally require a reasonable understanding of their\nmechanisms, properties and limitations, in order to better apprehend and\ninterpret their results.\n\nAccordingly, the goal of this thesis is to provide an in-depth\nanalysis of random forests, consistently calling into\nquestion each and every part of the algorithm, in order to shed new light on\nits learning capabilities, inner workings and interpretability. The first\npart of this work studies the induction of decision trees and the construction of\nensembles of randomized trees, motivating their design and purpose whenever\npossible. Our contributions follow with an original complexity\nanalysis of random forests, showing their good computational performance\nand scalability, along with an in-depth discussion of their\nimplementation details, as contributed within Scikit-Learn.\n\nIn the second part of this work, we analyze and discuss the interpretability of\nrandom forests in the eyes of variable importance measures. The core of our\ncontributions rests in the theoretical characterization of the Mean Decrease of\nImpurity variable importance measure, from which we prove and derive some of\nits properties in the case of multiway totally randomized trees and in\nasymptotic conditions. In consequence of this work, our analysis  demonstrates\nthat variable importances as computed from non-totally randomized trees (e.g.,\nstandard Random Forest) suffer from a combination of defects, due to masking\neffects, misestimations of node impurity or due to the binary structure of\ndecision trees.\n\nFinally, the last part of this dissertation addresses limitations of random\nforests in the context of large datasets. Through extensive experiments, we\nshow that subsampling both samples and features simultaneously provides on par\nperformance while lowering at the same time the memory requirements. Overall\nthis paradigm highlights an intriguing practical fact: there is often no need\nto build single models over immensely large datasets. Good performance can\noften be achieved by building models on (very) small random parts of the data\nand then combining them all in an ensemble, thereby avoiding all practical\nburdens of making large data fit into memory.\n", 
  "id": 15667482
}