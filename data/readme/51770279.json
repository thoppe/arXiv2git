{
  "read_at": 1462557800, 
  "description": "Learn deep local descriptors from weakly-labeled data", 
  "README.md": "# Learning Local Descriptors from Weakly-Labeled Data\n\nCurrent best local descriptors are learned on a large dataset of matching and non-matching keypoint pairs.\nHowever, data of this kind is not always available since detailed keypoint correspondences can be hard to establish (e.g., for non-image data).\nOn the other hand, we can often obtain labels for pairs of keypoint bags.\nFor example, keypoint bags extracted from two images of the same object under different views form a matching pair, and keypoint bags extracted from images of different objects form a non-matching pair.\nOn average, matching pairs should contain more corresponding keypoints than non-matching pairs.\nWe propose to learn local descriptors from such information where local correspondences are not known in advance.\n\n<center><img src=\"teaser.png\" alt=\"Teaser\" style=\"width: 512px;\"/></center>\n\nEach image in the dataset (first row) is processed with a keypoint detector (second row) and transformed into a bag of visual words (third row).\nSome bags form matching pairs (green arrow) and some form non-matching pairs (red arrows).\nOn average, matching pairs should contain more corresponding local visual words than non-matching pairs.\nWe propose to *learn local descriptors* by optimizing the mentioned local correspondence criterion on a given dataset.\nNote that prior work assumes local correspondences are known in advance.\n\nThe details of the method can be found in our technical report available on [arXiv](http://arxiv.org/abs/1603.09095).\nIf you use our results and/or ideas, please cite the report as (BibTeX)\n\n```\n@misc\n{\n\twlrn,\n\tauthor = {Nenad Marku\\v{s} and Igor S. Pand\\v{z}i\\'c and J\\\"{o}rgen Ahlberg},\n\ttitle = {{Learning Local Descriptors by Optimizing the Keypoint-Correspondence Criterion}},\n\tyear = {2016},\n\teprint = {arXiv:1603.09095}\n}\n```\n\n## Some results (to be updated soon)\n\nA network trained with our method (code in this repo) can be obtained from the folder `models/`.\nThis network extracts `64f` descriptors of unit length from local patches of size `32x32`.\nHere is its structure:\n\n```\nnn.Sequential {\n  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> output]\n  (1): nn.MulConstant\n  (2): nn.View\n  (3): nn.Sequential {\n    [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> output]\n    (1): nn.SpatialConvolution(3 -> 32, 3x3)\n    (2): nn.ReLU\n    (3): nn.SpatialConvolution(32 -> 64, 4x4, 2,2)\n    (4): nn.ReLU\n    (5): nn.SpatialConvolution(64 -> 128, 3x3)\n    (6): nn.SpatialMaxPooling(2,2,2,2)\n    (7): nn.SpatialConvolution(128 -> 32, 1x1)\n    (8): nn.SpatialConvolution(32 -> 64, 6x6)\n  }\n  (4): nn.View\n  (5): nn.Normalize(2)\n}\n```\n\nThe structure is specified in `models/3x32x32_to_64.lua`.\nThe net parameters are stored as a vector of floats at `models/3x32x32_to_64.params`.\nThis is to reduce the storage requirements (i.e., the repo size).\nUse the following code to deploy and use the net.\n\n```lua\n-- load the network parameters first\nparams = torch.load('models/3x32x32_to_64.params')\n\n-- create the network and initialize its weights with loaded data\nn = dofile('models/3x32x32_to_64.lua')(params):float()\n\n-- generate a random batch of five 32x32 patches (each pixel is a float from [0, 255])\np = torch.rand(5, 3, 32, 32):float():mul(255)\n\n-- propagate the batch through the net to obtain descriptors\n-- (note that no patch prepocessing is required (such as mean substraction))\nd = n:forward(p)\n\n-- an appropriate similarity between descriptors is, for example, a dot product ...\nprint(d[1]*d[2])\n\n-- ... or you can use the Euclidean distance\nprint(torch.norm(d[1] - d[2]))\n```\n\nNotice that although it was trained on `32x32` patches, the model can be applied in a fully-convolutional manner to images of any size (the third module of the architecture contains only convolutions, ReLUs and pooling operations).\n\n## How to repeat the training\n\nFollow these steps.\n\n#### 1. Prepare bags of keypoints\n\nDownload <http://46.101.250.137/data/ukb.tar> and extract the archive.\nIt contains two folders with JPG images: `ukb-trn/` and `ukb-val/`.\nImages from the first folder will be used for training and images from the second one for checking the validation error.\n\nMove to the folder `utils/` and compile `fast.cpp` and `extp.cpp` with the provided `makefile`.\nThese are the keypoint detection and patch extraction programs.\nUse the script `batch_extract.sh` to transform the downloaded images into bags of keypoints:\n```bash\nbash batch_extract.sh ukb-trn/ ukb-trn-bags/ 128 32\nbash batch_extract.sh ukb-val/ ukb-val-bags/ 128 32\n```\n\nExtracted patches should now be in `ukb-trn-bags/` and `ukb-val-bags/`.\nAs these are stored in the JPG format, you can inspect them with your favorite image viewer.\n\n### 2. Prepare data-loading scripts\n\nTo keep a desirable level of abstraction and enable large-scale learning, this code requires the user to provide his/her routines for generating triplets.\nAn example can be found at `utils/tripletgen.lua`.\nThe strings \"--FOLDER--\", \"--NCHANNELS--\" and \"--PROBABILITY--\" need to be replaced with appropriate ones, depending whether loading training or validation data.\nThe following shell commands will do this for you (replace each slash in the folder paths with backslash+slash as required by `sed`).\n```bash\ncp utils/tripletgen.lua trn-tripletgen.lua\nsed -i -e 's/--FOLDER--/\"ukb-trn-bags\"/g' trn-tripletgen.lua\nsed -i -e 's/--NCHANNELS--/3/g' trn-tripletgen.lua\nsed -i -e 's/--PROBABILITY--/0.33/g' trn-tripletgen.lua\n\ncp utils/tripletgen.lua val-tripletgen.lua\nsed -i -e 's/--FOLDER--/\"ukb-val-bags\"/g' val-tripletgen.lua\nsed -i -e 's/--NCHANNELS--/3/g' val-tripletgen.lua\nsed -i -e 's/--PROBABILITY--/1.0/g' val-tripletgen.lua\n```\n\nAfter executing them, you should find two Lua files, `trn-tripletgen.lua` and `val-tripletgen.lua`, next to `wlrn.lua`.\n\n#### 3. Specify the descriptor-extractor structure\n\nThe model is specified with a Lua script which returns a function for constructing the descriptor extraction network.\nSee the default model in `models/3x32x32_to_64.lua` for an example.\n\nOf course, you can try different architectures.\nHowever, to learn their parameters, some parts of `wlrn.lua` might need additional tweaking (such as learning rates).\n\n#### 4. Start the learning script\n\nFinally, learn the parameters of the network by running the traininig script:\n\n\tth wlrn.lua models/3x32x32_to_64.lua trn-tripletgen.lua -v val-tripletgen.lua -w params.t7\n\nThe training should finish in about a day on a GeForce GTX 970 with cuDNN.\nThe file `params.t7` contains the learned parameters of the descriptor extractor specified in `models/3x32x32_to_64.lua`.\nUse the following code to deploy them:\n```lua\nn = dofile('models/3x32x32_to_64.lua')():float()\np = n:getParameters()\np:copy(torch.load('params.t7'))\ntorch.save('net.t7', n)\n```\n\n## Contact\n\nFor any additional information contact me at <nenad.markus@fer.hr>.\n\nCopyright (c) 2016, Nenad Markus. All rights reserved.", 
  "id": 51770279
}