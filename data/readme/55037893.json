{
  "read_at": 1462555649, 
  "description": "", 
  "README.md": "**Paper Implemantation**  \n[Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images by Shuran Song and Jianxiong Xiao](http://dss.cs.princeton.edu/paper.pdf)  \n[Used some C/C++/CUDA and MATLAB code of said paper's implemenation](https://github.com/shurans/DeepSlidingShape)  \n\n**Tested on**  \nUbuntu 15.10  \n\n**Results**  \nVGG 16 Layers -> Scene: 654 Batch: 100 Accuracy: 89.87668195718415% Time: 36315.016382632 seconds  \nVGG 19 Layers -> Scene: 654 Batch: 100 Accuracy: 89.89831804281107% Time: 39391.945318995 seconds  \n\nVGG 16 Layer results were obtained by the latest commit  \nVGG 19 Layer results were obtained by commit cf6fb3ca63150382abc65373fd30c43d9ef974ca  \n\n**How to install**  \n\n*Requirements*:  \n\n[Knet](http://knet.readthedocs.org/en/dev/install.html)  \n[Julia Package \"MAT\"](https://github.com/simonster/MAT.jl)   \n[Julia Package \"ImageMagick\"](https://github.com/JuliaIO/ImageMagick.jl)  \n7zip (For extracting processed data)  \n\n*Read the README.md in the \"data\" folder to download and setup the data*  \n\n**How to run**  \nThere are 3 arguments:  \n\n\"sceneCount\" limits the number of scenes to be processed  \n\"VGGNetLayerCount\" determines the type of VGGNet; either 16 or 19 layer version  \n\"file\" contains the output of the terminal  \n\njulia Test.jl \"sceneCount\" \"VGGNetLayerCount\" 2>&1 | tee \"file\"  \n\nExamples:  \njulia Test.jl 2 16 2>&1 | tee ../experiments/vgg_16_layers_test_02_scenes_output.txt  \njulia Test.jl 2 19 2>&1 | tee ../experiments/vgg_19_layers_test_02_scenes_output.txt  \n\njulia Test.jl 10 16 2>&1 | tee ../experiments/vgg_16_layers_test_10_scenes_output.txt  \njulia Test.jl 10 19 2>&1 | tee ../experiments/vgg_19_layers_test_10_scenes_output.txt  \n\njulia Test.jl 654 16 2>&1 | tee ../experiments/vgg_16_layers_test_all_scenes_output.txt  \njulia Test.jl 654 19 2>&1 | tee ../experiments/vgg_19_layers_test_all_scenes_output.txt  \n\n**Test set**  \nI chose NYU as my test set since it has less scenes compared to SUNRGBD (654 vs 5050), and each scene has a maximum of 2000 bounding boxes in it, so this way it takes less time to get results.  \n*Note: NYU is a subset of SUNRGBD database*  \n\n**Differences**  \nThe paper uses the 16 layer version of VGGNet, mine also has the 19 layer version.  \nThe paper makes use of 7x7 Region-of-Interest (RoI) pooling, however I crop/resize inputs to 224x224 since Knet does not have RoI pooling.  \n\n*RoI Pooling*  \nhttp://arxiv.org/pdf/1504.08083.pdf Fast R-CNN  \nhttp://arxiv.org/pdf/1406.4729.pdf Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition  \nhttp://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf Fast R-CNN RoI Pooling Layer  \nhttps://github.com/rbgirshick/caffe-fast-rcnn/blob/fast-rcnn/include/caffe/fast_rcnn_layers.hpp  \nhttps://github.com/rbgirshick/caffe-fast-rcnn/blob/fast-rcnn/src/caffe/layers/roi_pooling_layer.cpp  \nhttps://github.com/rbgirshick/caffe-fast-rcnn/blob/fast-rcnn/src/caffe/layers/roi_pooling_layer.cu  \n", 
  "id": 55037893
}