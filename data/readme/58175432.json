{
  "id": 58175432, 
  "read_at": 1462556671, 
  "description": "Project to links the arXiv to github repos", 
  "README.md": "# arXiv2git\nLinks the [arXiv](http://arxiv.org/) to github repos\n\n## Starter roadmap\n\n- [x] Write basic repo searcher\n- [x] Build (this) git repo\n- [x] Use github tokens\n- [x] parse month by month\n- [x] build starter repo list\n- [x] Write downloader of READMEs\n- [ ] Allow downloader to only update changed READMEs\n- [ ] Download first pass of READMEs\n- [ ] Write parser of READMEs\n- [ ] Build set of links\n\n## Website roadmap\n\n- [ ] Turn list into static webpage\n- [ ] Get list on live AWS instance for auto-updates\n- [ ] Write API to query\n- [ ] Notification for arXiv\n- [ ] Feed for discovery\n\n## Plugin roadmap\n\n- [ ] Write browser plugin ", 
  "fetch_README.py": "import json\nimport os\nimport time\nimport glob\nimport codecs\nimport unidecode\nimport tqdm\nfrom calendar import monthrange\nfrom datetime import date\n\ncurrent_year = date.today().year\ncurrent_month = date.today().month\n\nfrom utils import get_login_params\nlogin_params = get_login_params()\n\nos.system('mkdir -p data data/readme')\n\nclone_cmd = 'git clone --depth 1 {url} {dest}'\n\ndef repo_iter():\n    gb = os.path.join('data','repos','*.json')\n    F_REPO = glob.glob(gb)\n\n    for f in F_REPO:\n        with open(f) as FIN:\n            js = json.loads(FIN.read())\n        for item in js:\n            yield item\n\ndef get_filename(id):\n    return os.path.join('data','readme','{}.json'.format(id))\n\ndef update_needed(id):\n    f_readme = get_filename(id)\n    if not os.path.exists(f_readme):\n        return True\n    return False\n\ndef gather_readme(item):\n\n    # Clear the space\n    os.system('rm -rf tmp')\n\n    # Clone the repo\n    cmd = clone_cmd.format(url=item[\"git_url\"],dest=\"tmp\")\n    os.system(cmd)\n\n    # Find the matching files\n    F_README = [x for x in os.listdir('tmp')\n                if 'readme' in x.lower()\n                and 'html' not in x.lower()]\n\n    data = {}\n    for f in F_README:\n        f = os.path.join('tmp',f)\n\n        if os.path.isdir(f):\n            continue\n        \n        with codecs.open(f,'r','utf-8',errors='replace') as FIN:\n            raw = FIN.read()\n            val = unidecode.unidecode(raw)\n            data[os.path.basename(f)] = val\n\n    # Clear the space\n    os.system('rm -rf tmp')\n    \n    return data\n\n\nprint \"Reading repo list\"\nREPOS = [r for r in repo_iter() if update_needed(r[\"id\"])]\n    \nfor r in tqdm.tqdm(REPOS):\n\n    id = r['id']\n    #if not update_needed(id):\n    #    print \"Skipping\", id\n    #    continue\n\n    print \"Cloning\", id\n       \n    data = gather_readme(r)\n    data[\"description\"] = r[\"description\"]\n    data['id'] = r['id']\n    data['read_at'] = int(time.time())\n\n    js = json.dumps(data,indent=2)\n\n    with codecs.open(get_filename(id),'w','utf-8') as FOUT:\n        FOUT.write(js)\n\n    print \"Sleeping\", 2\n    time.sleep(2)\n"
}