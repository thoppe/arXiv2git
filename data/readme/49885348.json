{
  "read_at": 1462553058, 
  "description": "Build sample-specific co-expression networks.", 
  "README.md": "# SamSpecCoEN\nBuild sample-specific co-expression networks.\n\n\nOverview\n========\nThe goal of this code is to build sample-specific co-expression networks: \nAll samples have the same network *structure*, but edge weights depend on the sample.\nThe co-expression networks are built from Pearson's correlation between gene expressions.\nEach individual weights reflects the sample's contribution/deviation from this population-wide correlation.\n\nThe two options are:  \n1. LIONESS [Kuijjer et al., 2015]: For a given sample, an edge-weight is the contribution of this sample to the global correlation.  \n2. For a given sample, the edge-weight is the distance of that sample to the regression line fitting the expression of both genes. This quantifies how much this sample deviates from the population-wide behaviour.  \n\nWe also use ideas from [Zhang and Horvath, 2005] to build the global (population-wide) network, i.e. that a co-expression network should be approximately scale-free.  \n\nThis code is meant to be used on the ACES gene expression data [Staiger et al., 2013].  \n\n\nRequirements\n============\nPython packages\n---------------\n* hd5py  \n* matplotlib  \n* numpy  \n* memory_profiler (optional, for profiling memory usage)\n* timeit (optional, for timing functions)\n\nACES\n----\n* Download from [http://ccb.nki.nl/software/aces/](http://ccb.nki.nl/software/aces/)\n* untar in this (SamSpecCoEN) directory\n* add an empty file __init__.py under ACES/experiments\n* make sure to have the required Python packages (in particular, xlrd)\n\nUsage\n=====\nCreating sample-specific co-expression networks\n-----------------------------------------------\n\nThe class for creating sample-specific co-expression neworks is `CoExpressionNetwork.py`. It can be used in the following manner:\n\n### Networks for a single data set\n```python CoExpressionNetwork.py DMFS outputs/U133A_combat_DMFS```\ncreates sample-specific coexpression networks for the entire dataset. The network structure (list of edges) is stored under `outputs/U122A_combat_DMFS/edges.gz` and its weights under `outputs/U122A_combat_DMFS/lioness/edges_weights.gz` (for the LIONESS approach) or `outputs/U122A_combat_DMFS/regline/edges_weights.gz` (for the regression line approach).\n\n### Networks in a cross-validation setting\n```python CoExpressionNetwork.py DMFS outputs/U133A_combat_DMFS -k 5```\ncreates 5 folds for cross-validation. The network structure (list of edges) is stored under `outputs/U122A_combat_DMFS/edges.gz` and its weights, for each fold from k=0 to k=4, under `outputs/U122A_combat_DMFS/<k>/lioness/edges_weights.gz` (for the LIONESS approach) or `outputs/U122A_combat_DMFS/<k>/regline/edges_weights.gz` (for the regression line approach). \n\n`outputs/U122A_combat_DMFS/<k>` also contains training and test indices and labels.\n\n### Networks in a subtype-stratified cross-validation setting\nIn order to evaluate the expressiveness of the representation of samples by their sample-specific co-expression network, we use a subtype-stratified cross-validation setting similar to that described in [Allahyar & de Ridder, 2015].\n\nTo create the corresponding data folds and networks:\n```\ndata_dir=data/SamSpecCoEN/outputs/U133A_combat_RFS/subtype_stratified\naces_dir=data/SamSpecCoEN/ACES # downloaded from http://ccb.nki.nl/software/aces/\nfor repeat in {0..9}\ndo\n    python setupSubtypeStratifiedCV_writeIndices.py data/SamSpecCoEN ${repeat}\ndone\n\nfor repeat in {0..9}\ndo\n    for fold in {0..9}\n    do\n\t    python setupCV_computeNetworks.py ${aces_dir} ${data_dir} ${fold} ${repeat}\n    done\ndone\n```\nThis process can easily be parallelized.\n\n**Warning** Note that for one repeat and fold, the networks take about 450 Mo of space, meaning that for 10 folds, 10 repeats, you need 43 Go of space to store the data. \n\n### Networks in a sampled leave-one-study-out cross-validation setting\nWe use a sampled leave-one-study-out cross-validation setting similar to that described in [Allahyar & de Ridder, 2015].\n\nTo create the corresponding data folds and networks:\n```\ndata_dir=data/SamSpecCoEN/outputs/U133A_combat_RFS/sampled_loso\naces_dir=data/SamSpecCoEN/ACES # downloaded from http://ccb.nki.nl/software/aces/\nfor repeat in {0..9}\ndo\n    python setupSampledLOSO_writeIndices.py data/SamSpecCoEN ${repeat}\ndone\n\nfor repeat in {0..9}\ndo\n    for fold in {0..9}\n    do\n\t   python setupCV_computeNetworks.py ${aces_dir} ${data_dir} ${fold} ${repeat}\n    done\n done\n```\nThis process can easily be parallelized.\n\n**Warning** Note that for one repeat and fold, the networks take about 450 Mo of space, meaning that for 10 folds, 10 repeats, you need 43 Go of space to store the data. \n\nCross-validation experiments\n----------------------------\nThe class for running a cross-validation experiment is `OuterCrossVal.py`. Internally, it uses `InnerCrossVal.py` to determine the best hyperparameters for the learning algorithm.\n\n```python OuterCrossVal.py ACES outputs/U133A_combat_DMFS lioness results -o 5 -k 5 -m 400``` \nruns a 5-fold cross-validation experiment on the data stored in folds under `outputs/U133A_combat_DMFS`, for the LIONESS edge weights, using a 5-fold inner cross-validation loop, and returning at most 400 genes (following ACES/FERAL). It uses both an l1-regularized logistic regression (results under the ```results/``` folder) and an l1/l2 (or Elastic Net)-regularized logistic regression  (results under the ```results/enet``` folder) .\n\n### Subtype-stratified cross-validation \n#### Parallelization at the repeat level\nTo run a cross-validation with 5-fold of inner cross-validation (for parameter setting), returning at most 1000 features:\n```\ndata_dir=data/SamSpecCoEN/outputs/U133A_combat_RFS/subtype_stratified\naces_dir=data/SamSpecCoEN/ACES # downloaded from http://ccb.nki.nl/software/aces/\n\nfor repeat in {0..9}\ndo\n    for network in lioness regline\n    do\n        python OuterCrossVal.py ${aces_dir} ${data_dir}/repeat${repeat} ${network}  \\ \n               ${data_dir}/repeat${repeat}/results/${network} -o 10 -k 5 -m 1000\n    done\ndone\n```\nThis runs both an l1-regularized and an l1/l2-regularized (or ElasticNet) logistic regression. The ```--edges```\n\nThe ```--nodes``` option allows you to run the exact same algorithm on the exact same folds, but using the node weights (i.e. gene expression data) directly instead of the edge weights, for comparison purposes (the network type is required but won't be used):\n\n```\npython OuterCrossVal.py ${aces_dir} ${data_dir}/repeat${repeat} lioness \\ \n       ${data_dir}/repeat${repeat}/results -o 10 -k 5 -m 1000 --nodes\n```\n\nThe ```--sfan``` option allows you to run sfan [Azencott et al., 2013] to select nodes, using the structure of the co-expression network, using an l2-regularized logistic regression on the values (normalize gene expression) of the selected nodes for final prediction. In this case ```${sfan_dir}``` points to the ```sfan/code``` folder that you can obtain from [sfan's github repository](https://github.com/chagaz/sfan). This will also run an l2-regularized logistic regression only on the nodes that are connected in the network (i.e. not using sfan at all); the results will be under ```${data_dir}/repeat${repeat}/results/nosel$```.\n\n```\npython OuterCrossVal.py ${aces_dir} ${data_dir}/repeat${repeat} lioness  \\\n       ${data_dir}/repeat${repeat}/results/sfan -o 10 -k 5 -m 1000 --nodes --sfan ${sfan_dir}\n```\n\n#### Parallelization at the repeat/fold level\nTo run a cross-validation with 5-fold of inner cross-validation (for parameter setting), returning at most 1000 features:\n```\ndata_dir=data/SamSpecCoEN/outputs/U133A_combat_RFS/subtype_stratified\naces_dir=data/SamSpecCoEN/ACES # downloaded from http://ccb.nki.nl/software/aces/\nfor repeat in {0..9}\ndo\n    for fold in {0..9}\n    do\n        for network in lioness regline\n        do\n            python InnerCrossVal.py ${aces_dir} ${data_dir}/repeat${repeat}/fold${fold} ${network} \\ \n                   ${data_dir}/repeat${repeat}/results/${network}/fold${fold} -k 5 -m 1000\n        done\n    done\ndone\n```\nFollowed by\n```\ndata_dir=data/SamSpecCoEN/outputs/U133A_combat_RFS/subtype_stratified\naces_dir=data/SamSpecCoEN/ACES # downloaded from http://ccb.nki.nl/software/aces/\n\nfor repeat in {0..9}\ndo\n    for network in lioness regline\n    do\n        python run_OuterCrossVal.py ${aces_dir} ${data_dir}/repeat${repeat} ${network} \\ \n               ${data_dir}/repeat${repeat}/results/${network} -o 10 -k 5 -m 1000\n    done\ndone\n```\n\nThe ```--nodes``` option allows you to run the exact same algorithm on the exact same folds, but using the node weights (i.e. gene expression data) directly instead of the edge weights, for comparison purposes (the network type is required but won't be used):\n\n```\npython InnerCrossVal.py ${aces_dir} ${data_dir}/repeat${repeat}/fold${fold} lioness \\\n       ${data_dir}/repeat${repeat}/results/fold${fold} -k 5 -m 1000 --nodes\n  \npython run_OuterCrossVal.py ${aces_dir} ${data_dir}/repeat${repeat} lioness \\\n       ${data_dir}/repeat${repeat}/results -o 10 -k 5 -m 1000 --nodes\n```           \n\nThe ```--sfan``` option allows you to run sfan [Azencott et al., 2013] to select nodes, using the structure of the co-expression network, using an l2-regularized logistic regression on the values (normalize gene expression) of the selected nodes for final prediction. In this case ```${sfan_dir}``` points to the ```sfan/code``` folder that you can obtain from [sfan's github repository](https://github.com/chagaz/sfan). This will also run an l2-regularized logistic regression only on the nodes that are connected in the network (i.e. not using sfan at all); the results will be under ```${data_dir}/repeat${repeat}/results/nosel$```.\n\n```\npython InnerCrossVal.py ${aces_dir} ${data_dir}/repeat${repeat}/fold${fold} lioness \\\n       ${data_dir}/repeat${repeat}/results/sfan/fold${fold} -k 5 -m 1000 --nodes --sfan ${sfan_dir}\n \npython run_OuterCrossVal.py ${aces_dir} ${data_dir}/repeat${repeat} lioness \\ \n       ${data_dir}/repeat${repeat}/results/sfan -o 10 -k 5 -m 1000  --sfan ${sfan_dir}     \n```  \n\n\nReferences\n==========\nAllahyar, A. and de Ridder, J. (2015). FERAL: network-based classifier with application to breast cancer outcome prediction. Bioinformatics, 31 (12).\n\nAzencott, C.-A., Grimm, D., Sugiyama, M., Kawahara, Y., and Borgwardt, K. M. (2013). Efficient network-guided multi-locus association mapping with graph cuts. Bioinformatics, 29(13): i171--i179.\n\nKuijjer, M.L., Tung, M., Yuan, G., Quackenbush, J., and Glass, K. (2015). Estimating sample-specific regulatory networks. arXiv:1505.06440 [q-Bio].  \n \nStaiger, C., Cadot, S., Gyorffy, B., Wessels, L.F.A., and Klau, G.W. (2013). Current composite-feature classification methods do not outperform simple single-genes classifiers in breast cancer prognosis. Front Genet 4.  \n  \nZhang, B., and Horvath, S. (2005). A General Framework for Weighted Gene Co-Expression Network Analysis. Statistical Applications in Genetics and Molecular Biology 4.\n\n", 
  "id": 49885348
}