{
  "read_at": 1462548956, 
  "description": "", 
  "README.md": "# Faster RNNLM (HS/NCE) toolkit\nIn a nutshell, the goal of this project is to create an rnnlm implementation that can be trained on huge datasets (several billions of words) and very large vocabularies (several hundred thousands) and used in real-world ASR and MT problems.\nBesides, to achieve better results this implementation supports such praised setups as ReLU+DiagonalInitialization [1], GRU [2], NCE [3], and RMSProp [4].\n\nHow fast is it?\nWell, on One Billion Word Benchmark [8] and 3.3GHz CPU the program with standard parameters (sigmoid hidden layer of size 256 and hierarchical softmax) processes more then 250k words per second in 8 threads, i.e. 15 millions of words per minute.\nAs a result an epoch takes less than one hour.\n\nThe distribution includes `./run_benchmark.sh` script to compare training speed on your machine among several impementations.\nThe scipts downloads Penn Tree Bank corpus and trains four models: Mikolov's rnnlm with class-based softmax from rnnlm.org, Edrenkin's rnnlm with HS from kaldi project, faster-rnnlm with hierarchical softmax, and faster-rnnlm with noise contrastive estimation.\nNote that while models with class-based softmax can achieve a little lower entropy then models hierarchical softmax, their training is infeaseble for large vocabularies.\nOn the other hand, NCE speed doesn't depend on the size of the vobaculary.\nWhats more, models trained with NCE is comparable with class-based models in terms of resulting entropy.\n\n## Quick start\nRun `./build.sh` to download Eigen library and build faster-rnnlm.\n\nTo train a simple model with GRU hidden unit and Noise Contrastive Estimation, use the following command:\n\n   `./rnnlm -rnnlm model_name -train train.txt -valid validation.txt -hidden 128 -hidden-type gru -nce 20 -alpha 0.01`\n\nFiles train.txt and test.txt must contain one sentence per line. All distinct words that are found in the training file will be used for the nnet vocab, their counts will determine Huffman tree structure and remain fixed for this nnet. If you prefer using limited vocabulary (say, top 1 million words) you should map all other words to <unk> or another token of your choice. Limited vocabulary is usually a good idea if it helps you to have enough training examples for each word.\n\nTo apply the model use following command:\n\n   `./rnnlm -rnnlm model_name -test train.txt`\n\nLogprobs (log10) of each sentence are printed to stdout. Entropy of the corpus in bits is printed to stderr.\n\n## Model architecture\nThe neural network has an input embedding layer, a few hidden layers, an output layer, and optional direct input-output connections.\n\n### Hidden layer\nAt the moment the following hidden layers are supported: sigmoid, tanh, relu, gru, gru-bias, gru-insyn, gru-full.\nFirst three types are quite standard.\nLast four types stand for different modification of Gated Recurrent Unit. Namely, gru-insyn follows formulas from [2]; gru-full adds bias terms for reset and update gates; gru uses identity matrices for input transformation without bias; gru-bias is gru with bias terms.\nThe fastest layer is relu, the slowest one is gru-full.\n\n### Output layer\nStandard output layer for classification problems is softmax.\nHowever, as softmax outputs must be normalized, i.e. sum over all classes must be one, its calculation is infeasible for a very large vocabulary.\nTo overcome this problem one can use either softmax factorization or implicit normalization.\nBy default, we approximate softmax via Hierarchical Softmax over Huffman Tree [6].\nIt allows to calculate softmax in logarithmic linear time, but reduces the quality of the model.\nImplicit normalization means that one calculates next word probability as in full softmax case, but without explicit normalization over all the words.\nOf course, it is not guaranteed that such *probabilities* will sum to up.\nBut in practice the sum is quite close to one due to custom loss function.\nCheckout [3] for more details.\n\n### Direct input-output connections\nAs was noted in [0], train neural network together with maximum entropy model could lead to significant improvement.\nIn a nutshell, maxent model tries to approximate probability of target as a linear combination of its history features.\nE.g. in order to estimate probability if word \"d\" in the sentence \"a b c d\", the model will sum the following features: f(\"d\") + f(\"c d\") + f(\"b c d\") + f(\"a b c d\").\nYou can use maxent with both HS and NCE output layers.\n\n## Command line options\nWe opted to use command line options that are compatible with [Mikolov's rnnlm](http://rnnlm.org).\nAs result one can just replace the binary to switch between implementations.\n\nThe program has three modes, i.e. traininig, evaluation, and sampling.\n\nAll modes require model name:\n\n```\n    --rnnlm <file>\n      Path to model file\n```\n\nWill create <file> and <file>.nnet files (for storing vocab/counts in the text form and the net itself in binary form).\nIf the <file> and <file>.nnet already exist, the tool will attempt to load them instead of starting new training.\nIf the <file> exists and <file>.nnet doesn't, the tool will use existing vocabulary and new weights.\n\nTo run program in test mode, you must provide test file. If you use NCE and would like to calculate entropy, you must use --nce_accurate_test flag. All other options are ignored in apply mode\n\n```\n    --test <file>\n      Test file\n    --nce-accurate-test (0 | 1)\n      Explicitly normalize output probabilities; use this option\n      to compute actual entropy (default: 0)\n```\n\nTo run program in sampling mode, you must select positive number of sentences to sample.\n\n```\n  --generate-samples <int>\n    Number of sentences to generate in sampling mode (default: 0)\n  --generate-temperature <float>\n    Softmax temperatute (use lower values to get robuster results) (default: 1)\n```\n\nTo train program, you must provide train and validation files\n\n```\n  --train <file>\n    Train file\n  --valid <file>\n    Validation file (used for early stopping)\n```\n\nModel structure options\n\n```\n  --hidden <int>\n    Size of embedding and hidden layers (default: 100)\n  --hidden-type <string>\n    Hidden layer activation (sigmoid, tanh, relu, gru, gru-bias, gru-insyn, gru-full)\n    (default: sigmoid)\n  --hidden-count <int>\n    Count of hidden layers; all hidden layers have the same type and size (default: 1)\n  --direct <int>\n    Size of maxent layer in millions (default: 0)\n  --direct-order <int>\n    Maximum order of ngram features (default: 0)\n```\n\nLearning reverse model, i.e. a model that predicts words from last one to first one, could be useful for mixture.\n\n```\n  --reverse-sentence (0 | 1)\n    Predict sentence words in reversed order (default: 0)\n```\n\n\nThe performance does not scale linearly with the number of threads (it is sub-linear due to cache misses, false HogWild assumptions, etc).\nTesting, validation and sampling are always performed by a single thread regardless of this setting.\nAlso checkout \"Performance notes\" section\n\n```\n  --threads <int>\n    Number of threads to use\n```\n\nBy default, recurrent weights are initialized using uniform distribution.\nIn [1] another method to initialize weights was suggested, i.e. identity matrix multiplied by some positive constant.\nThe option below corresponds to this constant.\n\n```\n  --diagonal-initialization <float>\n    Initialize recurrent matrix with x * I (x is the value and I is identity matrix)\n    Must be greater then zero to have any effect (default: 0)\n```\n\nOptimization options\n\n```\n  --rmsprop <float>\n    RMSprop coefficient; rmsprop=1 disables rmsprop and rmsprop=0 equivalent to RMS\n    (default: 1)\n  --gradient-clipping <float>\n    Clip updates above the value (default: 1)\n  --learn-recurrent (0 | 1)\n    Learn hidden layer weights (default: 1)\n  --learn-embeddings (0 | 1)\n    Learn embedding weights (default: 1)\n  --alpha <float>\n    Learning rate for recurrent and embedding weights (default: 0.1)\n  --maxent-alpha <float>\n    Learning rate for maxent layer (default: 0.1)\n  --beta <float>\n    Weight decay for recurrent and embedding weight, i.e. L2-regularization\n    (default: 1e-06)\n  --maxent-beta <float>\n    Weight decay for maxent layer, i.e. L2-regularization (default: 1e-06)\n```\n\nThe program supports truncated back propagation through time.\nGradients from hidden to input are back propagated on each time step.\nHowever gradients from hidden to previous hidden are propagated for bptt steps within each bppt-period block.\nThis trick could speed up training and wrestle gradient explosion.\nSee [7] for details.\nTo disable any truncation set bptt to zero.\n\n```\n  --bptt <int>\n    Length of truncated BPTT unfolding\n    Set to zero to back-propagate through entire sentence (default: 3)\n  --bptt-skip <int>\n    Number of steps without BPTT;\n    Doesn't have any effect if bptt is 0 (default: 10)\n```\n\nEarly stopping options (see [0]).\nLet `ratio' be a ratio of previous epoch validation entropy to new one.\n\n```\n  --stop <float>\n    If `ratio' less than `stop' then start leaning rate decay (default: 1.003)\n  --lr-decay-factor <float>\n    Learning rate decay factor (default: 2)\n  --reject-threshold <float>\n    If (whats more) `ratio' less than `reject-threshold' then purge the epoch\n    (default: 0.997)\n  --retry <int>\n    Stop training once `ratio' has hit `stop' at least `retry' times (default: 2)\n```\n\nNoise Contrastive Estimation is used iff number of noise samples (--nce option) is greater then zero.\nOtherwise HS is used.\nReasonable value for nce is 20.\n\n```\n  --nce <int>\n    Number of noise samples; if nce is position then NCE is used instead of HS\n    (default: 0)\n  --use-cuda (0 | 1)\n    Use CUDA to compute validation entropy and test entropy in accurate mode,\n    i.e. if nce-accurate-test is true (default: 0)\n  --nce-unigram-power <float>\n    Discount power for unigram frequency (default: 1)\n  --nce-lnz <float>\n    Ln of normalization constant (default: 9)\n  --nce-unigram-min-cells <float>\n    Minimum number of cells for each word in unigram table (works\n    akin to Laplacian smoothing) (default: 5)\n  --nce-maxent-model <string>\n    Use given the model as a noise generator\n    The model must a pure maxent model trained by the program (default: )\n```\n\nOther options\n\n```\n  --epoch-per-file <int>\n    Treat one pass over the train file as given number of epochs (default: 1)\n  --seed <int>\n    Random seed for weight initialization and sampling (default: 0)\n  --show-progress (0 | 1)\n    Show training progress (default: 1)\n  --show-train-entropy (0 | 1)\n    Show average entropy on train set for the first thread (default: 0)\n    Train entropy calculation doesn't work for NCE\n\n```\n\n\n## Performance notes\nTo speed up matrix operations we use [Eigen](http://eigen.tuxfamily.org/) (C++ template library for linear algebra).\nBesides, we use data parallelism with sentence-batch HogWild [5].\nThe best performance could be achieved if all the threads are binded to the same CPU (one thread per core). This could be done by means of `taskset` tool (available by default in most Linux distros).\nE.g. if you have 2 CPUs and each CPU has 8 real cores + 8 hyper threading cores, you should use the following command:\n\n```\ntaskset -c 0,1,2,3,4,5,6,7 ./rnnlm -threads 8 ...\n```\n\nIn NCE mode CUDA is used to accelarate validation entropy calculation.\nOf course, if you don't have GPU, you can use CPU to calculate entropy, but it will take a lot of time.\n\n## Usage advice\n\n  - You don't need to repeat structural parameters (hidden, hidden-type, reverse, direct, direct-order) when using an existing model. They will be ignored. The vocabulary saved in the model will be reused.\n  - The vocabulary is built based on the training file on the first run of the tool for a particular model. The program will ignore sentences with OOVs in train time (or report them in test time).\n  - Vocabulary size plays very small role in the performance (it is logarithmic in the size of vocabulary due to the Huffman tree decomposition). Hidden layer size and the amount of training data are the main factors.\n  - Usually NCE works better then HS in terms of both PPL and WER.\n  - Direct connections could dramatically improve model quality. Especially in case of HS. Reasonable values to start from are `-direct 1000 -direct-order 4`.\n  - The model will be written to file after a training epoch if and only if its validation entropy improved compared to the previous epoch.\n  - It is a good idea to shuffle sentences in the set before splitting them into training and validation sets (GNU shuf & split are one of the possible choices to do it). For huge datasets use --epoch-per-file option.\n\n\n## References\n[0] Mikolov, T. (2012). Statistical language models based on neural networks. Presentation at Google, Mountain View, 2nd April.\n\n[1] Le, Q. V., Jaitly, N., & Hinton, G. E. (2015). A Simple Way to Initialize Recurrent Networks of Rectified Linear Units. arXiv preprint arXiv:1504.00941.\n\n[2] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.\n\n[3] Chen, X., Liu, X., Gales, M. J. F., & Woodland, P. C. (2015). Recurrent neural network language model training with noise contrastive estimation for speech recognition.\n\n[4] T. Tieleman and G. Hinton, \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude,\" COURSERA: Neural Networks for Machine Learning, vol.  4, 2012\n\n[5] Recht, B., Re, C., Wright, S., & Niu, F. (2011). Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems (pp. 693-701).\nChicago\n\n[6] Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.\n\n[7] Sutskever, I. (2013). Training recurrent neural networks (Doctoral dissertation, University of Toronto).\n\n[8] Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., & Robinson, T. (2013). One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005.\n", 
  "id": 42483622
}