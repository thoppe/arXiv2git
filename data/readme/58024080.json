{
  "read_at": 1462556755, 
  "description": "Currently favored techniques in deep learning", 
  "README.md": "- Weight initialization\n  - [Random walk initialization for training very deep feedforward\n    networks](http://arxiv.org/abs/1412.6558) **(preferred)**\n    - View deep network as random walk on logs of norms of vectors\n    - Rescale weight matrices to make walk unbiased\n    - Used by http://arxiv.org/abs/1511.03771\n  - For RNNs\n    - [iRNN](https://arxiv.org/abs/1504.00941)\n      - Initialize weight matrix between hidden states to identity to avoid\n        vanishing / exploding gradients\n      - [uRNN paper](http://arxiv.org/abs/1511.06464) claims that it doesn't\n        work very well:\n\n        > We found the IRNN to be particularly unstable; it only ran without\n        > blowing up with\n        > incredibly low learning rates and gradient clipping. Since the\n        > performance was so poor relative to\n        > other models we compare against, we do not show IRNN curves in the\n        > figures.\n    - [npRNN](http://arxiv.org/abs/1511.03771)\n      - Modify idea of iRNN by initializing with random positive definite\n        matrix with all eigenvalues <=1\n      - Claims improved performance\n- Architecture\n  - Nonlinearity\n    - ReLUs **(preferred)**\n    - tanh is more popular for RNNs\n  - RNNs\n    - [LSTM](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) **(preferred)**\n    - [uRNN](http://arxiv.org/abs/1511.06464)\n      - Enforces that weight matrix has eigenvalues all identically 1\n      - Does this by decomposing transition matrices and making them\n        complex-valued\n      - Seems intriguing to force matrices to be unitary by design, but\n        unsatisfyingly complicated solution.  Feels like there should be a\n        simpler way\n      - [code](https://github.com/amarshah/complex_RNN)\n      - [interesting discussion on reddit](https://www.reddit.com/r/MachineLearning/comments/3uk2q5/151106464_unitary_evolution_recurrent_neural/)\n  - [Batch normalization](http://arxiv.org/abs/1502.03167) **(preferred)**\n    - Renormalizes the outputs from each layer to save higher layers from\n      having to constantly learn the variances of the layers below\n    - Defines *Internal Covariate Shift*\n    - Allows much higher learning rates and less careful initialization of\n      parameters\n    - [Applied to RNNs](http://arxiv.org/abs/1603.09025)\n- Regularization\n  - [Dropout](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n    **(preferred)**\n    - Randomly turn off nodes during training, and rescale output during\n      testing\n    - Extremely simple yet effective form of regularization\n    - [Applied to RNNs](http://arxiv.org/abs/1603.05118)\n  - L2 weight decay\n  - [Batch normalization](http://arxiv.org/abs/1502.03167) claims to be a form\n    of regularization, though they still seem to use L2 in addition\n- Learning algorithm\n  - Adaptive learning rate\n    - RMSProp **(preferred)**\n    - [ESGD](http://arxiv.org/abs/1502.04390) *(promising)*\n      - Alternative preconditioner trying to attach saddle points\n    - [AdaGrad](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf)\n  - Momentum **(preferred)**\n    - TODO: More info\n- Tricks\n  - Grid search on learning rate and dropout percentage\n    - Used in [npRNN](http://arxiv.org/abs/1511.03771)\n- RNNs (TODO: fill out more, referencing above papers)\n  - Vanishing / Exploding gradients\n    - [Orthogonal RNNs](http://arxiv.org/abs/1602.06662)\n      - Explicitly constructs RNN solutions to 2 common toy problems\n      - Uses this to understand why iRNNs vs orthogonal RNN work well for some\n        tasks\n      - Introduces something that mixes between them\n    - Parameter initialization (iRNN, npRNN)\n    - Architecture (uRNN)\n    - Gradient clipping\n    - LSTM\n    - 3 approaches described in iRNN paper\n      - Optimization algorithm\n", 
  "id": 58024080
}