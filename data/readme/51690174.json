{
  "read_at": 1462557509, 
  "description": "experimental binary net implementation in chainer", 
  "README.md": "# binary_net by chainer\n\nThis is an experimental code for reproducing [1]'s result using chainer. \nNo optimization is used for binary operations. I just binalize weight and activation at computation, and use a straight through estimator for gradient computation. \n\n- [1] \"BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\", Matthieu Courbariaux, Yoshua Bengio\nhttp://arxiv.org/abs/1602.02830\n\nCode is almost equivalent to chainer/examples/mnist/ except:\n\n- Use binary weight, binary activation, batch_normalization (net.py, bst.py, link_binary_linear.py function_binary_linear.py)\n- Use weight clip, optimizer.add_hook(weight_clip.WeightClip()) (weight_clip.py)\n\n\nUsage\n```\n# cpu\n./train_mnist.py\n\n# gpu (use device id=0)\n./train_mnist.py --gpu=0\n```\n\nResult\n```\nload MNIST dataset\nepoch 1\ngraph generated\ntrain mean loss=0.573861178756, accuracy=0.92756666926\ntest  mean loss=0.473955234885, accuracy=0.957400003672\nepoch 2\ntrain mean loss=0.456328810602, accuracy=0.963833337426\ntest  mean loss=0.436628208458, accuracy=0.966100006104\nepoch 3\ntrain mean loss=0.431186137001, accuracy=0.970866675178\ntest  mean loss=0.425710965991, accuracy=0.968000004292\nepoch 4\ntrain mean loss=0.417045980394, accuracy=0.975233343144\ntest  mean loss=0.417223671675, accuracy=0.969800002575\nepoch 5\ntrain mean loss=0.409991853635, accuracy=0.977583343883\ntest  mean loss=0.407217691839, accuracy=0.972200006247\nepoch 6\ntrain mean loss=0.400645414094, accuracy=0.979883343577\ntest  mean loss=0.40729173243, accuracy=0.972400006652\nepoch 7\ntrain mean loss=0.395223465959, accuracy=0.981483343343\ntest  mean loss=0.402929984331, accuracy=0.972300007343\nepoch 8\ntrain mean loss=0.389928704053, accuracy=0.983366676569\ntest  mean loss=0.402315998375, accuracy=0.97280000627\nepoch 9\ntrain mean loss=0.389456737339, accuracy=0.983600010673\ntest  mean loss=0.39955814153, accuracy=0.973400005698\nepoch 10\ntrain mean loss=0.385094682376, accuracy=0.984783343176\ntest  mean loss=0.401046113968, accuracy=0.972200005651\nepoch 11\ntrain mean loss=0.38257016028, accuracy=0.986000010371\ntest  mean loss=0.393966214061, accuracy=0.974400005937\nepoch 12\ntrain mean loss=0.379689370046, accuracy=0.986583343049\ntest  mean loss=0.396037294269, accuracy=0.974900006056\nepoch 13\ntrain mean loss=0.378962427129, accuracy=0.986783343355\ntest  mean loss=0.392184624076, accuracy=0.974600006342\nepoch 14\ntrain mean loss=0.375957165956, accuracy=0.987533342044\ntest  mean loss=0.394931056798, accuracy=0.974400007725\nepoch 15\ntrain mean loss=0.375070895106, accuracy=0.988500009278\ntest  mean loss=0.393464969695, accuracy=0.974700006843\nepoch 16\ntrain mean loss=0.37365236491, accuracy=0.988550009727\ntest  mean loss=0.397632206976, accuracy=0.972800005078\nepoch 17\ntrain mean loss=0.372001686394, accuracy=0.989216675361\ntest  mean loss=0.392721504271, accuracy=0.973500006795\nepoch 18\ntrain mean loss=0.369835597078, accuracy=0.989616675278\ntest  mean loss=0.388620298207, accuracy=0.975900007486\nepoch 19\ntrain mean loss=0.369737090866, accuracy=0.98986667484\ntest  mean loss=0.391184872091, accuracy=0.974400007725\nepoch 20\ntrain mean loss=0.368466852009, accuracy=0.98983334144\ntest  mean loss=0.389251522124, accuracy=0.976600005627\nsave the model\nsave the optimizer\n```\n", 
  "id": 51690174
}