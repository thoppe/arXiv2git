{
  "read_at": 1462557978, 
  "description": "", 
  "README.md": "# Welcome to ChefNet\n\nWhat can we teach computer's about food? ChefNet is a convolutional neural network that identifies the ingredients in a dish by analyzing a picture of it.\n\nFor example, what ingredients do you think are in this delicious looking cake?\n\n<img src=\"images/carrot_cake.jpg\" width=\"300\">\n<br><br>\n\n# Table of Contents\n\n1. [Data](#data)\n    * [Ingredient Labeling](#ingredient-label-wrangling)\n    * [Image Processing](#image-processing)\n    * [Neural Network Architecture](#neural-network-architecture)\n1. [Results](#results)\n    * [Overall Results](#overall-results)\n    * [Example Predictions](#example-predictions)\n    * [Data Issues](#data-issues)\n1. [Try ChefNet yourself](#try-chefnet-yourself)\n1. [Next Steps](#next-steps)\n1. [Recipe Generation](#recipe-generation-with-recurrent-neural-networks)\n1. [Thank you](#thank-you)\n    * [References](#references)\n\n# Data\n\nTo create a labeled dataset of images of food, I scraped the recipes and user submitted photos of 17,000 recipes, totaling 230,000 user photos, from allrecipes.com. The scraper was run on an AWS instance and took about a day to run. Massive speedups may be attributed to parallelizing and threading the scraping process. Code is in the [Web Scrapers](/Scripts/Web_scrapers) folder under Scripts.\n\n### Ingredient label wrangling\n\n<img src=\"figures/vocab_wordcloud.png\" width=\"400\">\n\nIn order to train a neural net, I needed to create consistent labels for ingredients. I took two approaches. My first approach was to start with the scraped list of ingredients, and identify the keyword using the indico keyword extraction api, while iteratively remove all words not critical to the underlying food item (Code may be found in [Recipe Generation RNN](/Scrips/Recipe-Generation-RNN)). My second approach, which was ultimately used to train the [Ingredent Identifier](/Scripts/Ingredient_identifier), was to start with a cleaned list of ingredients initially scraped from enchantedlearning.com\n\nIt is critical that image labels are as clean as possible, otherwise the neural network will have difficulty learning. It was also important to allow the model to have multi-word labels to represent items such as bell pepper. A useful extension of this project may be to vectorize the labels, so that the net will learn that the ingredient similarity, for example, misclassifying beef and steak is closer than chicken and peas. Vectorization methods require tokenization of text first. This may be an area worth explore further.\n\n### Image Processing ([Code](/Scripts/Preprocessing))\n\nNeural networks were trained with raw image data, and convolved imaged data that was passed though the 2014 image net winner, VGG-16 from Oxford. Transfer learning proved more fruitful given the limited size of my dataset. Activations were taken at the end of layer 30, before flattening to dense layers. It would be interesting to compare results using activations taken after these dense layers, but I did not have time to explore this comparison. Here is a rough illustration of how the image vectorization process:\n\n<img src=\"figures/image_vectorization.png\" width=\"\">\n<br><br>\n\nImages were downsized to 100x100 so that I could iterate through training multiple models, in the time allotted for capstone projects.\n\n### Neural Network Architecture ([Code](/Scripts/Ingredient_identifier/build_models.py))\n\nMy architecture went though multiple iterations, ultimately I settled on preprocessing images with VGG-16, and passing those activations into 3 hidden dense layers. My output layer consists of a sigmoid activation for each ingredient, and uses binary crossentropy loss.\n\n# Results\n\nThis section provides some high level results for how my final net performed, and then some examples of how it predicts on a particular image.\n\n### Overall results ([Code](/Scripts/Ingredient_identifier/evaluate_models.py))\n\n__ChefNet:__ 48% Recall, 38% Precision\n\n__ChefNet without VGG-16 Processing:__ 46% Recall, 35% Precision\n\n__Naive simulation:__ 6% Recall, 21% Precision\n\nThe algorithm was able to perform better for some ingredients than other. Below you may see what classes had best recall (top 10 ranged from 75%-100%). The net had better recall for those ingredients that were more frequent in the dataset:\n\n<img src=\"figures/recall_wordcloud.png\" width=\"400\">\n\nBelow represents the top classes in terms of Precision (top 10 range form 60%-100%), note that these classes are different. In general, the net was more precise with ingredients it only predicted a few times:\n\n<img src=\"figures/precision_wordcloud.png\" width=\"400\">\n\n### Example Predictions\n\nHere are some examples of how well ChefNet predicted:\n\n##### Carrot Cake\n\n<img src=\"figures/Carrot_cake_slide.png\" width=\"\">\n\n##### My Lunch last Monday\n\n<img src=\"figures/Monday_lunch_slide.png\" width=\"\">\n\n### Data Issues\n\nThe data is not perfect, below is a slide that shows two different images for the same recipe. Not only can the sugar cookie look completely different based on the decoration decisions, but there are also misplaced pictures.\n\nAs an example, below you may see two user submitted images for the same sugar cookie recipe. The first illustrates how a cooking may look like just about anything, the second shows how users misplace images:\n\n<img src=\"figures/User_imgs.png\" width=\"\">\n\n# Try ChefNet yourself\n\nFirst you will need to install these dependencies, in addition to Conda:\n\n* [Keras](http://keras.io/)\n\n* [Skimage](http://scikit-image.org/)\n\n* [HDF5](http://docs.h5py.org/en/latest/build.html)\n\nYou will need to download the weights of my trained convolutional neural net and place the .h5 in the [Models](/models) folder\n\nMy weights: [CNN Weights](https://drive.google.com/file/d/0B53_Ht6DdCsGMy1GTDkwR0piODg/view?usp=sharing)\n\nYou will also need to download the weights of trained VGG-16 and place the .h5 file in the [VGG Weights](/vgg_weights) folder.\n\nVGG-16 weights: [vgg16_weights.h5](https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/view)\n\nNext you should move any image file you would like to predict on into the [images](/images) folder.\n\nNow you can run [predict_ingredients.py](/Scripts/Ingredient_identifier) to have the model make your predictions (make sure you navigate to the [ingredient identifier](/Scripts/Ingredient_identifier) before running ipython).\n\nIf you run the script in ipython, you may just run `predict_user_photo(model, vocab)` to predict additional photos without reloading the model.\n\nHere is an example of how it should look:\n\n<img src=\"figures/demo.png\" width=\"\">\n\n# Next Steps\n\nThere are a number of next steps that can be taken with this project.\n\n* The model may benefit from further tuning, and more neural network structures could be explored. It may also benefit from training on full size images.\n\n* Additional data may be scraped from other recipe websites to create a larger dataset.\n\n* Another extension may involve true image captioning at a character or word level. I started exploring this option, but found that is was less useful toward my motivation of predicting underlying ingredients.\n\n### Recipe Generation with Recurrent Neural Networks\n\nSeparate from the ingredient identifier, I've also developed a model that will generate novel ingredients character by character, based on the cleaned recipes scraped from allrecipes.com. Here are a couple examples, I'm not sure how well they would turn out:\n\n* banana, lemon juice, cream mushroom soup, milk, cheese sauce\n\n* garlic, cheese, cheddar cheese, salt, cheese, sausage, garlic, pork, bacon hotme, chicken, sesame, asparagus, bread, cheese, bacon, hamburger bun\n\nYou may notice __bacon hotme__ is not an actual ingredient. This is an odd result that can come out of character by character text generation.\n\nAll scripts for this maybe found in [Recipe Generation RNN](Scripts/Recipe_Generation_RNN).\n\n# Thank you\n\nBig thank you to Jesse Lieman-Sifry for the inspiration behind this project, as well as to my Galvanize Instructors and peers for the continuous help and suggestions along the way. It was a pleasure to work with all of you.\n\n### References\n\n* VGG Net Representation:\n\n``` Very Deep Convolutional Networks for Large-Scale Image Recognition\nK. Simonyan, A. Zisserman\narXiv:1409.1556 ```\n", 
  "id": 52681534
}