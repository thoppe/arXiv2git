{
  "read_at": 1462548568, 
  "description": "A list of papers and resources dedicated to deep reinforcement learning", 
  "README.md": "# Deep Reinforcement Learning Papers\n\nA list of papers and resources dedicated to deep reinforcement learning.\n\nPlease note that this list is currently work-in-progress and far from complete.\n\n## TODOs\n\n - Add more and more papers\n - Improve the way of classifying papers (tags may be useful)\n - Create a policy of this list: curated or comprehensive, how to define \"deep reinforcement learning\", etc.\n\n## Contributing\n\nIf you want to inform the maintainer of a new paper, feel free to contact [@mooopan](https://twitter.com/mooopan). Issues and PRs are also welcome.\n\n## Table of Contents\n\n - [Papers](#papers)\n - [Talks/Slides](#talksslides)\n - [Miscellaneous](#miscellaneous)\n\n## Papers\n\n - [Deep Value Function](#deep-value-function)\n - [Deep Policy](#deep-policy)\n - [Deep Actor-Critic](#deep-actor-critic)\n - [Deep Model](#deep-model)\n - [Application to Non-RL Tasks](#application-to-non-rl-tasks)\n - [Unclassified](#unclassified)\n\n### Deep Value Function\n\n - S. Lange and M. Riedmiller, **Deep Learning of Visual Control Policies**, ESANN, 2010. [pdf](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2010-87.pdf)\n   - Deep Fitted Q-Iteration (DFQ)\n - V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonglou, D. Wierstra, and M. Riedmiller, **Playing Atari with Deep Reinforcement Learning**, NIPS 2013 Deep Learning Workshop, 2013. [pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n   - Deep Q-Network (DQN) with experience replay\n - V. Mnih, K. Kavukcuoglu, D. Silver, A. a Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, **Human-level control through deep reinforcement learning**, Nature, 2015. [pdf](http://home.uchicago.edu/~arij/journalclub/papers/2015_Mnih_et_al.pdf) [code](https://sites.google.com/a/deepmind.com/dqn/)\n   - Deep Q-Network (DQN) with experience replay and target network\n - T. Schaul, D. Horgan, K. Gregor, and D. Silver, **Universal Value Function Approximators**, ICML, 2015. [pdf](http://schaul.site44.com/publications/uvfa.pdf)\n - A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. De Maria, M. Suleyman, C. Beattie, S. Petersen, S. Legg, V. Mnih, and D. Silver, **Massively Parallel Methods for Deep Reinforcement Learning**, ICML Deep Learning Workshop, 2015. [pdf](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications_files/gorila.pdf)\n   - Gorila (General Reinforcement Learning Architecture)\n - K. Narasimhan, T. Kulkarni, and R. Barzilay, **Language Understanding for Text-based Games Using Deep Reinforcement Learning**, EMNLP, 2015. [pdf](http://people.csail.mit.edu/karthikn/pdfs/mud-play15.pdf) [supplementary](http://people.csail.mit.edu/karthikn/pdfs/mud-supp.pdf) [code](http://people.csail.mit.edu/karthikn/mud-play/)\n   - LSTM-DQN\n - M. Hausknecht and P. Stone, **Deep Recurrent Q-Learning for Partially Observable MDPs**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1507.06527) [code](https://github.com/mhauskn/dqn/tree/recurrent)\n - M. Lai, **Giraffe: Using Deep Reinforcement Learning to Play Chess**, arXiv. 2015. [arXiv](http://arxiv.org/abs/1509.01549) [code](https://bitbucket.org/waterreaction/giraffe)\n - H. van Hasselt, A. Guez, and D. Silver, **Deep reinforcement learning with double q-learning**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1509.06461)\n   - Double DQN\n - F. Zhang, J. Leitner, M. Milford, B. Upcroft, and P. Corke, **Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control**, in ACRA, 2015. [pdf](http://juxi.net/papers/others/zhang2015acra-submission.pdf)\n - T. Schaul, J. Quan, I. Antonoglou, and D. Silver, **Prioritized Experience Replay**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1511.05952)\n - Z. Wang, N. de Freitas, and M. Lanctot, **Dueling Network Architectures for Deep Reinforcement Learning**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1511.06581)\n - V. Francois-Lavet, R. Fonteneau, and D. Ernst, **How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies**, NIPS Deep Reinforcement Learning Workshop, 2015. [arXiv](http://arxiv.org/abs/1512.02011)\n - I. Sorokin, A. Seleznev, M. Pavlov, A. Fedorov, and A. Ignateva, **Deep Attention Recurrent Q-Network**, NIPS Deep Reinforcement Learning Workshop, 2015. [arXiv](http://arxiv.org/abs/1512.01693)\n - A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick, R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell, **Policy Distillation**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1511.06295)\n - M. G. Bellemare, G. Ostrovski, A. Guez, P. S. Thomas, and R. Munos, **Increasing the Action Gap: New Operators for Reinforcement Learning**, AAAI, 2016. [arXiv](http://arxiv.org/abs/1512.04860)\n - D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis, **Mastering the game of Go with deep neural networks and tree search**, Nature, 2016. [pdf](https://storage.googleapis.com/deepmind-data/assets/papers/deepmind-mastering-go.pdf)\n - T. Zahavy, N. Ben Zrihem, and S. Mannor, **Graying the black box: Understanding DQNs**, arXiv, 2016. [arXiv](http://arxiv.org/abs/1602.02658)\n - J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson, **Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks**, arXiv, 2016. [arXiv](http://arxiv.org/abs/1602.02672)\n - I. Osband, C. Blundell, A. Pritzel, and B. Van Roy, **Deep Exploration via Bootstrapped DQN**, arXiv, 2016. [arXiv](http://arxiv.org/abs/1602.04621)\n - T. Salimans and D. P. Kingma, **Weight Normalization : A Simple Reparameterization to Accelerate Training of Deep Neural Networks**, arXiv, 2016. [arXiv](http://arxiv.org/abs/1602.07868)\n - S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, **Continuous Deep Q-Learning with Model-based Acceleration**, arXiv, 2016. [arXiv](http://arxiv.org/abs/1603.00748)\n - J. Heinrich and D. Silver, **Deep Reinforcement Learning from Self-Play in Imperfect-Information Games**, arXiv, 2016. [arXiv](http://arxiv.org/abs/1603.01121)\n - T. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, **Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation**, arXiv, 2016. [arXiv](http://arxiv.org/abs/1604.06057)\n\n### Deep Policy\n\n - S. Levine, C. Finn, T. Darrell, and P. Abbeel, **End-to-End Training of Deep Visuomotor Policies**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1504.00702)\n   - partially observed guided policy search\n - J. Schulman, S. Levine, P. Moritz, M. Jordan, and P. Abbeel, **Trust Region Policy Optimization**, ICML, 2015. [pdf](http://jmlr.org/proceedings/papers/v37/schulman15.pdf)\n\n### Deep Actor-Critic\n\n - J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, **High-Dimensional Continuous Control Using Generalized Advantage Estimation**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1506.02438)\n - T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, **Continuous control with deep reinforcement learning**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1509.02971)\n - D. Balduzzi and M. Ghifary, **Compatible Value Gradients for Reinforcement Learning of Continuous Deep Policies**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1509.03005)\n - M. Hausknecht and P. Stone, **Deep Reinforcement Learning in Parameterized Action Space**, arXiv. 2015. [arXiv](http://arxiv.org/abs/1511.04143)\n - N. Heess, J. J. Hunt, T. P. Lillicrap, and D. Silver, **Memory-based control with recurrent neural networks**, NIPS Deep Reinforcement Learning Workshop, 2015. [arXiv](http://arxiv.org/abs/1512.04455)\n - V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, **Asynchronous Methods for Deep Reinforcement Learning**, arXiv, 2016. [arXiv](http://arxiv.org/abs/1602.01783)\n\n### Deep Model\n\n - B. C. Stadie, S. Levine, and P. Abbeel, **Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1507.00814)\n - J. Oh, X. Guo, H. Lee, R. Lewis, and S. Singh, **Action-Conditional Video Prediction using Deep Networks in Atari Games**, NIPS, 2015. [arXiv](http://arxiv.org/abs/1507.08750)\n - J. M. Assael, W. Om, T. B. Schon, and M. P. Deisenroth, **Data-Efficient Learning of Feedback Policies from Image Pixels using Deep Dynamical Models**, arXiv, 2015 [arXiv](http://arxiv.org/abs/1510.02173)\n - N. Heess, G. Wayne, D. Silver, T. Lillicrap, Y. Tassa, and T. Erez, **Learning Continuous Control Policies by Stochastic Value Gradients**, NIPS, 2015. [arXiv](http://arxiv.org/abs/1510.09142) [video](https://www.youtube.com/watch?v=PYdL7bcn_cM)\n - J. Schmidhuber, **On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1511.09249)\n - K. Fragkiadaki, P. Agrawal, S. Levine, and J. Malik, **Learning Visual Predictive Models of Physics for Playing Billiards**, ICLR, 2016. [arXiv](http://arxiv.org/abs/1511.07404)\n\n### Application to Non-RL Tasks\n\n- J. C. Caicedo and S. Lazebnik, **Active Object Localization with Deep Reinforcement Learning**, ICCV, 2015. [pdf](http://web.engr.illinois.edu/~slazebni/publications/iccv15_active.pdf)\n- H. Guo, **Generating Text with Deep Reinforcement Learning**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1510.09202)\n- S. Hansen, **Using Deep Q-Learning to Control Optimization Hyperparameters**, arXiv, 2016. [arXiv](http://arxiv.org/abs/1602.04062)\n\n### Unclassified\n\n - X. Guo, S. Singh, H. Lee, R. Lewis, and X. Wang, **Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning**, NIPS, 2014. [pdf](http://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning.pdf) [video](https://sites.google.com/site/nips2014atari/)\n - S. Mohamed and D. J. Rezende, **Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1509.08731)\n - Y. Liang, M. C. Machado, E. Talvitie, and M. Bowling, **State of the Art Control of Atari Games Using Shallow Reinforcement Learning**, arXiv, 2015. [arXiv](http://arxiv.org/abs/1512.01563)\n - A. Tamar, S. Levine, and P. Abbeel, **Value Iteration Networks**, arXiv, 2016. [arXiv](http://arxiv.org/abs/1602.02867)\n\n## Talks/Slides\n\n - S. Levine, **Deep Learning for Decision Making and Control**, 2015. [video](https://www.youtube.com/watch?v=EtMyH_--vnU)\n - D. Silver, **Deep Reinforcement Learning**, ICLR, 2015. [video1](https://www.youtube.com/watch?v=EX1CIVVkWdE) [video2](https://www.youtube.com/watch?v=zXa6UFLQCtg) [slides](http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf)\n - D. Silver, **Deep Reinforcement Learning**, UAI, 2015. [video](https://www.youtube.com/watch?v=qLaDWKd61Ig)\n\n## Miscellaneous\n\n - [Deep Q-Learning - Google Group](https://groups.google.com/forum/#!forum/deep-q-learning)\n - [CS 294 Deep Reinforcement Learning, Fall 2015](http://rll.berkeley.edu/deeprlcourse/)\n - [Deep Reinforcement Learning Workshop, NIPS 2015](http://rll.berkeley.edu/deeprlworkshop/)\n - [junhyukoh/deep-reinforcement-learning-papers](https://github.com/junhyukoh/deep-reinforcement-learning-papers)\n - [DeepMind's Nature Paper and Earlier Related Work](http://people.idsia.ch/~juergen/naturedeepmind.html)\n\n", 
  "id": 43203362
}