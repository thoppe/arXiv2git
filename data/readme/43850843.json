{
  "read_at": 1462556655, 
  "description": "LSST Bootcamp Notes", 
  "README.md": "# LSST Bootcamp Notes\n\nSee slides at: https://github.com/lsst-dm/Oct15_bootcamp\n\n## Introduction to LSST (Mario)\n\nSpecs:\n\n- 18K deg sq\n- 10mas astrometry\n- r<24.5 (<27.5 after 10 years)\n- ugrizy\n- 0.5-1% photometry at bright end\n- 3.2 Gpix camera\n- 30 sec exposure (split into 15 sec); 4 sec readout\n- 825 visits\n- 15 TB/night\n- 37B objects\n\nEnd goal is to turn the sky into a database.\nPeople will use the survey catalogs and images entirely.\n\n### Areas\n\n1. Time domain\n\n   - GRB, nova, supernova\n   - Source characterization of objects\n   - instantaneous discovery of new things\n\n2. Census of the solar system\n\n   - NEOs, Cometa, KBOs, Oort Cloud\n\n3. Map the Milky Way's tidal streams and galactic structure. Use RR Lyrae etc for distances. LSST covers just narrow beams through MW.\n\n4. Dark energy and dark matter.\n\n   - Strong lensing\n   - Weak lensing\n   - Constraining the nature of dark energy\n\n### Site\n\nCerro Pachon. Summit is called? XX\n\nDome is built to get the best ground effect and optimal seeing.\n\n### Camera\n\n- 3.2 Gigapixels\n- 0.2 arcsec pixels\n- 9.6 sq deg FOV\n- 2 sec readout\n- 6 filters\n- camera diameter is 1.65 m (5' 5\")\n\n### Data Management\n\n- 5 PB/yr of imaging\n- Each data release costs $120M (given project costs)\n\nOur challenges\n\n### Processing data in a way that enables science\n\n- `Model <- inference <- Data`\n- Ideally you'd *want* to do completely degenerative modelling of telescope+camera+galaxy to fit the galaxy, for example\n- Instead do `Model <- inference <- catalog <- data processing <- Data` (data processing means instrumental calibration + measurement)\n- In other words, the data is *reprojecting* and possibly *compressing* the dataset to make it easier to performance **inference** against.\n- LSST does more than most observatories! Most observatories require astronomers to do inference against raw data.\n\nGuiding Principles of how to process data and what to measure into catalogs:\n\n1. Maximize science enabled by the catalogs. Most LSST science will come from just the catalogs.\n2. Minimize information loss\n3. Provide and document the transformation (software)\n\nThis is captured in the LSE-163 Data Products Defintion Document. http://ls.st/dpdd\n\n### LSST from a user's perspective\n\n- **Level 1.** Stream of 10M time domain events per night. Detected and transmitted to brokers within 60 seconds. L1 also updates a catalog orbits for 6M bodies in the Solar System.\n- **Level 2.** Deep coadded images. Catalog of 37 billion objects (20B galaxies, 17B stars). 7T single-epoch detections and 30T forced sources.\n- **Level 3.** Ability to work with the catalogs and add analyses to the pipelines.\n\n### Project\n\n- KT: Planning and execution\n- Mario: vision\n- Jeff: resources and standards compliance\n\nLSST Team is about 50 people.\n\n### DM System Vision\n\nWe will enable LSST science by creating a well documented, state-of-the-art, high-performance, scalable, multi-camera, open-source, O/IR survey data processing and analysis system.\n\nWe will extensively test this system on pre-cursor data and surveys.\n\nWe will make it easy for anyone to deploy and run the software.\n\n### Sites\n\n- Summit/base: \n- 2 long-haul networks (200 Gbps to La Serena; 2x40 Gbit long hault)\n- NCSA pope\n\n### Applications\n\nApplications (Science Pipelines) carry the core scientific algorithms that process or analyze raw LSST data to generate output Data Products\n\nLSST Science Pipelines encompass a huge variety of analysis tasks. Slide of WBS numbers\n\n### Software choices\n\n- Python 2.7 codebase. Use Python whenever possible.\n- C++ for computationally intensive code made available via SWIG.\n- Modularity. Virtually everything is a Python module. 100+ packages.\n- Scons, Eups, git.\n\nWe're re-writing out own pipeline because the existing ones were hard to adapt.\n\n### Middleware.\n\n1. NSCA: Enable execution of science poplines on hundreds of thousands of cores\n\n   - Frameworks to construct pipelines\n   - Orchestration to execute on thousands of cores\n   - Control and monitor the whole DM system\n\n2. SLAC:\n\n   - FIXME Check slides\n\n### Delivery DBs with a UI\n\n- SLAC: Provide Qserve database. Parallel, distributed fault-tolerant relational database\n- Science User Interfaces to enable and access LSST data\n\n### Level 1\n\nRapid follow-up.\nNeed to transmit alerts within 60 seconds of readout.\n\nImage differencing algorithms need to work in 60 secs without adding false positives and measure interesting quantities.\n\n- position\n- flux, size and shape\n- light curves in all bands (up to a year; stretch: all time)\n- Variability characterization\n- Cut-outs centered on the object\n\nTimeline: see slides.\n\nNOTE: it'd be cool to make a D3 timeline with the sequence diagram.\n\nClose collaboration is needed to make this pipeline hum.\n\n### Level 2\n\n- Year 1 will have two data releases\n- Each DR will encompass all data take up to that time\n- DR1 will have 11B objets; 37B by DR10.\n\nMultifit: fit models of objects on individual images rather than measuring on the co-adds.\n\n### Level 3\n\nTo enable the community to create new products using LSST's software, services or computing resources.\nThis means\n\n- Providing the software primitives to construct custom measurement/inference codes\n- Enabling the users to run those codes at the LSST data center, leveraging the investment in I/O (piggyback on LSST's data trains).\n\nSoftware may well be the long-term legacy of LSST.\n\n**Users may create the most successful catalogs to come out of LSST; DR catalogs may just be one example that is most broadly applicable.**\n\n**We want to engineer our sofware and hardware to make this possible.**\n\n### Conclusions\n\n1. Keep the system vision in mind at all times.\n2. Question, learn, own the problem. You need to be a part of DM team culture.\n\n   - **Understand**: Think! Understand your work and why you're doing it. Don't just make it work; make it the best. Always understand the big picture.\n   - **Question and be curious**: Always strive to learn about why things are the way they are. **But don't settle; propose and argue for improvements. Don't fear giving feedback.** But also expect to be questioned about your decisions. Understand our Benevolent Dictator-ish model and commit to decisions that are taken.\n   - **Work with the Team, Deliver, and Earn Trust**. The ability to work with the team, to solve problems, and (ultimately) to deliver, is the currency of the team. This is how we earn trust from each other, and how we make progress.\n\nRead:\n\n- LSST Overview Paper arxiv.org/pdf/0805.2366\n- LSE-163 DPDD\n- LDM-148 System Design\n- LDM-151 Science Pipelines\n- LDM-152 Middleware\n- LDM-153 Database\n\n## Basic afw and Data Butler Concepts (KT-Lim)\n\nA summary of how to use the Stack from an end-user's perspective. More details about AFW tomorrow.\n\n### Why a stack?\n\n- Build a toolkit/framwork that can be plugged together from individual parts\n- Need standardized interfaces\n- Built to be scalable and portable (high performance computing)\n\n### Task abstraction\n\n(FITS not used internally? All data in memory in tasks?)\n\n- Tasks (algorithms) operate on *objects*, not physical representations (i.e., on files)\n- Escaping from the binary program + file metaphor\n- Allow tasks to be invoked in many contexts (CLI, large-scale production, from SUI/T, SDQA)\n- Allows data to be stored in many formats?\n\n  - Filesystem, tape, object store (like an archive), database\n  - Local or remote\n\n### What is afw?\n\n- Applications framework\n- Applications is really Science Pipelines\n- Framework is really a library or toolkit\n- Therefore: Library of astronomical image processing obects that can be used to build algorithms and pipelines\n\n### Image Data\n\n#### Image\n\n- 2D array of pixels\n- Pixel types: uint16, int32, float, double\n\n#### Masks are special types of images; bit masks (16-bits typically; probably 32-bit).\n\n- Every bit has a name associated with it.\n- Eventually add custom bits\n- Mask object contains metadata information (dict) that describes the bits.\n- TODO: we should publish a table of standard bits.\n\n#### Exposure\n\nThree planes plus metedata\n\n1. Image\n2. Mask\n3. variance\n4. plus metadata and additional objects (PSF)\n\nMetadata includes\n\n- WCS\n- PSF\n- Filter\n- Calib\n- Detector\n- PropertyList (key-value pairs of metadata)\n\n### Skymap\n\n- Overlapping **tangent plane projections (tracts)**\n\n  - Dodecahedral, declination band, specific positions, rings, HEADPixes.\n\n- Each divided into overlapping rectangular segments (patches)\n\n  - Inner regions exactly tile tract, overlap border within tract\n\nNote: we *could* decide to use a different pixelization in the future.\n\n### Table/Record\n\n- Like a relational (SQL) table or a FITS table\n- Columns of varying types, defined by a schema\n- Rows (records) for different measurements\n\nThis is completely custom. Lots of maintenance for a generic data type. But it is convenient.\n\nSQL aspect: tables can be related to other tables. **But** afw tables don't actually do SQL-like joins.\n\n### Data Butler\n\nAka *Data Client Access Services*.\n\n- Butler gets data, puts data, lists data\n- Butler manages translations between physical formats and internal afw objects\n- Butler does not do I/O itself; it is a framework for allowing I/O to be configured. The afw objects themselves know how to do I/O. Think of afw as a router or a switch.\n\nButler is really an interface layer. The actual work is implemented in a mapper\n\n#### Butler manages Repositories\n\n- A repository is a collection of datasets referenced by a path or a URL\n- In practice, today, it is a directory tree.\n- Repository structure defined by its mapper (and its mapper's configuration).\n- The mapper is selected by the `_mapper` file in a repository.\n- Mappers typically written by the camera/instrument/observatory experts that know the layout of camera's file.\n\n##### Repositories can be chained\n\nOutput repositories are automatically chained to the parent input repository.\nDatasets from anywhere in the parent chain can be retrieved.\nPointing to a child repository also lets you see/get objects from the parent repositories.\nThis is implemented in such a way that it *looks* like the parent data is part of the child's repository.\n\n#### Datasets\n\n- Anything that can be persisted or retrieved\n- Some datasets can *only* be retrieved; possibly only persisted\n- Range from numbers to images with metadata to entire catalogs\n\n##### Identifying Datasets\n\n- Dataset type\n\n   * Not Python or C++ dtype\n   * Each mapper can have its own list of dataset types (though there are some standard ones)\n\nHello world.\n\n- Data `id`\n  * key/value pairs\n  * Each dataset type can have its own list of keys\n  * Different mapper can require different keys\n- **Documentation of dataset types and data id keys is sparse.** Currently documented ad-hoc in `obs_*` package policy files.\n  * Need docs for available keys\n  * Need docs for available types\n- Data reference (for programming)\n  * Combination of Butler and data id\n  * It *does not refer to a dataset type.* Can be applied to multiple dataset types (if appropriate).\n\n##### Common Data ID Keys\n\n- `visit`, `ccd`, `filter`\n- `tract`, `patch`\n\n##### Partial Identification\n\nOnly unique key/value pairs need to be provided in a data id:\n\n- others are looked up in a registry database (registry is created by a script for a given camera)\n- registry typically generated from raw input data\n- allows for data to be looked up if there is no degeneracy to identify a dataset\n\nRendezvous:\n\n- Info from one dataset can be used to look up another dataset\n- For example: used to determine what calibration images apply\n\nYou could write your own rendezvous code and put it in the mapper.\n\n##### Common Dataset Types\n\n- Images and exposures\n\n  - raw (postISRCCD, visitim, icSrc, icMatch)\n  - calexp\n  - coaddTempExp\n  - deepCoadd-calexp (deepCoadd)\n\n- Calibration Exposures\n\n  - bias, dark, flat, fringe\n\n- Detection and Measurement Tables\n\n  - `src` (`srcMatch`), `src_schema`, `transformed_src`, `transformed_src_schema`\n\n##### Other Metadata Types\n\nFIXME See slides.\n\n### Summary\n\n**Tasks** use the Butler interface to operate on datasets, often Images or Exposures, identified by data ids within repositories, producing new dtasets, often new Exposures or Tables, in an output repository chained to the input repository. The Butler uses a camera-specific Mapper to define the repository structure, available datasets, and data id keys.\n\n## Using Tasks (John Swinbank)\n\n### What is a task\n\n- A coherent unit of work\n  - Unit of work varies from the trivial (add two numbers) to the complex (do everything needed to detect and measure sources, including ISR, calibration, source finding, etc)).\n- Tasks are combined hierarchically.\n\n```\nsetup pipe_tasks -t v11_0\ncd ${PIPE_TASKS_DIR}/examples\n```\n\nGrab some data from afwdata (FIXME)\n\n### Run on CLI\n\n```\n./exampleStatsTask.py small.fits\n```\n\n### Run from Python\n\nTasks can also be run form Python\n\n```\ntask = ExampleSimpleStatsTask()\nresult = task.run(maskedImage)\n```\n\nNote; there's nothing special about `run()`. In theory you can call any method on a task object. We don't use ``_call__()`` in order to be more transparent.\n\nThe `result` is a struct. You access results from attributes on the Struct.\n\n### Configuration\n\n```\nconfig = ExampleSigmaClippedStatsTask.ConfigClass(numSigmaClip=1)\nconfig.numSigmaClip = 2  # change attribute later\ntask = ExampleSigmaClippedSatsTask(config=config)\n```\n\n### CLI\n\nYou could wrap all the tasks in a homebrewed command line interface; but we shouldn't.\n\nUsing CmdLineTask provides us with a standard interface across all our tasks.\n\n- Includes interaction with Butler (read/write data, store task config and metadata)\n- Set and show config, parallelization\n- FIXME a third point.\n\n### Processing Model\n\n- Butler integration adds complexity\n- Rather than specify a filename on the CL, we specify the path to a repository and the data id\n\n```\nmyTask.py /path/to/repository --id data_id [options]\n```\n\nThe middleware will iterate over everything in the repository that matches `data_id` and call the task's `run()` method\n\nTasks come with a set of default configuration.\nThese defaults can be overridden within the hierarchy of sub-tasks.\n\n``--id`` on a task, without arguments, run on *all data in a repository*.\n\n``--show data`` will tell you what data it will work on.\n\n``--id filter=g`` selects data with that metadata\n\n``--id visit=1..3`` (include range)\n\n``--id visit=1..3:2`` with a *stride* of 2.\n\n``--id visit=1^3`` does an OR.\n\nSometimes you see both `id` and `--selectID`.\n\n### Configuration\n\n`--show config` is essentially python code\n\nSome Python configs can be done on the command line; but not all.\n\nInstead, use a Python config file. Pass it on the command line with ``-C``.\n\nMost tasks store their configuration ot the repositoy when they are run.\n\nTasks will refuse to run again if config on CL is different from that already persisted in the repository.\n\n### Composition\n\nWant to compose large tasks out of simple tasks.\n\n`--show tasks`\n\nWe can swap (or retarget) another task that has the same interface.\n\n### Additional repositories\n\n- Most tasks produce data products\n- These are normally written to the repo provided on the CL\n- We can use the `--output` option to specify a different repository\n\n### Debugging\n\n`--debug`. This causes it to import a `debug.py`. This `debug.py` has a `DebugInfo` class that can be called to help produce debugging information.\n\nNOTE: this needs to be combined with the logging framework.\n\nThe debugging options are stored separately of the tasks, so its easy for those docs to bit rot.\n\nCan also `--loglevel DEBUG`.\n\nNote: this is *not* the same as `--debug`.\nUse `--debug` to open DS9, etc.\n\nTo abort on error, `--doraise`. This way it stops before going on to the next data.\n\n### Parallelism\n\nUse `-j N` to run multiple data ids at once. This uses Python's `multiprocessing`.\n\nLarge scale parallelization will be done by the NCSA middleware.\n\n### Documentation\n\nSee `pipe_base` doxygen and the list of available tasks and their documentation on Doxygen.\n\n## EUPS (John Swinbank)\n\nExtended Unix Product Support.\n\nIt is a tool for managing multiple versions of interdependent software packages.\n\nIt allows for multiple version of the packages installed at once for both development and science.\n\n- Develop with today's version of the stack\n- fix bugs in yesterdays\n- reproduce your science run from last year\n\nEUPS manipulates your environment to make this tractable.\n\n```\nloadLSST.bash\neups path\n# retarget\nunsetup lsst\nmkdir -p\n```\n\nNOTE: need to get product/package terminology straight.\n\nMost commands are in `eups`. `setup` and `unsetup` are special and used on their own.\n\nPackages Table (.table) files are used tell eups how to prepend the PYTHONPATH to include source from the package.\n\n```\neups list\neups tags\n```\n\nHow are the current/latest tags being uploaded? Is that only if you're sharing a stack?\n\nCommands to try\n\n```\neups list\neups list -s\nsetup -v lsst_apps\nmore ${LSST_APPS_DIR}/ups/lsst_apps.table\n```\n\n### Eups distrib\n\nThe package distribution mechanism used with eups.\n\n```\neups distrib path\n```\n\nFetch and install packages defined on some remote server\n\n```\neups distrib list lsst_apps\neups distrib install -t v11_0 lsst_apps\n```\n\nTo see tags go to https://sw.lsstcorp.org/eupspkg/tags\n\n### Conclusions\n\n- EUPS is a third party package\n- GitHub issue tracker for problems with EUPS itself\n- Report stack installation problems on JIRA\n- Tips on trac: https://dev.lsstcorp.org/trac/wiki/EupsTips\n\n## Data Management Organization and Management (Jeff Kantor)\n\nWe're a NSF MREFC Project. More on that later.\n\n### DM Team\n\n- 28 new Team members added nsince MREFC\n- DM Staff is now 48 people\n- FY16 is to have 50 FTE\n- Trick is to keep the expertise/experience high as we staff up\n- DM is a highly distributed team. Multi-technology. Multi-cadence.\n  - IPAC\n  - SLAC\n  - UC Davis\n  - UW\n  - NCSA\n  - SLAC\n  - NOAO\n  - FIU\n  - REUNA\n\nThe project organization is described in [LDM-294](http://ls.st/LDM-294).\n\n### Institution Organization\n\n- Each institution has a Technical/Cost Account Manager (T/CAM) and a Scientific/Technical Lead (S/TL). They can two people or a single person.\n- T/CAM plays the management role (staffing, etc)\n- DM Leadership Team of S/TLs and T/CAMs meets weekly.\n- Science/Architecture Team (SAT)\n  - Meets monthly (or as superseded by RFD).\n  - Working Groups (WG): Applications, Middleware, Infrastructure, Database, Operations\n- Technical Control Team (TCT)\n  - Collection-2511 - this is the technical baseline/commitment for what we deliver.\n  - Meets as necessary\n\n### LSST DM is part of a NSF MREFC\n\n- We are members of the LSST Project\n- The LSST Project is being funded and conducted as a joint project by the NSF and the DOE, with participation by Chile and other International Partners\n- The LSST Camera is funded as Major Item of Equipment (MIE)\n- The other parts of the project (PMO, DM, EPO, System Engineering) are funded as Major Research Equipment and Facility Construction (MREFC).\n- We get money yearly; overall Construction has $1B from NSF, other amount from DOE.\n- There are **many** rules about what is allowed on project funding\n  - The rules vary somewhat between NSF and DOE\n  - With NSF, DM pipelines is *ok*, but doing general science is *not ok*.\n  - E.g. Can write papers about the pipelines, but not about the science.\n\n### Regularly Scheduled Meetings\n\n- Online meetings\n  - Online meetings (SAT, DMLT, TCT, WGs)\n  - WBS Oriented Meetings (SUI, Infrastructure)\n  - Technical Coordination Meetings (e.g. coordination stand-ups)\n- Face to Face meetings\n  - with Entire DM Team\n    - LSST Project and Community Workshop in August, location varies.\n    - Annual DM All hands Meeting in February, location varies. Usually February.\n  - DM LT: At DM Team Meetings plus in October/November, May/June, location varies.\n\nFor schedule DM Meetings and DM Travel google calendars.\n\n### Collaboration Tools\n\n- Mailing lists (lists.lsst.org). dm-announce, dm-staff, dmlt, etc. Some are *controlled* subscription to reach out to certain people. Others are self-subscribing.\n- [HipChat](http://hipchat.org). Use for informal discussions. May end up retiring it/moving to SLAC.\n- [Community Forum](http://community.lsst.org)\n- [Confluence](http://confluence.lsst.org). Project standard.\n- [JIRA](http://jira.lsstcorp.org). Project standard.\n- [URL shortener ls.st](http://http://ls.st)\n- [Google Hangouts](http://ls.st/sup)\n- [project.lsst.org](http://project.lsst.org)\n  - Travel\n  - Contacts\n  - Calendars\n  - Interactions database: add your interaction there to give NSF advance notice. If you're not representing NSF/DOE/or funding profile, it doesn't matter.\n  - Risk Management\n\n### DM Planning Process\n\n1. Top Level\n   * LSST System Requirements and Operations Plans\n   * Proejct Risks and Milesotones\n2. DM System Reqs and Roadmap (LDM-240)[http://ls.st/LDM-240]\n   - High-level picture\n   - 6-month granularity\n3. PMCS\n   - Institutional-level resource assignments\n   - Budgeting\n   - Earned-value management\n   - Basis of montly reports\n4. FIXME see slides for next level\n\n### JIRA\n\n- Software Development and sme non-software efforts are planned and managed at the task-level using JIRA and JIRA Agile\n  - Web based interface\n  - Issues cover essentially all task-based work\n  - Tracks all history and actions on the Issue being updated\n  - Excellent tool to collect status on the work being performed\n- Add JIRA issue as soon as you suspect something is an issue or needs to be done\n- Use Request for Comment to notify and discuss an issue or need that is more complex or larger scope.\n  - If no one raises issues, you have implicit approval\n\n### JIRA Agile\n\nJIRA agile adds agile process to JIRA in the form of stories, epics and sprints, kanbans.\n\n- Software development is generally schedule in sprints, with other activity are generally kanbands\n- Developers and T/CAMs create stories (new developments) improvements (to existing code) and bugs (fixes)\n- Stories are preferably 2-20 story points\n- Stories are complete or not; we don't track progress in side stories\n- Story completion adds to the completion level of their Epic\n- New Stories can be added to an Epic during the cycle\n- Report when a story is done to your T/CAM (same day if possible) or mark it done in JIRA. Real-time update is necessary to show monthly progress to NSF.\n- Deprecate stories by marking 'Invalid' or 'Won't Fix'.\n- If a Story or Epic becomes irrelevant, smaller, or larger, let your T/CAM know.\n\n### JIRA to PMCS\n\nJIRA activities get sucked into the Project Management Control System (PMCS) for scheduling and Earned Vallue Management (EVM).\n\nTo enable EVM, we do a \"rolling wave\" plan for six-month release cycles.\n\n### Documentation and Document Management\n\n- Working versions of documents are developed in a variety of tools (wikis, LaTeX, Acrobat.\n- Official baseline versions are in docushare\n- LSST requirements and interface control documents are in collection-2808 and collection-2807.\n- LSST monthly technical progress reports are in collection-3826\n- DM requirements and design documents are in collection-2511\n- DM monthly progress is in collection-221\n- There are many more related documents, both management and technical, ask your T/CAM if you need to find something.\n\n## DM Code Structure (Tim Jenness)\n\n### Overview\n\n- C++11 and Python 2.7\n  - Use python whenever possible\n  - Use C++ for performance only\n- We do not use Cython/Numba.\n- Intending to support Python 3 soon\n- GCC 4.8 minimum supported compiler\n- Clang supported\n- SUI uses Java 1.6 server side, JavaScript on client side\n- Linux CenOS 6 and 7; OS X Yosemite, Mavericks are test platforms. No El Cap\n- All code in lsst namespace\n\n### Package overview\n\n- orchestration: crtl\n- Data access framework (Butler): daf\n- pipeline execution frameworks: pex\n- afw\n- Image Processing: ip\n- Measurement: meas\n- Coaddition: coadd\n- Pipeline infrastructure: pipe\n- Control display tools: display\n- Camera specific mappings: obs\n- Web vis of images/catalogs: firefly (Java)\n- Data access: dax\n\nSee confluence for a list of pages\n\n### Middleware: log\n\n- log package has C++ / Python interfaces\n- Runs on log4cxx\n- INFO/WARN/DEBUG/TRAVEL levels\n\n### Middleware: exceptions\n\n- pex_exceptions proves a set of C++ exceptions that can be caught in Python code.\n- Use native Python exceptions when appropriate.\n- Exceptions include: LogicError, DomainError, InvalidParameterError, etc\n\n### Support packages\n\n- `daf_base`:\n  - Datetime handling\n  - PrpertySet/List\n- `ndarray`: C++ implementation of numpy. Translated into numpy on Python side; use `eigen` for fancy stuff.\n- `geom`: cartesian and spherical geometry\n- `db`: database access utilities\n\n### Third-party packages\n\n- 3rd party packages are distributed as EUPS-managed packages installable with `eups distrib` or `lsstsw`\n  - They are versioned\n  - LSST packages list them as explicity dependencies\n- Numpy and matplotlib are presumed to have been installed by other means (but this is checked)\n- Some 3rd party packages are not expected to be called directly\n- New packages can be requested via the RFC process\n  - Using 3rd party packages is encouraged rather than reimplementing a wheel\n  - It's easy to make a new eups package\n- 3rd party Python packages are currently distributed in this way and not via pip (there's an RFC for this)\n\nAvailable packages include:\n\n- cfitsio\n- eigen (linear algebra)\n- boost (though we're trying to move away from boost towards C++11)\n- wcslib\n- fftw\n- gsl\n- minuit2\n\nFull list at https://confluence.lsstcorp.org/display/DM/DM+Third+Party+Software\n\nTODO: move this page to git/the docs.\n\n### Structure of a package\n\n```\nbin  # executables / python scripts\ndoc  # doxygen\nexamples  # examples\ninclude\nlib\npython\nsrc\ntests\nups\n```\n\n### Building a package: scons\n\n- Scons is used to build LSST software\n- To build and test a package\n  - `setup -k -r .` to ensure tests use the newly-built code\n  - `scons -j 4 opt=3` for parallelized build\n- `scone -j 4 python` will just build the python code\n- Sconstruct file is used by scons to configure the build\n- SConstruct files are used for subsidiary configuration in subdirs.\n\n### LSST extensions to Scons: sconsUtils\n\n- EUPS version and dependency tracking\n- Compiler detection (clang vs gcc and how to add C++11 support)\n- Swig interface building\n- How to run tests\n\nSee Sconsfile example in slide\n\n### Python code\n\n- Lives in the `python/lsst` directory\n- If your package is named something_else the code will be located in python/lsst/something/else\n- Each sub-directory (lsst and below) must have an `__init__.py` that contains\n\n```\nimport pkgutil, lsstimport\n__path__ = pkgutil.extend_path(__path__, __name__)\n```\n\n(`import lsstimport` seems to be for Swig support)\n\nThis boilerplate is required to setup the python namespace.\n\n### Swig\n\n- http://www.swig.org\n- Parses C++ header files and generated Python wrapper code\n- Interface is defined in `.i` files that live in the `python/lsst` tree.\n- `meas_base` `.i` files are in `python/lsst/meas/base`\n- Swig generates .py file and shared library. For `meas_base` baseLib.i generates baseLib.py and `_baseLib.so`.\n  - The other .i files are included by baseLib.i\n  - The Sconscript file in the same directory tells scons that only baseLib is relevant\n  - Look in the `_wrap.cc` file to see what Swig has generated.\n\n### Third-party package structure\n\n- 3rd party packages are wrappers around the standard distribution files\n- EUPS, via eupspkg, knows how to build different styles of distro: Python's setup.py, autoconf and cmake.\n\nDirectories:\n\n- `ups`: EUPS configuration, including dependencies and build instructions\n- `upstream`: upstream unmodified distribution tar file\n- `patches`: patches to be applied to the distribution before building it (this is generally an anti-pattern)\n\n### Testing\n\n- Each package has associated unit tests\n- Python tests use unittest\n  - `assertEqual`, `assertLess`\n  - Only use `assertTrue` if you are really testing truth\n- Use decorators to skip tests; don't comment those tests out\n- C++ tests use boost\n- Aim for new code to come with associated unit tests\n  - Code coverage is less than ideal at present but aiming to begin gathering metrics on this\n  - Integration tests are used to test the stack as a whole\n- Python tests will soon be run via a standard python test environment such as `nose` or `py.test`.\n\n  These will give significantly better test output handling in the Jenkins continuous integration system.\n\n### Documentation\n\n- Currently the doc directory.\n- Doxygen format used for method and class descriptions inline.\n- Currently moving to reST and numpydoc format and aiming to integrate into ReadTheDocs.\n\n### The ups directory\n\n- The `ups` directory teaches EUPS and `sconsUtils` how the pacakge relates to other pacakges and how to configure it when it is setup.\n- The `.table` file lists dependencies and environment variables for EUPS\n- The `.cfg` file contains configuration information for `sconsUtils`\n- The `eupspkg.cfg.sh` provides overrides and additional information to allow eupspkg to build a package.\n\n### Coding standards\n\n- We want to move our code standard to a 'diff' against PEP8\n- **New code can immediate use PEP8 naming**\n- C++ coding standard\n\n## afw (Simon Krughoff)\n\n### cameraGeom\n\n- A system for representing transformation between different coordinate systems in the optical system\n- Utilities for building cameras\n\n### coord\n\n- Coordinate construction and conversion\n- Implements the following\n  - FK5, etc\n  - FIXME add more\n\n### detection\n\n- threshold\n- footprint (collection of pixels + metadata)\n- HeavyFootprint (FIXME what is a HeavyFootprint?)\n- Psf\n\n### geom\n\n- low-level image geometry\n- Angle, Box, Extent, Point, Span\n- Ellipse, Polygon\n- XYTransforms: Affine, Identity, Inverted, Multi, Radial, Separable\n  - these are *reversible* transformations\n\n### image\n\n- utilities to actually manipulate images\n- Image things:\n  - Image: single grid of pixels (float, double, int, uint)\n  - Mask: grid of bit mask pixels with associated mask plane definitions\n  - DecoratedImage - Image with metadata (deprecated)\n  - MaskedImage - Image + Mask + Variance. Most of the stack uses these\n  - Exposure - MaskedImage with associated image things: WCS, Psf, metadata, calibration info.\n  - Side note: we should *always* use Exposures as the interface between tasks\n- Other associated things:\n  - Defect\n  - Filter\n  - Calib (this is really photometric calibration), Wcs\n\n### math\n\n- Statistics (mean, stdev, var, median, inner quartile range, clipped stats, min\n- Kernels\n- Convolution\n- Interpolation and approximation\n- Fitting\n- Functions\n- Splines\n- Random number generator\n- Warping - Lanczos, bilinear, etc\n\n### table\n\n- Tables are really catalogs with fixed schema. The schema is flexible and can be set up to do lots of things\n  - Store amp electronigcs info\n  - Source catalogs\n  - Match catalogs\n\n### How to find things\n\n- Doxygen: but it's hard to find Python documentation in the doxygen\n- GitHub code\n- unit tests (ugh, so bad we use unit tests for docs)\n- Help strings in Python (but not useful when Swig'd)\n- Search with an editor\n\n### Example\n\nSee code on bootcamp repo\n\nModifiers on method names (e.g., BoxI) are due to Swig.\n\n```\nbox - afwGeom.BoxI(afwGeom.PointI(200, 500),\n                   afwGeom.ExtendI())\nim = afwImage.ImageF(box)\n```\n\n**Gotcha** the LLC is not 0, 0. This bounding box is relative to a global coordinate system called PARENT (the default)\n\nSub images are *views* on the original array\n\nCan't add images of different types. Need to explicity use the *convertX()* method.\n\nafw.detection can be used for low-level detection based on thresholding to create *footprints*.\n\n```\nafw_detect.FootprintSet(masked_im, threshold, 'DETECTED')\n```\n\nwhere DETECTED is the name of a bit in the mask.\n\nafw_math has background estimation tools.\n\nOnce you have footprints set, you can easily do detections on each footprint.\n\n```\nim, mask, var = masked_im.getArrays()\n```\n\n(those are views)\n\nThe `>>=` operator does?? reflections??\n\nYou can also do operations on the numpy arrays.\n\nNumpy uses (y, x) indexing with afw.image uses (x,y) indexing.\n\n### Understanding afw\n\nThe original intent was to keep the C++ environment rich.\nThis led to classes not just functions defined in C++.\n\nTakeaway: the Python/C++ line is hard to draw. May be better not to expose all of afw C++ to python.\n\n## Source Measurements and Tables (Jim Bosch)\n\nMeasurement and tables are related because measurements are delivered in tables.\nthe talk will discuss how to write a measurement task and plugin.\n\n### Footprint\n\nRegular footprints describe on regions of pixels.\n\nHeavy footprints describe both regions and the values of the pixels.\n\n### Flow\n\n#### Detect\n\n1. Exposure\n2. pixels, Psf\n3. SourceDetectionTask\n4. parent Footprints\n5. Source Catalog\n6. SourceDetectionTask sends DETECT mask plane back to the Exposure\n\nThe DETECT step creates Footprints\n\n#### Deblend\n\nTakes Footprints. May split them to create Heavy footprints (i.e., child footprints) that divide flux of the regular (parent) footprint.\n\n#### Measure\n\nTake footprints, wcs information, etc., and now measure quantities for the \n\n```\nreplace all detections with noise\n\nfor record in catalog:\n    restore pixels from HeavyFootprint\n    run plugins\n    re-replace pixels with noise\n\napply aperture corrections\n```\n\nNote this potentially double-counts flux.\n\n### Running a Single Frame Measurement Task\n\n1. Initialize a minimal schema: `lsst.afw.table.SourceTable.makeMinimalSchema()`\n2. Customize the detection task\n3. Call the Detection task, passing the config and the schema.\n4. Initialize other tasks (deblend, etc) to fully setup the schema\n\n#### Notes on configuring a measurement task\n\n- list of active plugins is a set\n- need to 'bless' which plugin's output is called the \"Model Flux\"\n\n#### SourceTable\n\n- SourceTable is really a factory for SouceRecords, not a container for them\n- Pass Source table and an Exposure to the Task.run() method.\n- Pass Exposure and Catalog to  SourceDeblendTask.\n- Finally run the measure task.\n\n### Writing a  SingleFramePlugin\n\n1. Config class\n2. Plugin\n   - registers with `@lsst.meas.base.register('ext_BoxFlux')`\n   - `ConfigClass = BoxFluxClass`\n   - Classmethod `getExecutionOrder`\n   - All Plugins have hte same `__init__` signature\n   - `measure` method takes a measRecord and exposure where all neighbours are replaced with noise\n\nThe Plugin has a run order. BasePlugin has the execution orders.\n\nSee slides/video for tutorial in writing a task.\n\nDiscussion of schema column names. There are strong conventions.\n\nUse `schema.join()` to add fields to the schema. Some algorithms have strong naming conventions.\n\n```\nmeasRecord.get()  # get and set always work\nmeasRecord[]  # does not always work;\n```\n\n### Error Handling in Plugins\n\n- If the plugin will fail on all sources because of mis-configuration, raise `lsst.eas.base.FatalAlgorithmError`\n- If a known failure mode, set at least two flags\n  - Set flags as fields in `measure()` method, or\n  - Re-raise as `lsst.meas.base.MeasurementError` and set flags in `fail()`.\n- All other exceptions will trigger warnings, and `fail()` flags should be set\n\n### Unit Transformations\n\nSee slides from talk.\n\n### Plugins in C++\n\nPlugins can also be written in C++ (and most current ones are). Unfortunately, the APIs make it easier to actually write plugins in C++.\n\n### Forced Measurement\n\nMeasure sources on an image while holding the centroids fixed (or other values).\n\n### Tables\n\n#### Records and Keys\n\nBase record has\n\n- data\n- block\n- table (a record remembers the table that created it and relies on it to know is own schema)\n\nA Key has an offset\n\nRecord access is some kind of magic BS.\n\n### Records and Tables\n\nTable table has\n\n- data\n- metadata (`PropertyList`)\n- block\n\nThis is why a table is actually just a factory that knows its schema.\n\nA BaseCatalog is really just a vector of pointers to records.\n\n- No guarantee the recors are in the orer they were allocated in\n- no guarantee that the reords are from the same Block\n- No guarantee that records are even from the same Table or schema.\n\n### Source Records\n\nSource Records differ from BaseRecords in that they have a **footprint**.\n\nSourceRecords have a minimal schema\n\nSourceRecords and SourceCatalogs have special getters for certain predefined field names. E.g.,\n\n- `getX` -> get(`slow_Centroid_x`)\n\n### Record/Table subclasses\n\n- `SimpleRecord` used by reference entries for astrometry\n- `ExposureRecord` holds metadata about an exposure, including the Psf and Wcs. Mostly used by CoaddInputs and CoaddPsf\n- `AmpInfoRecord` used by the camera geomtry module to store both structured and flexible information about amplifiers\n- `PeakRecord` used to store peaks within a Footprint\n\n### Getting arrays from columns in Catalogs\n\n```\narray = catalog['field']\n```\n\ngets a numpy view of a column.\n\nThis only works for contiguous in memory; otherwise\n\n```\ncatalog = catalog.copy(deep=True)\n```\n\nUse the `isContiguous` method to test.\n\n### Catalog Indexing\n\n- Pass strong or Key to extract column (return numpy ndarray)\n- Pass integer, a slice, or boolean array as `catalog[x]` to index the rows (returning a record or a subset catalog)\n\nGotchas\n\n- You can't pass an array of row indices (not implemented yet)\n- Boolean indices don't force a copy so they generally return a noncontiguous Catalog (this is intentional - we want all copies explicit)\n\n### Adding/removing columns\n\nOnce we've created a table, we can't change the Schema.\n\n- Whenever you ask a Record/Table/Catalog you get a copy, so modifying it won't do what you want.\n\n### To add a column (schema mappers)\n\nNeed to\n\n1. Get a SchemaMapper\n2. Add a minimal schema from the original catalog\n3. Add a field to the schema\n4. Create an empty catalog with the Schema Mapper's output schema\n5. Add all records from the old catalog to the new one, using the SchemaMapper to copy values\n\n### Flag Fields\n\nKey for a flag field has\n\n- offset\n- bit\n\nFlag column arrays are copies, not views.\n\nOnly get/set are supported, not `[]`.\n\n### Slots and Aliases\n\nWe've already mentioned the \"slot\" system, which adds getters to SourceRecord and SourceCatalog for some predefined names. Those names are usually *aliases* to real field names.\n\nA Schema holds its aliases in an attached AliasMap. Aliases are *just* string mappings. There is some globbing-ish functionality to define lots of aliases at once.\n\n### FunctorKeys\n\n`FunctorKeys` provide a mechanism to get first-class objects our of one or more fields in a record.\n\nFor example, creating coordinates from separate x/y columns (i.e., packing/unpacking data structures)\n\nCan even do things like get magnitudes from flux values.\n\nShould be possible to create FunctorKeys in Python (there are separate implementations in Python and C++).\n\n### Summary of afw.table Idiosyncracies\n\n- [record, table, catalog.schema returns a copy. **But** that copy shares an AliasMap wth the original!\n- Can't use `[]` on Flag fields (only get/set)\n- Using an array of booleans to index a catalog returns a noncontiguous view\n- another one\n\n### More info\n\n- afw.table reference doxygen page is decent\n- print(schema) to get a useful introspection of the schema\n- docstrings are useful\n- Doxygen reference documentation is quite complete, but often only applies to the C++ interface.\n- If something seems weird, just ask particularly on community.lsst.org.\n\n## Orchestration and Control (Steve at NCSA)\n\n- Develop and test locally\n- Take same code and run it in production\n\n### LSST package ctrl_orca - Orca\n\n- software setup, execution, monitoring and shutdown across multiple machines\n- internall uses condor, but the user doesn't need to know\n- User specified parameters in Config files\n\n### ctrl_execute\n\n- Simplifies the execution or orca\n- writes configuration files for orca\n- duplicates local execution environment remotely\n\nLocal packages that are setup with eUPS are duplicated remotes to more easily replicate any errors locally\n\n### Quickstart\n\n```\nsetup ctrl_execute\nsetup ctrl_platform_lsst\n```\n\n```\nrunOrca.py -p lsst -c \"processCcdSdss.py sdss /home/user/input --output ./output [more args]\"\n```\n\nSimple test\n\n```\nrunOrca.py -p lsst -c \"/bin/echo\" -i $HOME/ids.txt -e ~/lsstsw\n```\n\nuse\n\n```\ncondor_q\n```\n\nto see the status of jobs.\n\nHTCondor output files are put into FIXME a directory.\n\nThe application output is put into `/lsst/DC3root/{usr}_{date}_{id}`\n\nYou can use `condor_rm` to quit jobs.\n\nSee https://confluence.lsstcorp.org/display/DM/Orchestration for more information.\n\n## Russell Owen\n\nSee the doxygen page for LSST tasks\n\nNot all tasks are documented at this time.\n\nHere we'll look at three Example tasks.\n\n### How to write a task\n\nTasks are subclasses of `lsst.pipe.base.task`\n\nEvery task needs a configuration.\nConfiguration parameters are set once and apply to any dataset being passed.\nThey should be defaults.\nThings that change all the times shouldn't be Configs, they should be arguments.\n\nSee `ExampleSigmaClippedStatsTask`.\n\nThe Config is a subclass of `pexConfig.Config`. You add configs; each gets a type, documentation and default value.\n\nConfiguration fields\n\n- range with min/max acceptable fields\n- subtasks `ConfigurableField` (this way you can retarget subtasks by modifying the configuration)\n\n`_defaultName` is required for Command Line tasks. This is crucial for top-level (command-line) tasks. The name of the task defines where the configuration is persisted.\nName of the subtask is `{top level _defaultName}.{subtask _defaultName}`\n\n`RunnerClass` is needed by Command line tasks that allows multiprocessing\n\n`canMultiprocess` is only needed for command-line tasks; see canMultiprocess for more information\n\n#### init method of tasks\n\n`__init__`\n\n- must make any subtasks with `self.subtask`. The argument of this method then becomes the *attribute* of the task.\n- you will also want to construct the schema since you need to construct a schema before you construct any table.\n\n**All instance variables in a task must be static; pass those as arguments. This is probably for thread safety.**\n\n#### Run method of tasks\n\n`run` is the primary entry point. not special; some tasks have multiple entry points.\n\nDon't *put all code in the run method*. You want to break it down into sub-tasks so that re-targeting is possible.\n\nThe `run` method taskes a `dataRef`.\n\nWhen entering run, say what data you're processing with self.log.info(\"Processing data ID %s\")\n\nRun\n\n```\nif self.config.doFail:\n    raise pipeBase.TaskError(...)\n```\n\nif it was a data-generated error that shouldn't print a traceback\n\nWe get the data by asking the dataRef, e.g.:\n\n```\nrawExp = dataRef.get(\"raw\")\nmaskedImage = rawExp.getMaskedImage()\n```\n\n#### Using lsst.debug\n\n```\nimport lsstDebug\ndisplay = lsstDebug.Info(__name__).display\nif display:\n    frame = 1\n    mtv(rawExp, frame=frame, title=\"exposure\")\n```\n\nThe task docs should tell you how the display is used and what arguments it takes.\n\nNote there's newer display code available than the example shown here.\n\n#### Subtask\n\n- document it!\n- subtasks should return a `pipeBase.Struct` (dictionaries with named attributes). This way a subtask can return *extra* data without breaking interfaces.\n\n#### Stats\n\n```\nafwMath.makeStatistics(maskedImage, afwMath.MEANCLIP | afwMath.STDEVCLIP | afwMath.ERRORS, stats_control)\n```\n\n### Command-line Tasks\n\nCommand line tasks always take a dataRef and have a run method.\n\nCommand line tasks are subclasses of `lsst.pipe.base.CmdLineTask`.\n\nBy default has a run method that takes a dataRef. You need more arguments, you need to provide an ArgumentParker and often a CustomTaskRunner.\n\nEvery command line task needs a `TaskRunner`; the default one is usually good.\n\nThese task runners are put into `bin` and get added to the global task namespace.\n\n#### Persisting Config and MetaData\n\nUse dataset types\n\n* `_DefaultName_config` for configuration\n* `_DefaultName_metadata`` for metadata\n\nIf these are `None`, there's no persistence. Useful for one-off tasks.\n\n#### Argument Parser\n\nDefault argument parser is appropriate for working with raw exposures.\nIf you want to work with coadds, you'll need a new arguemnt parker.\n\nAdding new DatasetTypes will be awesome with the new Butler.\n\n#### Custom task runners\n\nNeed a custom task runner fi you need different arguments, or don't need data references/a butler.\n\nE.g.:\n\n```\nclass CoaddTaskRunner(pipeBase.TaskRunner):\n    @staticmethod\n    def getTargetList(parsedCmd, **kwargs):\n        return pipeline.TaskRunner.getTargetList(...)\n```\n\n## Creating an obs_ package\n\nThere is an obs package for each observatory.\nDefines keys in header of raw data; camera geometry; overrides for tasks that specific to the camera, and color term corrections.\n\n### Camera mapper\n\ncamera maper makes the following available to butler\n\n- policy file with organization of data in repositroy\n- camera geometry\n- defect registry (locations fo bad pixels)\n\nAnd also contain\n\n- camera specific configuration for tasks\n\nButler 2.0 will make it easier to define new dataset types. Camera mapper and policy files will be affected in the future.\n\nCamera mapppers subclass lsst.daf.butlerUtils.CameraMapper. \n\n### Policy file\n\n- type and location of each supported \"dataset type\" (e.g., where raws are, where calibrated exposures go, etc.)\n- location of camera geometry information\n\n### Camera geometry\n\n- estimate plate scale and optical distortion\n- position of the detector in teh focal plane\n- amplifier gain and read noise\n- serial number of each detector\n\nSome information might be time variant. e.g., new filer. new detector. This is a known limitation that hopefully the new Butler will fix.\n\n### Making a camera mapper\n\nthe `\\_makeCamera` method unpersists the camera geomtry. You may override this method, but the default is good.\n\nthe default requires hat you policy have n entry \"camera\" containing a camera.py and has a FITS-based amp info table.\n\nYou get this information from a camera team. Then probably write a script that takes data from teh observatory and transforms it into the camera's format.\n\nThe known bad pixels are in an sqllite db file.\n\n### Camera specific tasks and configurations\n\n- config/taskanem.py\n- config/cameraname/taskname.py\n\nUse the configs in here to retarget subtasks to use camera-specific subtasks (especially useful for ISR).\n\nnote processCcdSdss.py exists because its so different from normal data (already reduced). You can't retarget a top-level task.\n\nnote that colorterms, at least in HSC, are in config files. These should really be part of a database.\n", 
  "id": 43850843
}