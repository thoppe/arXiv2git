{
  "read_at": 1462550898, 
  "description": "Implement feature hashing with R", 
  "README.bib": "@inproceedings{DBLP:conf/kdd/McMahanHSYEGNPDGCLWHBK13,\n  author    = {H. Brendan McMahan and\n               Gary Holt and\n               David Sculley and\n               Michael Young and\n               Dietmar Ebner and\n               Julian Grady and\n               Lan Nie and\n               Todd Phillips and\n               Eugene Davydov and\n               Daniel Golovin and\n               Sharat Chikkerur and\n               Dan Liu and\n               Martin Wattenberg and\n               Arnar Mar Hrafnkelsson and\n               Tom Boulos and\n               Jeremy Kubica},\n  editor    = {Inderjit S. Dhillon and\n               Yehuda Koren and\n               Rayid Ghani and\n               Ted E. Senator and\n               Paul Bradley and\n               Rajesh Parekh and\n               Jingrui He and\n               Robert L. Grossman and\n               Ramasamy Uthurusamy},\n  title     = {Ad click prediction: a view from the trenches},\n  booktitle = {The 19th {ACM} {SIGKDD} International Conference on Knowledge Discovery\n               and Data Mining, {KDD} 2013, Chicago, IL, USA, August 11-14, 2013},\n  pages     = {1222--1230},\n  publisher = {{ACM}},\n  year      = {2013},\n  url       = {http://doi.acm.org/10.1145/2487575.2488200},\n  doi       = {10.1145/2487575.2488200},\n  timestamp = {Tue, 10 Sep 2013 10:11:57 +0200},\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/kdd/McMahanHSYEGNPDGCLWHBK13},\n  bibsource = {dblp computer science bibliography, http://dblp.org}\n}\n\n\n@article{zhang2014real,\ntitle={Real-Time Bidding Benchmarking with iPinYou Dataset},\nauthor={Zhang, Weinan and Yuan, Shuai and Wang, Jun and Shen, Xuehua},\njournal={arXiv preprint arXiv:1407.7073},\nyear={2014}\n} \n\n@inproceedings{DBLP:conf/icml/WeinbergerDLSA09,\n  author    = {Kilian Q. Weinberger and\n               Anirban Dasgupta and\n               John Langford and\n               Alexander J. Smola and\n               Josh Attenberg},\n  title     = {Feature hashing for large scale multitask learning},\n  booktitle = {Proceedings of the 26th Annual International Conference on Machine\n               Learning, {ICML} 2009, Montreal, Quebec, Canada, June 14-18, 2009},\n  pages     = {1113--1120},\n  year      = {2009},\n  editor    = {Andrea Pohoreckyj Danyluk and\n               L{\\'{e}}on Bottou and\n               Michael L. Littman},\n  url       = {http://doi.acm.org/10.1145/1553374.1553516},\n  doi       = {10.1145/1553374.1553516},\n  timestamp = {Mon, 29 Sep 2014 17:13:49 +0200},\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icml/WeinbergerDLSA09},\n  bibsource = {dblp computer science bibliography, http://dblp.org}\n}\n\n@proceedings{DBLP:conf/icml/2009,\n  editor    = {Andrea Pohoreckyj Danyluk and\n               L{\\'{e}}on Bottou and\n               Michael L. Littman},\n  title     = {Proceedings of the 26th Annual International Conference on Machine\n               Learning, {ICML} 2009, Montreal, Quebec, Canada, June 14-18, 2009},\n  series    = {{ACM} International Conference Proceeding Series},\n  volume    = {382},\n  publisher = {{ACM}},\n  year      = {2009},\n  isbn      = {978-1-60558-516-1},\n  timestamp = {Thu, 19 May 4451261 05:22:40 +},\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icml/2009},\n  bibsource = {dblp computer science bibliography, http://dblp.org}\n}\n\n", 
  "README.Rmd": "---\noutput: \n  html_document: \n    keep_md: yes\n---\nFeatureHashing\n==============\n\nLinux: [![Travis-ci Status](https://travis-ci.org/wush978/FeatureHashing.svg?branch=master)](https://travis-ci.org/wush978/FeatureHashing)\nWin : [![Build status](https://ci.appveyor.com/api/projects/status/bm4lpxn5f07d8klj/branch/master?svg=true)](https://ci.appveyor.com/project/wush978/featurehashing/branch/master)\nOS X: [![Travis-ci Status](https://travis-ci.org/wush978/FeatureHashing.svg?branch=osx)](https://travis-ci.org/wush978/FeatureHashing)\n\n\n[![Coverage Status](https://img.shields.io/coveralls/wush978/FeatureHashing.svg)](https://coveralls.io/r/wush978/FeatureHashing?branch=master)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/FeatureHashing)](http://cran.r-project.org/package=FeatureHashing/)\n[![rstudio mirror downloads](http://cranlogs.r-pkg.org/badges/FeatureHashing)](https://github.com/metacran/cranlogs.app)\n\nImplement feature hashing with R\n\n```{r setup, include=FALSE}\nlibrary(knitcitations)\nbib <- read.bibtex(\"README.bib\")\n# citep(bib[[1]])\n```\n\n## Introduction\n\n[Feature hashing](http://en.wikipedia.org/wiki/Feature_hashing), also called as the hashing trick, is a method to\ntransform features to vector. Without looking the indices up in an\nassociative array, it applies a hash function to the features and uses their\nhash values as indices directly.\n  \nThe package FeatureHashing implements the method in `r citep(bib[[\"DBLP:conf/icml/WeinbergerDLSA09\"]])` to transform\na `data.frame` to sparse matrix. The package provides a formula interface similar to model.matrix \nin R and Matrix::sparse.model.matrix in the package Matrix. Splitting of concatenated data, \ncheck the help of `test.tag` for explanation of concatenated data, during the construction of the model matrix.\n\n## Installation\n\nTo install the stable version from Cran, run this command:\n```r\ninstall.packages(\"FeatureHashing\")\n```\n\nFor up-to-date version, please install from github. Windows user will need to install [RTools](http://cran.r-project.org/bin/windows/Rtools/) first.\n\n```r\ndevtools::install_github('wush978/FeatureHashing')\n```\n\n### When should we use Feature Hashing?\n\nFeature hashing is useful when the user does not easy to know the dimension of the feature vector. \nFor example, the bag-of-word representation in document classification problem requires scanning entire dataset to know how many words we have, i.e. the dimension of the feature vector.\n\nIn general, feature hashing is useful in the following environment:\n\n- Streaming Environment\n- Distirbuted Environment\n\nBecause it is expensive or impossible to know the real dimension of the feature vector.\n\n## Getting Started\n\nThe following scripts show how to use the `FeatureHashing` to construct `Matrix::dgCMatrix` and train a model in other packages which supports `Matrix::dgCMatrix` as input.\n\nThe dataset is a sample from iPinYou dataset which is described in `r citep(bib[[\"zhang2014real\"]])`.\n\n### Logistic Regression with [`glmnet`](http://cran.r-project.org/package=glmnet)\n\n```{r lr}\n# The following script assumes that the data.frame\n# of the training dataset and testing dataset are \n# assigned to variable `ipinyou.train` and `ipinyou.test`\n# respectively\n\nlibrary(FeatureHashing)\n\n# Checking version.\nstopifnot(packageVersion(\"FeatureHashing\") >= package_version(\"0.9\"))\n\ndata(ipinyou)\nf <- ~ IP + Region + City + AdExchange + Domain +\n  URL + AdSlotId + AdSlotWidth + AdSlotHeight +\n  AdSlotVisibility + AdSlotFormat + CreativeID +\n  Adid + split(UserTag, delim = \",\")\n# if the version of FeatureHashing is 0.8, please use the following command:\n# m.train <- as(hashed.model.matrix(f, ipinyou.train, 2^16, transpose = FALSE), \"dgCMatrix\")\nm.train <- hashed.model.matrix(f, ipinyou.train, 2^16)\nm.test <- hashed.model.matrix(f, ipinyou.test, 2^16)\n\n# logistic regression with glmnet\n\nlibrary(glmnet)\n\ncv.g.lr <- cv.glmnet(m.train, ipinyou.train$IsClick,\n  family = \"binomial\")#, type.measure = \"auc\")\np.lr <- predict(cv.g.lr, m.test, s=\"lambda.min\")\nauc(ipinyou.test$IsClick, p.lr)\n```\n\n### Gradient Boosted Decision Tree with [`xgboost`](http://cran.r-project.org/package=xgboost)\n\nFollowing the script above, \n\n```{r xgboost}\n# GBDT with xgboost\n\nlibrary(xgboost)\n\ncv.g.gdbt <- xgboost(m.train, ipinyou.train$IsClick, max.depth=7, eta=0.1,\n  nround = 100, objective = \"binary:logistic\", verbose = ifelse(interactive(), 1, 0))\np.lm <- predict(cv.g.gdbt, m.test)\nglmnet::auc(ipinyou.test$IsClick, p.lm)\n```\n\n\n### Per-Coordinate FTRL-Proximal with $L_1$ and $L_2$ Regularization for Logistic Regression\n\nThe following scripts use an implementation of the FTRL-Proximal for Logistic Regresion, which is published in `r citep(bib[[\"DBLP:conf/kdd/McMahanHSYEGNPDGCLWHBK13\"]])`, to predict the probability (1-step prediction) and update the model simultaneously.\n\n\n```{r ftprl}\nsource(system.file(\"ftprl.R\", package = \"FeatureHashing\"))\n\nm.train <- hashed.model.matrix(f, ipinyou.train, 2^16, transpose = TRUE)\nftprl <- initialize.ftprl(0.1, 1, 0.1, 0.1, 2^16)\nftprl <- update.ftprl(ftprl, m.train, ipinyou.train$IsClick, predict = TRUE)\nauc(ipinyou.train$IsClick, attr(ftprl, \"predict\"))\n```\n\nIf we use the same algorithm to predict the click through rate of the 3rd season of iPinYou, the overall AUC will be 0.77 which is comparable to the overall AUC of the 3rd season 0.76 reported in `r citep(bib[[\"zhang2014real\"]])`.\n\n## Supported Data Structure\n\n- character and factor\n- numeric and integer\n- array, i.e. concatenated strings such as `c(\"a,b\", \"a,b,c\", \"a,c\", \"\")`\n\n## Reference\n\n```{r bibliograph, results='asis', echo=FALSE}\nbibliography()\n```\n", 
  "id": 22476756, 
  "README.md": "FeatureHashing\n==============\n\nLinux: [![Travis-ci Status](https://travis-ci.org/wush978/FeatureHashing.svg?branch=master)](https://travis-ci.org/wush978/FeatureHashing)\nWin : [![Build status](https://ci.appveyor.com/api/projects/status/bm4lpxn5f07d8klj/branch/master?svg=true)](https://ci.appveyor.com/project/wush978/featurehashing/branch/master)\nOS X: [![Travis-ci Status](https://travis-ci.org/wush978/FeatureHashing.svg?branch=osx)](https://travis-ci.org/wush978/FeatureHashing)\n\n\n[![Coverage Status](https://img.shields.io/coveralls/wush978/FeatureHashing.svg)](https://coveralls.io/r/wush978/FeatureHashing?branch=master)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/FeatureHashing)](http://cran.r-project.org/package=FeatureHashing/)\n[![rstudio mirror downloads](http://cranlogs.r-pkg.org/badges/FeatureHashing)](https://github.com/metacran/cranlogs.app)\n\nImplement feature hashing with R\n\n\n\n## Introduction\n\n[Feature hashing](http://en.wikipedia.org/wiki/Feature_hashing), also called as the hashing trick, is a method to\ntransform features to vector. Without looking the indices up in an\nassociative array, it applies a hash function to the features and uses their\nhash values as indices directly.\n  \nThe package FeatureHashing implements the method in (Weinberger, Dasgupta, Langford, Smola, and Attenberg, 2009) to transform\na `data.frame` to sparse matrix. The package provides a formula interface similar to model.matrix \nin R and Matrix::sparse.model.matrix in the package Matrix. Splitting of concatenated data, \ncheck the help of `test.tag` for explanation of concatenated data, during the construction of the model matrix.\n\n## Installation\n\nTo install the stable version from Cran, run this command:\n```r\ninstall.packages(\"FeatureHashing\")\n```\n\nFor up-to-date version, please install from github. Windows user will need to install [RTools](http://cran.r-project.org/bin/windows/Rtools/) first.\n\n```r\ndevtools::install_github('wush978/FeatureHashing')\n```\n\n### When should we use Feature Hashing?\n\nFeature hashing is useful when the user does not easy to know the dimension of the feature vector. \nFor example, the bag-of-word representation in document classification problem requires scanning entire dataset to know how many words we have, i.e. the dimension of the feature vector.\n\nIn general, feature hashing is useful in the following environment:\n\n- Streaming Environment\n- Distirbuted Environment\n\nBecause it is expensive or impossible to know the real dimension of the feature vector.\n\n## Getting Started\n\nThe following scripts show how to use the `FeatureHashing` to construct `Matrix::dgCMatrix` and train a model in other packages which supports `Matrix::dgCMatrix` as input.\n\nThe dataset is a sample from iPinYou dataset which is described in (Zhang, Yuan, Wang, and Shen, 2014).\n\n### Logistic Regression with [`glmnet`](http://cran.r-project.org/package=glmnet)\n\n\n```r\n# The following script assumes that the data.frame\n# of the training dataset and testing dataset are \n# assigned to variable `ipinyou.train` and `ipinyou.test`\n# respectively\n\nlibrary(FeatureHashing)\n```\n\n```\n## Loading required package: methods\n```\n\n```r\n# Checking version.\nstopifnot(packageVersion(\"FeatureHashing\") >= package_version(\"0.9\"))\n\ndata(ipinyou)\nf <- ~ IP + Region + City + AdExchange + Domain +\n  URL + AdSlotId + AdSlotWidth + AdSlotHeight +\n  AdSlotVisibility + AdSlotFormat + CreativeID +\n  Adid + split(UserTag, delim = \",\")\n# if the version of FeatureHashing is 0.8, please use the following command:\n# m.train <- as(hashed.model.matrix(f, ipinyou.train, 2^16, transpose = FALSE), \"dgCMatrix\")\nm.train <- hashed.model.matrix(f, ipinyou.train, 2^16)\nm.test <- hashed.model.matrix(f, ipinyou.test, 2^16)\n\n# logistic regression with glmnet\n\nlibrary(glmnet)\n```\n\n```\n## Loading required package: Matrix\n## Loading required package: foreach\n## Loaded glmnet 2.0-2\n```\n\n```r\ncv.g.lr <- cv.glmnet(m.train, ipinyou.train$IsClick,\n  family = \"binomial\")#, type.measure = \"auc\")\np.lr <- predict(cv.g.lr, m.test, s=\"lambda.min\")\nauc(ipinyou.test$IsClick, p.lr)\n```\n\n```\n## [1] 0.5076923\n```\n\n### Gradient Boosted Decision Tree with [`xgboost`](http://cran.r-project.org/package=xgboost)\n\nFollowing the script above, \n\n\n```r\n# GBDT with xgboost\n\nlibrary(xgboost)\n\ncv.g.gdbt <- xgboost(m.train, ipinyou.train$IsClick, max.depth=7, eta=0.1,\n  nround = 100, objective = \"binary:logistic\", verbose = ifelse(interactive(), 1, 0))\np.lm <- predict(cv.g.gdbt, m.test)\nglmnet::auc(ipinyou.test$IsClick, p.lm)\n```\n\n```\n## [1] 0.6554945\n```\n\n\n### Per-Coordinate FTRL-Proximal with $L_1$ and $L_2$ Regularization for Logistic Regression\n\nThe following scripts use an implementation of the FTRL-Proximal for Logistic Regresion, which is published in (McMahan, Holt, Sculley, Young, Ebner, Grady, Nie, Phillips, Davydov, Golovin, Chikkerur, Liu, Wattenberg, Hrafnkelsson, Boulos, and Kubica, 2013), to predict the probability (1-step prediction) and update the model simultaneously.\n\n\n\n```r\nsource(system.file(\"ftprl.R\", package = \"FeatureHashing\"))\n\nm.train <- hashed.model.matrix(f, ipinyou.train, 2^16, transpose = TRUE)\nftprl <- initialize.ftprl(0.1, 1, 0.1, 0.1, 2^16)\nftprl <- update.ftprl(ftprl, m.train, ipinyou.train$IsClick, predict = TRUE)\nauc(ipinyou.train$IsClick, attr(ftprl, \"predict\"))\n```\n\n```\n## [1] 0.5986472\n```\n\nIf we use the same algorithm to predict the click through rate of the 3rd season of iPinYou, the overall AUC will be 0.77 which is comparable to the overall AUC of the 3rd season 0.76 reported in (Zhang, Yuan, Wang, et al., 2014).\n\n## Supported Data Structure\n\n- character and factor\n- numeric and integer\n- array, i.e. concatenated strings such as `c(\"a,b\", \"a,b,c\", \"a,c\", \"\")`\n\n## Reference\n\n[1] H. B. McMahan, G. Holt, D. Sculley, et al. \"Ad click\nprediction: a view from the trenches\". In: _The 19th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\nKDD 2013, Chicago, IL, USA, August 11-14, 2013_. Ed. by I. S.\nDhillon, Y. Koren, R. Ghani, T. E. Senator, P. Bradley, R. Parekh,\nJ. He, R. L. Grossman and R. Uthurusamy. ACM, 2013, pp. 1222-1230.\nDOI: 10.1145/2487575.2488200. <URL:\nhttp://doi.acm.org/10.1145/2487575.2488200>.\n\n[2] K. Q. Weinberger, A. Dasgupta, J. Langford, et al. \"Feature\nhashing for large scale multitask learning\". In: _Proceedings of\nthe 26th Annual International Conference on Machine Learning, ICML\n2009, Montreal, Quebec, Canada, June 14-18, 2009_. Ed. by A. P.\nDanyluk, L. Bottou and M. L. Littman. 2009, pp. 1113-1120. DOI:\n10.1145/1553374.1553516. <URL:\nhttp://doi.acm.org/10.1145/1553374.1553516>.\n\n[3] W. Zhang, S. Yuan, J. Wang, et al. \"Real-Time Bidding\nBenchmarking with iPinYou Dataset\". In: _arXiv preprint\narXiv:1407.7073_ (2014).\n"
}