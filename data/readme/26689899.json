{
  "read_at": 1462553780, 
  "description": "A powerful machine learning algorithm utilizing Q-Learning and Neural Networks, implemented using Torch and Lua.", 
  "README.md": "DeepQLearning\n=============\n\nWritten by Blake Milner and Jeff Soldate, with help from Eugenio Culurciello and his lab. Work was\ndone as part of a project for BME495, a Computational Neuroscience course at Purdue. The original\ncode, written in JavaScript, was developed by Andrej Karpathy, a Ph.D. student at Stanford University.\n\nDeep Q Learning is a powerful machine learning algorithm utilizing Q-Learning. The state space is implemented using Neural Networks, thus bypassing inefficient static look up tables. This aplication was implemented using Torch 7 and Lua.\n\nIn many practical engineering scenarios it is often necessary for an algorithm to perform a\nseries of decisions in order to accomplish a given task. However, that task itself is not always\nwell-defined and the intermediate decisions to accomplish it are often complex and ever-changing.\nFurthermore, information that contributes to accomplishing the task is often not readily available\nuntil critical intermediate decisions have already been made. Video games are a good example of\nsituations in which a series of actions is required in order to accomplish a task.\n\nThis application also presents an AI based approach to learning a game where the rules aren't\nimmediately known. In recent years very robust algorithms utilizing these concepts have been developed and applied \nsuccessfully to retro Atari video games: http://arxiv.org/pdf/1312.5602v1.pdf.\n\nReinforcement learning methods that encourage both exploration and strategizing have been developed in\norder to address this problem. One of these methods, called Q-learning, utilizes a policy in order to\nselect an optimal action.\n\nThe Q-learning algorithm hinges on a utility function called the Q-function. This function\naccepts a state that contains all pertinent information about the playing field along with a possible\naction. The function returns a number that describes the utility of that action. In Q-learning the utility\nof an action is evaluated based on the immediate reward gained from taking that action and the\npossibility of a delayed reward that the action may lead to. For large games with many states and possible\nactions the above approach is very time-consuming and computationally intense. Using a neural network to\nrepresent the Q-function can solve many of these issues by eliminating the need for enumeration in order to completely\nsupport the exploration of the state space.\n\nAn implementation of the method described above (written in JavaScript) exists and is freely available:\nhttp://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html\n\nHowever, this package is designed for a browser and used primarily as a learning tool. DeepQLearning is a \npartial port of the Q-learning component of this package to the Lua scripting language. The Neural Network \ncomponent is powered by Torch 7, a scientific computing framework used for machine learning. It is the hope \nof the authors that this package can be used to fuel further scientific inquiry into this topic.\n\nThe JS implementation of DeepQLearning can be found at: \nhttps://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html\n\nThis page also contains a broswer game that the JS Qlearning engine learns from scratch. If the settings are optimized\nthen after about 15 minutes the application will have learned to play the game flawlessy.\n\n\nInstallation and Use\n====================\n\nRequirements:\n\n * Torch7 (with nnx and optim package) \n-- A scientific computing framework with wide support for machine learning algorithms. (https://github.com/torch/torch7)\n\n\nUsage:\n\nThe DeepQLearning module can be easily included in a Lua scipt using:\n\n```bash\nBrain = require 'deepqlearn'\n```\n\nThe brain must then be initialized with the number of expected inputs and outputs:\n\n```bash\nBrain.init(num_inputs, num_outputs)   \n```\n\nAn action can be selected from an input state space using:\n\n```bash\naction = Brain.forward(state); \n```\n\nLearning can be affected from the last state space input to Brian.forward by giving a reward value:\n\n```bash\nBrain.backward(reward); \n```\n", 
  "id": 26689899
}