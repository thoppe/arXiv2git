{
  "read_at": 1462547726, 
  "description": "summaries and notes on neural language learning papers", 
  "README.md": "# neural language notes\nSimple notes and comments on papers about neural language learning from arxiv, ACL, EMNLP, NAACL, etc. Originally, this is inspired by [Denny Britz's notes](https://github.com/dennybritz/deeplearning-papernotes).\n\n#### TODO\n- Pointing the Unknown Words\n- Attend, Infer, Repeat Fast Scene Understanding with Generative Models\n- How NOT To Evaluate Your Dialogue System An Empirical Study of\n- Revisiting Semi-Supervised Learning with Graph Embeddings\n- Neural Summarization by Extracting Sentences and Words\n- Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints\n- LSTM-BASED DEEP LEARNING MODELS FOR NONFACTOID\n- Generating Visual Explanations\n- A Compositional Approach to Language Modeling [[arxiv](http://arxiv.org/abs/1604.00100)]\n- AttSum: Joint Learning of Focusing and Summarization with Neural Attention [[arxiv](http://arxiv.org/abs/1604.00125)]\n- Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems [[arxiv](http://arxiv.org/abs/1511.06931)]\n- The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations [[arxiv](http://arxiv.org/abs/1511.02301)]\n- Building Machines That Learn and Think Like People [[arxiv](Building Machines That Learn and Think Like People)]\n- A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories [[arxiv](https://arxiv.org/abs/1604.01696)]\n- Revisiting Summarization Evaluation for Scientific Articles [[arxiv](http://arxiv.org/abs/1604.00400)]\n- Reasoning About Pragmatics with Neural Listeners and Speakers [[arxiv](http://arxiv.org/abs/1604.00562)]\n- Character-Level Question Answering with Attention [[arxiv](http://arxiv.org/abs/1604.00727)]\n- Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks [[arxiv](http://arxiv.org/abs/1604.00734)]\n- Top-down Tree Long Short-Term Memory Networks [[arxiv](http://arxiv.org/abs/1511.00060)]\n- Recurrent Neural Network Grammars [[arxiv](http://arxiv.org/abs/1602.07776)]\n- Pointing the Unknown Words [[arxiv](http://arxiv.org/abs/1603.08148)]\n- Neural Programmer: Inducing Latent Programs with Gradient Descent [[arxiv](http://scholar.google.com/scholar_url?url=https://research.google.com/pubs/archive/44927.pdf&hl=en&sa=X&scisig=AAGBfm2VedkF99f2i9IB7m_Ki5ELxJ-SCQ&nossl=1&oi=scholaralrt)]\n- Adversarial Autoencoders []\n- Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition\n- Net2Net: Accelerating Learning via Knowledge Transfer\n- Neural Programmer: Inducing Latent Programs with Gradient Descent\n- A Neural Conversational Model\n- Document embedding with paragraph vectors\n- Neural Language Correction with Character-Based Attention [[arxiv](http://arxiv.org/abs/1603.09727)]\n- Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks [[arxiv](http://arxiv.org/abs/1604.01178)]\n- Automatic Annotation of Structured Facts in Images\n- Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves [[arxiv](https://arxiv.org/abs/1604.02038)]\n- Building Machines That Learn and Think Like People [[arxiv](http://arxiv.org/pdf/1604.00289.pdf)]\n- LARGER-CONTEXT LANGUAGE MODELLING WITH RECURRENT NEURAL NETWORK [[arxiv](http://arxiv.org/pdf/1511.03729v2.pdf)]\n- A Diversity-Promoting Objective Function for Neural Conversation Model [[arxiv](http://arxiv.org/pdf/1510.03055v2.pdf)]\n- TOWARDS PRINCIPLED UNSUPERVISED LEARNING [[arxiv](http://arxiv.org/pdf/1511.06440v2.pdf)]\n- Sentence-Level Grammatical Error Identification as Sequence-to-Sequence\n  Correction [[arxiv](https://arxiv.org/abs/1604.04677)]\n- Hierarchical Attention Networks for Document Classification [[arxiv](http://scholar.google.co.kr/scholar_url?url=http://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf&hl=ko&sa=X&scisig=AAGBfm3Yksk0QUL3dBcaokFPC3DOF8CFvg&nossl=1&oi=scholaralrt)]\n- Visual Storytelling [[arxiv](https://arxiv.org/abs/1604.03968)]\n- Using Sentence-Level LSTM Language Models for Script Inference [[arxiv](https://arxiv.org/abs/1604.02993)]\n- ABCNN: Attention-Based Convolutional Neural Network for Modeling\n  Sentence Pairs [[arxiv](https://arxiv.org/abs/1512.05193)]\n- Character-Level Question Answering with Attention [[arxiv](https://arxiv.org/abs/1604.00727)]\n- Abstractive Text Summarization Using Sequence-to-Sequence RNNs and\n  Beyond [[arxiv](https://arxiv.org/abs/1602.06023)]\n- Sentence Compression by Deletion with LSTMs [[link](http://research.google.com/pubs/pub43852.html)]\n- A Simple Way to Initialize Recurrent Networks of Rectified Linear Units [[arxiv](http://arxiv.org/abs/1504.00941)]\n- DenseCap: Fully Convolutional Localization Networks for Dense Captioning [[arxiv](http://cs.stanford.edu/people/karpathy//densecap.pdf)]\n- TRAINING NEURAL NETWORKS ON PARTITIONED TRAINING DATA [[paper]()]\n- Nonextensive information theoretical machine [[arxiv](https://arxiv.org/abs/1604.06153)]\n- Training Deep Nets with Sublinear Memory Cost [[arxiv](https://arxiv.org/abs/1604.06174)]\n- What we write about when we write about causality: Features of causal statements across large-scale social discourse [[arxiv](https://arxiv.org/abs/1604.05781)]\n- Dialog-based Language Learning [[arxiv](https://arxiv.org/abs/1604.06045)]\n- Question Answering via Integer Programming over Semi-Structured\n  Knowledge [[arxiv](https://arxiv.org/abs/1604.06076)]\n- Dialog-based Language Learning [[arxiv](http://arxiv.org/pdf/1604.06045.pdf)]\n- Bridging LSTM Architecture and the Neural Dynamics during Reading [[arxiv](https://arxiv.org/abs/1604.06635)]\n- Neural Generative Question Answering [[arxiv](https://arxiv.org/abs/1512.01337)]\n- Recurrent Memory Networks for Language Modeling [[arxiv](https://arxiv.org/abs/1601.01272)]\n- Colorful Image Colorization [[paper](http://arxiv.org/abs/1603.08511)]  [[code](https://github.com/richzhang/colorization)] [[note](/notes/Colorful-Image-Colorization.md)]\n- ...\n", 
  "id": 55370917
}