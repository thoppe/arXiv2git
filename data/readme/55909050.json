{
  "read_at": 1462547783, 
  "description": "Asynchronous Methods for Deep Reinforcement Learning", 
  "README.md": "# async_deep_reinforce\n\nAsynchronous deep reinforcement learning\n\n## About\n\nAn attempt to repdroduce Google Deep Mind's paper \"Asynchronous Methods for Deep Reinforcement Learning.\"\n\nhttp://arxiv.org/abs/1602.01783\n\nAsynchronous Advantage Actor-Critic (A3C) method for playing \"Atari Pong\" is implemented with TensorFlow.\n\nLearning result movment after 24 hour is like this. (click image below to preview)\n\n[![Learning result after 24 hour](https://img.youtube.com/vi/cFWL_y9BVaQ/0.jpg)](https://www.youtube.com/watch?v=cFWL_y9BVaQ)\n\nAny advice or suggestion is strongly welcomed in issues thread.\n\nhttps://github.com/miyosuda/async_deep_reinforce/issues/1\n\n## How to build\n\nFirst we need to build multi thread ready version of Arcade Learning Enviroment.\nI made some modification to it to run it on multi thread enviroment.\n\n    $ git clone https://github.com/miyosuda/Arcade-Learning-Environment.git\n    $ cd Arcade-Learning-Environment\n    $ cmake -DUSE_SDL=ON -DUSE_RLGLUE=OFF -DBUILD_EXAMPLES=ON .\n    $ make -j 4\n\t\n    $ pip install .\n\nI recommend to install it on VirtualEnv environment.\n\n## How to run\n\nTo train,\n\n    $python a3c.py\n\nTo display the result with game play,\n\n    $python a3c_disp.py\n\n## Acknowledgements\n\n- [@aravindsrinivas](https://github.com/aravindsrinivas) for providing information for some of the hyper parameters.\n\n", 
  "id": 55909050
}