{
  "read_at": 1462557538, 
  "description": "", 
  "README.md": "# RST Discourse Parsing using Deep Neural Nets\nImplementations of RST discourse paring models represented in \"Recursive Deep Models for Discourse Parsing\" and \"When Are Tree Structures Necessary for Deep Learning of Representations? \". Bi-directional LSTMs are applied to EDU sequences and Tree LSTMs are applied for tree construction.\n\n## Requirements:\nGPU \n\nmatlab >= 2014b\n\nFor any pertinent question, feel free to contact jiweil@stanford.edu\n\n##Folders\nBinary:  a binary structure\nclassifier to determine whether two adjacent text\nunits should be merged to form a new subtree.\n\nMulti: a multi-class classifier to determine which discourse\nrelation label should be assigned to the new subtree.\n\nInfer: Doing inference on testing dataset.\n\n## Training\nrun binary/discourse_binary.m\n\nrun multi/discourse_multi.m\n## Testing\ninfer/Evaluation.m\n\ndownload [data,embeddings](http://cs.stanford.edu/~bdlijiwei/discourse_data.tar)\n\n```latex\n\n@inproceedings{li2014recursive,\n    title={Recursive Deep Models for Discourse Parsing.},\n    author={Li, Jiwei and Li, Rumeng and Hovy, Eduard H},\n    booktitle={EMNLP},\n    pages={2061--2069},\n    year={2014}\n}\n\n@article{li2015tree,\n    title={When are tree structures necessary for deep learning of representations?},\n    author={Li, Jiwei and Jurafsky, Dan and Hovy, Eudard},\n    journal={arXiv preprint arXiv:1503.00185},\n    year={2015}\n}\n\n```\n", 
  "id": 51962256
}