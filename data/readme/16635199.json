{
  "read_at": 1462543149, 
  "description": "Implements SFO minibatch optimizer in Python and MATLAB, and reproduces figures from paper.", 
  "README.md": "Sum of Functions Optimizer (SFO)\n================================\n\nSFO is a function optimizer for the case where the target function breaks into a sum over minibatches, or a sum over contributing functions.  It combines the benefits of both quasi-Newton and stochastic gradient descent techniques, and will likely converge faster and to a better function value than either.  It does not require tuning of hyperparameters.  It is described in more detail in the paper:\n> Jascha Sohl-Dickstein, Ben Poole, and Surya Ganguli<br>\n> Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods<br>\n> International Conference on Machine Learning (2014)<br>\n> arXiv preprint arXiv:1311.2115 (2013)<br>\n> http://arxiv.org/abs/1311.2115\n\nThis repository provides easy to use Python and MATLAB implementations of SFO, as well as functions to exactly reproduce the figures in the paper.<br>\n\n## Use SFO\n\nSimple example code which trains an autoencoder is in **sfo_demo.py** and **sfo_demo.m**, and is reproduced at the end of this README.\n\n### Python package\n\nTo use SFO, you should first import SFO,  \n`from sfo import SFO`  \nthen initialize it,    \n`optimizer = SFO(f_df, theta_init, subfunction_references)`    \nthen call the optimizer, specifying the number of optimization passes to perform,    \n`theta = optimizer.optimize(num_passes=1)`.\n\nThe three required initialization parameters are:    \n- *f_df* - Returns the function value and gradient for a single subfunction\n            call.  Should have the form\n                `f, dfdtheta = f_df(theta, subfunction_references[idx])`,\n            where *idx* is the index of a single subfunction.    \n- *theta_init* - The initial parameters to be used for optimization.  *theta_init* can\n            be either a NumPy array, an array of NumPy arrays, a dictionary of NumPy\n            arrays, or a nested combination thereof.  The gradient returned by *f_df*\n            should have the same form as *theta_init*.    \n- *subfunction_references* - A list containing an identifying element for\n            each subfunction.  The elements in this list could be, eg, numpy\n            matrices containing minibatches, or indices identifying the\n            subfunction, or filenames from which target data should be read.\n            **If each subfunction corresponds to a minibatch, then the number of\n            subfunctions should be approximately \\[number subfunctions\\] = sqrt(\\[dataset size\\])/10**.\n\nMore detailed documentation, and additional options, can be found in **sfo.py**.\n\n### MATLAB package\n\nTo use SFO you must first initialize the optimizer,    \n`optimizer = sfo(@f_df, theta_init, subfunction_references, [varargin]);`    \nthen call the optimizer, specifying the number of optimization passes to perform,    \n`theta = optimizer.optimize(20);`.\n\nThe initialization parameters are:    \n- *f_df* - Returns the function value and gradient for a single subfunction\n            call.  Should have the form\n                `[f, dfdtheta] = f_df(theta, subfunction_references{idx}, varargin{:})`,\n            where *idx* is the index of a single subfunction.    \n- *theta_init* - The initial parameters to be used for optimization.  *theta_init* can\n            be either a vector, a matrix, or a cell array with a vector or\n            matrix in every cell.  The gradient returned by *f_df* should have the\n            same form as *theta_init*.    \n- *subfunction_references* - A cell array containing an identifying element\n            for each subfunction.  The elements in this list could be, eg,\n            matrices containing minibatches, or indices identifying the\n            subfunction, or filenames from which target data should be read.\n            **If each subfunction corresponds to a minibatch, then the number of\n            subfunctions should be approximately \\[number subfunctions\\] = sqrt(\\[dataset size\\])/10**.\n- *[varargin]* - Any additional parameters, which will be passed through to *f_df* each time\n            it is called.\n\nSlightly more documentation can be found in **sfo.m**.\n\n### Special situations\n\nEmail jascha@stanford.edu with questions if you don't find your answer here.\n\n#### Reducing overhead\n\nIf too much time is spent inside SFO relative to inside the objective function, then reduce the number of subfunctions by increasing the minibatch size or merging subfunctions.\n\n#### Replacing minibatches / using SFO in the infinite data limit\n\nIn the Python version of the code, a subfunction or minibatch can be replaced by calling the function `replace_subfunction`.  See the documentation in ***sfo.py*** for more details.  Note that replacing a subfunction without calling `replace_subfunction` will cause the optimizer to fail, since SFO relies on subfunctions returning consistent gradients.\n\n#### Using with dropout\n\nStochastic gradients will break SFO, because it uses the change in the minibatch/subfunction gradient to estimate the Hessian matrix.  The benefits of noise regularization can be achieved without making the gradients stochastic by using frozen noise.  That is, in the case of dropout, assign a random dropout mask to each datapoint.  Every time that datapoint is evaluated however, use the same dropout mask.  This makes the gradients consistent across multiple evaluations of the minibatch.\n\n## Reproduce figures from paper\nTo reproduce the figures from the paper, run **generate\\_figures/figure\\_\\*.py**.  Several of the figures rely on a subdirectory **figure_data/** with training data.  This can be downloaded from https://www.dropbox.com/sh/h9z4djlgl2tagmu/GlVAJyErf8 .\n\n## Example code\n\nThe following code blocks train an autoencoder using SFO in Python and MATLAB respectively.  Identical code is in **sfo_demo.py** and **sfo_demo.m**.\n\n### Python example code\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy.random import randn\nfrom sfo import SFO\n\n# define an objective function and gradient\ndef f_df(theta, v):\n    \"\"\"\n    Calculate reconstruction error and gradient for an autoencoder with sigmoid\n    nonlinearity.\n    v contains the training data, and will be different for each subfunction.\n    \"\"\"\n    h = 1./(1. + np.exp(-(np.dot(theta['W'], v) + theta['b_h'])))\n    v_hat = np.dot(theta['W'].T, h) + theta['b_v']\n    f = np.sum((v_hat - v)**2) / v.shape[1]\n    dv_hat = 2.*(v_hat - v) / v.shape[1]\n    db_v = np.sum(dv_hat, axis=1).reshape((-1,1))\n    dW = np.dot(h, dv_hat.T)\n    dh = np.dot(theta['W'], dv_hat)\n    db_h = np.sum(dh*h*(1.-h), axis=1).reshape((-1,1))\n    dW += np.dot(dh*h*(1.-h), v.T)\n    dfdtheta = {'W':dW, 'b_h':db_h, 'b_v':db_v}\n    return f, dfdtheta\n\n# set model and training data parameters\nM = 20 # number visible units\nJ = 10 # number hidden units\nD = 100000 # full data batch size\nN = int(np.sqrt(D)/10.) # number minibatches\n# generate random training data\nv = randn(M,D)\n\n# create the array of subfunction specific arguments\nsub_refs = []\nfor i in range(N):\n    # extract a single minibatch of training data.\n    sub_refs.append(v[:,i::N])\n\n# initialize parameters\ntheta_init = {'W':randn(J,M), 'b_h':randn(J,1), 'b_v':randn(M,1)}\n# initialize the optimizer\noptimizer = SFO(f_df, theta_init, sub_refs)\n# run the optimizer for 1 pass through the data\ntheta = optimizer.optimize(num_passes=1)\n# continue running the optimizer for another 20 passes through the data\ntheta = optimizer.optimize(num_passes=20)\n\n# plot the convergence trace\nplt.plot(np.array(optimizer.hist_f_flat))\nplt.xlabel('Iteration')\nplt.ylabel('Minibatch Function Value')\nplt.title('Convergence Trace')\n\n# test the gradient of f_df\noptimizer.check_grad()\n```\n\n### MATLAB example code\n\n```MATLAB\n% set model and training data parameters\nM = 20; % number visible units\nJ = 10; % number hidden units\nD = 100000; % full data batch size\nN = floor(sqrt(D)/10.); % number minibatches\n% generate random training data\nv = randn(M,D);\n\n% create the cell array of subfunction specific arguments\nsub_refs = cell(N,1);\nfor i = 1:N\n    % extract a single minibatch of training data.\n    sub_refs{i} = v(:,i:N:end);\nend\n\n% initialize parameters\n% Parameters can be stored as a vector, a matrix, or a cell array with a\n% vector or matrix in each cell.  Here the parameters are \n% {[weight matrix], [hidden bias], [visible bias]}.\ntheta_init = {randn(J,M), randn(J,1), randn(M,1)};\n% initialize the optimizer\noptimizer = sfo(@f_df_autoencoder, theta_init, sub_refs);\n% run the optimizer for half a pass through the data\ntheta = optimizer.optimize(0.5);\n% run the optimizer for another 20 passes through the data, continuing from \n% the theta value where the prior call to optimize() ended\ntheta = optimizer.optimize(20);\n\n% plot the convergence trace\nplot(optimizer.hist_f_flat);\nxlabel('Iteration');\nylabel('Minibatch Function Value');\ntitle('Convergence Trace');\n\n% test the gradient of f_df\noptimizer.check_grad();\n```\n\nThe subfunction/minibatch objective function and gradient for the MATLAB code is defined as follows,\n```MATLAB\nfunction [f, dfdtheta] = f_df_autoencoder(theta, v)\n    % [f, dfdtheta] = f_df_autoencoder(theta, v)\n    %     Calculate L2 reconstruction error and gradient for an autoencoder\n    %     with sigmoid nonlinearity.\n    %     Parameters:\n    %         theta - A cell array containing\n    %              {[weight matrix], [hidden bias], [visible bias]}.\n    %         v - A [# visible, # datapoints] matrix containing training data.\n    %              v will be different for each subfunction.\n    %     Returns:\n    %         f - The L2 reconstruction error for data v and parameters theta.\n    %         df - A cell array containing the gradient of f with each of the\n    %              parameters in theta.\n\n    W = theta{1};\n    b_h = theta{2};\n    b_v = theta{3};\n    \n    h = 1./(1 + exp(-bsxfun(@plus, W * v, b_h)));\n    v_hat = bsxfun(@plus, W' * h, b_v);\n    f = sum(sum((v_hat - v).^2)) / size(v, 2);\n    dv_hat = 2*(v_hat - v) / size(v, 2);\n    db_v = sum(dv_hat, 2);\n    dW = h * dv_hat';\n    dh = W * dv_hat;\n    db_h = sum(dh.*h.*(1-h), 2);\n    dW = dW + dh.*h.*(1-h) * v';\n    % give the gradients the same order as the parameters\n    dfdtheta = {dW, db_h, db_v};\nend\n```\n", 
  "id": 16635199
}