{
  "read_at": 1462557045, 
  "description": "", 
  "README.md": "#Multi-gpu using Theano and PyCUDA\n\nDemonstration of training the same neural network with multipe GPUs using Theano and PyCUDA\n\nSee [theano_alexnet](https://github.com/uoguelph-mlrg/theano_alexnet) and this [technical report](http://arxiv.org/abs/1412.2302) for how to use this to train AlexNet.\n\nIf you use this in your research, we kindly ask that you cite the above report:\n\n```bibtex\n@article{ding2014theano,\n  title={Theano-based Large-Scale Visual Recognition with Multiple GPUs},\n  author={Ding, Weiguang and Wang, Ruoyan and Mao, Fei and Taylor, Graham},\n  journal={arXiv preprint arXiv:1412.2302},\n  year={2014}\n}\n```\n\n## Dependencies\n\nPackages\n\n* [numpy](http://www.numpy.org/)\n* [Theano](http://deeplearning.net/software/theano/)\n* [PyCUDA](http://mathema.tician.de/software/pycuda/)\n* [zmq](http://zeromq.org/bindings:python)\n\nFiles needed to be in the same folder\n\n* [logistic_sgd.py](http://deeplearning.net/tutorial/code/logistic_sgd.py)\n* [mlp.py](http://deeplearning.net/tutorial/code/mlp.py)\n\nDownload [mnist.pkl.gz](http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz) and change the shared_args['dataset'] to where you save it.\n\n\n## Description\n\n* dual_mlp.py : This script trains a multi-layer perceptron with 2 gpus. It uses data parallelism, where 2 minibatches trained separately on 2 gpus are combined to be a larger minibatch. This is by no means the best of way using 2 gpus. The purpose of this code is to show a way of using theano with\nmultiprocessing and multiple gpus.\n\n\n## How to run\nIn terminal run:\n\nTHEANO_FLAGS=mode=FAST_RUN,floatX=float32 python dual_mlp.py arg1 arg2\n\nwhere arg1 is the index of the 1st gpu and arg2 is the index of the 2nd\ngpu. These 2 gpus need to be connected directly by PCI-e, otherwise the\np2p transfer won't work.\n\nFor people at University of Guelph, on GPU1~10, run\n\nTHEANO_FLAGS=mode=FAST_RUN,floatX=float32 python dual_mlp.py 1 2\n\non GPU11, run\n\nTHEANO_FLAGS=mode=FAST_RUN,floatX=float32 python dual_mlp.py 0 2\n\n## Acknowledgement\n*Frederic Bastien*, for providing the original page of [Using Multiple GPUs](https://github.com/Theano/Theano/wiki/Using-Multiple-GPUs)\n\n*Lev Givon*, for providing help on inter process communication between 2 gpus with PyCUDA, Lev's original script https://gist.github.com/lebedov/6408165\n\n*Fei Mao*, for extensive discussions on GPUs, CUDA, and debugging\n\n*Graham Taylor*, for extensive suggestions\n\n*Guangyu Sun*, for help on debugging the code\n", 
  "id": 27655268
}