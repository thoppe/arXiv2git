{
  "id": 34348490, 
  "read_at": 1462511750, 
  "README.rst": "nbodykit\n========\n\nnbodykit is a software kit for cosmological datasets from\nN-body simulations and large scale structure surveys.\n\nDriven by the optimistism regarding the abundance and availability of \nlarge scale computing resources in the future, \nthe development of nbodykit\ndistinguishes itself from other similar software packages\n([nbodyshop]_, [pynbody]_, [yt]_, [xi]_) by focusing on :\n\n- a **unified** treatment of simulation and observational data sets; \n  insulating algorithms from data containers;\n\n- reducing wall-clock time by **scaling** to thousands of cores;\n\n- **deployment** and availability on large super computing facilities.\n\nAll algorithms are parallel, and run with Message Passing Interface (MPI).\n\n.. todo::\n\n    Nbodykit is a set of extension points and a(two) main programs\n    that wire the plugins together. \n\n    We need to write about this in a graceful way. The extension framework\n    sets the design of nbodykit aside from others. \n\nBuild Status\n------------\n\nWe perform integrated tests for the toplevel executables in a\nminiconda environment for Python 2.7 and Python 3.4. \n\n.. image:: https://api.travis-ci.org/bccp/nbodykit.svg\n    :alt: Build Status\n    :target: https://travis-ci.org/bccp/nbodykit/\n\n\nTop Level Executables\n---------------------\n\nThe algorithms implemented in nbodykit can be invoked by\n\n- ``bin/nbkit.py``: the main executable that can run a variety of algorithms (see below)\n\n- ``bin/nbkit-batch.py``: runs algorithms in batch-mode, using a task manager to control the iteration over a set of configuration files\n\nRun them with '-h' to see the inline help.\n\n.. code:: bash\n\n    nbkit.py --help\n\nFor mpirun / mpiexec, it is import to launch nbkit.py via Python:\n\n.. code:: bash\n\n    mpirun -n 4 python `which nbkit.py` AlgorithmName [config]\n\n\nAlgorithms\n----------\n\nAlgorithms are implemented as Plugins. \n\nTo obtain a list of algorithms, use\n\n.. code:: bash\n\n    nbkit.py --list-algorithms\n\nTo see help of a particular algorithm, use\n\n.. code:: bash\n\n    nbkit.py AlgorithmName --help\n\nor\n\n.. code:: bash\n\n    nbkit.py --list-algorithms AlgorithmName\n\nwhere AlgorithmName can be one of the following:\n\n- **FFTPower** : Power spectrum (Isotropic, Anisotropic, and multipoles) in a periodic box, via Fast Fourier Transforms\n\n- **BianchiFFTPower** : power spectrum multipoles using FFTs for a data survey with non-trivial geometry, as detailed in [Bianchi]_ et al. 2015\n\n- **FFTCorrelation**: Correlation function (Isotropic, Anisotropic, and multipoles) in a periodic box, via Fast Fourier Transforms\n\n- **PairCountCorrelation**: Correlation function (Isotropic, Anisotropic, and multipoles) via simple pair-counting\n\n- **FOF** : Friend of Friend halos (usually called groups)\n\n- **FOF6D** : 6D Friend of Friend halos (usually called subhalos or galaxies)\n\n- **Describe** : Describes the max and min of a column from any DataSource\n\n- **Subsample** : Create a subsample from a DataSource, and evaluate density (:math:`1 + \\delta`) smoothed \n  at the given scale\n\n- **TraceHalo** : Trace the center of mass and velocity of particles between two different DataSource, joining\n  by ID.\n\n- **FiberCollisions** : Assign fibers to a galaxy survey assuming a fixed angular collision radius, such that the number of galaxies collided out of the survey is minimized\n\nIt is very easy to contribute a new algorithm to nbodykit. We will document this later.\n\n.. todo::\n\n    Write about this.\n\nDatasources\n-----------\n\nnbodykit insulates algorithms from data containers via 'DataSource' plugins.\n\nThe datasource ensures the input to the algorithms are transformed to MPC/h units,\nand particles are moved to the redshift space (if requested).\n\nTo obtain a list of datasources, use\n\n.. code:: bash\n\n    nbkit.py --list-datasources\n\nor to obtain help for a particular DataSource, use\n\n.. code:: bash\n\n    nbkit.py --list-datasources DataSourceName\n\nThe list of DataSource plugins:\n\n- **FOFGroups** : Friend of Friend catalog generated by the FOF algorithm.\n\n- **HaloLabel** : Halo label (halo id per particle) file by the FOF algorithm.\n\n- **FastPM** : Snapshot files of FastPM\n\n- **PlainText** : Plain text file\n\n- **TPMLabel** : Halo label  (halo id per particle) file generated from Martin's TPM.\n\n- **Gadget** : A flavor of Gadget 2 files (experimental)\n\n- **GadgetGroupTab** : A flavor of Gadget 2 FOF catalogue (experimental)\n\n- **TPMSnapshot** : snapshot files from Martin's TPM.\n\n- **Grid** : A special data source representing a simple 3D grid (not particles)\n\n- **Pandas** : Pandas flavored HDF5 and text files.\n\n- **RaDecRedshift** : read (ra, dec, z) from a plaintext file, returning Cartesian coordinates\n\n- **TracerCatalog** : a catalog of tracer objects, with associated `data` and `randoms` catalogs to fully specify the geometry of the survey \n\nIt is very easy to contribute a new datasource to nbodykit. We will document this later.\n\n.. todo::\n\n    Write about this.\n\nThe DataSource plugins implement three reading modes: simple-read, streaming and full-read:\n\n- **simple-read** reads in the entire data set from a single rank, which is then split and scattered to other ranks. This is the minimal \n  requirement of a DataSource.\n\n- **full-read** reads in the entire dataset; by default it calls simple-read to read.\n\n- **streaming** reads in the data chunk by chunk; by default it does a full-read.\n\nPlugins\n-------\n\nnbodykit can be extended by adding plugins. Several plugins are distributed together\nwith nbodykit main source code, but users can add new plugins as well.\n\n.. todo::\n\n    Write about this.\n\nExamples\n--------\n\nThere are example scripts (which also act as integrated tests) in examples directory.\nThe supporting data for these scripts can be retrieved from \n\n    https://s3-us-west-1.amazonaws.com/nbodykit/nbodykit-data.tar.gz\n\nCheck get_data.sh for details.\n\nDependencies\n------------\n\nThe software is built on top of existing tools. Please refer to their\ndocumentations:\n\n- [pfft]_    : massively parallel fast fourier transform with pencil domains\n- [pfft-python]_  : python binding of pfft\n- [pmesh]_     :  particle mesh framework in Python\n- [kdcount]_   : pair-counting and friend-of-friend clustering with KD-Tree\n- [bigfile]_   :  A reproducible massively parallel IO library for hierarchical data\n- [MP-sort]_   : massively parallel sorting \n- [sharedmem]_ : in-node parallelism with fork and copy-on-write.\n\nSome better established dependencies are\n\n- [scipy]_,  [numpy]_   : the foundations for Scientific Python.\n- [mpi4py]_   : MPI for python\n- [h5py]_     : Support for HDF5 files\n\nOptional Dependencies\n---------------------\n\n- [pandas]_, [pytables]_ are required to access the PANDAS subset of HDF5 and fast parsing of plain text files.\n\nBuild\n-----\n\nThe software is designed to be installed with the ``pip`` utility like a regular\npython package.\n\nUsing nbodykit from the source tree is not supported. See 'Development mode' for\ndetails.\n\nThe steps listed here is intended for a commodity Linux based cluster \n(e.g. a Rocks Cluster [rocksclusters]_) or a Linux based workstation / laptop.\nPlease note that there are slight changes to the procedure on systems running\na Mac OS X operating system and \nCray super-computers \nas explictly noted below in `Special notes for Mac and Cray`_.\n\nInstall the main ``nbodykit`` package, as well as the external dependencies \nlisted above, into the default python installation directory with:\n\n.. code:: sh\n   \n    git clone http://github.com/bccp/nbodykit\n    cd nbodykit\n\n    # It may take a while to build fftw and pfft.\n    # Mac and Edison are special, see notes below\n\n    pip install -r requirements.txt\n    pip install -U --force --no-deps .\n\nA different installation directory can be specified via the ``--user`` or ``--root <dir>`` \noptions of the ``pip install`` command. \n\nThe pure-python ``nbodykit`` package (without external dependencies) can be installed by \nomitting the ``-r requirements.txt`` option, with such an installation only requiring ``numpy``. \nThe caveat being that the functionality of the package is greatly diminished -- package behavior \nin this instance is not tested and considered undefined. \n\n\nThe dependencies of nbodykit are not fully stable, thus we recommend updating\nthe external dependencies occassionally via the ``-U`` option of ``pip install``. \nAlso, since nbodykit is\nnot yet stable enough for versioned releases, ``--force`` ensures the current \nsourced version is installed:\n\n.. code:: sh\n\n    pip install -U -r requirements.txt\n    pip install -U --force --no-deps .\n\nTo confirm that nbodykit is working, we can type, in a interactive python session:\n(please remember to jump to bin/ directory to avoid weird issues about importing in-tree)\n\n.. code:: python\n\n    import nbodykit\n    print(nbodykit)\n\n    import kdcount\n    print(kdcount)\n\n    import pmesh\n    print(pmesh)\n\nOr try the scripts in the bin directory:\n\n.. code:: bash\n\n    cd bin/\n    mpirun -n 4 python-mpi nbkit.py -h\n\nDevelopment Mode\n++++++++++++++++\n\nnbodykit can be installed with the development mode (``-e``) of pip\n\n.. code::\n\n    pip install -r requirements.txt -e .\n\nIn addition to the dependency packages, the 'development' installation\nof nbodykit may require a forced update from time to time:\n\n.. code::\n\n    pip install -U --force --no-deps -e .\n\nIt is sometimes required to manually remove the ``nbodykit`` directory in \n``site-packages``, if the above command does not appear to update the installation\nas expected.\n\n\nSpecial notes for Mac and Cray\n------------------------------\n\nMac Notes\n+++++++++\n\nautotools are needed on a Mac\n\n.. code::\n\n    sudo port install autoconf automake libtool\n    \nOn Mac, the `LDSHARED` environment variable must be explicitly set. In bash, the command is\n\n.. code::\n\n    export LDSHARED=\"mpicc -bundle -undefined dynamic_lookup -DOMPI_IMPORTS\"; pip install -r requirements.txt .\n    \nOn recent versions of MacPorts, we also need to tell mpicc to use gcc rather than the default clang\ncompiler, which doesn't compile fftw correctly due to lack of openmp support.\n\n.. code::\n    \n    export OMPI_CC=gcc\n \nNERSC Notes\n+++++++++++\n\nWe maintain a ``daily`` build of nbodykit on NERSC systems that works with the 2.7-anaconda python via the python-mpi-bcast helper. Here is an example job script, that\nruns a friend of friend finder (FOF) then measures the halo power spectrum for halos of mass greater than 1e13 (Msun/h). \n\n.. code:: bash\n\n    #! /bin/bash\n\n    #SBATCH -n 32\n    #SBATCH -p debug\n    #SBATCH -t 10:00\n\n    set -x\n    module load python/2.7-anaconda\n\n    source /project/projectdirs/m779/nbodykit/activate.sh\n\n    srun -n 16 python-mpi $NBKITBIN FOF <<EOF\n    nmin: 10\n    datasource:\n        plugin: FastPM\n        path: data/fastpm_1.0000\n    linklength: 0.2 \n    output: output/fof_ll0.200_1.0000.hdf5\n    calculate_initial_position: True\n    EOF\n\n    srun -n 16 python-mpi $NBKITBIN FFTPower <<EOF\n\n    mode: 2d      # 1d or 2d\n    Nmesh: 256     # size of mesh\n\n    # here are the input fields \n    field:\n      DataSource:\n        plugin: FOFGroups\n        path: output/fof_ll0.200_1.0000.hdf5\n        m0: 2.27e12  # mass of a particle: use OmegaM * 27.75e10 * BoxSize ** 3/ Ntot\n        rsd: z       # direction of RSD, usually use 'z', the default LOS direction.\n    #    select: Rank < 1000 # Limits to the first 1000 halos for abundance matching or\n        select: LogMass > 13 # limit to the halos with logMass > 13 (LRGs). \n    output: output/power_2d_fofgroups.dat  # output\n    EOF\n\nNote that we need to provide the \nmass of a single particle for FOFGroups. The number is :math:`27.75\\times10^{10} \\Omega_M (L / N)^3 h^{-1} M_\\odot`, where :math:`L` is the boxsize in Mpc/h, and \n:math:`N` is the number of particles per side -- :math:`N^3` is the total number of particles. \n\nDetail on NERSC\n----------------\nIf you would like to do development on NERSC (not recommended), then more work is involved.\n\nTo use nbodykit on a NERSC Cray system (e.g. [Edison]_, [Cori]_), we need to ensure the python environment\nis setup to working efficiently on the computing nodes.\n\nIf darshan [darshan]_ or altd are loaded by default, be sure to unload them since they tend to interfere\nwith Python:\n\n.. code::\n\n    module unload darshan\n    module unload altd\n\nand preferentially, use GNU compilers from PrgEnv-gnu\n\n.. code::\n\n    module unload PrgEnv-intel\n    module unload PrgEnv-cray\n    module load PrgEnv-gnu\n\nthen load the Anaconda [anaconda]_ python distribution,\n\n.. code::\n\n    module load python/2.7-anaconda\n\nWe will need to set up the fast python start-up on a Cray computer, since\nthe default start-up scales badly with the number of processes. Our\npreferred method is to use [fast-python]_ . \n\n1. Modify the shell profile, and set PYTHONUSERBASE to a unique location.\n   (e.g. a path you have access on /project) for each machine.\n\n   For example, this is excertion from the profile of \n   a typical user on NERSC (``.bash_profile.ext``),\n   that has access to ``/project/projectdirs/m779/yfeng1``.\n\n.. code:: bash\n\n    if [ \"$NERSC_HOST\" == \"edison\" ]; then\n        export PYTHONUSERBASE=/project/projectdirs/m779/yfeng1/local-edison\n    fi\n\n    if [ \"$NERSC_HOST\" == \"cori\" ]; then\n        export PYTHONUSERBASE=/project/projectdirs/m779/yfeng1/local-cori\n    fi\n\n    export PATH=$PYTHONUSERBASE/bin:$PATH\n    export LIBRARY_PATH=$PYTHONUSERBASE/lib\n    export CPATH=$PYTHONUSERBASE/include\n\n2. Install nbodykit to your user base with ``pip install --user``. \n   Also, create a bundle (tarball) of nbodykit. \n   Repeat this step if nbodykit (or any dependency) is updated.\n\n.. code:: bash\n\n    cd nbodykit;\n\n    MPICC=cc pip install --user -r requirements $PWD\n\n    # enable python-mpi-bcast (On NERSC)\n    source /project/projectdirs/m779/python-mpi/activate.sh\n\n    # create the bundle\n    MPICC=cc bundle-pip nbodykit.tar.gz -r requirements.txt $PWD\n\n\n\nReferences\n==========\n\n.. [nbodyshop] http://www-hpcc.astro.washington.edu/tools/tools.html\n\n.. [pynbody] https://github.com/pynbody/pynbody\n\n.. [yt] http://yt-project.org/\n    \n.. [pfft-python] http://github.com/rainwoodman/pfft-python\n\n.. [pfft] http://github.com/mpip/pfft\n\n.. [pmesh] http://github.com/rainwoodman/pmesh\n\n.. [kdcount] http://github.com/rainwoodman/kdcount\n\n.. [sharedmem] http://github.com/rainwoodman/sharedmem\n\n.. [MP-sort] http://github.com/rainwoodman/MP-sort\n\n.. [h5py] http://github.com/h5py/h5py\n\n.. [numpy] http://github.com/numpy/numpy\n\n.. [scipy] http://github.com/scipy/scipy\n\n.. [pandas] http://pandas.pydata.org/\n\n.. [pytables] http://pandas.pydata.org/\n\n.. [mpi4py] https://bitbucket.org/mpi4py/mpi4py\n\n.. [fast-python] https://github.com/rainwoodman/python-mpi-bcast\n\n.. [bigfile] https://github.com/rainwoodman/bigfile\n\n.. [rocksclusters] http://rocksclusters.org\n\n.. [xi] http://github.com/bareid/xi\n\n.. [edison] https://www.nersc.gov/users/computational-systems/edison/\n\n.. [cori] https://www.nersc.gov/users/computational-systems/cori/\n\n.. [darshan] http://www.mcs.anl.gov/research/projects/darshan/\n\n.. [anaconda] http://docs.continuum.io/anaconda/index\n\n.. [Bianchi] http://arxiv.org/abs/1505.05341\n\n", 
  "description": "Data analysis kit for cosmology simulations, the massively parallel way"
}