{
  "read_at": 1462554073, 
  "description": "The toolbox for the Google Refexp dataset proposed in this paper: http://arxiv.org/abs/1511.02283", 
  "README.md": "# Python toolbox for the Google Referring Expressions Dataset\n\n\nThe Google RefExp dataset is a collection of text descriptions of objects in \nimages which builds on the publicly available [MS-COCO](http://mscoco.org/) \ndataset. Whereas the image captions in MS-COCO apply to the entire image, this \ndataset focuses on\ntext descriptions that allow one to uniquely identify a single object or region \nwithin an image.\nSee more details  in this paper: [Generation and Comprehension of Unambiguous Object Descriptions](http://arxiv.org/abs/1511.02283)\n\n## Sample of the data <a name=\"google_refexp\"></a>\n\nGreen dot is the object that is being referred to.\nSentences are generated by humans in a way that uniquely describes the\nchosen object.\n\n<table width=\"100%\">\n  <tr>\n    <td  align=\"center\"><img src=\"http://www.stat.ucla.edu/~junhua.mao/projects/obj_descrip_folder/sample_images/pic_001.jpg\" alt=\"Mountain View\" width=\"95%\"></td>\n    <td  align=\"center\"><img src=\"http://www.stat.ucla.edu/~junhua.mao/projects/obj_descrip_folder/sample_images/pic_002.jpg\" alt=\"Mountain View\" width=\"95%\" align=\"center\"></td>\n    <td  align=\"center\"><img src=\"http://www.stat.ucla.edu/~junhua.mao/projects/obj_descrip_folder/sample_images/pic_003.jpg\" alt=\"Mountain View\" width=\"95%\" align=\"center\"></td>\n  </tr>\n  \n  <tr>\n    <td valign=\"top\">\n      <ul>\n      <li>A girl wearing glasses and a pink shirt.</li>\n      <li>An Asian girl with a pink shirt eating at the table.</li>\n      </ul>\n    </td>\n    <td valign=\"top\">\n      <ul>\n      <li>A boy brushing his hair while looking at his reflection.<br></li>\n      <li>A young male child in pajamas shaking around a hairbrush in the mirror.</li>\n      </ul>\n    </td>\n    <td valign=\"top\">\n      <ul>\n      <li>Zebra looking towards the camera.<br></li>\n      <li>A zebra third from the left.</li>\n      </ul>\n    </td>\n  </tr>\n</table>\n\n\n## Requirements\n- python 2.7 (Need numpy, scipy, matlabplot, PIL packages. All included in \n[Anaconda](https://store.continuum.io/cshop/anaconda/))\n\n## Setup and data downloading\n\n\n### Easy setup \n\n  ```\n  cd $YOUR_PATH_TO_THIS_TOOLBOX\n  python setup.py\n  ```\n  \nRunning the setup.py script will do the following five things:\n\n1.  Download Google Refexp Data\n2.  Download and compile the COCO toolbox\n3.  Download COCO annotations\n4.  Download COCO images\n5.  Align the Google Refexp Data with COCO annotations\n\nAt each step you will be prompted by keyboard whether or not you would like to \nskip a step.\nNote that the MS COCO images (13GB) and annotations (158MB) are very large and \nit takes some time to download all of them. \n\n### Manual downloading and setup\n\nYou can \ndownload the GoogleRefexp data directly from this \n[link](https://storage.googleapis.com/refexp/google_refexp_dataset_release.zip).\n\n\nIf you have already played with MS COCO and do not want to have two copies of \nthem, you can choose to create a symbolic link from external to your COCO toolkit. E.g.:\n\n  ```\n  cd $YOUR_PATH_TO_THIS_TOOLBOX\n  ln -sf $YOUR_PATH_TO_COCO_TOOLBOX ./external/coco\n  ```\n\nPlease make sure the following on are on your path:\n\n1. compiled PythonAPI at \nexternal/coco/PythonAPI\n2. the annotation file at \nexternal/coco/annotations/instances_train2014.json\n3. COCO images at  external/coco/images/train2014/.\n\nYou can create symbolic links if you have \nalready downloaded the data and compiled the COCO toolbox.\n\nThen run *setup.py* to download the Google Refexp data and compile this toolbox. \nYou can skip steps 2, 3, 4.\n\n## Demos\n\nFor **visualization** and **utility** functions, please see \n*google_refexp_dataset_demo.ipynb*.\n\nFor automatic and Amazon Mechanical Turk (AMT) **evaluation** of the comprehension \nand generation tasks, please see *google_refexp_eval_demo.ipynb*; The \nappropriate output format for a comprehension/generation algorithm is described \nin ./evaluation/format_comprehension_eval.md and \n./evaluation/format_generation_eval.md\n\nWe also provide two sample outputs for reference. For the comprehension task, \nwe use a naive baseline which is a random shuffle of the region candidates \n(./evaluation/sample_results/sample_results_comprehension.json). For the \ngeneration task, we use a naive baseline which outputs the class name of an \nobject (./evaluation/sample_results/sample_results_generation.json).\n\nIf you are not familiar with AMT evaluations, please see this \n[tutorial](http://docs.aws.amazon.com/AWSMechTurk/latest/RequesterUI/amt-ui.pdf)\nThe interface and APIs provided by this toolbox have already grouped 5 \nevaluations into one HIT. In our experiment, paying 2 cents for one HIT leads to \nreasonable results.\n\n\n## Citation\n\nIf you find the dataset and toolbox useful in your research, \nplease consider citing:\n\n    @article{mao2015generation,\n      title={Generation and Comprehension of Unambiguous Object Descriptions},\n      author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan and Murphy, Kevin},\n      journal={arXiv preprint arXiv:1511.02283},\n      year={2015}\n    }\n    \n## License\n\nThis data is released by Google under the following license:\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>.\n    \n## Toolbox Developers\n\n[Junhua Mao](https://www.stat.ucla.edu/~junhua.mao/) and [Oana Camburu](https://www.cs.ox.ac.uk/people/oana-maria.camburu/)\n", 
  "id": 47009782
}