{
  "read_at": 1462553013, 
  "description": "Implementation in Haskell of the one-sided mean kernel and other sequence kernels", 
  "README.md": "### About\n\nThis is a very small toy project that I made to experiment with the Haskell programming language.\n\nIt reads a set of sequences from file `example_data.txt`, computes a Gram matrix of these sequences using the _one-sided mean alignment kernel_, computes the eigenvalues of this matrix and displays them.\nAs the one-sided mean alignment kernel is provably positive definite, the eigenvalues are all positive.\n\nThe project also contains code to generate random dummy time series.\n\n### Usage\n\n1. If you do not have stack installed you can on OS X with `Homebrew`:\n\n    ```bash\n    brew update\n    brew install haskell-stack\n    ```\n2. Then build and run the program:\n\n    ```bash\n    cd haskell-sequence-kernels\n    stack setup\n    stack build\n    stack exec haskell-sequence-kernels-exe\n    ```\n\n### Performance\n\nPerformance is terrible.\n\nThis is because currently dynamic programming is implemented by keeping intermediate results in a `Data.Map` structure which does not have constant access time like vectors.\nIn the future I plan on using a mutable vector or matrix with the `ST` monad.\nNevertheless, on this dataset the Haskell program compiled with `O2` optimization level is quite surprisingly almost faster than the Python equivalent.\n\n### About the one-sided mean alignment kernel\n\nWell-known kernel methods such as for example SVM and Kernel PCA rely on a kernel which is used to compute a Gram matrix which can generally be interpreted as a matrix of similarity values between any pair of samples in the dataset.\nThese algorithms require that the kernel be _positive definite_, which guarantees that the resulting optimization programs are convex whatever the samples.\n\nWhen dealing with vector data the most common choice is generally a Gaussian kernel, but when the samples are time-series (or more generally sequences) custom kernels must be used.\nThere are not many sensible choices, as merely using a classic _dynamic time warping_ distance does not lead to a positive definite kernel.\n\nTo this end we propose the one-sided mean kernel, which has many advantages:\n* Provably positive definite,\n* Faster than competing techniques with a time complexity of `O(l x (m - l))` instead of `O(l x m)` for sequences of length `l < m`,\n* Consistent with a vector kernel when time series are of equal length,\n* Does not suffer from issues of diagonal dominance like the global alignment kernel for example.\n\nFor comparison an implementation in Python is available [here](https://github.com/nchrys/python-sequence-kernels).\n\n\n### TODO\n\n1. Rewrite the algorithm using the `ST` monad,\n2. Implement other kernels such as classic dynamic time warping and the global alignment kernel,\n3. Add support for multivariate time-series,\n4. Automatic selection of the kernel bandwith,\n\n### References\n\n* N. Chrysanthos, P. Beauseroy, H. Snoussi, E. Grall-Maes, __Theoretical properties and implementation of the one-sided mean kernel for time series__, in: _Neurocomputing_ 169 (2015) 196-204\n* M. Cuturi, J.-P. Vert, O. Birkenes, T. Matsui, __A kernel for time series based on global alignments__, in: _IEEE International Conference on Acoustics, Speech and Signal Processing_, 2007\n* J.-P. Vert, __The Optimal Assignment Kernel is not Positive Definite__ in: _arXiv:0801. 4061_\n", 
  "id": 49076855
}