{
  "read_at": 1462546151, 
  "description": "A collection of Kaggle solutions. Not very polished.", 
  "README.md": "# Kaggle solutions\n\nI've been using Kaggle as an excuse to learn techniques in machine\nlearning/artificial intelligence.\n\n## Resources I've been learning from\n\nHere are some primary resources I've been learning from (in rough\nchronological order). For reference, I started from an extensive\nprogramming background, a decent but rusty math background, and a\nrudimentary background in machine learning.\n\n- http://karpathy.github.io/2015/05/21/rnn-effectiveness/: It was fun\n  to play with the released code, even though I didn't yet know what\n  many of the parameters meant.\n\n- http://www.pyimagesearch.com/2014/09/22/getting-started-deep-learning-python/:\n  Didn't worry too much about the details of Deep Belief Networks,\n  since I've been told those aren't behind the most recent advances in\n  deep learning. However, I found a lot of value in actually getting a\n  not-completely-black-boxed neural network up and running.\n\n- http://neuralnetworksanddeeplearning.com/: It wasn't until I worked\n  through this book that I really felt like I understood what was\n  going on (or at least knew enough to have a sense of what I didn't\n  know). I highly recommend going through the entire thing. I was\n  particularly impressed by the author's ability to anticipate my\n  confusions and objections, and also convey intuitions and\n  motivations.\n\n- https://class.coursera.org/ml-005/lecture: Andrew Ng's Machine\n  Learning course. Contains helpful details on a number of topics I\n  hadn't seen before. However, the course moves slower than I was\n  hoping, but with the right cherry-picking it felt pretty useful.\n\n- http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf: Useful for\n  learning some practical tricks for getting better performance out of\n  your neural network. The first half is very useful and readable\n  (though I didn't work through all of the math). The second half\n  seems less so: as they conclude the paper, \"Classical second-order\n  methods are impractical in almost all useful cases\".\n\n- http://deeplearning.net/tutorial/lenet.html: Decent introduction to\n  convolutional neural networks. I wasn't previously familiar with\n  convolutions and didn't fully understand it until I'd read\n  http://www.songho.ca/dsp/convolution/convolution.html.\n\n- http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf:\n  Contains thorough background on recurrent neural networks, with many\n  experiments and tricks for training your own.\n\n- http://andrew.gibiansky.com/blog/machine-learning/conjugate-gradient/:\n  This whole blog is great. It has good exposition on some of the more\n  mathematically-involved techniques.\n\n- http://arxiv.org/pdf/1211.5063v2.pdf: A better choice of activation\nfunction and initialization scheme.\n\n- http://deepdish.io/2015/02/24/network-initialization/: More recent\n  overview of initialization techniques.\n\n- http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html:\n  Great overview of how to think about deep neural networks, and how\n  to train them in practice.\n\n- https://christopherolah.wordpress.com/: Many amazing blog posts\n  which explore deep concepts in accessible ways.\n", 
  "id": 38395872
}