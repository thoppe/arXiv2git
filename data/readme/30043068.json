{
  "read_at": 1462551527, 
  "description": "Fast GPU kernels for convolutional networks.", 
  "README.md": "### Preview release of convolutional kernels\n\nThis is a proof-of-concept preview release of the main GPU kernels\nused in a convolutional neural network (CNN). They are being\nincorporated into a forthcoming release of Nervana's full-featured\nDeep Learning Library, which is currently in limited beta. The preview\nincludes convolutional fprop-backprop-update kernels, dense matrix\nmultiply (GEMM) kernels, and automatically generated element-wise\nkernels. The kernels use an underlying 16-bit representation used in a\nrecent paper by Courbariaux et al.<sup>[1](#refs)</sup> from the Bengio lab,\nwhere they demonstrate that representations as low as 12 bits are\nsufficient for both learning and inference for several state-of-the-art\ndeep networks.\n\nWe are releasing this library to solicit feedback for our full\nrelease. In addition to being very high performance, it will facilitate\nexplorations of limited bit-width, integer-like numerical\nrepresentations for deep networks.\n\n#### Features\n\nThe kernels were designed using\n[MaxAs](https://github.com/NervanaSystems/maxas), our assembler for\nthe NVIDIA Maxwell GPU.\n\nHere are some features of the kernels:\n\n1. They achieve near full utilization on the Maxwell GPU for all\nkernels, including convolutional backprop and update, and are one of\nthe fastest implementations we know of.\n\n2. The 16-bit representation means twice the memory I/O bandwidth and\ntwice the amount of data elements that can fit into DDR.\n\n3. The kernels allow you to set your precision for each operand\nenabling algorithmic explorations at different bit widths.\n\n4. The convolutional kernel features and function arguments are\nidentical to those of cuDNN, with the addition of 3-D convolutions.\n\nThis release supports a 16-bit representation using a 15-bit\nmantissa and sign bit. The integer word length (iwl) can be specified\nper tensor operand. The underlying operations use the native 32-bit \nfloating point arithmetic of the GPU.\n\nWe have a library of fast kernels we use for our internal\nefforts. These kernels support a wider range of problem sizes,\noperations, and numerical formats, and flexible bit-widths. We \nplan to share them in a future release.\n\n#### Prior work\n\nVanhoucke et al.<sup>[2](#refs)</sup> demonstrated limited-precision\nfixed-point SIMD optimizations for CPUs with significant speed\nimprovements for inference in a mixed HMM/NN large vocabulary\nsystem. Coubariaux et al.<sup>[1](#refs)</sup> recently showed success\nwith limited precision for both inference and learning.\n\nWe modeled our kernels' functionality on NVIDIA's cuDNN library\ndescribed in Chetlur et al.<sup>[3](#refs)</sup>. The GEMM kernels are\nalso based on NVIDIA's work as we previously noted\n[here](https://github.com/NervanaSystems/maxas/wiki/SGEMM).\n\nThere is a large number of academic results with CNNs and high quality\nopen source packages<sup>[4-6](#refs)</sup> from which we have\nlearned.  Note that a Fourier domain approach such as Vasilache et\nal.<sup>[5](#refs)</sup> can outperform our kernels for certain\nproblem sizes at the cost of additional memory and flexibility.\n\nAndrew Lavin very recently demonstrated<sup>[7](#refs)</sup> a similar\napproach using [MaxAs](https://github.com/NervanaSystems/maxas) for\nperforming fprop in 32-bit floating point with full utilization\non Maxwell ([repository](https://github.com/eBay/maxDNN),\n[writeup](http://arxiv.org/abs/1501.06633)). The differences with our\nkernels are our 16-bit representation, support for backprop and update\nin addition to fprop, in-place zero padding, upscaling, and 3D\nconvolutions.\n\n#### How to run\n\nThe kernels are wrapped using Python classes which have `numpy` and\n`pycuda` as dependencies. The wrapper classes borrow from `pycuda's`\nGPUArray class. The kernels run on NVIDIA Maxwell only and have been\ntested on Ubuntu 14.04.\n\nThe following demonstration scripts are included:\n\n- `convolution.py` is a simple script that runs forward prop, backprop\nand update kernels.\n\n        % python convolution.py\n        fprop_conv  NCK: (128,192,384) DHW:[1, 13, 13]\n                    TRS:[1, 3, 3] MPQ:[1, 11, 11]:  4.478526 msecs 4589.5 gflops\n        bprop_conv  NCK: (128,192,384) DHW:[1, 13, 13]\n                    TRS:[1, 3, 3] MPQ:[1, 11, 11]:  4.429529 msecs 4640.3 gflops\n        update_conv NCK: (128,192,384) DHW:[1, 13, 13]\n                    TRS:[1, 3, 3] MPQ:[1, 11, 11]:  4.437881 msecs 4631.5 gflops\n\n- `gemm.py` is a simple script that runs several GEMM kernels.\n\n- `bench.py` is a script to generate some of the numbers for Soumith Chintala's\n[page](https://github.com/soumith/convnet-benchmarks).\n\n        % python bench.py\n\n        Our numbers (Layers 1-5 and total in msecs, lower is better):\n\n          forward:    [  41.  112.   42.    5.    9.]  total=208\n          backward:   [ 139.  240.   87.    9.   18.]  total=493\n          gradInput:  [  81.  124.   46.    5.    9.]  total=264\n          gradWeight: [  58.  116.   41.    4.    9.]  total=228\n\n        For comparison, cuDNN R2 reference numbers for GK110, from\n        https://github.com/soumith/convnet-benchmarks (accessed 2/2/15):\n\n          forward:    [  90.  218.   79.    9.   17.]  total=413\n          backward:   [ 189.  606.  230.   27.   47.]  total=1099\n          gradInput:  [  91.  344.  130.   15.   20.]  total=600\n          gradWeight: [  98.  262.  100.   12.   27.]  total=499\n\nThe scripts output basic performance metrics. For more detail, you can\nuse NVIDIA CUDA's\n[`nvprof`](http://docs.nvidia.com/cuda/profiler-users-guide/)\ntool. Keep in mind there are a number of subtle factors determining\nutilization and FLOPS numbers, such as the entropy of the\ninputs. Please consult the documentation.\n\n#### License\n\nPlease note the terms of the software in LICENSE.txt, including\nnon-commercial use and reverse engineering clauses. If you wish to\nlicense this or related software, please contact us at\nlicense@nervanasys.com.  Flexpoint&trade; refers to the different\nnumerical representations we use in our CPU and GPU libraries, as well\nas in Nervana's forthcoming hardware.\n\n#### References <a name=\"refs\"></a>\n\n1. Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre\nDavid. [*Low precision arithmetic for deep learning.*](http://arxiv.org/abs/1412.7024)\narXiv preprint arXiv:1412.7024 (2014).\n\n2. Vanhoucke, Vincent, Andrew Senior, and Mark\nZ. Mao. [*Improving the speed of neural networks on CPUs.*]\n(http://research.google.com/pubs/pub37631.html) Proc. Deep Learning\nand Unsupervised Feature Learning NIPS Workshop (2011).\n\n3. Chetlur, Sharan, Cliff Woolley, Philippe Vandermersch, Jonathan\nCohen, John Tran, Bryan Catanzaro, and Evan Shelhamer.\n[*cuDNN: Efficient primitives for deep learning.*](http://arxiv.org/abs/1410.0759)\narXiv preprint arXiv:1410.0759 (2014).\n\n4. Jia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev,\nJonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor\nDarrell. [*Caffe: Convolutional architecture for fast feature embedding.*](http://caffe.berkeleyvision.org/)\nIn Proceedings of the ACM International Conference on Multimedia,\npp. 675-678. ACM (2014).\n\n5. Vasilache, Nicolas, Jeff Johnson, Michael Mathieu, Soumith\nChintala, Serkan Piantino, and Yann LeCun.\n[*Fast Convolutional Nets With fbfft: A GPU Performance Evaluation.*](http://arxiv.org/abs/1412.7580)\narXiv preprint arXiv:1412.7580 (2014).\n\n6. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton.\n[*Imagenet classification with deep convolutional neural networks.*](https://code.google.com/p/cuda-convnet2/)\nNIPS (2012).\n\n7. Lavin, Andrew.\n[*maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell GPUs.*](http://arxiv.org/abs/1501.06633)\narXiv preprint arXiv:1501.06633 (2015).\n", 
  "id": 30043068
}