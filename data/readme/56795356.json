{
  "read_at": 1462548419, 
  "description": "Soft attention mechanism for video caption generation", 
  "README.md": "# SA-tensorflow\nTensorflow implementation of soft-attention mechanism for video caption generation. \n<center>\n<img src=\"./README_files/head.png\">\nAn example of soft-attention mechanism. The attention weight alpha indicates the temporal attention in one video based on each word.  \n</center>\n\n[[Yao et al. 2015 Describing Videos by Exploiting Temporal Structure]](http://arxiv.org/abs/1502.08029)\nThe original code implemented in Torch can be found [here](https://github.com/yaoli/arctic-capgen-vid).\n# Prerequisites\n* Python 2.7\n* [Tensorflow](https://www.tensorflow.org/) >= 0.7.1\n* NumPy\n* pandas\n* keras\n* java 1.8.0\n\n# Data\nWe pack the data into the format of HDF5, where each file is a mini-batch for training and has the following keys:\n```\n[u'data', u'fname', u'label', u'title']\n```\n\n```batch['data']``` stores the visual features. ```shape (n_step_lstm, batch_size, hidden_dim) ```\n\n```batch['fname']``` stores the filenames(no extension) of videos. ```shape (batch_size)```\n\n```batch['title']``` stores the description. If there are multiple sentences correspond to one video, the other metadata such as visual features, filenames and labels have to duplicate for one-to-one mapping. ```shape (batch_size)```\n\n```batch['label']``` indicates where the video ends. For instance, ```[-1., -1., -1., -1.,  0., -1., -1.]``` means that the video ends at index 4.\n\n```shape (n_step_lstm, batch_size)```\n\n# Usage\n\n## training\n```\n$ python Att.py --task train\n```\n## testing\nTest the model after a certain number of training epochs.\n\n```\n$ python Att.py --task test --net models/model-20\n```\n## Auther\n[Tseng-Hung Chen](https://github.com/paul0819)\n\n[Kuo-Hao Zeng](https://github.com/jacky55062003)\n\n## Acknowledgments\n\nWe modified the code from this repository [jazzsaxmafia/video\\_to\\_sequence](https://github.com/jazzsaxmafia/video_to_sequence) to the temporal-attention model.\n\n## Reference\n\n[1] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A. Courville. \nDescribing videos by exploiting temporal structure. arXiv:1502.08029v4, 2015.\n\n[2] chen:acl11,\n  title = \"Collecting Highly Parallel Data for Paraphrase Evaluation\",\n  author = \"David L. Chen and William B. Dolan\",\n  booktitle = \"Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-2011)\",\n  address = \"Portland, OR\",\n  month = \"June\",\n  year = 2011\n\n[3] [Microsoft COCO Caption Evaluation](https://github.com/tylin/coco-caption)\n", 
  "id": 56795356
}