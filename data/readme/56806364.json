{
  "read_at": 1462548176, 
  "description": "Applying Dense-Sparse-Dense training methodology to SqueezeNet, DSD training improved the top-1 accuracy of SqueezeNet by 4.3% on ImageNet without changing the model architecture and model size. ", 
  "README.md": "### SqueezeNet-DSD-Training\n\nThis repo demos Dense-Sparse-Dense(DSD) training methodology.\n\nThe repo contains the DSD-SqueezeNet caffemodel, which is obtained by applying Dense-Sparse-Dense training methodology to SqueezeNet v1.0. DSD training methodology improves the top-1 accuracy of SqueezeNet by 4.3% on ImageNet without changing the model architecture and model size. \n\nDSD training is an interesting byproduct of network pruning: re-densifying and retraining from a sparse model can improve the accuracy. That is, compared to a dense CNN baseline, dense-sparse-dense (DSD) training yielded higher accuracy. \n\nWe now explain our DSD training strategy. On top of the sparse SqueezeNet (pruned 3x), we let the killed weights recover, initializing them from zero. We let the survived weights keeping their value. We retrained the whole network using learning rate of 1e - 4. After 20 epochs of training, we observed that the top-1 ImageNet accuracy improved by 4.3 percentage-points; \n\nSparsity is a powerful form of regularization. Our intuition is that, once the network arrives at a local minimum given the sparsity constraint, relaxing the constraint gives the network more freedom to escape the saddle point and arrive at a higher-accuracy local minimum. So far, we trained in just three stages of density (dense-sparse-dense), but regularizing models by intermittently pruning parameters10 throughout training would be an interesting area of future work.\n\n\n### Usage:\n\n    $CAFFE_ROOT/build/tools/caffe test --model=trainval.prototxt --weights=DSD_SqueezeNet_top1_0.617579_top5_0.834742.caffemodel --iterations=1000 --gpu 0\n\n### Result:\n      \n    I0421 13:58:46.246104  5184 caffe.cpp:293] accuracy_top1 = 0.617579\n    I0421 13:58:46.246115  5184 caffe.cpp:293] accuracy_top5 = 0.834742\n    I0421 13:58:46.246126  5184 caffe.cpp:293] loss = 1.7059 (* 1 = 1.7059 loss)    \n\n    \n# Related SqueezeNet repo and paper:\n[SqueezeNet](https://github.com/DeepScale/SqueezeNet)\n\n[SqueezeNet-Deep-Compression](https://github.com/songhan/SqueezeNet-Deep-Compression)\n\n[SqueezeNet-Generator](https://github.com/songhan/SqueezeNet-Generator)\n\n[SqueezeNet-DSD-Training](https://github.com/songhan/SqueezeNet-DSD-Training)\n\n[SqueezeNet-Residual](https://github.com/songhan/SqueezeNet-Residual)\n\n\nIf you find SqueezeNet, DSD training, network pruning and Deep Compression useful in your research, please consider citing the paper:\n\n    @article{SqueezeNet,\n      title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5MB model size},\n      author={Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},\n      journal={arXiv preprint arXiv:1602.07360},\n      year={2016}\n    }\n\n    @article{DeepCompression,\n      title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},\n      author={Han, Song and Mao, Huizi and Dally, William J},\n      journal={International Conference on Learning Representations (ICLR)},\n      year={2016}\n    }\n\n    @inproceedings{han2015learning,\n      title={Learning both Weights and Connections for Efficient Neural Network},\n      author={Han, Song and Pool, Jeff and Tran, John and Dally, William},\n      booktitle={Advances in Neural Information Processing Systems (NIPS)},\n      pages={1135--1143},\n      year={2015}\n    }\n\n    @article{han2016eie,\n      title={EIE: Efficient Inference Engine on Compressed Deep Neural Network},\n      author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},\n      journal={International Conference on Computer Architecture (ISCA)},\n      year={2016}\n    }\n\n", 
  "id": 56806364
}