{
  "read_at": 1462559105, 
  "description": "Fast Nearest-Neighbor Retrieval under the Dynamic Time Warping", 
  "README.md": "# LBImproved C++ Library\n\n**Author:** Daniel Lemire \n\n**License:** GPL v3\n\nThis library comes in the form of one short C++ header file. The documentation\nis in the C++ comments and in this file.\n\nThis C++ library implements fast nearest-neighbor retrieval under the dynamic\ntime warping (DTW). This library includes the dynamic programming solution, a\nfast implementation of the LB_Keogh lower bound, as well as a improved lower\nbound called LB_Improved. This library was used to show that LB_Improved can be\nused to retrieve nearest neighbors three times faster on several data sets\nincluding random walks time series or shape time series.\n\nThe library assumes that your time series have the same length. If they don't, you need to preprocess them (perhaps with linear interpolation) so that they have the same length.\n\n**Reference:**\n- Daniel Lemire, Faster Retrieval with a Two-Pass Dynamic-Time-Warping Lower Bound, Pattern Recognition Volume 42, Issue 9, September 2009, Pages 2169-2180. http://arxiv.org/abs/0811.3301\n- Original source: http://code.google.com/p/lbimproved/\n\n## Key features\n\n1) Fast Dynamic Time Warping nearest neighbor retrieval.\n2) Persistence\n3) External-memory: you need only a constant amount of RAM\n\n\n## PREREQUISITES\n\n- You must first build and install the spatial index library \n  (http://research.att.com/~marioh/spatialindex/index.html)\n  I built this software with release 1.3.2 - May 23rd, 2008.\n- While not strictly necessary, SWIG (http://www.swig.org) is strongly recommended.\n  I interact with the library using swig and python.\n- If you are using SWIG, Python is recommended. I have used Python 2.5.\n\n## OPERATING SYSTEM\n\nI built and ran this software with Mac OS 10.4. It also builds under Linux\nif you have Python 2.5 and swig installed. It should be possible to\nuse  any other Unix-like operating system, or even Windows. \n\n## BUILD\n\ntype \"make\"\n\n\n## TESTING\n\ntype \"python unitesting.py\"\n\n##USAGE\n\n```python\nimport dtw\nconstraint = 0.1\nn = 128\nc = int(constraint*n)\nrtree = dtw.TimeSeriesTree(\"mytmpfile.bin\",c,reducdim)\n# randomwalk(n) return a size n array\nfor i in xrange(1000):\n    rtree.add(randomwalk(n))\nx = randomwalk(n)\nfor mode in [rtree.LINEAR, rtree.TREE]:\nfor algo in [ rtree.NAIVE,rtree.LB_KEOGH, rtree.LB_IMPROVED]:\n    rtree.getNearestNeighborCost(x,algo,mode) \nrtree.close()\n# to reopen the tree, just do this:\nrtree = dtw.TimeSeriesTree(\"mytmpfile.bin\")\n```\n\n```c\n#include \"dtw.h\"\n(...)\n// compute the DTW between two vectors:\ndouble fd = mDTW.fastdynamic(x,y);\n// to seek a nearest neighbor, construct a LB_Improved object:\nLB_Improved pruner = new LB_Improved(targetvector, constraint);// constraint can be 10% of vector length\n// then repeatedly call the test method, which returns the best distance so far\ndouble bestdistancesofar = pruner.test(candidate);\n// the test method is typically much cheaper than a full DTW\n```\n", 
  "id": 12219991
}