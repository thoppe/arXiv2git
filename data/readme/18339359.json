{
  "read_at": 1462550753, 
  "description": "Spearmint Salad is a combination of Spearmint and Agnostic Bayes to create an ensemble of predictors over the hyperparameter space while using the fast hyperparameter search approach of Spearmint.", 
  "README.md": "# Spearmint Salad\nThis python software is an implementation of the Sequential Model-Based Ensemble Optimization (ESMBO) algorithm [[1](http://arxiv.org/abs/1402.0796)].  It combines [Spearmint](https://github.com/JasperSnoek/spearmint) [[2](https://nips.cc/Conferences/2012/Program/event.php?ID=3571)] for fast hyperparameter optimization with the Agnostic Bayes theory [[3](http://jmlr.org/proceedings/papers/v32/lacoste14.html)] to generate an ensemble of learning algorithms over the hyperparameter space for increasing the generalization performances.\n\n## Features\n* **Fast hyperparameter optimization** via [Spearmint](https://github.com/JasperSnoek/spearmint), based on a gaussian process modelization of the hyperparameter search space.\n* **State of the art ensemble of learning algorithms** with no extra computational cost at learning time. \n* **Compatible with any programming language** via a configurable command line interface (*coming soon*).\n* **3D vizualization** of the hyperparameter space based on [mayavi](http://code.enthought.com/projects/mayavi/) (Optional dependency).\n* **Easy parallelization** via python's multiprocessing or an implementation of an mpi queue for running on a computer grid (*alpha*).\n* **Crash recovery** (*coming soon*). \n* **Anytime algorithm.** You can obtain the best predictor so far and visualize the behavior at anytime.\n* **MongoDB compatible**. All information gathered during the optimization process is stored in a MongoDB-like structure. \n\n## Dependencies\n* numpy, scipy\n* matplotlib (Optional, for visualization)\n* mayavi, traits (Optional, for 3D visualization)\n* mpi4py, openmpi (Optional, for parallelization over computational grid)\n* scikit-learn (Optional, for playing with the provided examples)\n\n## Usage\n\nFirst, you have to define a hyperparameter search space. To do so, we provide an elegant pythonic tool that mimics the way you would normally instantiate your learning algorithm. Example using SVR from scikit learn:\n```python\nfrom spearmint_salad import hp\nfrom sklearn.svm import SVR\n\n# Encapsulate the class into a hp.Obj to be able to instantiate using variable parameters \n# or constant parameters if necessary.\nhp_space = hp.Obj(SVR)(\n    C = hp.Float( min_val=0.01, max_val=1000, hp.log_scale ), # variable\n    kernel = 'rbf', # constant\n    gamma = hp.Float( min_val=10**-5, max_val=1000, hp.log_scale ), # variable\n    epsilon = hp.Float(min_val=0.01, max_val=1, hp.log_scale), # variable\n)\n```\nNext, it suffices to start the optimization for a given metric on a particular dataset\n\n```python\nfrom spearmint_salad import metric\n\nmetric = metric.SquareDiffLoss()\nmake_salad( hp_space, metric, dataset_path)\n```\nDuring the optimization, you can launch the visualization tool\n```bash\n$ viz.py\n```\n\n![](https://raw.github.com/wiki/recursix/spearmint-salad/viz.png)\n\n## References\n[1] Lacoste, Alexandre, Hugo Larochelle, Francois Laviolette, and Mario Marchand. \"Sequential Model-Based Ensemble Optimization.\" *arXiv* preprint arXiv:1402.0796 (2014).\n\n[2] Snoek, Jasper, Hugo Larochelle, and Ryan P. Adams. \"Practical Bayesian Optimization of Machine Learning Algorithms.\" In *NIPS*, pp. 2960-2968. 2012.\n\n[3] Lacoste, Alexandre, Mario Marchand, Francois Laviolette, and Hugo Larochelle. \"Agnostic Bayesian Learning of Ensembles.\" In *Proceedings of The 31st International Conference on Machine Learning*, pp. 611-619. 2014.\n\n", 
  "id": 18339359
}