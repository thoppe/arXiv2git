{
  "read_at": 1462547827, 
  "description": "G2P with Tensorflow", 
  "README.md": "# Sequence-to-Sequence G2P toolkit\n\nThe tool does Grapheme-to-Phoneme (G2P) conversion using recurrent\nneural network (RNN) with long short-term memory units (LSTM). LSTM\nsequence-to-sequence models were successfully applied in various tasks,\nincluding machine translation [1] and grapheme-to-phoneme [2].\n\nThis implementation is based on python\n[TensorFlow](https://www.tensorflow.org/versions/r0.8/tutorials/seq2seq/index.html),\nwhich allows an efficient training on both CPU and GPU.\n\n## Requirements\n\nThe tool requires TensorFlow. Please see the installation\n[guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md)\nfor details\n\n## Running G2P\n\nA 2-layer LSTM with 64 hidden units is [available for download on cmusphinx website](https://sourceforge.net/projects/cmusphinx/files/G2P%20Models/g2p-seq2seq-cmudict.tar.gz/download).\nUnpack the model after download. It is trained on [CMU English dictionary](http://github.com/cmusphinx/cmudict)\n\nThe easiest way to check how the tool works is to run it the interactive mode\n```\n  python g2p.py --interactive --model [model_folder_path]\n\n```\n\nthen type the words\n\n\nTo generate pronunciations for an English word list with a trained model, run\n\n```\n  python g2p.py --decode [your_wordlist] --model [model_folder_path]\n\n```\nThe wordlist is a text file: one word per line\n\n\nTo count Word Error Rate  of the trained model, run\n\n```\n  python g2p.py --evaluate [your_wordlist] --model [model_folder_path]\n\n```\nThe wordlist is a text file: one word per line\n\n\n## Training G2P system\n\nTo train G2P you need a dictionary (word and phone sequence per line).\nSee an [example dictionary](http://github.com/cmusphinx/cmudict)\n\n```\n  python g2p.py --train [train_dictionary.dic] --model [model_folder_path]\n```\n\nYou can set up maximum training steps:\n```\n  \"--max_steps\" - Maximum number of training steps (Default: 10000).\n     If 0 train until no improvement is observed\n```\n\nIt is a good idea to play with the following parameters:\n```\n  \"--size\" - Size of each model layer (Default: 64).\n     We observed much better results with 512 units, but the training becomes slow\n\n  \"--num_layers\" - Number of layers in the model (Default: 1). \n     For example, you can try 1 if the train set is not large enough, \n     or 3 to hopefully get better results\n\n  \"--learning_rate\" - Initial Learning rate (Default: 0.5) \n\n  \"--learning_rate_decay_factor\" - Learning rate decays by this much (Default: 0.8)\n```\n\nYou can manually point out Development and Test datasets:\n```\n  \"--valid\" - Development dictionary (Default: created from train_dictionary.dic)\n  \"--test\" - Test dictionary (Default: created from train_dictionary.dic)\n```\n\n\n#### Word error rate on 12K test set of CMU dictionary\n\nSystem | WER,%\n--- | --- \nBaseline WFST (Phonetisaurus) | 28.0\nLSTM num_layers=2, size=64    | 32.0\nLSTM num_layers=2, size=512   | **24.7**\n\n\n## References\n---------------------------------------\n\n[1] Ilya Sutskever, Vinyals Oriol and V. Le Quoc. \"Sequence to sequence\nlearning with neural networks.\" In Advances in neural information\nprocessing systems, pp. 3104-3112. 2014.\n\n[2] Yao, Kaisheng, and Geoffrey Zweig. \"Sequence-to-sequence neural net\nmodels for grapheme-to-phoneme conversion.\" arXiv preprint\narXiv:1506.00196, 2015.\n\n", 
  "id": 57041432
}