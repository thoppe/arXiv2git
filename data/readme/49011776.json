{
  "read_at": 1462553009, 
  "description": "Calibrate with Confidence", 
  "README.md": "# CWC\nCalibrate with Confidence\nThese files contain the code for the Calibrate with Confidence method and data for two case studies.\n\n\nFrequently, a set of objects has to be evaluated by a panel of assessors, but not every object is assessed by each assessor and different assessors  may have different standards or use the marking scale differently, which we term \"assessor bias\". \nIn addition to assessor bias, the levels of confidence in each assessment may vary according to the match in expertise between assessor and object and the stringency of assessment.\n\nExamples include the evaluation of: grant proposals submitted to a funding panel; candidates for appointment to a lectureship;  students' performance in an options system where they choose to take examinations from a list of options; the quality of research submitted to national research assessment exercise.\n\nA problem facing such panels is how to account for different levels of bias and confidence amongst panel members. \nWe propose a method to achieve robust calibration assessors and hence robust scores for outputs. \nIt is based upon the premise that we may infer something about assessors' relative standards by examining discrepancies between their evaluations when their evaluations have objects in common,  also taking into account possible differences in declared confidences in their evaluations.\n\nWe have submitted a paper to PLOS ONE. \nOur algorithm is described in detail in that paper which is titled \"Calibration with confidence: A robust and efficient method for panel assessment\" and authored by R.S. MacKay, S. Parker, R. Kenna & R.J. Low.\n\nA preprint version of the paper is available at http://arxiv.org/abs/1504.00340\nOur algorithm is fully described in that preprint too.\nThere is an associated website: http://www.calibratewithconfidence.co.uk\nThere, the data of relevance for the paper and associated code are also available.\nPlease feel free to download and test our algorithm which has been designed to run on both Windows  and Unix based operating systems via Python and Microsoft Excel macro packages. \nIn order to test and improve our algorithm we would be very grateful if you could please leave feedback on your findings via http://www.calibratewithconfidence.co.uk.\n\n\nAdditionally, and in order to satisfy PLOS ONE requirements that data and algorithm be deposited in a stable, public repository so that the data are permanently accessible, we store the data and code here on github.\n\nThe two files here contain the excel code for the Calibrate with Confidence method and data for two case studies.\n\nThe file Casestudy1.xlsx contains the averaged results generated by multiple simulations using the algorithm described in the paper.\nThese are the data reported in Case Study 1 of the paper. \n\nThe file CaseStudy2.xlsm contains the raw (real) data and processed data associated with Case Study 2.\nIt also contains the data generated by application of the algorithm.\nThese are the data reported upon in the paper.\nThe notation is explained in the paper.\n\n\n\nWe welcome questions and feedback.\n", 
  "id": 49011776
}