{
  "read_at": 1462554299, 
  "description": "TripletLoss criterion for Chainer", 
  "README.md": "# TripletChain\nTripletLoss criterion for Chainer\n\n## Usage example\n\n```python\nfrom chainer import Chain, Variable\nfrom loss import triplet_loss\n\nclass TripletNet(Chain):\n\n    def forward_once(self, x_data, train=True):\n        x = Variable(x_data)\n        h = self.layer1(x)\n        ...\n        y = self.layerN(h)\n  \n        return y\n  \n    def forward(self, a, p, n, train=True):\n        y_a = self.forward_once(a, train)\n        y_p = self.forward_once(p, train)\n        y_n = self.forward_once(n, train)\n  \n        return triplet_loss(y_a, y_p, y_n)\n```\n\n## Sampling function example (using chainer)\n\n```python\nfrom scipy.spatial.distance import cdist\nimport numpy as np\nimport itertools\n\ndef sample_softnegative_triplets(X, y, xp, model, margin = 0.2):\n    '''\n    Returns soft-negative triplets as described in Google 2015 paper:\n        http://arxiv.org/pdf/1503.03832.pdf\n    X - train data\n    y - labels\n    xp - chainer.cupy/numpy container (depends wether you have Cuda or not,\n    you can use simple NumPy if you dont have cuda)\n    model - chainer model\n    '''\n    # forward passing X batch through network to get embeddings for every image\n    embeddings = model.forward_once(X, train=False)\n    try:\n        emb = xp.asnumpy(embeddings.data)\n    except Exception:\n        emb = embeddings.data\n    pairwise_distances = cdist(emb, emb, 'sqeuclidean')\n    a_p = []\n    triplets = []\n    for label in np.unique(y):\n        # forming a-p pairs or each class\n        a_p.extend(list(itertools.combinations(np.where(y==label)[0], 2)))\n    for a_i, p_i in a_p:\n        # extracting current pair distance\n        a_p_dist = pairwise_distances[a_i, p_i]\n        # extracting all indexes of negative samples\n        negatives = np.where(y!=y[a_i])[0]\n        # subtracting current pair distance from negative pairs (ie a-n)\n        diff = pairwise_distances[a_i, negatives] - a_p_dist\n        # extracting soft negatives indexes, ie triplets with a-n distance greater than a-p and within margin\n        soft_negatives = np.where((diff>0.) & (diff<margin))[0]\n        # extracting index of most hard among soft-negatives\n        if len(soft_negatives) > 0:\n            # finding min index of differences, pointing to soft negatives index, pointing to negative index\n            n_i = negatives[soft_negatives[diff[soft_negatives].argmin()]]\n            # adding triplet indexes to result list\n            triplets.append(np.array((a_i, p_i, n_i)))\n    # return soft-negative triplets and AP samples amount\n    return np.array(triplets, dtype=np.int32), len(a_p)\n    \n# Can be used in training like this:\ndef train_batch(X, y, xp, optimizer, model):\n    triplets, AP_pairs = sample_softnegative_triplets(xp.asarray(X, dtype=xp.float32), y, xp, model)\n    if len(triplets) == 0:\n        return 0., AP_pairs, 0.\n    a = xp.asarray(X[triplets[:, 0]], dtype=xp.float32)\n    p = xp.asarray(X[triplets[:, 1]], dtype=xp.float32)\n    n = xp.asarray(X[triplets[:, 2]], dtype=xp.float32)\n    optimizer.zero_grads()\n    loss = model.forward(a, p, n)\n    loss.backward()\n    optimizer.update()\n    batch_loss = float(loss.data) * len(triplets)\n    return batch_loss, AP_pairs, len(triplets)\n```\n", 
  "id": 47039936
}