{
  "read_at": 1462556099, 
  "description": "code for Holistically-Nested Edge Detection", 
  "README.md": "## Holistically-Nested Edge Detection\n\nCreated by Saining Xie at UC San Diego\n\n### Introduction:\n\n<img src=\"http://pages.ucsd.edu/~ztu/hed.jpg\" width=\"400\">\n\nWe develop a new edge detection algorithm, holistically-nested edge detection (HED), which performs image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets.  HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are important in order to resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSD500 dataset (ODS F-score of .790) and the NYU Depth dataset (ODS F-score of .746), and do so with an improved speed (0.4s per image). Detailed description of the system can be found in our [paper](http://arxiv.org/abs/1504.06375).\n\n### Citations\n\nIf you are using the code/model/data provided here in a publication, please cite our paper:\n\n    @InProceedings{xie15hed,\n      author = {\"Xie, Saining and Tu, Zhuowen\"},\n      Title = {Holistically-Nested Edge Detection},\n      Booktitle = \"Proceedings of IEEE International Conference on Computer Vision\",\n      Year  = {2015},\n    }\n    \n  \n### Changelog\n\nIf you have downloaded the previous version (testing code) of HED, please note that we updated the code base to the new version of Caffe. We uploaded a new pretrained model with better performance. We adopted the python interface written for the FCN paper instead of our own implementation for training and testing. The evaluation protocol doesn't change.\n\n### Pretrained model\n\nWe provide the pretrained model and training/testing code for the edge detection framework Holistically-Nested Edge Detection (HED). Please see the Arxiv or ICCV paper for technical details. The pretrained model (fusion-output) gives ODS=.790 and OIS=.808 result on BSDS benchmark dataset.\n  0. Download the pretrained model (56MB) from (http://vcl.ucsd.edu/hed/hed_pretrained_bsds.caffemodel) and place it in examples/hed/ folder.\n\n### Installing \n 0. Install prerequisites for Caffe(http://caffe.berkeleyvision.org/installation.html#prequequisites)\n 0. Modified-caffe for HED: https://github.com/s9xie/hed.git\n\n### Training HED\nTo reproduce our results on BSDS500 dataset:\n 0. data: Download the augmented BSDS data (1.2GB) from (http://vcl.ucsd.edu/hed/HED-BSDS.tar) and extract it in data/ folder\n 0. initial model: Download fully convolutional VGG model (248MB) from (http://vcl.ucsd.edu/hed/5stage-vgg.caffemodel) and put it in examples/hed folder\n 0. run the python script **python solve.py** in examples/hed\n\n### Testing HED\nPlease refer to the IPython Notebook in examples/hed/ to test a trained model. The fusion-output, and individual side-output from 5 scales will be produced after one forward pass.\n \nNote that if you want to evaluate the results on BSDS benchmarking dataset, you should do the standard non-maximum suppression (NMS) and edge thinning. We used Piotr's Structured Forest matlab toolbox available here **https://github.com/pdollar/edges**. Some helper functions are also provided in the eval/ folder. \n\n### Precomputed Results\nIf you want to compare your method with HED and need the precomputed results, you can download them from (http://vcl.ucsd.edu/hed/eval_results.tar).\n\n\n### Acknowledgment: \nThis code is based on Caffe. Thanks to the contributors of Caffe. Thanks @shelhamer and @longjon for providing fundamental implementations that enable fully convolutional training/testing in Caffe.\n\n    @misc{Jia13caffe,\n      Author = {Yangqing Jia},\n      Title = { {Caffe}: An Open Source Convolutional Architecture for Fast Feature Embedding},\n      Year  = {2013},\n      Howpublished = {\\url{http://caffe.berkeleyvision.org/}}\n    }\n\nIf you encounter any issue when using our code or model, please let me know.\n", 
  "id": 43477752
}