{
  "read_at": 1462548384, 
  "description": "Python implementation of tabular asynchronous actor critic", 
  "README.md": "## Summary\nThis repo contains a process-based implementation of tabular, 1-step,\nasynchronous advantage actor critic. It's pretty different from the A3C \nalgorithm from the Asynchronous Methods for Deep \nReinforcement Learning paper: http://arxiv.org/abs/1602.01783\nin that it does not use a function approximator and in that it\ndoes not incorporate (the forward view) of eligibility traces.\n\nIt is similar in that it implements advantage actor critic\nwith multiple agents updating weights in parallel.\n\nThere's also a simple test maze markov decision process (MDP).\nWhether or not running with muiltiple processes is beneficial\nseems to depend upon the specific maze MDP you use.\n\n## Results\n### single process:\n- 2x5 maze: 5.4 seconds\n- 2x10 maze: 21.9 seconds\n- 1x30 maze: 49.2 seconds\n- 5x3 maze: 11.9 seconds\n\n### two processes:\n- 2x5 maze: 3.5 seconds\n- 2x10 maze: 22.1 seconds\n- 1x30 maze: 79.2 seconds\n- 5x3 maze: 18.5 seconds\n\n## File Descriptions\n- run_experiment.py: Script for running an experiment.\n- async_actor_critic.py: Contains the implementation of asynchronous actor critic.\n- experiment.py: Contains implementations for the Experiment and MultiProcessExperiment\nclasses.\n- maze_mdp.py: Defines a simple MDP on which to test the actor critic implementation.\n- utils.py: Utilities used by the algorithm and for plotting.\n", 
  "id": 57194097
}