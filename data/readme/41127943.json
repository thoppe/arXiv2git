{
  "README": "# Copyright 2015 Yin Zheng, Yu-Jin Zhang, Hugo Larochelle. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without modification, are\n# permitted provided that the following conditions are met:\n#\n#    1. Redistributions of source code must retain the above copyright notice, this list of\n#       conditions and the following disclaimer.\n#\n#    2. Redistributions in binary form must reproduce the above copyright notice, this list\n#       of conditions and the following disclaimer in the documentation and/or other materials\n#       provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY Yin Zheng, Yu-Jin Zhang, Hugo Larochelle ``AS IS'' AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL\n# Yin Zheng, Yu-Jin Zhang, Hugo Larochelle OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,\n# INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED\n# TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\n# THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n#\n# The views and conclusions contained in the software and documentation are those of the\n# authors and should not be interpreted as representing official policies, either expressed\n# or implied, of Yin Zheng, Yu-Jin Zhang, Hugo Larochelle.\n\n\n\t\t\n'''\n@reference: A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data, IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI)\n\n\t\t\tProject page: https://sites.google.com/site/zhengyin1126/home/supdeepdocnade\n\n@Authors:     Yin Zheng, Received Ph.D from Tsinghua University, 2015. Homepage: https://sites.google.com/site/zhengyin1126/\n              Yu-Jin Zhang, Tsinghua University,\n              Hugo Larochelle, University of Sherbrooke and Twitter. Homepage: http://www.dmi.usherb.ca/~larocheh/index_en.html\n\n@contact: Yin Zheng, yzheng3xg@gmail.com\n          \n'''\n\n\nWe provide:\n1. SupDeepDocNADE.py : Classes of DeepDocNADE and SupDeepDocNADE, which is used by run_pretrain_DeepDocNADE.py and run_SupDeepDocNADE.py\n2. run_pretrain_DeepDocNADE.py : Python script to pretrain DeepDocNADE model on MIR Flickr Unlabeled data\n3. run_SupDeepDocNADE.py : Python script to train and test SupDeepDocNADE mdoel.\n4. gen_dataset_labeled.py: Python script to generate the labeled dateset files needed by the model\n5. gen_dataset_unlabeled.py: Python script to generate the unlabeled dateset files needed by the model\n6. MIR_Flickr_Theano_lab.py: Python script to load labeled dataset and provide interface to SupDeepDocNADE model\n7. MIR_Flickr_Theano_Unlab.py: Python script to load unlabeled dataset and provide interface to DeepDocNADE model\n\n===================================================================================================================== \t\n\n0. Install liblinear (http://www.csie.ntu.edu.tw/~cjlin/liblinear/) and put liblinear into the PYTHONPATH\n\tNOTE: make sure that you can use liblinear in python.  To test whether you config liblinear successfully, you could\n\tuse \"from liblinearutil import *\" in python.\n\n1. Generate the dataset:\n\t1) Download the dataset from Nitsh's homepage: http://www.cs.toronto.edu/~nitish/multimodal/index.html\n\t2) Run \"python gen_dataset_labeled.py\" to generate the labeled dataset. \n\t\ta) Read the comments in the script about how to use it.\n\t\tb) create a file IN THE SAME PATH OF THE LABELED DATASET named \"meta.txt\" with the following lines in the meta.txt\n\t\t\tvoc_size:2000\n\t\t\tn_regions:14\n\t\t\ttext_voc_size:2000\n\t\t\tglobal_feat_size:1857\n\t\t\tn_classes:38\n\t\tc) Create a file IN THE SAME PATH OF THE LABELED DATASET named \"sizes.txt\" with the following content:\n\t\t\t10000\n\t\t\t5000\n\t\t\t10000\n\t\n\t3) Run \"python gen_dataset_unlabeled.py\" to generate the unlabeled dataset. \n\t\ta) Read the comments in the script about how to use it.\n\t\tb) create a file IN THE SAME PATH OF THE UNLABELED DATASET with the name meta.txt, the content is:\n\t\t\tvoc_size:2000\n\t\t\tn_regions:14\n\t\t\ttext_voc_size:2000\n\t\t\tglobal_feat_size:1857\n\t\t\tdataset_split:50\n\t\tc) create a file IN THE SAME PATH OF THE UNLABELED DATASET named \"sizes.txt\" with the following content:\n\t\t\t975000\n\t\t\t\n\t\t\n\t\t\t\n2. Run run_pretrain_DeepDocNADE.py to pretrain DeepDocNADE model on unlabeled data:\n\t\tpython run_pretrain_DeepDocNADE.py n_pretrain pre_learning_rate hidden_size activation_function  dropout_rate model_file_dir unlab_dataset_dir batch_size anno_weight  polyakexp_weight model_init\n\t\n\n\tThe parameters of the script is as follows:\n\t\n\t\tn_pretrain : number of iterations\n\t\tpre_learning_rate : learning rate of the pretraining \n\t\thidden_size : the hidden size of the model, e.g. 2048_2048_2048 is a 3 hidden layers model with 2048 units each layer\n\t\tactivation_function : the activation function of the hidden layers, \"sigmoid, relu or tanh\" \n\t\tdropout_rate: the dropout rate for each hidden layer, e.g. \"0.5_0.5_0.5\" means dropout rate is 0.5 for each layer\n\t\tmodel_file_dir: path to save the pretrained model\n\t\tunlab_dataset_dir: the path to the unlabeled dataset \n\t\tbatch_size: the batch size \n\t\tanno_weight: annotation weight \n\t\tpolyakexp_weight: polyak weight\n\t\tmodel_init: path to the saved pretrain model, which is used to continue_training based on the trained model. It could be NULL if no model saved before.\n\t\n\tOne example of the scipt is: \n\t\tpython run_pretrain_DeepDocNADE.py 6000 0.03 2048_2048_2048 reclin 0.5_0.5_0.5 PATH_TO_SAVE_THE_MODEL PATH_TO_UNLABELED_DATA 500 12000 std 0.9995 PATH_TO_SAVED_MODEL\n\n\n3. Run run_SupDeepDocNADE.py to train SupDeepDocNADE model on labeled data, which could be trained from scratch or initialize from pretrained model on unlabeled data :\n\t\tpython run_SupDeepDocNADE.py folder_ID use_pretrain max_iter look_ahead hidden_size learning_rate unsup_weight activation_function Linear_minC, Linear_maxC, dropout_rate uniresult_dir Pretrain_model_name lab_dataset_dir batch_size anno_weight polyakexp_weight\n\t\n\tThe parameters are as follows:\n\t\tfolder_ID : ID of the dataset ( 1 to 5 )\n\t\tuse_pretrain: Whether use pretrained model or training from randomly initialized parameters (True or False)\n\t\tmax_iter: number of max iterations\n\t\tlook_ahead: early stop if number of iterations without improvement exceed the number of look ahead\n\t\thidden_size: the hidden size of the model, e.g. 2048_2048_2048 is a 3 hidden layers model with 2048 units each layer\n\t\tlearning_rate: learning rate of training process\n\t\tunsup_weight: the weight of the unsupervised part \n\t\tactivation_function: the activation function of the hidden layers, \"sigmoid, relu or tanh\" \n\t\tLinear_minC: the minimum value of C for linear SVM (in log_2 space) \n\t\tLinear_maxC: the max value of C for linear SVM (in log_2 space) \n\t\tdropout_rate: the dropout rate for each hidden layer, e.g. \"0.5_0.5_0.5\" means dropout rate is 0.5 for each layer\n\t\tuniresult_dir: the path to save the results\n\t\tPretrain_model_name: the path and the name of the pretrained model if you set use_pretrain True. Otherwise, any string\n\t\tlab_dataset_dir: the path to the labeled dataset\n\t\tbatch_size : the mini-batch size\n\t\tanno_weight: the weight of the annotation words\n\t\tpolyakexp_weight: polyak weight\n\t\n\tOne example of the script is: \n\t\t\n\t\tpython run_SupDeepDocNADE.py 1 True 20000 300 2048_2048_2048 0.01 0.25 reclin -17 10 0.5_0.5_0.5 PATH_TO_SAVE_RESULTS PATH_NAME_OF_PRETRAINED_MODEL PATH_TO_LABELED_DATA 500 12000  0.9995\n\t\n\t\n\t\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n", 
  "read_at": 1462544931, 
  "description": "A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data, TPAMI, http://arxiv.org/abs/1409.3970", 
  "id": 41127943
}