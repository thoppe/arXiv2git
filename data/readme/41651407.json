{
  "read_at": 1462544980, 
  "description": "An implementation of the paper 'A Neural Algorithm of Artistic Style'.", 
  "README.md": "# Implementation of 'A Neural Algorithm of Artistic Style'\n\nThis is a Torch7 implementation of the method described in the paper \n'A Neural Algorithm of Artistic Style' by Leon Gatys, Alexander Ecker, and Matthias Bethge (http://arxiv.org/abs/1508.06576).\n\n![](examples/starry_eiffel_short.gif)\n\n[(Longer animation)](http://gfycat.com/UnawareUnfortunateEkaltadeta)\n\n## Dependencies\n\n- [Torch7](https://github.com/torch/torch7)\n- [imagine-nn](https://github.com/szagoruyko/imagine-nn) (for Inception network)\n- CUDA 6.5+ (unless running on CPU -- see below)\n\nimagine-nn (and any other Torch packages you're missing) can be installed via Luarocks:\n\n```\nluarocks install inn\n```\n\n## Usage\n\nFirst, download the models by running the download script:\n\n```\nbash download_models.sh\n```\n\nThis downloads the model weights for the VGG and Inception networks.\n\nBasic usage:\n\n```\nqlua main.lua --style <style.jpg> --content <content.jpg> --style_factor <factor>\n```\n\nwhere `style.jpg` is the image that provides the style of the final generated image, and `content.jpg` is the image that provides the content. `style_factor` is a constant that controls the degree to which the generated image emphasizes style over content. By default it is set to 2E9.\n\nThis generates an image using the VGG-19 network by Karen Simonyan and Andrew Zisserman (http://www.robots.ox.ac.uk/~vgg/research/very_deep/).\n\nOther options:\n\n- `model`: {inception, vgg}. Convnet model to use. Inception refers to Google's [Inception architecture](http://arxiv.org/abs/1409.4842). Default is VGG.\n- `num_iters`: Number of optimization steps. Default is 500.\n- `size`: Long edge dimension of the generated image. Set to 0 to use the size of the content image. Default is 500.\n- `display_interval`: Number of iterations between image displays. Set to 0 to suppress image display. Default is 20.\n- `smoothness`: Constant that controls the smoothness of the generated image (total variation norm regularization strength). Useful when using Inception model (set to ~5E-3). Default is 0.\n- `init`: {image, random}. Initialization mode for the optimized image. `image` initializes with the content image; `random` initializes with random Gaussian noise. Default is `image`.\n- `backend`: {cunn, cudnn}. Neural network CUDA backend. `cudnn` requires the [Torch bindings](https://github.com/soumith/cudnn.torch/tree/R3) for CuDNN R3.\n- `optimizer`: {sgd, lbfgs}. Optimization algorithm. `lbfgs` is slower per iteration and consumes more memory, but may yield better results. Default is `sgd`.\n- `cpu`: Optimize on CPU instead of GPU (only VGG model supported).\n\n### Out of memory?\n\nThe VGG network with the default L-BFGS optimizer gives the best results. However, this setting also requires a lot of GPU memory. If you run into CUDA out-of-memory errors, try running with the Inception architecture or with the SGD optimizer:\n\n```\nqlua main.lua --style <style.jpg> --content <content.jpg> --model inception --optimizer sgd\n```\n\nYou can also try reducing the size of the generated image:\n\n```\nqlua main.lua --style <style.jpg> --content <content.jpg> --size 300\n```\n\nIf all else fails (or if you don't have a CUDA-compatible GPU), you can optimize on CPU:\n\n```\nqlua main.lua --style <style.jpg> --content <content.jpg> --cpu\n```\n\n## Examples\n\nThe Eiffel Tower in the style of Edvard Munch's *The Scream*:\n\n![](examples/eiffel_scream_short.gif)\n\n[(Longer animation)](http://gfycat.com/WebbedValuableGreyhounddog)\n\nPicasso-fied Obama:\n\n![](examples/picasso_obama_short.gif)\n\n[(Longer animation)](http://gfycat.com/WeakPettyDevilfish)\n\n## Implementation Details\n\nWhen using the Inception network, the outputs of the following layers are used to optimize for style: `conv1/7x7_s2`, `conv2/3x3`, `inception_3a`, `inception_3b`, `inception_4a`, `inception_4b`, `inception_4c`, `inception_4d`, `inception_4e`.\n\nThe outputs of the following layers are used to optimize for content: `inception_3a`, `inception_4a`.\n\nBy default, the optimized image is initialized using the content image; the implementation also works with white noise initialization, as described in the paper.\n\nIn order to reduce high-frequency \"screen door\" noise in the generated image (especially when using the Inception network), total variation regularization is applied (idea from [cnn-vis](https://github.com/jcjohnson/cnn-vis) by [jcjohnson](https://github.com/jcjohnson)).\n\n## Acknowledgements\n\nThe weights for the Inception network used in this implementation were ported to Torch from the publicly-available [Caffe](https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet) distribution.\n\nThanks to the [Bethge Group](http://bethgelab.org/deepneuralart/) for providing the weights to the normalized VGG network used here.\n", 
  "id": 41651407
}