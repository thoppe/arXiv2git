{
  "id": 46604540, 
  "read_at": 1462554595, 
  "README.txt": "\nORB-SLAM is a versatile and accurate Monocular SLAM solution able to compute in real-time the camera trajectory and a sparse 3D reconstruction of the scene in a wide variety of environments, ranging from small hand-held sequences to a car driven around several city blocks. It is able to close large loops and perform global relocalisation in real-time and from wide baselines.\n\nRelated Publications:\n\n[1] Raul Mur-Artal, J. M. M. Montiel and Juan D. Tardos. ORB-SLAM: A Versatile and Accurate Monocular SLAM System. Submitted to IEEE Transactions on Robotics. arXiv preprint: http://arxiv.org/abs/1502.00956\n\n\n1) License\n\nORB-SLAM is released under a GPLv3 license. Please note that we provide along ORB-SLAM a modified version of g2o and DBoW2 which are both BSD. \n\nFor a closed-source version of ORB-SLAM for commercial purposes, please contact the authors. \n\nIf you use ORB-SLAM in an academic work, please cite:\n\n@article{murSubTro2015,\n  title={{ORB-SLAM}: a Versatile and Accurate Monocular {SLAM} System},\n  author={Mur-Artal, Ra\\'ul, Montiel, J. M. M. and Tard\\'os, Juan D.},\n  journal={Submitted to IEEE Transaction on Robotics. arXiv preprint arXiv:1502.00956},\n  year={2015}\n}\n\n\n2) Prerequisites (dependencies)\n\nBoost\nWe use the Boost library to launch the different threads of our SLAM system.\n\n\tsudo apt-get install libboost-all-dev \n\nROS\nWe use ROS to receive images from the camera or from a recorded sequence (rosbag), and for visualization (rviz, image_view). \nWe have tested ORB-SLAM in Ubuntu 12.04 with ROS Fuerte, Groovy and Hydro. \nIf you do not have already installed ROS in your computer, we recommend you to install the Full-Desktop version of ROS Fuerte. \n<http://wiki.ros.org/fuerte/Installation/Ubuntu>.\n\ng2o dependencies (Eigen, BLAS, LAPACK, CHOLMOD)\nWe use g2o to perform several optimizations. We include a modified copy of the library including only the components we need \nand also some changes that are listed in Thirdparty/g2o/Changes.txt. \nIn order to compile g2o you will need to have installed CHOLMOD, BLAS, LAPACK and Eigen3.\n\n\tsudo apt-get install libsuitesparse-dev\n\tsudo apt-get install libblas-dev\n\tsudo apt-get install liblapack-dev\n\tsudo apt-get install libeigen3-dev\n\nDBoW2\nWe make use of some components of the DBoW2 library for place recognition and feature matching. We include a modified copy of the library\nincluding only the components we need and also some modifications that are listed in Thirdparty/DBoW2/LICENSE.txt. \nIt only depends on OpenCV, but it should be included in the ROS distribution.\n\n\n3) Installation\n\n1. Make sure you have installed ROS and all library dependencies (boost, eigen3, cholmod, blas, lapack).\n\n2. In your ROS package path (check your environment variable ROS_PACKAGE_PATH) clone this repository:\n\n\t\tgit clone https://github.com/raulmur/ORB_SLAM.git ORB_SLAM\n\n3. Build g2o. Go into Thirdparty/g2o/ and execute:\n\n\t\tmkdir build\n\t\tcd build\n\t\tcmake .. -DCMAKE_BUILD_TYPE=Release\n\t\tmake \n\n\tTip: To achieve the best performance in your computer, set your favorite compilation flags in line 97 and 98 of Thirdparty/g2o/CMakeLists.txt \n\t\t  (by default -03 -march=native)\n\n4. Build DBoW2. Go into Thirdparty/DBoW2/ and execute:\n\n\t\tmkdir build\n\t\tcd build\n\t\tcmake .. -DCMAKE_BUILD_TYPE=Release\n\t\tmake  \n\n\tTip: Set your favorite compilation flags in line 4 and 5 of Thirdparty/DBoW2/CMakeLists.txt\n\t\t  (by default -03 -march=native)\n\n5. Build ORB_SLAM. In the ORB-SLAM root execute:\n\n\t\tmkdir build\n\t\tcd build\n\t\tcmake .. -DROS_BUILD_TYPE=Release\n\t\tmake\n\n\tTip: Set your favorite compilation flags in line 12 and 13 of Thirdparty/DBoW2/CMakeLists.txt\n\t\t  (by default -03 -march=native)\n\n4) Usage\n\nSee section 5 to run the Example Sequence.\n\n1. Launch ORB-SLAM from the terminal (roscore should have been already executed):\n\n\t\trosrun ORB_SLAM ORB_SLAM PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE\n\nYou have to provide the path to the ORB vocabulary and to the settings file. The paths must be absolute or relative to the ORB_SLAM directory.  \nWe already provide the vocabulary file we use in ORB_SLAM/Data. Uncompress the file, as it will be loaded much faster.\n\n2. The last processed frame is published to the topic /ORB_SLAM/Frame. You can visualize it using image_view:\n\n\t\trosrun image_view image_view image:=/ORB_SLAM/Frame _autosize:=true\n\n3. The map is published to the topic /ORB_SLAM/Map, the current camera pose and global world coordinate origin are sent through /tf in frames /ORB_SLAM/Camera and /ORB_SLAM/World respectively.  Run rviz to visualize the map:\n\t\n\t- for ROS Fuerte:\n\n\t\trosrun rviz rviz -d Data/rviz.vcg\n\n\t- for ROS Groovy and Hydro:\n\n\t\trosrun rviz rviz -d Data/rviz.rviz\n\n4. ORB_SLAM will receive the images from the topic /camera/image_raw. You can now play your rosbag or start your camera node. \nCurrently we do not support reading image files from disk. If you have a sequence with individual image files, \nyou will need to generate a bag from them. We provide a tool to do that: https://github.com/raulmur/BagFromImages\n\n\nTip: Use a roslaunch to launch ORB_SLAM, image_view and rviz from just one instruction. We provide an example:\n\n\t- for ROS Fuerte:\n\n\t\troslaunch ExampleFuerte.launch\n\n\t- for ROS Groovy and Hydro:\n\n\t\troslaunch ExampleGroovyHydro.launch\n\n\n5) Example Sequence\nWe provide the settings and the rosbag of an example sequence in our lab. In this sequence you will see a loop closure and two relocalisation from a big viewpoint change.\n\n1. Download the rosbag file:  http://webdiis.unizar.es/~raulmur/orbslam/downloads/Example.bag.tar.gz\nUncompress the file.\n\n2. Launch ORB_SLAM with the settings for the example sequence. You should have already uncompressed the vocabulary file (/Data/ORBvoc.yml.tar.gz)\n\n\t- for ROS Fuerte:\n\n\t\troslaunch ExampleFuerte.launch\n\n\t- for ROS Groovy or newer versions:\n\n\t\troslaunch ExampleGroovyHydro.launch\n\n3. Once the ORB vocabulary has been loaded, start playing the bag \n\n\t\trosbag play --pause Example.bag\n\n\tand press space to start\n\n\n6) The Settings File\n\nORB_SLAM reads the camera calibration and setting parameters from a YAML file. We provide an example in Data/Settings.yaml, where you will find all parameters and their description. We use the camera calibration model of OpenCV.\n\nPlease make sure you write and call your own settings file for your camera (copy the example file and modify the calibration)\n\n7) Failure Modes\n\nYou should expect to achieve good results in sequences similar to those in which we show results in our paper [1], in terms of camera movement and texture in the environment. In general our Monocular SLAM solution is expected to have a bad time in the following situations:\n- Pure rotations in exploration\n- Low texture environments\n- Many (or big) moving objects, especially if they move slowly.\n\nThe system is able to initialize from planar and non-planar scenes. In the case of planar scenes, depending on the camera movement relative to the plane, it is possible that the system refuses to initialize, see the paper [1] for details. \n\n8) Need Help?\n\nIf you have any trouble installing or running ORB-SLAM, contact the authors.\n\n\n", 
  "README.md": "# ORB_SLAM\n\nORB-SLAM is a versatile and accurate Monocular SLAM solution able to compute in real-time the camera trajectory and a sparse 3D reconstruction of the scene in a wide variety of environments, ranging from small hand-held sequences to a car driven around several city blocks. It is able to close large loops and perform global relocalisation in real-time and from wide baselines.\n\nSee our project webpage: http://webdiis.unizar.es/~raulmur/orbslam/\n\n###Related Publications:\n\n[1] Raul Mur-Artal, J. M. M. Montiel and Juan D. Tardos. **ORB-SLAM: A Versatile and Accurate Monocular SLAM System**. *Submitted to IEEE Transactions on Robotics. arXiv preprint: http://arxiv.org/abs/1502.00956*\n\n\n#1. License\n\nORB-SLAM is released under a GPLv3 license. Please note that we provide along ORB-SLAM a modified version of g2o and DBoW2 which are both BSD. \n\nFor a closed-source version of ORB-SLAM for commercial purposes, please contact the authors. \n\nIf you use ORB-SLAM in an academic work, please cite:\n\n    @article{murSubTro2015,\n      title={{ORB-SLAM}: a Versatile and Accurate Monocular {SLAM} System},\n      author={Mur-Artal, Ra\\'ul, Montiel, J. M. M. and Tard\\'os, Juan D.},\n      journal={Submitted to IEEE Transaction on Robotics. arXiv preprint arXiv:1502.00956},\n      year={2015}\n     }\n\n\n#2. Prerequisites (dependencies)\n\n##2.1 Boost\n\nWe use the Boost library to launch the different threads of our SLAM system.\n\n\tsudo apt-get install libboost-all-dev \n\n##2.2 ROS\nWe use ROS to receive images from the camera or from a recorded sequence (rosbag), and for visualization (rviz, image_view). \n**We have tested ORB-SLAM in Ubuntu 12.04 with ROS Fuerte, Groovy and Hydro**. \nIf you do not have already installed ROS in your computer, we recommend you to install the Full-Desktop version of ROS Fuerte (http://wiki.ros.org/fuerte/Installation/Ubuntu).\n\n##2.3 g2o (included)\nWe use g2o to perform several optimizations. We include a modified copy of the library including only the components we need \nand also some changes that are listed in `Thirdparty/g2o/Changes.txt`. \nIn order to compile g2o you will need to have installed CHOLMOD, BLAS, LAPACK and Eigen3.\n\n\tsudo apt-get install libsuitesparse-dev\n\tsudo apt-get install libblas-dev\n\tsudo apt-get install liblapack-dev\n\tsudo apt-get install libeigen3-dev\n\n##2.4 DBoW2 (included)\nWe make use of some components of the DBoW2 library for place recognition and feature matching. We include a modified copy of the library\nincluding only the components we need and also some modifications that are listed in `Thirdparty/DBoW2/LICENSE.txt`. \nIt only depends on OpenCV, but it should be included in the ROS distribution.\n\n\n#3. Installation\n\n1. Make sure you have installed ROS and all library dependencies (boost, eigen3, cholmod, blas, lapack).\n\n2. In your ROS package path (check your environment variable `ROS_PACKAGE_PATH`) clone this repository:\n\n\t\tgit clone https://github.com/raulmur/ORB_SLAM.git ORB_SLAM\n\n3. Build g2o. Go into `Thirdparty/g2o/` and execute:\n\n\t\tmkdir build\n\t\tcd build\n\t\tcmake .. -DCMAKE_BUILD_TYPE=Release\n\t\tmake \n\n\t*Tip: To achieve the best performance in your computer, set your favorite compilation flags in line 97 and 98 of* `Thirdparty/g2o/CMakeLists.txt` \n\t\t  (by default -03 -march=native)\n\n4. Build DBoW2. Go into Thirdparty/DBoW2/ and execute:\n\n\t\tmkdir build\n\t\tcd build\n\t\tcmake .. -DCMAKE_BUILD_TYPE=Release\n\t\tmake  \n\n\t*Tip: Set your favorite compilation flags in line 4 and 5 of* `Thirdparty/DBoW2/CMakeLists.txt` (by default -03 -march=native)\n\n5. Build ORB_SLAM. In the ORB_SLAM root execute:\n\n\t\tmkdir build\n\t\tcd build\n\t\tcmake .. -DROS_BUILD_TYPE=Release\n\t\tmake\n\n\t*Tip: Set your favorite compilation flags in line 12 and 13 of* `Thirdparty/DBoW2/CMakeLists.txt` (by default -03 -march=native)\n\n#4. Usage\n\n**See section 5 to run the Example Sequence**.\n\n1. Launch ORB-SLAM from the terminal (`roscore` should have been already executed):\n\n\t\trosrun ORB_SLAM ORB_SLAM PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE\n\n  You have to provide the path to the ORB vocabulary and to the settings file. The paths must be absolute or relative   to the ORB_SLAM directory.  \n  We already provide the vocabulary file we use in `ORB_SLAM/Data/ORBvoc.yml`. Uncompress the file, as it will be   loaded much faster.\n\n2. The last processed frame is published to the topic `/ORB_SLAM/Frame`. You can visualize it using `image_view`:\n\n\t\trosrun image_view image_view image:=/ORB_SLAM/Frame _autosize:=true\n\n3. The map is published to the topic `/ORB_SLAM/Map`, the current camera pose and global world coordinate origin are sent through `/tf` in frames `/ORB_SLAM/Camera` and `/ORB_SLAM/World` respectively.  Run `rviz` to visualize the map:\n\t\n\t*in ROS Fuerte*:\n\n\t\trosrun rviz rviz -d Data/rviz.vcg\n\n\t*in ROS Groovy or Hydro*:\n\n\t\trosrun rviz rviz -d Data/rviz.rviz\n\n4. ORB_SLAM will receive the images from the topic `/camera/image_raw`. You can now play your rosbag or start your camera node. \nIf you have a sequence with individual image files, you will need to generate a bag from them. We provide a tool to do that: https://github.com/raulmur/BagFromImages.\n\n\n**Tip: Use a roslaunch to launch `ORB_SLAM`, `image_view` and `rviz` from just one instruction. We provide an example**:\n\n*in ROS Fuerte*:\n\n\troslaunch ExampleFuerte.launch\n\n*in ROS Groovy or Hydro*:\n\n\troslaunch ExampleGroovyHydro.launch\n\n\n#5. Example Sequence\nWe provide the settings and the rosbag of an example sequence in our lab. In this sequence you will see a loop closure and two relocalisation from a big viewpoint change.\n\n1. Download the rosbag file:  \n\thttp://webdiis.unizar.es/~raulmur/orbslam/downloads/Example.bag.tar.gz. \n\n\tAlternative link: https://drive.google.com/file/d/0B8Qa2__-sGYgRmozQ21oRHhUZWM/view?usp=sharing\n\n\tUncompress the file.\n\n2. Launch ORB_SLAM with the settings for the example sequence. You should have already uncompressed the vocabulary file (`/Data/ORBvoc.yml.tar.gz`)\n\n  *in ROS Fuerte*:\n\n\t  roslaunch ExampleFuerte.launch\n\n\t*in ROS Groovy or newer versions*:\n\n\t  roslaunch ExampleGroovyHydro.launch\n\n3. Once the ORB vocabulary has been loaded, play the rosbag (press space to start):\n\n\t\trosbag play --pause Example.bag\n\n\n#6. The Settings File\n\nORB_SLAM reads the camera calibration and setting parameters from a YAML file. We provide an example in `Data/Settings.yaml`, where you will find all parameters and their description. We use the camera calibration model of OpenCV.\n\nPlease make sure you write and call your own settings file for your camera (copy the example file and modify the calibration)\n\n#7. Failure Modes\n\nYou should expect to achieve good results in sequences similar to those in which we show results in our paper [1], in terms of camera movement and texture in the environment. In general our Monocular SLAM solution is expected to have a bad time in the following situations:\n- Pure rotations in exploration\n- Low texture environments\n- Many (or big) moving objects, especially if they move slowly.\n\nThe system is able to initialize from planar and non-planar scenes. In the case of planar scenes, depending on the camera movement relative to the plane, it is possible that the system refuses to initialize, see the paper [1] for details. \n\n#8. Need Help?\n\nIf you have any trouble installing or running ORB-SLAM, contact the authors.\n\n", 
  "description": ""
}