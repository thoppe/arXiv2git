{
  "read_at": 1462555931, 
  "description": "Matlab and Octave GPU Accelerated Deep Learning Toolbox", 
  "README.md": "# Cortexsys 3.1 User Guide\n### May, 2016\n\n![Cortexsys Logo](media/cortexsys.png)\n![SNL Logo](media/snl.png)\n![DOE Logo](media/doe.png)\n  ----------------------------------------------------------------------\nIntroduction\n============\n\nCortexsys is a deep learning toolbox for Matlab and GNU Octave 4.0\nintended for researchers and algorithm developers who would like to\nrapidly implement and analyze new algorithms with the Matlab or Octave\nenvironment. Cortexsys strives to accomplish too goals which are often\nin conflict: (1) Easy to use and learn; (2) Flexible and adaptable for\nresearch, education and prototyping. If we have achieved this goal, a\nnew user should be able to instantly run and understand the examples,\nand eventually dig into and understand the underlying \"guts\" without\nexcessive confusion.\n\nBenefits of Cortexsys\n---------------------\n\n-   Matrix and mathematical oriented language and development\n    environment\n\n    -   For example, multiplying a matrix and a vector and adding a\n        constant is accomplished simply with the commands: A\\*b+c\n\n    -   Completely interpreted language provides easy and fast debugging\n\n    -   Interoperability with the wealth of existing toolboxes and code\n\n-   Easy to combine various types of neural network layers together\n\n    -   For example, a convolutional net can feed a recurrent net that\n        outputs to a fully connected feed-forward net.\n\n-   Simple support for GPU accelerator cards and cluster computing with\n    the Matlab Parallel Computing Toolbox.\n\n-   Explicit implementation of the training and backpropagation\n    algorithms that does not use automatic differentiation. This is\n    useful for optimization, educational purposes and porting code to\n    compiled software or dedicated hardware.\n\nAt present, Cortexsys implements both standard, feed forward deep neural\nnets (\"deep learning\"), including convolutional nets, and recurrent\nneural nets. Furthermore, all types can be combined in arbitrary\nways.\n\nSummary of Features\n-------------------\n\n-   Layer types\n\n    -   Fully connected feed forward\n\n    -   Convolutional\n\n    -   Average pooling\n\n    -   Recurrent\n\n    -   Long-short term memory (LSTM)\n\n-   Activation functions\n\n    -   Sigmoid\n\n    -   Hyperbolic tangent\n\n    -   Softplus\n\n    -   Softmax\n\n    -   Linear\n\n    -   Rectified linear\n\n    -   Leaky rectified linear\n\n-   Optimization algorithms\n\n    -   Mini-batch gradient descent with momentum\n\n    -   ADADELTA gradient descent\n\n    -   Mini-batch conjugate gradient with line-search\n\n-   Regularization\n\n    -   L~2~ norm of weights\n\n    -   Dropout\n\n    -   De-noising of input\n\n    -   Max-norm\n\n    -   Sparsity constraints\n\n    -   Tied weights (for auto-encoders)\n\n-   Learning types\n\n    -   Supervised classifiers\n\n    -   Unsupervised auto-encoders\n\n    -   Sequence prediction (recurrent nets)\n\n    -   Unsupervised layer-by-layer pre-training\n\n    -   Input maximization for classifiers\n\n        -   Learn an input that maximizes the activation for a\n            particular unit activation, such as the output units. This\n            allows us to generate \"optical illusions\" or to visualize\n            what various features may represent.\n\n-   Examples\n\n    -   Deep auto-encoder with pre-training\n\n    -   Deep classifier with pre-training\n\n    -   Convolutional net for image classification\n\n    -   Recurrent network for sequence prediction\n\n        -   Generate Shakespeare text via random sampling\n\n    -   Neural net \"optical illusion\" generator\n\n        -   Generate inputs that an MNIST classifier believes are digits\n            with &gt;99.9% confidence\n\nFeature To-do List\n------------------\n\nIn addition to the features above, the following features may be\nimplemented at some point in the future, and will be a useful addition\nto the code base.\n\n-   Layers\n\n    -   Max pooling layers for convolutional nets\n\n-   Optimization algorithms\n\n    -   Stochastic Hessian Free Optimization for deep and recurrent nets\n\n-   Regularization\n\n    -   Mini-batch normalization of layer outputs\n\n-   Examples\n\n    -   Image caption generator with recurrent net\n\nInitialization and Startup\n==========================\n\nBefore launching Cortexsys, it is necessary to compile the CUDA routines\nif convolutional neural nets are being used and GPU acceleration is\ndesired. The MMX routines must be compiled if using recurrent networks\nwith any network type, unless GPU acceleration is being used. These\nroutines accelerate Matlab's built in functions for specific operations\nwhere built-in routines were either unavailable or did not have the\ndesired performance.\n\nTo compile these routines, you must setup your MEX compiler (see Matlab\ndocumentation[^2]). To compile the CUDA routines for convolutional nets,\nyou must also have the NVIDIA CUDA Toolkit[^3] installed.\n\n1.  Enter the nn\\_core/mmx directory and run the build\\_mmx.m script.\n\n2.  Enter the nn\\_core/cuda directory and run the\n    cudaBuild\\_and\\_Test.m script. This will compile the cuda routines\n    and perform some numerical tests for accuracy and performance.\n\nBefore calling any Cortexsys routines, you must also add the necessary\ndirectories to your Matlab path. The examples use the following code to\naccomplish this. You must adjust the root part of the path '../../' to\nreflect the location of the Cortexsys directory (e.g.\n'\\~/user/Cortexsys/').\n\naddpath('../../nn\\_gui');\n\naddpath('../../nn\\_core');\n\naddpath('../../nn\\_core/cuda');\n\naddpath('../../nn\\_core/mmx');\n\naddpath('../../nn\\_core/Optimizers');\n\naddpath('../../nn\\_core/Activations');\n\naddpath('../../nn\\_core/Wrappers');\n\naddpath('../../nn\\_core/ConvNet');\n\nNext, you must initialize Cortexsys and setup some basic parameters that\nare stored in the definitions object. This object must be passed to many\nof the Cortexsys objects and routines. Call the definitions constructor:\n\ndefs = definitions(PRECISION, useGPU, whichGPU, plotOn);\n\nHere, PRECISION is a string that may be either 'double' or 'single'.\nAlso, useGPU and plotOn are booleans (set to true or false) that control\nwhether plotting is enabled and if a GPU should be used. If a GPU is\nused, whichGPU is the number of the GPU card to use.\n\nInput and Target (Output) Data\n==============================\n\nInput and output data (as well as layer activations and some other types\nof data) is stored in a varObj object. All objects in Cortexsys are\n\"handle objects\", which are simply pointers to objects. Therefore, if\nyou have a varObj called A, and you set B = A, both B and A now point to\nthe same object with the same data. The internal data is stored in the\npublic v member, which is accessed like A.v. When defining a varObj, you\ncan optionally specify the type of data, such as INPUT\\_TYPE or\nOUTPUT\\_TYPE. If the data is of one of these types, you can extract a\nmini-batch from the data and automatically load it into the GPU with the\ngetmb() method: A.getmb(). Note, the data stored in the varObj may be a\nsparse matrix in memory, from which a mini-batch is extracted and loaded\ninto the GPU.\n\nCortexsys is also capable of handling both single precision and double\nprecision floating point data. This can be useful for some inexpensive\nGPU cards that cannot perform double precision calculations quickly. For\nconvenience, the precision(A, defs) function will convert a raw matrix\nor tensor to a particular type depending on what was set during\ninitialization in the definitions object.\n\nLayer Structure and Network Architecture\n========================================\n\nNeural networks in Cortexsys are divided up into layers. Each layer may\nbe of a different type, such as \"fully connected\" or \"LSTM\". This is\ndefined in a layers structure that determines the activation function,\nthe size of the layer and the type of the layer.\n\nA typical feed-forward, fully connected MNIST classifier network can be\nas shown below. This network has LReLU (leaky rectified linear)\nactivation functions and has a softmax output layer with a cross-entropy\ncost function. The size of each layer, set in layers.sz, is a three\nelement array. Except for convolutional layers, only the first element\ncan be other than 1. For instance, \\[768 1 1\\] creates a layer with 768\nLReLU units. A convolutional layer of size \\[12 5 5\\] would have 12\nfeature maps with kernel sizes of 5x5 pixels each.\n\nlayers.af{1} = \\[\\];\n\nlayers.sz{1} = \\[input\\_size 1 1\\];\n\nlayers.typ{1} = defs.TYPES.INPUT;\n\nlayers.af{end+1} = LReLU(defs, defs.COSTS.SQUARED\\_ERROR);\n\nlayers.sz{end+1} = \\[768 1 1\\];\n\nlayers.typ{end+1} = defs.TYPES.FULLY\\_CONNECTED;\n\nlayers.af{end+1} = LReLU(defs, defs.COSTS.SQUARED\\_ERROR);\n\nlayers.sz{end+1} = \\[256 1 1\\];\n\nlayers.typ{end+1} = defs.TYPES.FULLY\\_CONNECTED;\n\nlayers.af{end+1} = softmax(defs, defs.COSTS.CROSS\\_ENTROPY);\n\nlayers.sz{end+1} = \\[output\\_size 1 1\\];\n\nlayers.typ{end+1} = defs.TYPES.FULLY\\_CONNECTED;\n\nIn contrast to some approaches, the input data is represented as layer\n1, and is effectively just another layer in the network.\n\nIt is possible for layers other than the output layer to have a cost\nfunction. This is necessary when layer-by-layer pre-training is being\nused, so that each layer can be trained as an unsupervised auto-encoder.\n\n### Visualizing the Network\n\nThe connectivity, type and size of the layers can be visualized with the\nnnShow method located in the nn\\_gui folder. Simply pass nnShow with a\nfigure number, the layers structure and the definitions object.\n\nnnShow(123, layers, defs);\n\nFor the recurrent net example, the following plot is generated:\n\n![](media/image4.wmf){width=\"4.583333333333333in\" height=\"3.4375in\"}\n\nFigure 1: Diagram of the recurrent net example. The dimensionality of\nthe weights is shown, along with a blue ellipse indicating that the\nmiddle layer with hyperbolic tangent activation function is recurrent.\nThe width of the layers represents the number of units in the layer.\n\nTraining Parameters\n===================\n\nA variety of settings exist that control how the network is trained.\nExamples include the learning rate, dropout regularization, sparsity\npenalties, tied weights, etc. These parameters must be defined in a\nstructure and then passed to the nnLayers object when the network object\nis created (to the constructor). In general, the values of these\nhyper-parameters are not known in advance, and it is best to play with\ndifferent values to get a feeling for what works best.\n\n% Training parameters\n\nparams.maxIter = 1000; % How many iterations to train for\n\nparams.miniBatchSize = 128; **% set size of mini-batches**\n\n% Regularization parameters\n\nparams.maxnorm = 0; **% enable max norm regularization if \\~= 0**\n\nparams.lambda = 1; **% enable L2 regularization if \\~= 0**\n\nparams.alphaTau = 0.25\\*params.maxIter; **% Learning rate decay**\n\nparams.denoise = 0.25; **% enable input denoising if \\~= 0**\n\nparams.dropout = 0.6; **% enable dropout regularization if \\~= 0**\n\nparams.tieWeights = false; **% enable tied weights for autoencoder?**\n\nparams.beta\\_s = 0; **% Strength of sparsity penalty; set to 0 to\ndisable**\n\nparams.rho\\_s0 = 0; **% Target hidden unit activation for sparsity\npenalty**\n\n% Learning rate parameters\n\nparams.momentum = 0.9; **% Momentum for stochastic gradient descent**\n\nparams.alpha = 0.01; **% Learning rate for SGD**\n\nparams.rho = 0.95; **% AdaDelta hyperparameter**\n\nparams.eps = 1e-6; **% AdaDelta hyperparameter**\n\n% Conjugate gradient parameters\n\nparams.cg.N = 10; **% Max CG iterations before reset**\n\nparams.cg.sigma0 = 0.01; **% CG Secant line search parameter**\n\nparams.cg.jmax = 10; **% Maximum CG Secant iterations**\n\nparams.cg.eps = 1e-4; **% Update threshold for CG**\n\nparams.cg.mbIters = 10; **% How many CG iterations per minibatch?**\n\nIf the data is time series data, where each time step is stored in a\ndifferent cell in a cell array, getmb() extracts the mini-batch and\nconverts it into a 3D tensor, where the third dimension is time.\n\nCreating a Network Object\n=========================\n\nOnce the initialization object, layer topology, input and output data\nand training parameters are defined, the neural network object can be\ncreated. This object ties together all of these aspects and represents\neverything about the neural net. It is also used to store some\npersistent data during training, such as layer activations and dropout\nmasks.\n\nThe nnLayers object stores the layer structure, as well as references to\nthe input and output data, and all of the parameters associated with\ntraining. It also has the job of initializing the layer weights. The\nweights and biases are also stored in this object.\n\nThe nnLayers object expects one structure containing the layers,\nincluding another containing parameters for training. It can optionally\naccept the input and output data objects and initial weights and biases.\nFor example, the object can be created and the weights and biases\ninitialized with the following code:\n\nnn = nnLayers(params, layers, X, Y, {}, {}, defs);\n\nnn.initWeightsBiases();\n\nOr, to use your own initial weights and biases, you can pass nnLayers W\nand b at creation:\n\nnn = nnLayers(params, layers, X, Y, W, b, defs);\n\nKeep in mind, the input and target data, X and Y, could be the same\nvarObj to save memory--as in the case of an auto-encoder.\n\nTraining a Neural Network\n=========================\n\nOnce you have defined your nnLayers object, you can train this network\nusing one of several optimization algorithms. At this time, stochastic\nmini-batch gradient descent, ADADELTA gradient descent, and stochastic\nmini-batch conjugate gradient with line search is provided.\n\nFirst, create a function handle to the function that the optimization\nroutine will use to compute the gradients for your network. This\nfunction uses the backpropagation algorithm to compute the gradients of\nthe weights and biases with respect to the cost function defined in the\nlayers structure.\n\ncostFunc = @(nn,r,newRandGen) nnCostFunctionCNN(nn,r,newRandGen);\n\nThe function handle takes three arguments: the nnLayers object, the\nmini-batch to use (indicies of data to extract from the data set), and\nwhether or not randomly generated elements of the network should be\nre-drawn (e.g. dropout or denoising). The second and third parameters\nare useful for optimization routines that must control the stochastic\nnature of the network during training, such as conjugate gradient.\n\nNext, pass the training function to the optimization routine:\n\ngradientDescentAdaDelta(costFunc, nn, defs, Xts, Yts, yts, y, 'Training\nEntire Network');\n\nThe routine expects the cost function handle, the nnLayers object and\nthe definitions object. Optionally, you can also provide\ncross-validation (test sets) for checking the accuracy during training.\nA figure title is provided so that different stages of training can be\nidentified, when plotting the training progress.\n\nEvaluating a Trained Network\n============================\n\nOnce a network is trained, it can be used to predict a sequence or\notherwise generate an output, such as a class prediction.\n\n### Non-recurrent Nets\n\nTo compute the output of a network on a set of input data, load the\nappropriate data into the first layer in the nnLayers object and pass it\nto the feedforward function.\n\nnn.A{1} = Xts;\n\nnn.Y = Yts;\n\nm = size(Yts.v, 2);\n\nAout = feedForward(nn, m);\n\n\\[pred, probs\\]= predictDeep(A, false, true));\n\nacc = mean(double(pred == yts)) \\* 100;\n\nfeedforward takes the number of examples in the data set, m, and returns\nthe output of the final layer. This output can be passed to the\npredictDeep function for evaluating the accuracy of a classifier. The\npredictDeep function simply makes the maximum likelihood prediction.\n\n### Recurrent Nets\n\nEvaluating the prediction of a recurrent network is a bit more\ncomplicated. Please consult the end of the LSTM\\_RNN example Shakespeare\ngenerator for more details. However, the feedForwardLSTM method for a\nrecurrent net also takes as an argument the particular time step to\noperate on. If you place feedForwardLSTM in a loop, and increment the\ntime step *t*, it will advance the network one time step at a time. For\nthis example, the network expects to receive as input the output from\nthe previous time step. Thus, in between evaluating time steps, we set\nthe input at A{1} to be the previous time step's output prediction.\n\nfor t= 2:T\\_samp\n\nnn.A{1}.v(:,:,t) = Aout(:,:,t-1); % NOT ACTUALLY DONE THIS WAY\n\n% Step the RNN forward\n\nAout = feedforwardLSTM(nn, 1, t, false, true);\n\nend\n\nAdditionally, there are some details with respect to how a recurrent\nnet's output must be evaluated, in this case. Instead of simply making\nthe maximum likelihood prediction, as above, it is necessary to draw (or\nsample) from the probability distribution provided by the network\nprediction. Otherwise, if this process is deterministic, this network\nwill make deterministic and uninteresting predictions.\n\nThe Matlab randsample function, from the Statistics and Machine Learning\nToolbox, is used to draw a random sample from the output distribution.\nThis choice represents a choice of a character. If this toolbox is not\navailable, Octave does have an open source implementation.\n\nRecurrent Networks\n==================\n\nRecurrent networks include a time-domain aspect that can operate on a\nsequence of data. For example, the input could be a video, where each\ntime step is a different image. Alternatively, each time step could be a\nword or character from a text, such as Shakespeare's works.\n\nWhen working with recurrent nets, an additional parameter that sets the\nlength of the time sequence must be defined.\n\nparams.T = 50; **% Length of sequence is 50 time steps**\n\nKeep in mind, time series data is actually represented as a sequence of\nlength T+2 (see Data Representations).\n\nAnother parameter, T~OS~, controls whether the initial output of the net\nis factored into the training and the computed cost. For example, it may\nnot be helpful to expect a Shakespeare sequence generator to accurately\npredict an entire sentence of text based only on the first character in\nthe sentence. In this case, if T~OS~ is not zero, the first *n* time\nsteps will not count toward the network training. This gives you the\nability to \"prime\" the net with a sequence before optimizing the\npredicted sequence.\n\nparams.Tos = 5; **% Length of sequence to ignore when training**\n\n<span id=\"_Ref443496257\" class=\"anchor\"><span id=\"_Toc443501116\" class=\"anchor\"></span></span>Data Representations\n==================================================================================================================\n\nGenerally speaking, there are three types of input, output and\nactivation layer data. Again, this data is commonly stored in a varObj\nobject.\n\n1.  2D, *n* x *m* matrix for non-recurrent, non-convolutional data\n\n2.  4D *k~1~* x *k~2~* x *n* x *m* tensor for non-recurrent,\n    convolutional data\n\n3.  1D Cell array (with *t* elements) of 2D *n* x *m* matrices for\n    recurrent, time-domain data\n\n    a.  This is converted internally to an *n* x *m* x *t* tensor by\n        the varObj::getmb() method.\n\n4.  1D Cell array (with *t* elements) of 4D *k~1~* x *k~2~* x *n* x *m*\n    tensors for recurrent, time-domain data that is\n    convolutional/image/feature map data.\n\nIn the above, ***m*** is the index of the training example (m = number\nof training examples or mini-batch size). ***n*** is the dimensionality\nof the data or number of feature maps (convolutional net). ***t*** is\nthe time step of the sequence. For convolutional/image data, ***k~1~***\nand ***k~2~*** are the dimensions of the image or convolutional kernel.\n\n### Non-time Series Data\n\nFor example, the MNIST set would be a 2D matrix of 784 x 60000.\nRepresented to a convolutional net, MNIST data would be 28 x 28 x 1 x\n60000 (784 = 28^2^). The output of a convolutional layer with 5 feature\nmaps and 5x5 kernels could be 23 x 23 x 5 x 60000, if operating directly\non the MNIST data.\n\n### Time Series Data\n\nFor recurrent (time series) data, each time step is stored as an element\nin a 1D cell array. Each element in the cell array is an identically\nsized matrix or tensor (just like as above) representing the data at\nthat time step.\n\n*Keep in mind, time series data is actually represented as a sequence of\nlength T+2*, where *t* = 1 is the initial conditions (all zeros) and *t*\n= T+2 is the ending boundary conditions (all zeros). *t* = 2 is, in\nfact, the first time step in the series. This is necessary to simplify\nimplementation. Since Matlab doesn't allow zero indexing of arrays, *t*\n= 0 is not the initial conditions, and so *t* = 2 must be the first real\ntime step in the data.\n\n### Sparse Data\n\nThe input and output data can be stored in a sparse matrix. This is\nuseful when a \"one hot\" or \"n vs. k\" representation is used. For\ninstance, we can represent the MNIST target data as one of 10 possible\nvalues (one for each digit), where a 10 dimension vector with a \"1\" at\nthe proper place represents the correct digit. Instead of having a large\nmatrix with many zeros, a sparse representation can be used. The\nvarObj::getmb() method automatically converts this to a full matrix and\nloads into the GPU, if enabled.\n\nNetwork Parameter (Weight and Bias) Representations\n===================================================\n\n### Non-recurrent, Non-convolutional\n\nThe weight and bias parameters define the trained network. For a\nnon-recurrent, non-convolutional network, the bias, b, is a column\nvector with dimensionality equal to the number of units in the layer.\nThe weights, W, is a matrix with dimensionality n~2~ x n~1~, where n~1~\nis the number of units on the input layer, and n~2~ is the number of the\nconnecting layer above. Thus, b has dimensionality n~1~.\n\n### Recurrent\n\nFor recurrent networks, the weights at every time step are identical.\nTherefore, they have the same shape as for the non-recurrent case.\n\n### Convolutional\n\nFor convolutional layers, the weights are 4D tensor of *k~1~* x *k~2~* x\n*n~1~* x *n~2~*, where:\n\n> *k~1~* x *k~2~*: kernel dimensions in x and y (convolutional filter)\n>\n> *n~1~*: number of feature maps in lower layer\n>\n> *n~2~*: number of feature maps in upper layer\n\nIt is helpful to think of convolutional weights and feature maps as the\n2D analog of weights in a typical, fully connected net. So for a fully\nconnected net, you can imagine that W is actually a *1* x *1* x *n~2~* x\n*n~1~* tensor, where the first two dimensions are singleton, and give\nsimply a scalar weight value. Of course, for a convolutional net, this\n2D \"weight\" is scanned across the input image or map.\n\nConvolutional units also have biases, which are scalar values added to\neach feature map. Thus, if a layer has *n* feature maps, it will also\nhave an *n* dimensional bias vector.\n\nReferences\n==========\n\n1.  LeCun, Yann, and Yoshua Bengio. \"Convolutional networks for images,\n    speech, and time series.\" *The handbook of brain theory and neural\n    networks* 3361.10 (1995): 1995.\n\n2.  Maas, Andrew L., Awni Y. Hannun, and Andrew Y. Ng. \"Rectifier\n    nonlinearities improve neural network acoustic models.\" *Proc.\n    ICML*. Vol. 30. 2013.\n\n3.  Srivastava, Nitish, et al. \"Dropout: A simple way to prevent neural\n    networks from overfitting.\" *The Journal of Machine Learning\n    Research* 15.1 (2014): 1929-1958.\n\n4.  Vincent, Pascal, et al. \"Stacked denoising autoencoders: Learning\n    useful representations in a deep network with a local denoising\n    criterion.\" *The Journal of Machine Learning Research* 11\n    (2010): 3371-3408.\n\n5.  Sutskever, Ilya, et al. \"On the importance of initialization and\n    momentum in deep learning.\" *Proceedings of the 30th international\n    conference on machine learning (ICML-13)*. 2013.\n\n6.  Hochreiter, Sepp, and Jurgen Schmidhuber. \"Long short-term memory.\"\n    *Neural computation* 9.8 (1997): 1735-1780.\n\n7.  Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. \"Reducing the\n    dimensionality of data with neural networks.\" *Science* 313.5786\n    (2006): 504-507.\n\n8.  Ng, Andrew. \"Sparse autoencoder.\" *CS294A Lecture notes* 72\n    (2011): 1-19.\n\n9.  Shewchuk, Jonathan Richard. \"An introduction to the conjugate\n    gradient method without the agonizing pain.\" (1994).\n\n10. Karpathy, Andrej. \"The unreasonable effectiveness of recurrent\n    neural networks.\" (2015).\n\n11. Zeiler, Matthew D. \"ADADELTA: an adaptive learning rate method.\"\n    *arXiv preprint arXiv:1212.5701* (2012).\n\n12. Graves, Alex. *Supervised sequence labelling*. Springer Berlin\n    Heidelberg, 2012.\n\n13. Nguyen A, Yosinski J, Clune J. Deep Neural Networks are Easily\n    Fooled: High Confidence Predictions for Unrecognizable Images. In\n    Computer Vision and Pattern Recognition (CVPR '15), IEEE, 2015.\n\n14. Christopher, M. Bishop. \"Pattern recognition and machine learning.\"\n    Company New York 16.4 (2006): 049901.\n\n15. Hopfield, John J. \"Neural networks and physical systems with\n    emergent collective computational abilities.\" Proceedings of the\n    national academy of sciences 79.8 (1982): 2554-2558.\n\n16. Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams.\n    \"Learning representations by back-propagating errors.\" Cognitive\n    modeling 5.3 (1988): 1.\n\n17. McCulloch, Warren S., and Walter Pitts. \"A logical calculus of the\n    ideas immanent in nervous activity.\" The bulletin of mathematical\n    biophysics 5.4 (1943): 115-133.\n\n[^2]: <http://www.mathworks.com/support/compilers/R2015b/index.html>\n\n[^3]: <https://developer.nvidia.com/cuda-toolkit>\n", 
  "id": 53676662
}