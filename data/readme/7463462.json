{
  "read_at": 1462551332, 
  "description": "Matching engine for http://www.papernautapp.com", 
  "README.md": "Papernaut Engine\n================\n\nThis is the matching engine for <http://www.papernautapp.com>.\n\nOverview\n--------\n\nThis engine consists of two main parts: the Loaders, and the query API.\n\nThe loaders may be invoked while the query API is down, and the query API may\nbe running without invoking the loaders.\n\nThe loaders load webpages via feeds and archives, extract references to\nacademic papers, and store those webpage-to-paper citations in the database.\nWhen parsing a page, a loader determines which outbound links point to academic\npapers by issuing calls to a Zotero translation-server.\n\nThe query API is an HTTP API for querying these citations by identifier (such\nas DOI, PubMed ID, or arXiv ID), so that someone who reads papers can query to\nfind online discussions of those papers.  The primary consumer of the API is\nthe papernaut-frontend web application.\n\nThe loaders depend on a running instance of the Zotero translation-server, a\nthird-party open-source project.  It must be available when running the\nloaders, but is not necessary for running the query API.\n\nGetting Started\n---------------\n\n1.  Get the translator-server project built and running: <https://github.com/zotero/translation-server>\n\n2.  Install gems:\n\n        bundle install\n\n3.  Create and migrate the databases.  PostgreSQL is used by default.\n\n        rake db:create db:migrate db:test:prepare\n\n4.  If the translation-server is running somewhere other than\n    <http://localhost:1969>, configure `ENV['ZOTERO_TRANSLATION_SERVER_URL']`\n    with the base URL.\n\n5. Run the test suite to ensure things work on your system:\n\n        rake\n\n6. Start the application with [foreman](https://rubygems.org/gems/foreman).\n   By default, the papernaut-frontend application expects papernaut-engine\n   to be running on <http://localhost:3001>, so set your `PORT` in `.env`:\n\n        echo \"PORT=3001\" > .env\n        foreman start\n\n\nLoading content\n---------------\n\nContent is loaded into the database in two steps: discussion loading and page\nidentification.\n\nIn the first step, a `Loader` in run to scrape a content feed, such as a blog\narchive or an RSS feed.  For each element in the feed, a `Discussion` is\ncreated, corresponding to the original piece of content in the feed.  Each\n`Discussion` links to one or more `Page` object, corresponding to the outgoing\nlinks from the discussion page.  These are typically a subset of the webpage\nlinks; for example, on a social news feed like Reddit, there will be a single\nlinked `Page`, the subject of discussion.  Other feed types (like blog\narticles) will have a corresponding `Page` for each outbound link in the\ncontent area, eschewing sidebar and navigation links.\n\nSee `lib/loaders/**/*.rb` for the loader code.  Each file should contain\nan example of how to run it.  For example, to load the first 5 pages\nof <http://reddit.com/r/science> with 20 items per page:\n\n```ruby\nLoaders::RedditRssLoader.new('science', 5, 20).load\n```\n\nIn the second step, the collected pages are identified.  Here they are\nsubmitted to the zotero-translator instance which must be running at the time\nof identification.\n\nThis can be executed from the console as:\n\n```ruby\nPage.unidentified.last.identify\n```\n\nor may be run in parallel (see `lib/parallel_identifier.rb`) or as a background\njob.\n\nDeploying to production\n-----------------------\n\nSee `DEPLOY.md` for information about deploying.\n\nLicense\n-------\n\nSee `LICENSE` file.\n", 
  "id": 7463462
}