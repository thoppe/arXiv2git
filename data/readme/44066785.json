{
  "read_at": 1462556254, 
  "description": "This contains all the code used in the azure python streaming data experiment", 
  "README.md": "# sciml\nThis contains all the code used in the azure python streaming data experiments that were described in the blog [on microservice performance architecture](http://esciencegroup.com/2015/10/08/performance-analysis-of-a-cloud-microservice-based-ml-classifier/) and [Processing Scholarly Event Streams in the Cloud](http://esciencegroup.com/2015/09/08/processing-scholarly-event-streams-in-the-cloud/).   The code has been rewritten to be free of all azure dependencies with the exception of the use of Azure Tables for the final storage from the table web service.   It is certainly possible to rewrite this to use another database.\n\n##The Data\n\nThere are four type\n\n1. The arxiv configuration files.  They take the form of config\\_name.json where name can be all4 (the top level), bio (the arxiv q-bio objects), compsci (computer science), math, phy (Physics), finance (finance).   \n2. The machine learning model files (as generated by doc\\_analysis\\_final.py described below)\n3. The raw daa from the streams.  There are three of these.   The sciml\\_data\\_arxiv is the original data set from arxiv.  sciml\\_data\\_arxiv\\_new\\_9\\_28\\_15 is a recent snapshot of arxiv data not used in the training a portion of this was used for the training set.  The sciml\\_data\\_scimags is the rss data from the various science mags. \n4. The output of the main (top level) classifer.  This was used to push events directly to the message broker for use in the performance analysis.  This takes the form dump\\_all\\_subtopic\\_name where name is one of q-bio, Physics, compsci, math or physics. (note these are not the same as the names on the config files.)\n \nThe data is stored in two places. \n\n1. The configuration data, the rss feed input data and model data is stored on a publicly readable oneDrive site.  The url for this is  [http://1drv.ms/1PCOT8l](http://1drv.ms/1PCOT8l) (you may need to cut this and paste it into your browser)\n2. The oneDrive files are ok for download from the browser, but not as efficient for program level access.  So the programs here read the files from an public, read-only account \"http://esciencegroup.blob.core.windows.net/scimlpublic\" The code for reading the files is included in the source codes.  \n\n##doc\\_analysis\\_final.py\n\nThis is the generic scifeed document analyzer.  \nIt reads in the discipline specific config file and it will load the data and go from there.\nTt will generate all the models used by the classifier.\nYou invoke it with two arguments:  the topic which is one of\nall4, bio, compsci, finance, math, phy, and\nan integer which is the max size of the training set. note:\nif the set of documents in a subcategory is less than the max size the entire set\nof documents in that subcategory, the entire subcategory is selected as the training set. \nA sample invocation is \n \n\tipython doc_analysis_final.py all4 1200 \n\nthis one will generate the machine learning modes for the top-level analysis for 1200 randomly selected items from arxiv\n\n##main\\_classifier.py\n\nThis is a version of the classifier that generates topic lists\nit is used to generate the entire set of predicted main topic\nclassifications used by the performance analysis\ninvoke it with two paramters like this\n\n\tipython main_classifer.py all4  1200\n\nthis will pick 1200 random items from arxiv and run the top level\nclasifier.   this can also be run with one of the other subtopics \nnote: the number of items selected may be less that 1200 after duplicates\nare removed. \n\n##predictor8big.py\nthis version reads the model data from local files \nand makes predictions based on thresh value\npick a threshold around between 0.1 and 1.0\nand some number of samples.   it will print the \nthe samples that are above the threshold of confidence.\nFor data is uses the new arxiv data (not used in the training) and the science magazine data (also not used in the training).\nA reasonable invocation is \n\n\tipython predictor8big.py all4 300 0.15\n\nor\n\n\tipython predictor8big.py bio 300 0.15\n\nthis is pulling a sample of the data and show you the results only if the prediction is above the threshold.   If you see nothing, they\nthe threshold  is probably too big.\n\n##classifer\nThis directory hold the Dockerized version of the classifier that uses the rabbimq service and it assumes the scimlservice is running.\nNoice in the Dockerfile there are a number of parameters that must be provided before you build the docker image.  They are\n\n1. the address of rabbitmq service\n2. the id of the rabbit user\n3. the password for the rabbit user\n4. the address of scimlservice\n\n##scimlservice\nThis is the table web service component.    you will see that it need the first three items above (rabbitmq identity) and also the \nAzure account and access key so that it can put things in the table.  \n\n##run\\_sciml\\_services.ipynb\nThis file requires that you install the ipython notebook.  it is a set of commands to the classifer service to wait on queues and to send data to those queues. \n", 
  "id": 44066785
}