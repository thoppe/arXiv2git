{
  "read_at": 1462551143, 
  "description": "python version of the No-U-Turn Sampler (NUTS) from Hoffman & Gelman, 2011", 
  "README.md": "No-U-Turn Sampler (NUTS) for python\n===================================\n\nThis package implements the No-U-Turn Sampler (NUTS) algorithm 6 from the NUTS paper ([Hoffman & Gelman, 2011][1]).\n\nContent\n-------\n\nThe package mainly contains:\n\n* `nuts.nuts6`              return samples using the NUTS                  \n* `nuts.numerical_grad`     return numerical estimate of the local gradient\n* `emcee_nuts.NUTSSampler`  emcee NUTS sampler, a derived class from `emcee.Sampler`\n\n\nA few words about NUTS\n----------------------\n\nHamiltonian Monte Carlo or Hybrid Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters, biggest weakness of many MCMC methods. Instead, it takes a series of steps informed by first-order gradient information.\n\nThis feature allows it to converge much more quickly to high-dimensional target distributions compared to simpler methods such as Metropolis, Gibbs sampling (and derivatives).\n\nHowever, HMC's performance is highly sensitive to two user-specified parameters: a step size, and a desired number of steps.  In particular, if the number of steps is too small then the algorithm will just exhibit random walk behavior, whereas if it is too large it will waste computations.\n\nHoffman & Gelman introduced NUTS or the No-U-Turn Sampler, an extension to HMC that eliminates the need to set a number of steps.  NUTS uses a recursive algorithm to find likely candidate points that automatically stops when it starts to double back and retrace its steps.  Empirically, NUTS perform at least as effciently as and sometimes more effciently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs.\n\nMoreover, Hoffman & Gelman derived a method for adapting the step size parameter on the fly based on primal-dual averaging.  NUTS can thus be used with no hand-tuning at all.\n\nIn practice, the implementation still requires a number of steps, a burning period and a stepsize. However, the stepsize will be optimized during the burning period, and the final values of all the user-defined values will be revised by the algorithm.\n\n**reference**: \n[arXiv:1111.4246][1]: Matthew D. Hoffman & Andrew Gelman, 2011, \"_The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo_\"\n\n[1]: http://arxiv.org/abs/1111.4246\n\nExample Usage\n-------------\n**sampling a 2d highly correlated Gaussian distribution**\nsee `nuts.test_nuts6`\n\n\n* define a log-likelihood and gradient function:\n\n```python\ndef correlated_normal(theta):\n    \"\"\" Example of a target distribution that could be sampled from using NUTS.  (Doesn't include the normalizing constant.)\n    Note: \n    cov = np.asarray([[1, 1.98],\n                      [1.98, 4]])\n    \"\"\"\n\n    #A = np.linalg.inv( cov )\n    A = np.asarray([[50.251256, -24.874372],\n                    [-24.874372, 12.562814]])\n\n    grad = -np.dot(theta, A)\n    logp = 0.5 * np.dot(grad, theta.T)\n    return logp, grad\n```\n\n* set your initial conditions: number of dimensions, _number of steps, number of adaptation/burning steps, initial guess, and initial step size.\n\n```python\nD = 2\nM = 5000\nMadapt = 5000\ntheta0 = np.random.normal(0, 1, D)\ndelta = 0.2\n\nmean = np.zeros(2)\ncov = np.asarray([[1, 1.98], \n                  [1.98, 4]])\n```\n\n* run the sampling:\n\n```python\nsamples, lnprob, epsilon = nuts6(correlated_normal, M, Madapt, theta0, delta)\n```\n\n* some statistics: expecting mean = (0, 0) and std = (1., 4.)\n\n```python\nsamples = samples[1::10, :]\nprint('Mean: {}'.format(np.mean(samples, axis=0)))\nprint('Stddev: {}'.format(np.std(samples, axis=0)))\n```\n* a quick plot:\n\n```python\nimport pylab as plt\ntemp = np.random.multivariate_normal(mean, cov, size=500)\nplt.plot(temp[:, 0], temp[:, 1], '.')\nplt.plot(samples[:, 0], samples[:, 1], 'r+')\nplt.show()\n```\nExample usage as an EMCEE sampler\n---------------------------------\n\nsee `emcee_nuts.test_sampler`\n\n* define a log-likelihood function:\n\n```python\ndef lnprobfn(theta):\n    return correlated_normal(theta)[0]\n```\n\n* define a gradient function (if not numerical estimates are made, but slower):\n\n```python\ndef gradfn(theta):\n    return correlated_normal(theta)[1]\n```\n\n* set your initial conditions: number of dimensions, _number of steps, number of adaptation/burning steps, initial guess, and initial step size._\n\n```python\nD = 2\nM = 5000\nMadapt = 5000\ntheta0 = np.random.normal(0, 1, D)\ndelta = 0.2\n\nmean = np.zeros(2)\ncov = np.asarray([[1, 1.98],\n                  [1.98, 4]])\n```\n\n* run the sampling:\n\n```python\nsampler = NUTSSampler(D, lnprobfn, gradfn)\nsamples = sampler.run_mcmc( theta0, M, Madapt, delta )\n```\n\n", 
  "id": 10871839
}