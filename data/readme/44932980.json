{
  "read_at": 1462556647, 
  "description": "Julia package for labelled mode tensors.", 
  "README.md": "# Tensors\n\nTensors in the sense of this package are multidimensional arrays with the additional twist that indices are distinguished based on labels rather than their position in some linear order. For example, the line \n```julia\nx = zeros([Mode(:apple,3), Mode(:orange,4)])\n``` \ncreates an all-zeros tensor with two *modes* (aka dimensions or indices) of size 3 and 4, respectively. This tensor is equivalent to a 3x4 matrix except that we do not refer to the indices as \"first\" and \"second\" but rather as \"apple\" and \"orange\". We believe that labelling modes in this manner is both more natural as well as much simpler than imposing an arbitrary order convention on the indices.\n\n\n## Basic Usage\n\nCreate tensor with modes `:a`, `:b`, `:c` of size 4x3x2 and random entries.\n```julia\njulia> x = rand([Mode(:a,4), Mode(:b,3), Mode(:c,2)])\nTensor{Float64}([Mode(:a,4), Mode(:b,3), Mode(:c,2)])\n```\nUnfold tensor to a multi-dimensional array.\n```julia\njulia> x[[:a],[:b],[:c]]\n4x3x2 Array{Float64,3}:\n[:, :, 1] =\n 0.412986   0.286382  0.774888 \n 0.0398485  0.85685   0.152157 \n 0.360799   0.57589   0.0533906\n 0.404407   0.560899  0.24279  \n\n[:, :, 2] =\n 0.697664  0.23961   0.741541 \n 0.217322  0.343162  0.0774094\n 0.514375  0.958773  0.320539 \n 0.16762   0.408356  0.484142 \n```\nThe brackets and the order of the arguments are important! Compare the above with \n```julia\njulia> x[[:a],[:b,:c]]\n4x6 Array{Float64,2}:\n 0.412986   0.286382  0.774888   0.697664  0.23961   0.741541 \n 0.0398485  0.85685   0.152157   0.217322  0.343162  0.0774094\n 0.360799   0.57589   0.0533906  0.514375  0.958773  0.320539 \n 0.404407   0.560899  0.24279    0.16762   0.408356  0.484142 \n\njulia> x[[:b,:c],[:a]]\n6x4 Array{Float64,2}:\n 0.412986  0.0398485  0.360799   0.404407\n 0.286382  0.85685    0.57589    0.560899\n 0.774888  0.152157   0.0533906  0.24279 \n 0.697664  0.217322   0.514375   0.16762 \n 0.23961   0.343162   0.958773   0.408356\n 0.741541  0.0774094  0.320539   0.484142\n```\nAccess a particular element.\n```julia\njulia> x[:a => 3, :b => 2, :c => 1]\n0.57589\n```\nAdvanced initialisation.\n```julia\njulia> x = init(i-> i[:a] + 3*(i[:b]-1), Int, [Mode(:a,3), Mode(:b,4)]);\njulia> x[[:a],[:b]]\n3x4 Array{Int64,2}:\n 1  4  7  10\n 2  5  8  11\n 3  6  9  12\n```\n\nPlease note how in the above code snippets we represent modes using different objects depending on the context. To create a tensor, we pass a `Mode` object, that is a pair of a mode label and a mode size. \n```julia\nimmutable Mode\n    mlabel::Any\n    msize::Int\nend\n```\nOnce we have a tensor, it becomes redundant to specify the mode sizes again hence we only mention the mode labels from then on.\n\n\n## Mode Product\n\nThe higher-dimensional analogue of the matrix product is the *mode product* defined as follows. Let `x`, `y` be two tensors with mode labels `[M;K]` and `[K;N]` where `M`, `K` and `N` are disjoint mode sets. Then, `z = x*y` is a tensor with mode set `[M;N]` defined through `z[M,N] = x[M,K]*y[K,N]` where here the `*` stands for the standard matrix product of the respective unfoldings. \n\n**Examples**\n - Vector inner product.\n```julia\njulia> x = mod(rand(Int, [Mode(:a,3)]), 6); \njulia> y = mod(rand(Int, [Mode(:a,3)]), 6); \njulia> println(\"x = \", x[[:a]])\nx = [1,5,4]\njulia> println(\"y = \", y[[:a]])\ny = [3,0,0]\njulia> println(\"x*y = \",scalar(x*y))\nx*y = 3\n```\n - Vector outer product.\n```julia\njulia> x = mod(rand(Int, [Mode(:a,3)]), 6); \njulia> y = mod(rand(Int, [Mode(:b,3)]), 6);\njulia> println(\"x = \", x[[:a]])\nx = [4,5,2]\njulia> println(\"y = \", y[[:b]])\ny = [5,0,3]\njulia> println(\"x*y = \\n\", (x*y)[[:a],[:b]])\nx*y = \n[20 0 12\n 25 0 15\n 10 0 6]\n```\n - Matrix vector product.\n```julia\njulia> A = mod(rand(Int, [Mode(:a,3), Mode(:b,3)]), 6); \njulia> x = mod(rand(Int, [Mode(:b,3)]), 6); \njulia> println(\"A = \\n\", A[[:a],[:b]])\nA = \n[0 3 0\n 2 3 2\n 4 0 2]\njulia> println(\"x = \", x[[:b]])\nx = [3,5,0]\njulia> println(\"A*x = \", (A*x)[[:a]])\nA*x = [15,21,12]\n```\n - Frobenius inner product.\n```julia\njulia> X = mod(rand(Int, [Mode(:a,2), Mode(:b,3)]),6); \njulia> Y = mod(rand(Int, [Mode(:a,2), Mode(:b,3)]),6);\njulia> println(\"X = \\n\", X[[:a],[:b]])\nX = \n[5 0 1\n 5 1 4]\njulia> println(\"Y = \\n\", Y[[:a],[:b]])\nY = \n[0 0 1\n 2 5 4]\njulia> println(\"X*Y = \", (X*Y)[])\nX*Y = 32\n```\n\nThe above definition of the mode product involved a little white lie as it suggested that the mode product `x*y` runs over all common modes of `x` and `y`. The actual truth is that a mode `k` of `x` is contracted with a mode `l` of `y` if the predicate `multiplies(k,l)` returns `true`. In most cases, contracting equal modes is the behaviour you want, therefore the default definition is `multiplies(k,l) = (k == l)`. There are situations, however, where different rules are more suitable. \n\nThe particular situation we have in mind are linear operators `A` from a tensor space with mode set `D` onto itself. These operators are naturally tensors with two modes for each mode `k in D`, namely one which is to be contracted with the input and one delivering the mode for the output. In the notation of this package, we distinguish these modes by *tagging* them with a `:C` (for column) or `:R` (for row) tag, respectively. Given a mode symbol `k`, this is done by writing `tag(:C,k)` which wraps `k` in a `Tag{:C}` object. \n```julia\nimmutable Tag{L} mlabel::Any end\ntag(L,k) = Tag{L}(k)\n```\nFor convenience, the `tag()` function is overloaded to work on both `Mode` objects as well as `Vector{Any}` and `Vector{Mode}`. \n\nThe natural rules for matching row and column modes in the mode product are different from the above default. We would like the expression `A*x` to indicate the application of an operator `A` to a tensor `x`, i.e. the column modes of `A` should be multiplied with the corresponding mode of `x` despite the fact that they do not have equal mode labels. Similarly, we want to allow chaining of operators as in `A*B` and right-sided application to vectors as in `x*A`. We thus add the following methods to `multiplies`.\n```julia\nmultiplies(k::Tag{:C}, l::Tag{:R}) = multiplies(k.mlabel, l.mlabel)\nmultiplies(k::Any    , l::Tag{:R}) = multiplies(k       , l.mlabel)\nmultiplies(k::Tag{:C}, l::Any    ) = multiplies(k.mlabel, l       )\n```\nAt this point, the expression `y = A*x` involving tensors `A` with modes `[C(D); R(D)]` and `x` with modes `D` would result in a tensor `y` with modes `R(D)` instead of `D`. To resolve this issue, we add the rule that if only either the `R(k)` or `C(k)` mode of a tensor is multiplied, the remaining mode gets renamed to `k`. \n\nIf these rules confuse you at first, do not worry! The key point is that row and column modes behave exactly as you would expect them to, as illustrated in the following example. \n```julia\njulia> A = mod(rand(Int, [Mode(k,2) for k in (tag(:R,:a), tag(:C,:a))]), 6); \njulia> b = mod(rand(Int, [Mode(:a,2)]),6); \njulia> println(\"A = \\n\", A[[tag(:R,:a)],[tag(:C,:a)]])\nA = \n[0 1\n 0 5]\njulia> println(\"b = \", b[[:a]])\nb = [4,5]\njulia> println(\"A*b = \", (A*b)[:a])\nA*b = [5,25]\njulia> println(\"b*A = \", (b*A)[:a])\nb*A = [0,29]\n```\n\nWe so far silently assumed that in a mode product `x*y` there is at most one mode `k` of `x` for every mode `l` of `y` such that `multiplies(k,l)` is true, and vice versa. It is hard to imagine a situation where this rule would not be naturally satisfied, but we would like to warn users that its violation results in undefined behaviour. \n\n\n## Tensor Factorisations\n\nThis package provides tensor analogues for the QR decomposition and the SVD. As for the mode product, the general pattern of these functions is\n - unfold the tensor into a matrix,\n - compute its matrix decomposition,\n - reshape the results into tensors.\n\n**Tensor QR Decomposition**\n\nLet `x` be a tensor with modes `D`, `M` a subset of `D` and `k` a mode label not in `D`. The expression `q,r = qr(x,M,k)` is defined through `q[setdiff(D,M),[k]], r[[k],M] = qr(x[setdiff(D,M),M])`. \n\n**Tensor SVD**\n\nLet `x` be a tensor with modes `D`, `M` a subset of `D`, `k` a mode label not in `D` and `rfunc` a function `(::Vector{Real}) -> ::Int`. The expression `u,s,v = svd(x,M,k,rfunc)` is defined through\n```julia\nU,S,V = svd(x[setdiff(D,M),M])\nr = rfunc(S)\nu[setdiff(D,M),[k]] = U[:,1:r]\ns[[k]] = S[1:r]\nv[M,[k]] = V[:,1:r]\n```\n\nThe following generators for `rfunc` are provided:\n - `fixed(r) = (S) -> r`.\n - `maxrank() = (s) -> length(s)`.\n - `adaptive(eps; rel = true) = (S) -> [ smallest r such that norm(S[r+1:end])/(rel ? norm(S) : 1) <= eps ]`. \n\n\n## Examples\n\n### Higher-Order SVD\n\n**References:** \n - Lieven De Lathauwer, Bart De Moor and Joos Vandewalle. 'A multilinear singular value decomposition'. In: SIAM. J. Matrix Anal. & Appl., 21(4), 1253-1278. URL: <http://dx.doi.org/10.1137/S0895479896305696>\n\n**Definition**\n```julia\nfunction hosvd(x, eps)\n    eps = eps*norm(x)/sqrt(ndims(x))\n    core = x\n    factors = Dict{Any,Tensor{eltype(x)}}()\n    for k in mlabel(x)\n        u,s,v = svd(core, [k], tag(:Rank,k), adaptive(eps, rel=false))\n        core = scale(u,s)\n        factors[k] = v\n    end\n    return core,factors\nend\n```\n\n**Test**\n\nGet a tensor and compute its HOSVD.\n```julia\nx = rand([Mode(k,4) for k in 1:10])\ncore,factors = hosvd(x, 0.8)\n```\nReassemble the tensor and check accuracy.\n```julia\nxx = core; for f in values(factors) xx *= f; end \njulia> norm(x - xx)/norm(x)\n0.49961117095943813\n```\nMonitor ranks.\n```julia\njulia> for k = 1:10 println(k, \" => \", msize(core,tag(:Rank,k))); end\n1 => 3\n2 => 3\n3 => 3\n4 => 2\n5 => 1\n6 => 1\n7 => 1\n8 => 1\n9 => 1\n10 => 1\n```\n\n\n### Tensor Train Decomposition\n\n**References:**\n - I. V. Oseledets and E. E. Tyrtyshnikov. 'Breaking the curse of dimensionality, or how to use SVD in many dimensions'. In: SIAM J. Sci. Comput., 31(5), 3744-3759. URL: <http://dx.doi.org/10.1137/090748330>\n - I. V. Oseledets. 'Tensor-Train Decomposition'. In: SIAM J. Sci. Comput., 33(5), 2295-2317. URL: <http://dx.doi.org/10.1137/090752286>\n\n\n**Definition**\n```julia\nfunction tt_tensor(x, order, eps)\n    @assert Set(mlabel(x)) == Set(order)\n    d = ndims(x)\n    eps = eps*norm(x)/sqrt(d-1)\n    tt = Vector{Tensor{eltype(x)}}(d)\n    u,s,v = svd(x, [order[d]], tag(:Rank,d-1), adaptive(eps; rel=false))\n    x = scale(u,s); tt[d] = v\n    for k in d-1:-1:2\n        u,s,v = svd(x, [tag(:Rank,k),order[k]], tag(:Rank,k-1), adaptive(eps; rel=false))\n        x = scale(u,s); tt[k] = v\n    end\n    tt[1] = x\n    return tt\nend\n```\n\n**Test**\n\nGet a tensor and compute its TT decomposition.\n```julia\nx = rand([Mode(k,4) for k in 1:10])\ntt = tt_tensor(x, 1:10, 0.8)\n```\nReassemble the tensor and check accuracy.\n```julia\njulia> norm(x - prod(tt))/norm(x)\n0.4995203414908285\n```\nMonitor ranks.\n```julia\njulia>  println([msize(tt[k], tag(:Rank,k)) for k in 1:9])\n[1,1,1,1,1,1,14,8,3]\n```\n\n### Cyclic vs. Tree Structured Tensor Networks\n\nIt is known that tensor network formats based on cyclic graphs are in general not closed (see <http://arxiv.org/abs/1105.4449>) and therefore appear to be unsuitable for numerical computations. An immediate follow-up question is whether there is any reason to consider cyclic graphs at all, i.e. whether there are tensors which can be represented more efficiently in cyclic rather than tree-based formats. We next construct a three-dimensional tensor and verify numerically that it can be compressed more efficiently on a triangle than a tree. \n\nThe construction is fairly simple: take a triangle, set all ranks equal to `r` and fill the vertex tensors with random entries. \n```julia\nfunction triangle(r)\n    n = r^2\n    return [\n        rand([Mode(1,n), Mode((1,2),r), Mode((1,3),r)]),\n        rand([Mode(2,n), Mode((1,2),r), Mode((2,3),r)]),\n        rand([Mode(3,n), Mode((1,3),r), Mode((2,3),r)]),\n    ]\nend\n```\nConverting this tensor to a tree requires separating single modes. We expect these separations to have rank `r^2` with probability 1, yet proving this conjecture would require showing linear independence of all slices \n```julia\n[\n    triangle(r)[1][(1,2) => r12, (1,3) => r13] \n    for r12 = 1:r, r13 = 1:r\n]\n```\n(obvious) and \n```julia\n[\n    (triangle(r)[2]*triangle(r)[3])[(1,2) => r12, (1,3) => r13] \n    for r12 = 1:r, r13 = 1:r\n]\n``` \n(not obvious). The second part is easily investigated numerically. \n```julia\nconjecture_valid = true\nfor r = 1:10\n    n = r^2\n    c2 = rand([Mode(2,n), Mode((1,2),r), Mode((2,3),r)])\n    c3 = rand([Mode(3,n), Mode((1,3),r), Mode((2,3),r)])\n    u,s,v = svd(c2*c3, [(1,2),(1,3)], tag(:Rank,1), adaptive(1e-3))\n    conjecture_valid &= (length(s) == r^2)\nend\nif conjecture_valid println(\"The conjecture appears to be valid.\")\nelse println(\"THE CONJECTURE IS NOT VALID!!!\")\nend\n```\nWe run this code several times and always obtain the answer `The conjecture appears to be valid.`. We therefore conclude that `prod(triangle(r))` can be represented with `n*r^2` floats in a triangle compared to `2*n*r^2 + n*r^4` in TT or `3*n*r^2 + r^6` in a star. \n", 
  "id": 44932980
}