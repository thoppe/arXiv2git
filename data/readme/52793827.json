{
  "read_at": 1462557951, 
  "description": "", 
  "README.md": "# Action Recognition with Deep Learning\n\n[![Build Status](https://travis-ci.org/yjxiong/caffe.svg?branch=action_recog)](https://travis-ci.org/yjxiong/caffe)\n\nThis branch hosts the code for the technical report [\"Towards Good Practices for Very Deep Two-stream ConvNets\"](http://arxiv.org/abs/1507.02159), and more.\n\n### Updates\n- Dec 23, 2015\n  * Refactored cudnn wrapper to control overall memory consumption. Will automatically find the best algorithm combination under memory constraint.\n- Dec 17, 2015\n  * cuDNN v4 support: faster convolution and batch normalization (around 20% performance gain).\n- Nov 22, 2015\n  * Now python layer can expose a `prefetch()` method, which will be run in parallel with network processing.\n\n[Full Change Log](CHANGELOG.md)\n\n### Features\n- `VideoDataLayer` for inputing video data.\n- Training on optical flow data. \n- Data augmentation with fixed corner cropping and multi-scale cropping.\n- Parallel training with multiple GPUs.\n- cuDNNv4 integration.\n\n### Usage\n\n*See more in* [Wiki](https://github.com/yjxiong/caffe/wiki).\n\nGenerally it's the same as the original caffe. Please see the original README. \nPlease see following instruction for accessing features above. More detailed documentation is on the way.\n\n- Video/optic flow data\n  - First use the [optical flow extraction tool](https://github.com/wanglimin/dense_flow) to convert videos to RGB images and opitcal flow images.\n  - A new data layer called `VideoDataLayer` has been added to support multi-frame input. See the UCF101 sample for how to use it.\n  - **Note:** The `VideoDataLayer` can only input the optical-flow images generated by the tool listed above.\n- Fixed corner cropping augmentation\n  - Set `fix_crop` to `true` in `tranform_param` of network's protocol buffer definition.\n- \"Multi-scale\" cropping augmentation\n  - Set `multi_scale` to `true` in `transform_param`\n  - In `transform_param`, specify `scale_ratios` as a list of floats smaller than one, default is `[1, .875, .75, .65]`\n  - In `transform_param`, specify `max_distort` to an integer, which will limit the aspect ratio distortion, default to `1`\n- cuDNN v4\n - The cuDNN v4 wrapper has optimized engines for convolution and batch normalization.\n - The solver protobuf config has a parameter `richness` which specifies the total GPU memory in MBs available to the cudnn convolution engine as workspaces. Default `richness` is 300 (300MB). Using this parameter you can control the GPU memory consumption of training, the system will find the best setup under the memory limit for you.\n- Training with multiple GPUs\n  - Requires OpenMPI > 1.7.4 ([Why?](https://www.open-mpi.org/faq/?category=runcuda)). **Remember to compile your OpenMPI with option `--with-cuda`**\n  - Specify list of GPU IDs to be used for training, in the solver protocol buffer definition, like `device_id: [0,1,2,3]`\n  - Compile using cmake and use `mpirun` to launch caffe executable, like \n```bash\nmkdir build && cd build\ncmake .. -DUSE_MPI=ON\nmake && make install\nmpirun -np 4 ./install/bin/caffe train --solver=<Your Solver File> [--weights=<Pretrained caffemodel>]\n```\n**Note**: actual batch_size will be `num_device` times `batch_size` specified in network's prototxt.\n\n### Working Examples\n- Action recognition on UCF101\n  - [Project Site](http://personal.ie.cuhk.edu.hk/~xy012/others/action_recog/)\n  - [Caffe Model Files](https://github.com/yjxiong/caffe/tree/action_recog/models/action_recognition)\n  - [Training scripts and data files examples](https://github.com/yjxiong/caffe/tree/action_recog/examples/action_recognition)\n  - [Optical Flow Data](http://mmlab.siat.ac.cn/very_deep_two_stream_model/ucf101_flow_img_tvl1_gpu.zip)\n- Scene recognition on Places205\n  - [Model Files](https://github.com/wanglimin/Places205-VGGNet)\n  - [Technical Report](http://wanglimin.github.io/papers/WangGHQ15.pdf)\n\n### Extension\nCurrently all existing data layers sub-classed from `BasePrefetchingDataLayer` support parallel training. If you have newly added layer which is also sub-classed from `BasePrefetchingDataLayer`, simply implement the virtual method \n```C++\ninline virtual void advance_cursor();\n```\nIts function should be forwarding the \"data cursor\" in your data layer for one step. Then your new layer will be able to provide support for parallel training.\n\n### Questions\nContact \n- [Limin Wang](http://wanglimin.github.io/)\n- [Yuanjun Xiong](http://personal.ie.cuhk.edu.hk/~xy012/)\n\n### Citation\nYou are encouraged to also cite the following report if you find this repo helpful\n\n```\n@article{MultiGPUCaffe2015,\n  author    = {Limin Wang and\n               Yuanjun Xiong and\n               Zhe Wang and\n               Yu Qiao},\n  title     = {Towards Good Practices for Very Deep Two-Stream ConvNets},\n  journal   = {CoRR},\n  volume    = {abs/1507.02159},\n  year      = {2015},\n  url       = {http://arxiv.org/abs/1507.02159},\n}\n```\n\n----\nFollowing is the original README of Caffe.\n\n# Caffe\n\nCaffe is a deep learning framework made with expression, speed, and modularity in mind.\nIt is developed by the Berkeley Vision and Learning Center ([BVLC](http://bvlc.eecs.berkeley.edu)) and community contributors.\n\nCheck out the [project site](http://caffe.berkeleyvision.org) for all the details like\n\n- [DIY Deep Learning for Vision with Caffe](https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit#slide=id.p)\n- [Tutorial Documentation](http://caffe.berkeleyvision.org/tutorial/)\n- [BVLC reference models](http://caffe.berkeleyvision.org/model_zoo.html) and the [community model zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo)\n- [Installation instructions](http://caffe.berkeleyvision.org/installation.html)\n\nand step-by-step examples.\n\n[![Join the chat at https://gitter.im/BVLC/caffe](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/BVLC/caffe?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nPlease join the [caffe-users group](https://groups.google.com/forum/#!forum/caffe-users) or [gitter chat](https://gitter.im/BVLC/caffe) to ask questions and talk about methods and models.\nFramework development discussions and thorough bug reports are collected on [Issues](https://github.com/BVLC/caffe/issues).\n\nHappy brewing!\n\n## License and Citation\n\nCaffe is released under the [BSD 2-Clause license](https://github.com/BVLC/caffe/blob/master/LICENSE).\nThe BVLC reference models are released for unrestricted use.\n\nPlease cite Caffe in your publications if it helps your research:\n\n    @article{jia2014caffe,\n      Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},\n      Journal = {arXiv preprint arXiv:1408.5093},\n      Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},\n      Year = {2014}\n    }\n", 
  "id": 52793827
}