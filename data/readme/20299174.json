{
  "README.Rmd": "```{r echo=FALSE}\nknitr::opts_chunk$set(\n  comment = \"#>\",\n  collapse = TRUE,\n  warning = FALSE,\n  message = FALSE\n)\n```\n\n```\n  _____     .__  .__   __                   __\n_/ ____\\_ __|  | |  |_/  |_  ____ ___  ____/  |_\n\\   __\\  |  \\  | |  |\\   __\\/ __ \\\\  \\/  /\\   __\\\n |  | |  |  /  |_|  |_|  | \\  ___/ >    <  |  |\n |__| |____/|____/____/__|  \\___  >__/\\_ \\ |__|\n                                \\/      \\/\n```\n\n__Get full text articles from (almost) anywhere__\n\n[![Build Status](https://api.travis-ci.org/ropensci/fulltext.png)](https://travis-ci.org/ropensci/fulltext)\n[![Build status](https://ci.appveyor.com/api/projects/status/y487h3ec5wc2s20m/branch/master?svg=true)](https://ci.appveyor.com/project/sckott/fulltext/branch/master)\n[![codecov.io](https://codecov.io/github/ropensci/fulltext/coverage.svg?branch=master)](https://codecov.io/github/ropensci/fulltext?branch=master)\n[![rstudio mirror downloads](http://cranlogs.r-pkg.org/badges/fulltext)](https://github.com/metacran/cranlogs.app)\n[![cran version](http://www.r-pkg.org/badges/version/fulltext)](http://cran.rstudio.com/web/packages/fulltext)\n\nrOpenSci has a number of R packages to get either full text, metadata, or both from various publishers. The goal of `fulltext` is to integrate these packages to create a single interface to many data sources.\n\n`fulltext` makes it easy to do text-mining by supporting the following steps:\n\n* Search for articles\n* Fetch articles\n* Get links for full text articles (xml, pdf)\n* Extract text from articles / convert formats\n* Collect bits of articles that you actually need\n* Download supplementary materials from papers\n\nAdditional steps we hope to include in future versions:\n\n* Analysis enabled via the [tm](https://cran.rstudio.com/web/packages/tm/) package and friends, and via [Spark-R](https://amplab-extras.github.io/SparkR-pkg/) to handle especially large jobs\n* Visualization\n\nData sources in `fulltext` include:\n\n* [Crossref](http://www.crossref.org/) - via the `rcrossref` package\n* [Public Library of Science (PLOS)](https://www.plos.org/) - via the `rplos` package\n* [Biomed Central](http://www.biomedcentral.com/)\n* [arXiv](https://arxiv.org) - via the `aRxiv` package\n* [bioRxiv](http://biorxiv.org/) - via the `biorxivr` package\n* [PMC/Pubmed via Entrez](http://www.ncbi.nlm.nih.gov/) - via the `rentrez` package\n* Many more are supported via the above sources (e.g., _Royal Society Open Science_ is \navailable via Pubmed)\n* We __will__ add more, as publishers open up, and as we have time...See the [master list here](https://github.com/ropensci/fulltext/issues/4#issuecomment-52376743)\n\nAuthorization: A number of publishers require authorization via API key, and some even more\ndraconian authorization processes involving checking IP addresses. We are working on supporting\nall the various authorization things for different publishers, but of course all the OA content\nis already easily available. \n\nWe'd love your feedback. Let us know what you think in [the issue tracker](https://github.com/ropensci/fulltext/issues)\n\nArticle full text formats by publisher:  [https://github.com/ropensci/fulltext/blob/master/vignettes/formats.Rmd](https://github.com/ropensci/fulltext/blob/master/vignettes/formats.Rmd)\n\n## Installation\n\nStable version from CRAN\n\n```{r eval=FALSE}\ninstall.packages(\"fulltext\")\n```\n\nDevelopment version from GitHub\n\n```{r eval=FALSE}\ndevtools::install_github(\"ropensci/fulltext\")\n```\n\nLoad library\n\n```{r}\nlibrary('fulltext')\n```\n\n## Extraction tools\n\nIf you want to use `ft_extract()` function, it currently has two options for how to extract text from PDFs: `xpdf` and `ghostscript`. \n\n* `xpdf` installation: See http://www.foolabs.com/xpdf/download.html for instructions on how to download and install `xpdf`. For OSX, you an also get `xpdf` via [Homebrew](https://github.com/homebrew/homebrew-x11/blob/master/xpdf.rb) with `brew install xpdf`. Apparently, you can optionally install Poppler, which is built on `xpdf`. Get it at http://poppler.freedesktop.org/\n* `ghostscript` installation: See http://www.ghostscript.com/doc/9.16/Install.htm  \nfor instructions on how to download and install `ghostscript`. For OSX, you an also get `ghostscript` via [Homebrew](https://github.com/Homebrew/homebrew/blob/master/Library/Formula/ghostscript.rb) with `brew install gs`.\n\n## Search\n\n`ft_search()` - get metadata on a search query.\n\n```{r}\nft_search(query = 'ecology', from = 'plos')\n```\n\n## Get full text links\n\n`ft_links()` - get links for articles (xml and pdf).\n\n```{r}\nres1 <- ft_search(query = 'ecology', from = 'entrez', limit = 5)\nft_links(res1)\n```\n\nOr pass in DOIs directly\n\n```{r}\nft_links(res1$entrez$data$doi, from = \"entrez\")\n```\n\n## Get full text\n\n`ft_get()` - get full or partial text of articles.\n\n```{r}\nft_get('10.1371/journal.pone.0086169', from = 'plos')\n```\n\n## Extract chunks\n\n```{r}\nlibrary(\"rplos\")\n(dois <- searchplos(q = \"*:*\", fl = 'id',\n   fq = list('doc_type:full',\"article_type:\\\"research article\\\"\"), limit = 5)$data$id)\nx <- ft_get(dois, from = \"plos\")\nx %>% chunks(\"publisher\") %>% tabularize()\n```\n\n```{r}\nx %>% chunks(c(\"doi\",\"publisher\")) %>% tabularize()\n```\n\nUse `dplyr` to data munge\n\n```{r}\nlibrary(\"dplyr\")\nx %>%\n chunks(c(\"doi\", \"publisher\", \"permissions\")) %>%\n tabularize() %>%\n .$plos %>%\n select(-permissions.license)\n```\n\n## Supplementary materials\n\nGrab supplementary materials for (re-)analysis of data\n\n`ft_get_si()` accepts article identifiers, and output from `ft_search()`, `ft_get()`\n\n```{r}\ncatching.crabs <- read.csv(ft_get_si(\"10.6084/m9.figshare.979288\", 2))\nhead(catching.crabs)\n```\n\n## Cache\n\nWhen dealing with full text data, you can get a lot quickly, and it can take a long time to get. That's where caching comes in. And after you pull down a bunch of data, if you do so within the R session, you don't want to lose that data if the session crashes, etc. When you search you _will be able to_ (i.e., not ready yet) optionally cache the raw JSON/XML/etc. of each request locally - when you do that exact search again we'll just give you the local data - unless of course you want new data, which you can do.\n\n```{r eval=FALSE}\nft_get('10.1371/journal.pone.0086169', from='plos', cache=TRUE)\n```\n\n## Extract text from PDFs\n\nThere are going to be cases in which some results you find in `ft_search()` have full text available in text, xml, or other machine readable formats, but some may be open access, but only in pdf format. We have a series of convenience functions in this package to help extract text from pdfs, both locally and remotely.\n\nLocally, using code adapted from the package `tm`, and various pdf to text parsing backends\n\n```{r}\npdf <- system.file(\"examples\", \"example2.pdf\", package = \"fulltext\")\n```\n\nUsing `ghostscript`\n\n```{r}\n(res_gs <- ft_extract(pdf, \"gs\"))\n```\n\nUsing `xpdf`\n\n```{r}\n(res_xpdf <- ft_extract(pdf, \"xpdf\"))\n```\n\nOr extract directly into a `tm` Corpus\n\n```{r}\npaths <- sapply(paste0(\"example\", 2:5, \".pdf\"), function(x) system.file(\"examples\", x, package = \"fulltext\"))\n(corpus_xpdf <- ft_extract_corpus(paths, \"xpdf\"))\n```\n\nExtract pdf remotely on the web, using a service called `PDFX`\n\n```{r extract_remote, eval=FALSE}\npdf5 <- system.file(\"examples\", \"example5.pdf\", package = \"fulltext\")\npdfx(file = pdf5)\n```\n\n```{r eval=FALSE, tidy=FALSE}\n#> $meta\n#> $meta$job\n#> [1] \"34b281c10730b9e777de8a29b2dbdcc19f7d025c71afe9d674f3c5311a1f2044\"\n#>\n#> $meta$base_name\n#> [1] \"5kpp\"\n#>\n#> $meta$doi\n#> [1] \"10.7554/eLife.03640\"\n#>\n#>\n#> $data\n#> <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n#> <pdfx xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"http://pdfx.cs.man.ac.uk/static/article-schema.xsd\">\n#>   <meta>\n#>     <job>34b281c10730b9e777de8a29b2dbdcc19f7d025c71afe9d674f3c5311a1f2044</job>\n#>     <base_name>5kpp</base_name>\n#>     <doi>10.7554/eLife.03640</doi>\n#>   </meta>\n#>    <article>\n#>  .....\n```\n\n## Meta\n\n* Please [report any issues or bugs](https://github.com/ropensci/fulltext/issues).\n* License: MIT\n* Get citation information for `fulltext`: `citation(package = 'fulltext')`\n* Please note that this project is released with a [Contributor Code of Conduct](CONDUCT.md). By participating in this project you agree to abide by its terms.\n\n[![rofooter](http://ropensci.org/public_images/github_footer.png)](http://ropensci.org)\n", 
  "read_at": 1462546619, 
  "description": "An R api to search across and get full text for open access journals", 
  "README.md": "\n\n```\n  _____     .__  .__   __                   __\n_/ ____\\_ __|  | |  |_/  |_  ____ ___  ____/  |_\n\\   __\\  |  \\  | |  |\\   __\\/ __ \\\\  \\/  /\\   __\\\n |  | |  |  /  |_|  |_|  | \\  ___/ >    <  |  |\n |__| |____/|____/____/__|  \\___  >__/\\_ \\ |__|\n                                \\/      \\/\n```\n\n__Get full text articles from (almost) anywhere__\n\n[![Build Status](https://api.travis-ci.org/ropensci/fulltext.png)](https://travis-ci.org/ropensci/fulltext)\n[![Build status](https://ci.appveyor.com/api/projects/status/y487h3ec5wc2s20m/branch/master?svg=true)](https://ci.appveyor.com/project/sckott/fulltext/branch/master)\n[![codecov.io](https://codecov.io/github/ropensci/fulltext/coverage.svg?branch=master)](https://codecov.io/github/ropensci/fulltext?branch=master)\n[![rstudio mirror downloads](http://cranlogs.r-pkg.org/badges/fulltext)](https://github.com/metacran/cranlogs.app)\n[![cran version](http://www.r-pkg.org/badges/version/fulltext)](http://cran.rstudio.com/web/packages/fulltext)\n\nrOpenSci has a number of R packages to get either full text, metadata, or both from various publishers. The goal of `fulltext` is to integrate these packages to create a single interface to many data sources.\n\n`fulltext` makes it easy to do text-mining by supporting the following steps:\n\n* Search for articles\n* Fetch articles\n* Get links for full text articles (xml, pdf)\n* Extract text from articles / convert formats\n* Collect bits of articles that you actually need\n* Download supplementary materials from papers\n\nAdditional steps we hope to include in future versions:\n\n* Analysis enabled via the [tm](https://cran.rstudio.com/web/packages/tm/) package and friends, and via [Spark-R](https://amplab-extras.github.io/SparkR-pkg/) to handle especially large jobs\n* Visualization\n\nData sources in `fulltext` include:\n\n* [Crossref](http://www.crossref.org/) - via the `rcrossref` package\n* [Public Library of Science (PLOS)](https://www.plos.org/) - via the `rplos` package\n* [Biomed Central](http://www.biomedcentral.com/)\n* [arXiv](https://arxiv.org) - via the `aRxiv` package\n* [bioRxiv](http://biorxiv.org/) - via the `biorxivr` package\n* [PMC/Pubmed via Entrez](http://www.ncbi.nlm.nih.gov/) - via the `rentrez` package\n* Many more are supported via the above sources (e.g., _Royal Society Open Science_ is \navailable via Pubmed)\n* We __will__ add more, as publishers open up, and as we have time...See the [master list here](https://github.com/ropensci/fulltext/issues/4#issuecomment-52376743)\n\nAuthorization: A number of publishers require authorization via API key, and some even more\ndraconian authorization processes involving checking IP addresses. We are working on supporting\nall the various authorization things for different publishers, but of course all the OA content\nis already easily available. \n\nWe'd love your feedback. Let us know what you think in [the issue tracker](https://github.com/ropensci/fulltext/issues)\n\nArticle full text formats by publisher:  [https://github.com/ropensci/fulltext/blob/master/vignettes/formats.Rmd](https://github.com/ropensci/fulltext/blob/master/vignettes/formats.Rmd)\n\n## Installation\n\nStable version from CRAN\n\n\n```r\ninstall.packages(\"fulltext\")\n```\n\nDevelopment version from GitHub\n\n\n```r\ndevtools::install_github(\"ropensci/fulltext\")\n```\n\nLoad library\n\n\n```r\nlibrary('fulltext')\n```\n\n## Extraction tools\n\nIf you want to use `ft_extract()` function, it currently has two options for how to extract text from PDFs: `xpdf` and `ghostscript`. \n\n* `xpdf` installation: See http://www.foolabs.com/xpdf/download.html for instructions on how to download and install `xpdf`. For OSX, you an also get `xpdf` via [Homebrew](https://github.com/homebrew/homebrew-x11/blob/master/xpdf.rb) with `brew install xpdf`. Apparently, you can optionally install Poppler, which is built on `xpdf`. Get it at http://poppler.freedesktop.org/\n* `ghostscript` installation: See http://www.ghostscript.com/doc/9.16/Install.htm  \nfor instructions on how to download and install `ghostscript`. For OSX, you an also get `ghostscript` via [Homebrew](https://github.com/Homebrew/homebrew/blob/master/Library/Formula/ghostscript.rb) with `brew install gs`.\n\n## Search\n\n`ft_search()` - get metadata on a search query.\n\n\n```r\nft_search(query = 'ecology', from = 'plos')\n#> Query:\n#>   [ecology] \n#> Found:\n#>   [PLoS: 31172; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0] \n#> Returned:\n#>   [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0]\n```\n\n## Get full text links\n\n`ft_links()` - get links for articles (xml and pdf).\n\n\n```r\nres1 <- ft_search(query = 'ecology', from = 'entrez', limit = 5)\nft_links(res1)\n#> <fulltext links>\n#> [Found] 2 \n#> [IDs] ID_26787189 ID_26477897 ...\n```\n\nOr pass in DOIs directly\n\n\n```r\nft_links(res1$entrez$data$doi, from = \"entrez\")\n#> <fulltext links>\n#> [Found] 2 \n#> [IDs] ID_26787189 ID_26477897 ...\n```\n\n## Get full text\n\n`ft_get()` - get full or partial text of articles.\n\n\n```r\nft_get('10.1371/journal.pone.0086169', from = 'plos')\n#> <fulltext text>\n#> [Docs] 1 \n#> [Source] R session  \n#> [IDs] 10.1371/journal.pone.0086169 ...\n```\n\n## Extract chunks\n\n\n```r\nlibrary(\"rplos\")\n(dois <- searchplos(q = \"*:*\", fl = 'id',\n   fq = list('doc_type:full',\"article_type:\\\"research article\\\"\"), limit = 5)$data$id)\n#> [1] \"10.1371/journal.pone.0068036\" \"10.1371/journal.pone.0064513\"\n#> [3] \"10.1371/journal.pone.0053239\" \"10.1371/journal.ppat.1000439\"\n#> [5] \"10.1371/journal.ppat.1000438\"\nx <- ft_get(dois, from = \"plos\")\nx %>% chunks(\"publisher\") %>% tabularize()\n#> $plos\n#>                                               publisher\n#> 1         Public Library of Science\\nSan Francisco, USA\n#> 2         Public Library of Science\\nSan Francisco, USA\n#> 3 Public Library of Science\\n        San Francisco, USA\n#> 4         Public Library of Science\\nSan Francisco, USA\n#> 5         Public Library of Science\\nSan Francisco, USA\n```\n\n\n```r\nx %>% chunks(c(\"doi\",\"publisher\")) %>% tabularize()\n#> $plos\n#>                            doi\n#> 1 10.1371/journal.pone.0068036\n#> 2 10.1371/journal.pone.0064513\n#> 3 10.1371/journal.pone.0053239\n#> 4 10.1371/journal.ppat.1000439\n#> 5 10.1371/journal.ppat.1000438\n#>                                               publisher\n#> 1         Public Library of Science\\nSan Francisco, USA\n#> 2         Public Library of Science\\nSan Francisco, USA\n#> 3 Public Library of Science\\n        San Francisco, USA\n#> 4         Public Library of Science\\nSan Francisco, USA\n#> 5         Public Library of Science\\nSan Francisco, USA\n```\n\nUse `dplyr` to data munge\n\n\n```r\nlibrary(\"dplyr\")\nx %>%\n chunks(c(\"doi\", \"publisher\", \"permissions\")) %>%\n tabularize() %>%\n .$plos %>%\n select(-permissions.license)\n#>                            doi\n#> 1 10.1371/journal.pone.0068036\n#> 2 10.1371/journal.pone.0064513\n#> 3 10.1371/journal.pone.0053239\n#> 4 10.1371/journal.ppat.1000439\n#> 5 10.1371/journal.ppat.1000438\n#>                                               publisher\n#> 1         Public Library of Science\\nSan Francisco, USA\n#> 2         Public Library of Science\\nSan Francisco, USA\n#> 3 Public Library of Science\\n        San Francisco, USA\n#> 4         Public Library of Science\\nSan Francisco, USA\n#> 5         Public Library of Science\\nSan Francisco, USA\n#>   permissions.copyright.year permissions.copyright.holder\n#> 1                       2013                  Singh et al\n#> 2                       2013                     Li et al\n#> 3                       2013                 Harper et al\n#> 4                       2009               Narvaiza et al\n#> 5                       2009                         <NA>\n#>   permissions.license_url\n#> 1                    <NA>\n#> 2                    <NA>\n#> 3                    <NA>\n#> 4                    <NA>\n#> 5                    <NA>\n```\n\n## Supplementary materials\n\nGrab supplementary materials for (re-)analysis of data\n\n`ft_get_si()` accepts article identifiers, and output from `ft_search()`, `ft_get()`\n\n\n```r\ncatching.crabs <- read.csv(ft_get_si(\"10.6084/m9.figshare.979288\", 2))\n#> Error in .grep.text(html, regexp, which): SI number '2' greater than number of detected SIs (1)\nhead(catching.crabs)\n#> Error in head(catching.crabs): object 'catching.crabs' not found\n```\n\n## Cache\n\nWhen dealing with full text data, you can get a lot quickly, and it can take a long time to get. That's where caching comes in. And after you pull down a bunch of data, if you do so within the R session, you don't want to lose that data if the session crashes, etc. When you search you _will be able to_ (i.e., not ready yet) optionally cache the raw JSON/XML/etc. of each request locally - when you do that exact search again we'll just give you the local data - unless of course you want new data, which you can do.\n\n\n```r\nft_get('10.1371/journal.pone.0086169', from='plos', cache=TRUE)\n```\n\n## Extract text from PDFs\n\nThere are going to be cases in which some results you find in `ft_search()` have full text available in text, xml, or other machine readable formats, but some may be open access, but only in pdf format. We have a series of convenience functions in this package to help extract text from pdfs, both locally and remotely.\n\nLocally, using code adapted from the package `tm`, and various pdf to text parsing backends\n\n\n```r\npdf <- system.file(\"examples\", \"example2.pdf\", package = \"fulltext\")\n```\n\nUsing `ghostscript`\n\n\n```r\n(res_gs <- ft_extract(pdf, \"gs\"))\n#> <document>/Library/Frameworks/R.framework/Versions/3.2/Resources/library/fulltext/examples/example2.pdf\n#>   Title: pone.0107412 1..10\n#>   Producer: Acrobat Distiller 9.0.0 (Windows); modified using iText 5.0.3 (c) 1T3XT BVBA\n#>   Creation date: 2014-09-18\n```\n\nUsing `xpdf`\n\n\n```r\n(res_xpdf <- ft_extract(pdf, \"xpdf\"))\n#> <document>/Library/Frameworks/R.framework/Versions/3.2/Resources/library/fulltext/examples/example2.pdf\n#>   Pages: 10\n#>   Title: pone.0107412 1..10\n#>   Producer: Acrobat Distiller 9.0.0 (Windows); modified using iText 5.0.3 (c) 1T3XT BVBA\n#>   Creation date: 2014-09-18\n```\n\nOr extract directly into a `tm` Corpus\n\n\n```r\npaths <- sapply(paste0(\"example\", 2:5, \".pdf\"), function(x) system.file(\"examples\", x, package = \"fulltext\"))\n(corpus_xpdf <- ft_extract_corpus(paths, \"xpdf\"))\n#> $meta\n#>           names                           class\n#> 1 content, meta PlainTextDocument, TextDocument\n#> 2 content, meta PlainTextDocument, TextDocument\n#> 3 content, meta PlainTextDocument, TextDocument\n#> 4 content, meta PlainTextDocument, TextDocument\n#> \n#> $data\n#> <<VCorpus>>\n#> Metadata:  corpus specific: 0, document level (indexed): 0\n#> Content:  documents: 4\n#> \n#> attr(,\"class\")\n#> [1] \"xpdf\"\n```\n\nExtract pdf remotely on the web, using a service called `PDFX`\n\n\n```r\npdf5 <- system.file(\"examples\", \"example5.pdf\", package = \"fulltext\")\npdfx(file = pdf5)\n```\n\n\n```r\n#> $meta\n#> $meta$job\n#> [1] \"34b281c10730b9e777de8a29b2dbdcc19f7d025c71afe9d674f3c5311a1f2044\"\n#>\n#> $meta$base_name\n#> [1] \"5kpp\"\n#>\n#> $meta$doi\n#> [1] \"10.7554/eLife.03640\"\n#>\n#>\n#> $data\n#> <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n#> <pdfx xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"http://pdfx.cs.man.ac.uk/static/article-schema.xsd\">\n#>   <meta>\n#>     <job>34b281c10730b9e777de8a29b2dbdcc19f7d025c71afe9d674f3c5311a1f2044</job>\n#>     <base_name>5kpp</base_name>\n#>     <doi>10.7554/eLife.03640</doi>\n#>   </meta>\n#>    <article>\n#>  .....\n```\n\n## Meta\n\n* Please [report any issues or bugs](https://github.com/ropensci/fulltext/issues).\n* License: MIT\n* Get citation information for `fulltext`: `citation(package = 'fulltext')`\n* Please note that this project is released with a [Contributor Code of Conduct](CONDUCT.md). By participating in this project you agree to abide by its terms.\n\n[![rofooter](http://ropensci.org/public_images/github_footer.png)](http://ropensci.org)\n", 
  "id": 20299174
}