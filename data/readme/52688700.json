{
  "read_at": 1462557960, 
  "description": "Given a law corpus, create a dictionary for similar words looking-up", 
  "README.md": "# law-dictionary\nGiven a law corpus, create a dictionary for similar words looking-up\n\n0.If you only want to try the result, skip process 1,2,3 and directly see 4.\n\n1.You need to make sure your law corpus under this directory. To obtain dataset you can also download from here. (https://drive.google.com/file/d/0B9tRxL7rm9hDTC00b095WndYajQ/view?usp=sharing)\n\n2.Run merge_files.sh and remove_punc.sh you will get the preprocessed corpus, called \"merged\".\n\n3.Generally, you are supposed to train the model but I've already train the model. If you wanna train the model yourself, you can find my complete code here. (http://www.cs.toronto.edu/~zqiu/resource/law-dic.zip)\n\n4.Assume the model has been trained already. In your terminal, run \"./distance vectors.bin\" (or you can download from http://www.cs.toronto.edu/~zqiu/resource/vectors.bin) for word and \"./distance vectors-phrase.bin\" (or you can download from http://www.cs.toronto.edu/~zqiu/resource/vectors-phrase.bin) for phrases. Then you can follow screen instruction to input your target word and the program will return you the closest words and their corresponding distance to your typing word.\n\nA few explanation towards my model:\n\n1.Why use google's word2vec as \"framework\" instead of gensim or many other version framework(java,etc)?\n- Gensim is fabulous. As a python user, I love gensim and done several projects before but gensim only implement skip-gram (no CBOW) and only use softmax (without negative sampling). However, if you really want to try, use the resource below.\n\n\nC\thttp://word2vec.googlecode.com/svn/trunk/\npython\thttp://radimrehurek.com/gensim/\t\t \nJava\thttps://github.com/ansjsun/Word2VEC_java \nC++\thttps://github.com/jdeng/word2vec\n\n2.What's the parameter I used for training my model?\n- Even though word2vec provide CBOW and skip-gram, I still use skip-gram. CBOW is more like \"given context, predict the word in the middle\", it takes longer and require even bigger corpus.\n- The dimensionality of my model is 48.\n- the window size is 5, which represents before and after 5 words will be taken into consideration.\n- -min-count and -alpha uses default.\n- Use hs instead of negtive.\n- -binary 1 means use binary to store.\n\n\n\nThanks to:\n1. word2vec, google. https://code.google.com/archive/p/word2vec/\n\n2. Tomas Mikolov, google. http://arxiv.org/pdf/1309.4168.pdf\n\n3. Also, University of Toronto professor.\n-  Prof. Hinton's paper \"Learning distributed representations of concepts\", 1986.\n-  Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler, \"Skip-thought vectors\", 2015.\n", 
  "id": 52688700
}