{
  "id": 41055692, 
  "read_at": 1462545891, 
  "README.rst": "==============================\nPylearn2: A machine learning research library\n==============================\n\nPylearn2 is a library designed to make machine learning research easy.\n\nPylearn2 has online `documentation <http://deeplearning.net/software/pylearn2/>`_.\nIf you want to build a local copy of the documentation, run\n\n    python ./doc/scripts/docgen.py\n\nMore documentation is available in the form of commented examples scripts\nand ipython notebooks in the \"pylearn2/scripts/tutorials\" directory.\n\nPylearn2 was initially developed by David\nWarde-Farley, Pascal Lamblin, Ian Goodfellow and others during the winter\n2011 offering of `IFT6266 <http://www.iro.umontreal.ca/~pift6266/>`_, and\nis now developed by the LISA lab.\n\n\nQuick start and basic design rules\n------------------\n- Installation instructions are available `here <http://deeplearning.net/software/pylearn2/#download-and-installation>`_.\n- Subscribe to the `pylearn-users Google group\n  <http://groups.google.com/group/pylearn-users>`_ for important updates. Please write\n  to this list for general inquiries and support questions.\n- Subscribe to the `pylearn-dev Google group\n  <http://groups.google.com/group/pylearn-dev>`_ for important development updates. Please write\n  to this list if you find any bug or want to contribute to the project.\n- Read through the documentation and examples mentioned above.\n- Pylearn2 should not force users to commit to the whole library. If someone just wants\n  to implement a Model, they should be able to do that and not need to implement\n  a TrainingAlgorithm. Try not to write library features that force users to buy into\n  the whole library.\n- When writing reference implementations to go in the library, maximize code re-usability\n  by decomposing your algorithm into a TrainingAlgorithm that trains a Model on a Dataset.\n  It will probably do this by minimizing a Cost. In fact, you can probably use an existing\n  TrainingAlgorithm.\n\nHighlights\n------------------\n- Pylearn2 was used to set the state of the art on MNIST, CIFAR-10, CIFAR-100, and SVHN.\n  See pylearn2.models.maxout or pylearn2/scripts/papers/maxout\n- Pylearn2 provides a wrapper around Alex Krizhevsky's extremely efficient GPU convolutional\n  network library. This wrapper lets you use Theano's symbolic differentiation and other\n  capabilities with minimal overhead. See pylearn2.sandbox.cuda_convnet.\n\nLicense and Citations\n---------------------\nPylearn2 is released under the 3-claused BSD license, so it may be used for commercial purposes.\nThe license does not require anyone to cite Pylearn2, but if you use Pylearn2 in published research\nwork we encourage you to cite this article:\n\n- Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin,\n  Mehdi Mirza, Razvan Pascanu, James Bergstra, Frederic Bastien, and\n  Yoshua Bengio.\n  `\"Pylearn2: a machine learning research library\"\n  <http://arxiv.org/abs/1308.4214>`_.\n  *arXiv preprint arXiv:1308.4214* (`BibTeX\n  <http://www.iro.umontreal.ca/~lisa/publications2/index.php/export/publication/594/bibtex>`_)\n", 
  "description": "sampling-based CNN"
}