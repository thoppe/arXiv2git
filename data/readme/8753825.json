{
  "read_at": 1462549131, 
  "description": "Local search based graph clustering software", 
  "README.md": "Local Search Optimization for graph clustering\n=============\n\nIntroduction\n------------\n\nThis program uses a local search optimization to find an approximately optimal clustering of a undirected graph according to some objective function.\nMany different objectives are supported.\nThere are bindings for octave and MATLAB.\n\nFormally, the problem that is solved is to find a clustering C of the nodes in a graph that minimizes the loss\n\n    loss(C) = f(sum_{c [?] C} g(c))\n\nThe optimization method is the one introduced by Blondel et.al. \\[2], and also used by \\[1] and \\[3].\n\nInstallation\n------------\n\nTo build the stand alone version:\n\n    make\n\nTo build the octave interface:\n\n    octave\n    make_octave\n\nTo build the matlab interface:\n\n    matlab\n    make_matlab\n\nUsage\n------------\n\nfor the Octave and MATLAB bindings, the usage is:\n\n    [c,l,k] = lso_cluster(A, <optional parameters>);\n\nWhere:\n * `A` is a symmetric weighted adjacency matrix. It can be sparse.\n * The optional parameters are a list of `'name',value` pairs, see below.\n * `c` is the labeling that represents the clustering. The first cluster will have label `0`, the next label `1`, etc.\n * `l` is the objective (loss) value of the clustering.\n * `k` is the number of clusters, `k = numel(unique(c))`.\n\nOptional parameters regarding the clustering:\n\n * `'eval', clustering`\n   Evaluate the objective function on `clustering`, do not optimize.\n   \n * `'init', clustering`\n   Use the given `clustering` as initial value for the optimization.\n   Default: each node is initially in a separate cluster, i.e. `clustering=1:length(A)`.\n   \nOptional parameters regarding the objective:\n\n * `'loss', loss`\n   Use the given loss/objective function. The loss is given a string name. See below for a list of supported loss functions.\n   Default: `loss = 'modularity'`\n   \n * `'total_volume', m`\n   Replace the total volume (sum of edges) by `m`.\n   Many objectives use the total volume for normalization, and changing it will change the scale at which clusters are found.\n   Usually increasing the total volume will result in larger clusters.\n   Default: `m = sum(sum(A))`\n \n * `'extra_loss', alpha`\n   Add a term to the loss function that penalizes the volume of clusters, with weight `alpha`.\n \n * `'num_clusters', n`\n   Force the solution to have the given number of clusters.\n   The algorithm uses a binary search to alter the objective until it finds a solution with the given number of clusters.\n   The alteration is the same as the one used by `extra_loss`.\n   \n * `'min_num_cluster', n`\n   Force the solution to have at least the given number of clusters.\n   \n * `'max_num_cluster', n`\n   Force the solution to have at most the given number of clusters.\n   \n * `'max_cluster_size', n`\n   Allow clusters to have at most n nodes.\n\nOptional parameters about internal algorithm details, you only need these if you know what you are doing:\n\n * `'seed', random_seed`\n   Use a given random seed.\n   By default a fixed seed is used, so repeated runs with the same input give the same output.\n   \n * `'num_repeats', n`\n   Repeat the search n times from scratch with different random seeds and return the best result.\n   Default: 1\n \n * `'num_partitions', n`\n   Number of times to try and break apart the clusters and re-cluster them\n   Default: 0\n \n * `'optimize_exhaustive', bool`\n   Use an exhaustive search instead of local search. This is of course very slow. Can be combined with `max_num_cluster`.\n   Default: false.\n    \n * `'optimize_higher_level', bool`\n   Use a hierarchical optimizer, where small clusters are considered as nodes of a higher level graph.\n   Default: true.\n    \n * `'always_consider_empty', bool`\n   Always consider the move of a node into a new singleton cluster.\n   Default: true.\n    \n * `'num_loss_tweaks', n`\n   Maximum number of iterations in the binary search to force the specified number of clusters.\n   Default: 32\n    \n * `'check_invariants', bool`\n   Check invariants of the algorithm (for debugging). Default: false.\n   \n * `'trace_file', filename`\n   Write a trace of the steps performed by the optimization algorithm to a file in JSON format.\n\n * `'verbose', level`\n   Level of debug output. Levels go up to 7 and are increasingly chatty.\n   Default: `level = 0`, i.e. no output.\n\n\nLoss functions\n------------\n\nThe optional parameter `'loss'` specifies the loss function to use. The default is modularity.\nIn this section the notation is:\n\n    v_c: The volume of cluster c, i.e. the sum of degrees/strengths of nodes in c.\n         In MATLAB notation:\n           v_c = sum(sum(A(C==c,:)))\n    w_c: The weight of edges within cluster c\n         In MATLAB notation:\n           w_c = sum(sum(A(C==c,C==c))\n    n_c: The number of nodes in cluster c\n    m:   The total volume of the graph,\n         m = v_V, where V is the set of all nodes.\n         This is altered by the 'total_volume' option.\n\nSince the loss is minimized, some objectives are negated compared to their usual definition.\n\nSome of the supported loss functions are:\n\n* `'modularity'`,\n  `loss = -sum_c (w_c/m - v_c^2/m^2)`\n  This is the negation of the usual definition\n\n* `'infomap'`: The infomap objective by [3].\n\n* `'ncut'`: Normalized cut,\n  `loss = sum_c (v_c - w_c) / n_c`\n\n* `'rcut'`: Ratio cut,\n  `loss = sum_c (v_c - w_c) / v_c`\n\n* `{'pmod',p}`: Modularity with a different power,\n  `loss = -sum_c (w_c/m - (v_c/m)^p / (p-1))`\n\n* `{'mom',m}`: Monotonic variant of modularity,\n  `loss = -sum_c (w_c/(m + 2v_c) - (v_c/(m + 2v_c))^2)`\n\n* `'w-log-v'`,\n  `loss = sum_c (w_c/m * log(v_c) )`\n\n* `num`\n  `loss = |C|`: Minimize the number of clusters, this leads to finding the connected components.\n\nSee `loss_functions.hpp` for the full list.\nSome loss functions have parameters, these are passed as a cell array where the first element is the name.\n\n\nExamples\n------------\n\nFind the clustering of a graph by optimizing modularity:\n\n    % Build a graph that has a cluster structure\n    A = (blkdiag(rand(4), rand(5), rand(6)) > 1-0.5) | (rand(15) > 1-0.05);\n    A = A | A';\n    % Find clusterings\n    c = greedy_cluster(A);\n    % c = [0 0 0 0 1 1 1 1 1 2 2 2 2 2 2]'\n\nEvaluate the modularity of a given clustering:\n\n    c = [0 0 0 0 5 5 5 1 1 2 2 2 2 2 2]\n    [ignored, l] = greedy_cluster(A, 'eval',c);\n    l\n    % l = -0.40633\n\nOptimize infomap:\n\n    c = greedy_cluster(A, 'loss','infomap');\n\nFind a solution with exactly 3 clusters:\n\n    c = greedy_cluster(A, 'num_clusters',3);\n\n\nHere is a larger example:\n\n    % An LFR graph with mixing 0.6 and 1000 nodes (See [4])\n    % The gaph is in A, the ground truth is in c\n    > load example-LFR.mat\n    \n    > [d,l,k] = lso_cluster(A,'loss','modularity');\n    > normalized_mutual_information(c,d)\n    ans = 0.93011\n    \n    % This solution has too few clusters\n    > k\n    k = 25\n    > numel(unique(c))\n    ans = 41\n    \n    % So, force the number of clusters to be 41\n    > [d,l,k] = lso_cluster(A,'loss','modularity','num_clusters',41);\n    > normalized_mutual_information(c,d)\n    ans = 1\n    \n    % Or use a different loss function\n    > [d,l,k] = lso_cluster(A,'loss','w-log-v');\n    > normalized_mutual_information(c,d)\n    ans = 1\n\nUsage, stand alone version\n------------\n\nGraphs should be given on in a text file whos name is passed as the first argument (or `-` for stdin). Each line should look like:\n\n     <from> <to> <weight>\n\nWhere `<from>` and `<to>` are node identifiers. These are either strings or integers from `0...n`. Only nodes that have edges will be included in the graph. If needed, you can add edges `<n> <n> 0` to force the graph to contain a node `n`.\nThe graph is made symmetric, i.e. the reversed edges `<to> <from> <weight>` are also added.\n\nThe output is written as text to stdout, with `-o <filename>` the found clustering is directed to a file instead.\nEach line of the output will have the format\n\n    <node> <clusterid>\n\nNodes will be listed in alphabetical order, or in numerical order if the `--numeric` flag is given.\n\nOther parameters are specified as `--parameter value`, with the same name as the matlab version parameters. For example `--loss infomap` uses the infomap loss function.\n\n    $ lso-cluster example.in\n\n\nReferences\n----------\n\n\\[1] Graph clustering: does the optimization procedure matter more than the objective function?;\n     Twan van Laarhoven and Elena Marchiori;\n     Physical Review E 87, 012812 (2013)\n     [\\[pdf\\]](http://cs.ru.nl/~T.vanLaarhoven/clustering2012/PhysRevE.87.012812.pdf)\n     [\\[publisher\\]](http://link.aps.org/doi/10.1103/PhysRevE.87.012812)\n     [\\[website\\]](http://cs.ru.nl/~T.vanLaarhoven/clustering2012/)\n\n\\[2] Fast unfolding of communities in large networks;\n     Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte and Etienne Lefebvre;\n     J. Stat. Mech. Theory Exp. 2008, P10008 (2008),\n     [\\[publisher\\]](http://iopscience.iop.org/1742-5468/2008/10/P10008/)\n     [\\[arxiv\\]](http://arxiv.org/abs/0803.0476)\n\n\\[3] Maps of random walks on complex networks reveal community structure;\n     M. Rosvall and C. T. Bergstrom;\n     Proc. Natl. Acad. Sci. USA 105, 1118 (2008).\n     [\\[publisher\\]](http://dx.doi.org/10.1073/pnas.0706851105)\n     [\\[arxiv\\]](http://arxiv.org/abs/0707.0609)\n     [\\[code\\]](http://www.tp.umu.se/~rosvall/code.html)\n\n\\[4] Benchmark graphs for testing community detection algorithms;\n     A. Lancichinetti, S. Fortunato and F. Radicchi;\n     Physical Review E 78, 046110 (2008)\n     [\\[pdf\\]](https://sites.google.com/site/santofortunato/benchmark.pdf?attredirects=0)\n     [\\[website\\]](https://sites.google.com/site/santofortunato/inthepress2)\n", 
  "id": 8753825
}