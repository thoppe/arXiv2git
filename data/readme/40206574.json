{
  "read_at": 1462545974, 
  "description": "Vocabulary using n-grams", 
  "README.md": "# vocab\nVocab - Moz's package for generating and using n-grams. \n\nThere are many applications where you might want to create a vocabulary. \nOne example is Moz's word2gauss project (https://github.com/seomoz/word2gauss),\nwhich computes word embeddings for the vocabulary provided to it.\n\nThis repo, vocab, allows you to generate and use a vocabulary consisting of n-grams.\nVocabulary creation is done from the corpus that you provide.\n\n## Dependencies\nThe dependency list is short: numpy and Cython (>= 0.21.1).\n\n## Usage\n\n### Tokenization\nTo create and use a vocabulary, you will need a tokenizer. If left unspecified,\na default tokenizer will be used. The default tokenizer breaks on whitespace and \npunctuation (which is removed) and keeps only tokens that contain at least one \nASCII character. The input text must be encodable as UTF-8, and the tokens are \nlowercased.\n\nIf you supply your own tokenizer, the input will be a string and the output should\nbe an iterable of tokens.\n\n### Creating a vocabulary\nThe vocab constructor has a few optional arguments. The *tokenizer* parameter will\nallow you to specify your own tokenizer function, as described above. If you'd\nprefer to use your own stopword file, you can set *stopword_file*. \n\nCreating n-grams from a corpus requires you to pass a tuple for each \nn-gram order: the maximum number to keep, the minimum count needed per ngram, \nand a discounting coefficient, which prevents too many ngrams consisting of very \ninfrequent words to be formed. See equation 6 in this paper:\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. \nDistributed Representations of Words and Phrases and their Compositionality. \nIn Proceedings of NIPS, 2013.  (http://arxiv.org/pdf/1310.4546.pdf)\n\nAs an example, the following code creates uni, bi and trigrams. It will create\nup to 75000 unigrams, 25000 bigrams, and 10000 trigrams. In this example, all\nngrams need a minimum count of 350, and 350 is the discounting coefficient. The\ninput, corpus, is an iterable that produces one string per document. In the example\nbelow, the corpus is contained in a bzipped-file that contains one document per line.\n\n```python\nwith BZ2File('my_corpus.bz2', 'r') as corpus:\n    v = Vocabulary()\n    v.create(corpus, [(75000, 350, 350), (25000, 350, 350), (10000, 350, 350)])\n```\n\nThe create function also takes an optional flag, *keep_unigrams_stopwords* (True by\ndefault) to allow you the option of not keeping stopwords in the unigram set.\nFor word2gauss, we want to compute the embeddings for stopwords, but for an \napplication like topic modeling, you may want to exclude stopwords from the \nunigrams.\n\nOne assumption that we make when building the n-grams is that for bigrams and \nhigher, valid ngrams do not start or end with stopwords. \n\nNgrams are composed of unigram tokens and stored with underscores to delimit the tokens. \nFor example, \"statue of liberty\" is stored as \"statue_of_liberty\".\n\n### Saving and loading a vocabulary\nYou can save a vocabulary to gzipped file:\n```python\nv.save('my_vocab.gz')\n```\nLater, you can create a Vocabulary instance by loading the file:\n\n```python\nv = Vocabulary.load('my_vocab.gz')\n```\n\nThe load function assumes a gzipped-file.\n\nThe load function has optional arguments to specifiy the tokenizer to be used \nwith this vocabulary instance (*tokenizer*), the index lookup table size (*table_size*), \nand the power used to build the index lookup table (*power*). See the \"Negative sampling\"\nsection below for a description of the index lookup table.\n\n\n### Updating a vocabulary\nOnce a vocabulary is created, you can add n-grams to it by calling *add_ngrams*.\n\n```python\nv.add_ngrams(['iphone_6, 'samsung_galaxy_6'])\n```\n\nYou can also update token counts by passing a corpus to *update_counts*, where\nthe corpus is an iterable that produces one string per document.\n\n```python\nv.update_counts(corpus)\n```\n\n### Using a vocabulary\nEach ngram in the vocabulary is assigned an id when it is added. You can look\nup an ngram by id:\n\n```python\nv.id2word(100)  \n```\n(example output: 'statue_of_liberty')\n\nor an id by ngram:\n\n```python\nid = v.word2id('statue_of_liberty')\n```\n(example output: 100)\n\nThe function *tokenize* will return the ngrams for the input string:\n\n```python\nv.tokenize('The Statue of Liberty is in New York.')\n```\n(example output: ['the', 'statue_of_liberty', 'is', 'in', 'new_york'])\n\nIf the input contains tokens that are not part of the vocabulary, they will be \nremoved unless you set the optional parameter *remove_oov* to False. In this case,\nthe token \"OOV\" will be returned.\n\nIf you prefer the ids, you can call *tokenize_id*. The value -1 is returned for \nout-of-vocabulary tokens.\n\nTo get the size of the vocabulary, just call len:\n\n```python\nlen(v)\n```\n\n### Negative sampling\nSome word embedding algorithms use the idea of negative sampling, which is \ndescribed in the paper by Mikolov et al. cited above. To enable this, we build an index \nlookup table from the vocabulary counts when you load a vocabulary from file or\ncreate a vocabulary. The size and power used for this table can be modified in the\nload function or the constructor.\n\nThe functions, *random_id* and *random_ids* allow you to sample the vocabulary \nfrom this table:\n \n ```python\n id = v.random_id()        # return a random id\n ids = v.random_ids(100)   # return a list of 100 random ids\n ```\n", 
  "id": 40206574
}