{
  "read_at": 1462547909, 
  "description": "", 
  "README.md": "# artistic-videos\r\n\r\nThis is the torch implementation for the paper \"[Artistic style transfer for videos](http://arxiv.org/abs/1604.08610)\", based on neural-style code by Justin Johnson https://github.com/jcjohnson/neural-style .\r\n\r\nOur algorithm allows to transfer the style from one image (for example, a painting) to a whole video sequence and generates consistent and stable stylized video sequences.\r\n\r\n**Example video:**\r\n\r\n[![Artistic style transfer for videos](http://img.youtube.com/vi/Khuj4ASldmU/0.jpg)](https://www.youtube.com/watch?v=Khuj4ASldmU \"Artistic style transfer for videos\")\r\n\r\n## Setup\r\n\r\nTested with Ubuntu 14.04.\r\n\r\n* Install torch7, loadcaffe and the CUDA backend (otherwise you have to use CPU mode which is horribly slow) and download the VGG model, as described by jcjohnson: [neural-style#setup](https://github.com/jcjohnson/neural-style#setup).\r\n* To use the temporal consistency constraints, you need to set up an utility which estimates the optical flow between two images and creates a flow file in the [middlebury file format](http://vision.middlebury.edu/flow/code/flow-code/README.txt). For example, you can use [DeepFlow](http://lear.inrialpes.fr/src/deepflow/) which we also used in our paper. Then you can make use of the script `makeOptFlow.sh` to generate the optical flow for all frames as well as the certainty of the flow field. Specify the path to the optical flow utility in the first line of this script file.\r\n\r\n## Simple style transfer\r\n\r\nTo perform style transfer with mostly the default parameters, execute `stylizeVideo.sh <path_to_video> <path_to_style_image>`. This script will perform all the steps necessay to create a stylized version of the video. Note: you still need to specify the path to the optical flow utility as described above. And you have to have ffmpeg (or libav-tools for Ubuntu 14.10 and earlier) installed.\r\n\r\n## Advanced Usage\r\n\r\nPlease read the script `stylizeVideo.sh` to see which setps you have to perform in advance exactly. Basically you have to save the frames of the video as individual image files and you have to compute the optical flow between all adjacent frames as well as the certainty of the flow field (both can be accomplished with `makeOptFlow.sh`).\r\n\r\nThere are two versions of this algorithm, a single-pass and a multi-pass version. The multi-pass version yields better results in case of strong camera motion, but needs more iterations per frame.\r\n\r\nBasic usage:\r\n\r\n```\r\nth artistic_video.lua <arguments> [-args <fileName>]\r\n```\r\n\r\n```\r\nth artistic_video_multiPass.lua <arguments> [-args <fileName>]\r\n```\r\n\r\nArguments can be given by command line and/or written in a file with one argument per line. Specify the path to this file through the option `-args`. Arguments given by command line will override arguments written in this file.\r\n\r\n**Basic arguments**:\r\n* `-style_image`: The style image.\r\n* `-content_pattern`: A file path pattern for the individual frames of the videos, for example `frame_%04d.png`.\r\n* `-num_images`: The number of frames.\r\n* `-start_number`: The index of the first frame. Default: 1\r\n* `-gpu`: Zero-indexed ID of the GPU to use; for CPU mode set `-gpu` to -1.\r\n\r\n**Arguments for the single-pass algorithm** (only present in `artistic_video.lua`)\r\n* `-flow_pattern`: A file path pattern for files that store the backward flow between the frames. The placeholder in square brackets refers to the frame position where the optical flow starts and the placeholder in braces refers to the frame index where the optical flow points to. For example `flow_[%02d]_{%02d}.flo` means the flow files are named *flow_02_01.flo*, *flow_03_02.flo*, etc. If you use the script included in this repository (makeOptFlow.sh), the filename pattern will be `backward_[%d]_{%d}.flo`.\r\n* `-flowWeight_pattern`: A file path pattern for the weights / certainty of the flow field. These files should be a grey scale image where a white pixel indicates a high flow weight and a black pixel a low weight, respective. Same format as above. If you use the script, the filename pattern will be `reliable_[%d]_{%d}.pgm`.\r\n* `-flow_relative_indices`: The indices for the long-term consistency constraint as comma-separated list. Indices should be relative to the current frame. For example `1,2,4` means it uses frames *i-1*,*i-2* and *i-4* warped for current frame at position *i* as consistency constraint. Default value is 1 which means it uses short-term consistency only. If you use non-default values you have to compute the corresponding long-term flow as well.\r\n\r\n**Arguments for the multi-pass algorithm** (only present in `artistic_video_multiPass.lua`)\r\n* `-forwardFlow_pattern`: A file path pattern for the forward flow. Same format as in `-flow_pattern`.\r\n* `-backwardFlow_pattern`: A file path pattern for the backward flow. Same format as above.\r\n* `-forwardFlow_weights_pattern`: A file path pattern for the forward-flow. Same format as above.\r\n* `-backwardFlow_weights_pattern`: A file path pattern for the backward flow. Same format as above.\r\n* `-num_passes`: Number of passes. Default: 15.\r\n* `-use_temporalLoss_after`: Uses temporal consistency loss in given pass and afterwards. Default: `8`.\r\n* `-blendWeight`: The blending factor of the previous stylized frame. The higher this value, the stronger the temporal consistency. Default value is `1` which means that the previous stylized frame is blended equally with the current frame.\r\n\r\n**Optimization options**:\r\n* `-content_weight`: How much to weight the content reconstruction term. Default is 5e0.\r\n* `-style_weight`: How much to weight the style reconstruction term. Default is 1e2.\r\n* `-temporal_weight`: How much to weight the temporal consistency loss. Default is 1e3. Set to 0 to disable the temporal consistency loss.\r\n* `-temporal_loss_criterion`: Which error function is used for the temporal consistency loss. Can be either `mse` for the mead squared error or `smoothl1` for the [smooth L1 criterion](https://github.com/torch/nn/blob/master/doc/criterion.md#nn.SmoothL1Criterion).\r\n* `-tv_weight`: Weight of total-variation (TV) regularization; this helps to smooth the image.\r\n  Default is 1e-3. Set to 0 to disable TV regularization.\r\n* `-num_iterations`:\r\n  * Single-pass: Two comma-separated values for the maximum number of iterations for the first frame and for subsequent frames. Default is 2000,1000.\r\n  * Multi-pass: A single value for the number of iterations *per pass*.\r\n* `-tol_loss_relative`: Stop if the relative change of the loss function in an interval of `tol_loss_relative_interval` iterations falls below this threshold. Default is `0.0001` which means that the optimizer stops if the loss function changes less than 0.01% in the given interval. Meaningful values are between `0.001` and `0.0001` in the default interval.\r\n* `-tol_loss_relative_interval`: Se above. Default value: `50`.\r\n* `-init`:\r\n  * Single-pass: Two comma-separated values for the initialization method for the first frame and for subsequent frames; one of `random`, `image`, `prev` or `prevWarped`.\r\n  Default is `random,prevWarped` which uses a noise initialization for the first frame and the previous stylized frame warped for subsequent frames. `image` initializes with the content frames. `prev` initializes with the previous stylized frames without warping.\r\n  * Multi-pass: One value for the initialization method. Either `random` or `image`.\r\n* `-optimizer`: The optimization algorithm to use; either `lbfgs` or `adam`; default is `lbfgs`.\r\n  L-BFGS tends to give better results, but uses more memory. Switching to ADAM will reduce memory usage;\r\n  when using ADAM you will probably need to play with other parameters to get good results, especially\r\n  the style weight, content weight, and learning rate; you may also want to normalize gradients when\r\n  using ADAM.\r\n* `-learning_rate`: Learning rate to use with the ADAM optimizer. Default is 1e1.\r\n* `-normalize_gradients`: If this flag is present, style and content gradients from each layer will be\r\n  L1 normalized. Idea from [andersbll/neural_artistic_style](https://github.com/andersbll/neural_artistic_style).\r\n\r\n**Output options**:\r\n* `-output_image`: Name of the output image. Default is `out.png` which will produce output images of the form *out-\\<frameIdx\\>.png* for the single-pass and *out-\\<frameIdx\\>_\\<passIdx\\>.png* for the multi-pass algorithm.\r\n* `-output_folder`: Directory where the output images should be saved. Must end with a slash.\r\n* `-print_iter`: Print progress every `print_iter` iterations. Set to 0 to disable printing.\r\n* `-save_iter`: Save the image every `save_iter` iterations. Set to 0 to disable saving intermediate results.\r\n* `-save_init`: Set to 1 to save the initialization image; 0 otherwise.\r\n\r\n**Other arguments**:\r\n* `-content_layers`: Comma-separated list of layer names to use for content reconstruction.\r\n  Default is `relu4_2`.\r\n* `-style_layers`: Comman-separated list of layer names to use for style reconstruction.\r\n  Default is `relu1_1,relu2_1,relu3_1,relu4_1,relu5_1`.\r\n* `-style_blend_weights`: The weight for blending the style of multiple style images, as a\r\n  comma-separated list, such as `-style_blend_weights 3,7`. By default all style images\r\n  are equally weighted.\r\n* `-style_scale`: Scale at which to extract features from the style image, relative to the size of the content video. Default is `1.0`.\r\n* `-proto_file`: Path to the `deploy.txt` file for the VGG Caffe model.\r\n* `-model_file`: Path to the `.caffemodel` file for the VGG Caffe model.\r\n  Default is the original VGG-19 model; you can also try the normalized VGG-19 model used in the paper.\r\n* `-pooling`: The type of pooling layers to use; one of `max` or `avg`. Default is `max`.\r\n  The VGG-19 models uses max pooling layers, but the paper mentions that replacing these layers with average\r\n  pooling layers can improve the results. I haven't been able to get good results using average pooling, but\r\n  the option is here.\r\n* `-backend`: `nn` or `cudnn`. Default is `nn`. `cudnn` requires\r\n  [cudnn.torch](https://github.com/soumith/cudnn.torch) and may reduce memory usage.\r\n* `-cudnn_autotune`: When using the cuDNN backend, pass this flag to use the built-in cuDNN autotuner to select\r\n  the best convolution algorithms for your architecture. This will make the first iteration a bit slower and can\r\n  take a bit more memory, but may significantly speed up the cuDNN backend.\r\n\r\n## Acknowledgement\r\n* This work was inspired by the paper [A Neural Algorithm of Artistic Style](http://arxiv.org/abs/1508.06576) by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, which introduced an approach for style transfer in still images.\r\n* Our implementation is based on Justin Johnson's implementation [neural-style](https://github.com/jcjohnson/neural-style).\r\n\r\n## Citation\r\n\r\nIf you use this code or its parts in your research, please cite the following paper:\r\n\r\n```\r\n@TechReport{RuderDB2016,\r\n  author = {Manuel Ruder and Alexey Dosovitskiy and Thomas Brox},\r\n  title = {Artistic style transfer for videos},\r\n  institution  = \"arXiv:1604.08610\",\r\n  year         = \"2016\",\r\n}\r\n```\r\n", 
  "id": 56937993
}