{
  "id": 55419973, 
  "read_at": 1462547722, 
  "readme.txt": "Developed by Yining Wang (http://www.yining-wang.com)\nPlease cite the following paper:\nFast and guaranteed tensor decomposition via sketching. arXiv:1506.04448.\nBy Yining Wang, Hsiao-Yu Tung, Alex Smola and Anima Anandkumar.\n\nRequired packages:\nFFTW\nMatlab (>=2013)\n\n============================================================================================\nFast tensor decomposition\n\n1. Generating a (noisy) low-rank tensor\n./fftspec synth_lowrank [dim] [rank] [nsr]\n\n[dim]: the ambient dimension of the tensor\n[rank]: the intrinsic dimension of the tensor\n[nsr]: noise-to-signal ratio.\n\nThe resulting tensor will be saved to \"tensor.dat\" file. See Dense tensor format below for the format of the saved tensor.\n\n2. Generating a (noisy) high-rank tensor\n./fftspec synth_highrank [dim] [nsr]\n\nThe resulting tensor would have eigenvalues linearly decaying. That is, lambda_i = 1.0 / i\n\n3. Brute-force robust tensor power method\n./fftspec slow_rbp [input tensor data file] [rank] [L] [T] [result file]\n\n[input tensor data file]: the tensor to be decomposed. See below sections for formatting instructions.\n[rank]: target rank\n[L, T]: parameters for robust tensor power method. See paper for details.\n[result file]: store the factorization of input tensor. \nFormat: (binary file)\ndim (int), rank (int)\nlambda (double x rank)\nu1 (double x dim) (the first eigenvector)\nu2 (double x dim) (the second eigenvector)\n...\n\n4. Fast sketch-based robust tensor power method\n./fftspec slow_rbp [input tensor data file] [rank] [L] [T] [B] [b] [result file]\n\n[input tensor data file]: the tensor to be decomposed. See below sections for formatting instructions.\n[rank]: target rank\n[L, T]: parameters for robust tensor power method. See paper for details.\n[B, b]: parameters for sketching based tensor decomposition. See papaer for details.\n[result file]: store the factorization of input tensor. \n\n5. Brute-force Alternating Least Squares (ALS)\n./fftspec slow_als [input tensor data file] [rank] [T] [result_file]\n\n[input tensor data file]: the tensor to be decomposed. See below sections for formatting instructions.\n[rank]: target rank\n[T]: parameters for ALS. See paper for details.\n[result file]: store the factorization of input tensor. \n\nNote that for ALS, we have three (possibly different) aspects A, B and C. The result file will store all of them, in the exact order.\n\n6. Fast sketch sketch-based ALS\n./fftspec fast_als [input tensor data file] [rank] [T] [B] [b] [result_file]\n\n[input tensor data file]: the tensor to be decomposed. See below sections for formatting instructions.\n[rank]: target rank\n[T]: parameters for ALS. See paper for details.\n[B, b]: parameters for sketching based tensor decomposition. See papaer for details.\n[result file]: store the factorization of input tensor. \n\n============================================================================================\nFast spectral LDA\n\nBefore running, please first open the \"fftlda.cc\" file and set parameters for the LDA model to be learnt:\nV: number of vocabularies\nK: number of topics\nB, b: sketch parameters. See paper for details.\nL, T: robust tensor power method parameters. See paper for details.\nnum_threads: number of threads to use\nalpha0: parameter for spectral LDA (see paper for details). **strongly recommended to set as 1**\n\n1. Generating synthetic corpus: \n./fftlda synth [D] [m]\n\nD: number of documents\nm: number of words per documents\n\nThe synthesized corpus and model will be stored in \"synth.corpus\" and \"synth.model\"\n\nFor formats of corpus and model files, see below sections.\n\n2. Running (and evaluating) topic models\n./fftlda fast_speclda [corpus file] [model file] \nor \n./fftlda fast_speclda [corpus file] [model file] [reference model file]\n\ncorpus file: file name for the corpus\nmodel file: file name to store the fitted model\nreference model file: OPTIONAL. If provided, the program will try to evaluate the model trained with respect to the provided reference model file. This is not the only way to evaluate the obtained LDA model. One can always evaluate an obtained LDA model by its perplexity (see next) without any knowledge of a reference model.\n\n3. Evaluating perplexity\n./fftlda eval [model file] [test corpus file] [result file]\n\nmodel file: the model to be evaluated\ntest corpus file: evaluate the perplexity of given model on this particular test corpus\nresult file: store evaluation results\n\nWARNING: evaluation is extremely slow! Make sure your test corpus has at most 1000 documents.\n\n4. Producing topic assignments\n./fftlda gibbs [model file] [corpus file] [topic assignment file]\n\n[model file]: the trained spectral LDA model\n[corpus file]: the training corpus\n[topic assignment file]: topic assignments for every word in the training corpus\n\nPerform Gibbs sampling to produce a topic assignment. Could then be used as initialization of a collapsed Gibbs sampler and obtain much more accurate results.\n\n============================================================================================\nDense tensor format (*.dat, binary file):\ndim (int)\ndata (double x dim*dim*dim)\n\nNOTE: ALL WORDS MUST BE LISTED IN ASCENDING ORDER W.R.T. WORD ID\nCorpus format (*.corpus, binary file)\n# of documents (int)\n\tdoc_id1 (long long); # of items (int); word ids (int x # of items); word occurrences (int x # of items)\n    doc_id2 (long long); # of items (int); word ids (int x # of items); word occurrences (int x # of items)\n    doc_id3 (long long); # of items (int); word ids (int x # of items); word occurrences (int x # of items)\n    ...\n\n\nLDA model format (*.model, binary file)\nV (int), K (int)\nalpha (double x K)\nbeta (double x V)\nphi_1 (double x V)\nphi_2 (double x V)\n...\nphi_K (double x V)\n\nGibbs sampling data format\n# of topics (int)\n# of documents (int)\n\tdoc_id (long long); h (double x K); z (int x K)\n\nTensor power method pre-process data format\n# of documents (int)\nK (int)\nW * n_1 (double * K) # of words (int)\nW * n_2 (double * K) # of words (int)\n...\nW * n_d (double * K) # of words (int)\n", 
  "README.md": "# fftlda\nYining Wang's code for Tensor Decomposition via Sketching http://arxiv.org/abs/1506.04448\n", 
  "description": "Yining Wang's code for Tensor Decomposition via Sketching http://arxiv.org/abs/1506.04448"
}