{
  "id": 41311244, 
  "read_at": 1462544933, 
  "README.rst": "Theano implementation of the auto-classifiers-encoders (ACE) from \"Towards universal neural nets: Gibbs machines and ACE\", Galin Georgiev, \nhttp://arxiv.org/abs/1508.06585\n\n---------------------------\nTime spent on GTX 970 GPU:\n\n-non-generative ACE: \t \t      2 sec/epoch for batch size = 10000\n-generative ACE:\n\t-classification task (2): \t  8 sec/epoch for batch size = 10000\n\t-density estimation task (1): 15 sec/epoch for batch size = 1000\n---------------------------\nGenerative ACE options:\n\nDensity estimator (task=1) or classifier (task=2).\nSampling density is either Gaussian (sampling_class=1) or Laplacian (sampling_class=2). \n\n---------------------------\nGenerative ACE output for MNIST:\n\nEvery 20 epochs, a 30 x 30 matrix of 900 images is saved: \n\t-the top 300 images are from the so-called \"creative\" regime (see sub-section 3.2 in paper). \n\t-the middle 300 images are the model reconstructions of the first 300 test images in the \"non-creative\" regime.\n\t-the bottom 300 images are the  first 300 test images from the original (possibly binarized)  data set. \n\n--------------------------\nRaw data: \n\nObtain the files below from http://yann.lecun.com/exdb/mnist/ and change datasets_dir in toolbox.py to the desired location:\n\nMNIST:\ntrain-images.idx3-ubyte\ntrain-labels.idx1-ubyte\nt10k-images.idx3-ubyte\nt10k-labels.idx1-ubyte", 
  "description": "code referenced in \"Towards universal neural nets: Gibbs machines and ACE\", Galin Georgiev, http://arxiv.org/abs/1508.06585"
}