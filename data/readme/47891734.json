{
  "read_at": 1462549989, 
  "description": "Simple, opinionated explanations of various things encountered in Deep Learning", 
  "README.md": "\n# Deep Learning Glossary\n\nSimple, opinionated explanations of various things encountered in\nDeep Learning / AI / ML. \n\nContributions welcome - there may be errors here!\n\n\n\n### AlexNet\n\nWinner of ILSVRC 2012. Made a huge jump in accuracy using CNN. \nUses Dropout, ReLUs, and a GPU.\n\nKind of the canonical deep convolution net.\n\nhttp://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks\n\n\n### Attention\n\nNot well defined. The idea of processing only part of the input at each time\nstep. In visual attention, this is a low resolution window that jumps around\nthe high resolution input image. Determining where to focus attention is\ncontrolled by a RNN. \n\nTypical convolutional networks have input around 300 x 300 inputs, which is\nrather low resolution. Computation complexity grows at least linearly in the\nnumber of pixel inputs.  Attention models seem like a promising method of\nhandling larger inputs.\n\n[Recurrent Models of Visual Attention](http://arxiv.org/abs/1406.6247) Attention applied to MNIST.\n\n[Multiple Object Recognition with Visual Attention](http://arxiv.org/abs/1412.7755) Essentially v2 of the above model.\n\n[DRAW: A Recurrent Neural Network For Image Generation](http://arxiv.org/abs/1502.04623) A generative attention model\n\n\n### Batch Normialization (BN)\n\nNormalizing the inputs to each activation function can dramatically speed up\nlearning.\n\n[Ioffe & Szegedy, 2015](http://arxiv.org/abs/1502.03167)\n\n\n### CIFAR-10 Dataset\n\n60000 32x32 colour images in 10 classes, with 6000 images per class. There are\n50000 training images and 10000 test images. \n\nThere's also a CIFAR-100 with 100 classes.\n\nhttps://www.cs.toronto.edu/~kriz/cifar.html\n\n![cifar-10 example](cifar.png)\n\n\n### Convolutional Neural Network (CNN)\n\nThis is the best intro: http://cs231n.github.io/convolutional-networks/ \n\n[LeCun demoing one in '93](https://www.youtube.com/watch?v=FwFduRA_L6Q)\n\n\n### Deep Q Network (DQN)\n\nThe atari playing model. This is the closest thing to a real \"AI\" out there. It learns\nend-to-end from its input and makes actions. \n\n(Volodymyr Mnih Koray Kavukcuoglu David Silver Alex Graves Ioannis Antonoglou\nDaan Wierstra Martin Riedmiller)\n\n\n### Discriminative Model\n\nA model that outputs a category based on its input. As opposed to generative model.\n\nImage classification systems are discriminative.\n\n\n### Dropout\n\nIntroduced in AlexNet? Randomly zero out 50% of inputs during the forward pass.\nSimple regularizer.\n\n\n### Exponential Linear Units (ELUs)\n\nAnother activiation function designed to avoid vanishing gradients.\n\n```\nelu(x) = x          when x >= 0\n         exp(x) - 1 when x < 0\n```\n\n![elu graph](elu.png)\n\n[Clevert, Unterthiner, Hochreiter 2015](http://arxiv.org/abs/1511.07289)\n\n\n### FTRL-proximal algorithm, Follow-the-regularized-leader\n\n[Google, 2013](https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf)\n\n\n### Generative Adversarial Networks (GAN)\n\nFor generative networks (that is networks that produce high dimensional data like images or sound)\ntypical loss functions cannot express the complexity of the data. Goodfellow et al proposed a scheme\nwhere you train two networks: the generator and the discriminator. The generator tries to fool the\ndiscriminator into think it's producing real data. The discriminator outputs a single probability\ndeciding if it got input from the dataset or from the generator.\n\n[Generative Adversarial Networks (GAN)](http://arxiv.org/abs/1406.2661) (2014)\n\n[Adversarial Autoencoders](http://arxiv.org/abs/1511.05644) (2015)\n\n\n### Generative Model\n\nA model that produces output like the dataset it is training on. As opposed to a discriminative model.\n\nA model that creates images or sound would be a generative model.\n\n\n### Grid-LSTM\n\n(Kalchbrenner et al., 2015)\n\n\n### IAM Handwriting Dataset\n\nhttp://www.iam.unibe.ch/fki/databases/iam-handwriting-database\n\nFamously used in Graves's handwriting generation RNN: http://www.cs.toronto.edu/~graves/handwriting.html\n\nhttp://yann.lecun.com/exdb/mnist/\n\n\n### ImageNet Large Scale Visual Recognition Competition  (ILSVRC)\n\nThe most prominent computer vision contest, using the largest data set of images (ImageNet).\nThe progress in the classification task has brought CNNs to dominate the field of computer vision.\n\n| Year          | Model                     | Top-5 Error | Layers | Paper\n| ------------- |:-------------------------:| -----------:|-------:|------------------------------\n| 2012          | AlexNet                   |     17.0 %  |      8 | http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks\n| 2013          | ZFNet                     |     17.0 %  |      8 | http://arxiv.org/abs/1311.2901\n| 2014          | VGG-19                    |      8.43%  |     19 | http://arxiv.org/abs/1409.1556\n| 2014          | GoogLeNet / Inception     |      7.89%  |     22 | http://arxiv.org/abs/1409.4842\n| 2015          | Inception v3              |      5.6%   |        | http://arxiv.org/abs/1512.00567\n| 2015          | ResNet                    |      4.49%  |    152 | http://arxiv.org/abs/1512.03385\n\n\n### ImageNet dataset\n\nLargest image dataset. Each image is tagged with a WordNet nouns. One of the\nkey peices in the CNN progress along with GPUs and dedicated researchers.\n\n### Leaky ReLU \n\nVariation on ReLU that some have success with.\n\n```\nleaky_relu(x) = x          when x >= 0\n                0.01 * x   when x < 0\n```\n\n\n### Long Short Term Memory (LSTM)\n\nA type of RNN that solves the exploding/vanishing gradient problem.\n\nhttp://colah.github.io/posts/2015-08-Understanding-LSTMs/\n\n[This paper](http://arxiv.org/abs/1503.04069) is a great exploration of\nvariations. Concludes that [vanilla LSTM](http://www.sciencedirect.com/science/article/pii/S0893608005001206) is best.\n\nOriginally invented by [Hochreiter & Schmidhuber, 1997](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n\n\n### MNIST dataset\n\nHandwritten digits. 28x28 images. 60,000 training images and 10,000 testing images\n\n![MNIST example](mnist.png)\n\n\n### Momentum \n\nMomentum is an often used improvement to SGD - follow past gradients with some\nweight.\n\n\n### Neural Random-Access Machine (NRAM)\n\nEssentially v2 of NTM.\n\n[Kurach, Andrychowicz, Sutskever, 2015](http://arxiv.org/pdf/1511.06392v1)\n\n\n### Neural Turing Machine (NTM)\n\n(Graves et al., 2014)\n\n\n### Parametric Rectified Linear Unit (PReLU)\n\nhttp://arxiv.org/pdf/1502.01852v1\n\n\n### Rectified Linear Unit (ReLU)\n\nRectified linear unit is a common activation function which was first proved useful in AlexNet.\nRecommend over sigmoid and tanh activiations.\n\n```\nrelu(x) = max(x, 0)\n```\n\n[Nair & Hinton, 2010](http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)\n\n\n### ResNet\n\nA Microsoft Research model that won several categories of ILSVRC 2015.\nThe key insight is that as networks get deeper they can get worse, by adding\nidentify short cut connections over every two convolutional layers they are \nable to improve performance up to a 152 layer deep network. \nHere is a table from the paper that roughly shows the architectures they tried\n\n![ResNet arch](resnet.png)\n\n[Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 2015](http://arxiv.org/abs/1512.03385)\n\n\n### R-CNN, Regions with Convolutional Neural Networks\n\nObject detection model.\n\nGirshick et al., 2014\n\n\n### Stochastic Gradient Descent (SGD)\n\nThe backbone of Machine Learning.\n\n\n### TIMIT Speech database \n\n(Garofolo et al., 1993) \n\n\n### Variational Autoencoder (VAE)\n\nGenerative model. Compare to GAN.\n\n[Deep AutoRegressive Networks](http://arxiv.org/abs/1310.8499) (October 2013)\n\n[Aaron Courville's slides](https://ift6266h15.files.wordpress.com/2015/04/20_vae.pdf)  (April 2015)\n\n[Karol Gregor on Variational Autoencoders and Image Generation](https://www.youtube.com/watch?v=P78QYjWh5sM) (March 2015)\n\n\n### VGG-16 / VGG-19\n\nClose second place winner in ILSVRC 2014. Very simple CNN architecure using only\n3x3 convolutions, max pooling, ReLUs, dropout\n\n\n\n\n\n", 
  "id": 47891734
}