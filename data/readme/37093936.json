{
  "read_at": 1462549254, 
  "README.org": "#+TITLE: BigOptim -- Large Scale Finite Sums Cost functions Optimization for R\n\n[[https://travis-ci.org/IshmaelBelghazi/bigpoptim][https://travis-ci.org/IshmaelBelghazi/bigoptim.svg]]\n[[https://coveralls.io/github/IshmaelBelghazi/bigoptim?branch=master][https://coveralls.io/repos/IshmaelBelghazi/bigoptim/badge.svg?branch=master&service=github]]\n* Description\nBigOptim is an R package that implements the Stochastic Average Gradient(SAG)[1] optimization method. For strongly convex problems, SAG achieves batch gradient descent convergence rates while keeping the iteration complexity of stochastic gradient descent. This allows for efficient training of machine learning algorithms with convex cost functions.\n* Setup\n#+BEGIN_SRC R\ninstall.packages(\"devtools\")\ndevtools::install_github(\"hadley/devtools\")  ## Optional\ndevtools::install_github(\"IshmaelBelghazi/bigoptim\")\n#+END_SRC\n\n* Example: Fit with Linesearch\n#+BEGIN_SRC R\n## Loading Data set\ndata(covtype.libsvm)\n## Normalizing Columns and adding intercept\nX <- cbind(rep(1, NROW(covtype.libsvm$X)), scale(covtype.libsvm$X))\ny <- covtype.libsvm$y\ny[y == 2] <- -1\n## Setting seed\n#set.seed(0)\n## Setting up problem\nmaxiter <- NROW(X) * 10  ## 10 passes throught the dataset\nlambda <- 1/NROW(X) \nsag_ls_fit <- sag_fit(X=X, y=y, lambda=lambda,\n                      maxiter=maxiter, \n                      tol=1e-04, \n                      family=\"binomial\", \n                      fit_alg=\"linesearch\",\n                      standardize=FALSE)\n## Getting weights\nweights <- coef(sag_ls_fit)\n## Getting cost\ncost <- get_cost(sag_ls_fit)\n#+END_SRC\n* Example: Demo -- Monitoring gradient norm\n#+BEGIN_SRC R\ndemo(\"monitoring_training\")\n#+END_SRC\n#+CAPTION: Gradient norm after each effective pass through the dataset\n#+NAME: gradien_monitoring\n[[misc/readme/grad_norm_covtype.png]]\n* Runtime comparison\nRan on intel i7 4710HQ 16G with intel MKL and compilers.\n#+BEGIN_SRC R\ndemo(\"run_times\")\n#+END_SRC R\n** Dense dataset: Logistic regression on covertype\n*Logistic Regression on Covertype -- 581012 sample points, 55 variables*\n|                                          | constant | linesearch | adaptive |   glmnet |\n|------------------------------------------+----------+------------+----------+----------|\n| Cost at optimum                          | 0.513603 |   0.513497 | 0.513676 | 0.513693 |\n| Gradient L2 norm at optimum              | 0.001361 |   0.001120 | 0.007713 | 0.001806 |\n| Approximate gradient L2 norm  at optimum | 0.001794 |   0.000146 | 0.000214 |       NA |\n| Time(seconds)                            |    1.930 |      2.392 |    8.057 |    8.749 |\n\n** Sparse dataset: Logistic regression on rcv1_train\n*Logistic Regression on RCV1_train -- 20242 sample points, 47237 variables* \n|                                         |     constant |   linesearch |     adaptive |       glmnet |\n|-----------------------------------------+--------------+--------------+--------------+--------------|\n| Cost at optimum                         |     0.046339 |     0.046339 |     0.046339 |     0.046342 |\n| Gradient L2 norm at optimum             | 3.892572e-07 | 4.858723e-07 | 6.668943e-10 | 7.592185e-06 |\n| Approximate gradient L2 norm at optimum | 3.318267e-07 | 4.800463e-07 | 2.647663e-10 |           NA |\n| Time(seconds)                           |        0.814 |        0.872 |        1.368 |        4.372 |\n\n* References\n\n[1] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing Finite Sums with the Stochastic Average Gradient. arXiv:1309.2388 [cs, math, stat], September 2013. arXiv: 1309.2388. [ [[http://ishmaelbelghazi.bitbucket.org/SAG_proposal/proposal_IshmaelB_bib.html#schmidt_minimizing_2013][bib]] | [[http://arxiv.org/abs/1309.2388][http]] ] \n\n  \n", 
  "description": "Large Scale Machine learning Optimization through Stochastic Average Gradient", 
  "id": 37093936
}