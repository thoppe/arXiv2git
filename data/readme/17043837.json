{
  "read_at": 1462543254, 
  "description": "My Home Page", 
  "README.md": "## About Me \n\nJeff Hammond, Computational Scientist, jeff_hammond@acm.org\n\nMy detailed CV is available as [PDF](https://github.com/jeffhammond/jeffhammond.github.io/raw/master/cv/master.pdf).\n\n[Edit this page (authentication required)](https://github.com/jeffhammond/jeffhammond.github.io/edit/master/README.md)\n\n### Education and Research Positions\n\n**Intel Labs**    \nParallel Computing Laboratory (May 2014 - present).    \nTitle: Research Scientist    \nSupervisor: Drs. Tim Mattson and Pradeep Dubey    \n\n**Argonne National Laboratory**    \nLeadership Computing Facility (June 2011 - May 2014).    \nTitle: Assistant Computational Scientist in the Performance Engineering group    \nSupervisors: Drs. Kalyan Kumaran and Ray Bair    \n\n**The University of Chicago**    \nComputation Institute (February 2011 - May 2014).    \nTitle: Fellow (since September 2011)    \n\n**Argonne National Laboratory**    \nLeadership Computing Facility (June 2009 - May 2011).    \nTitle: Argonne Scholar (Director's Postdoctoral Fellowship)    \nSupervisor: Dr. Ray Bair    \n\n**Pacific Northwest National Laboratory**    \nEMSL MSCF (June 2006 - May 2009).    \nTitle: Alternate Sponsored Fellow (DOE-CSGF practicum)    \nSupervisors: Drs. Karol Kowalski and Wibe A. de Jong    \n\n**University of Chicago**    \nDept. of Chemistry (September 2003 to May 2009).    \nPhD in Chemistry, May 2009; MS in Chemistry, August 2004.    \nSupervisors: Professors Karl F. Freed and L. Ridgway Scott    \n\n**University of Washington**    \nDept. of Chemistry (January 2001 to August 2003).    \nBS in Chemistry with Distinction; BA in Mathematics; Minor in Applied Mathematics.    \nSupervisor: Professor Weston T. Borden    \n\n### Online Profiles\n\n* [LinkedIn](https://www.linkedin.com/in/jeffhammond)\n* [Twitter](https://twitter.com/JeffDotScience)\n* [Frontiers in Theoretical and Computational Chemistry](http://community.frontiersin.org/people/JeffHammond/110723)\n* [DOE-CSGF](http://www.krellinst.org/doecsgf/alumni/info/fship.cgi?-w+hammond2005)\n* [Chemistry Tree](http://academictree.org/chemistry/tree.php?pid=67742)\n* [Wordpress Blog](http://aragingnerd.wordpress.com/) (empty right now)\n* [StackExchange](http://scicomp.stackexchange.com/users/150/jeff)\n* [StackOverflow](http://stackoverflow.com/users/2189128/jeff)\n* [Academia.edu](https://independent.academia.edu/JeffHammond)\n\n### Publications \n* [Google Scholar](http://scholar.google.com/citations?user=UtykSKIAAAAJ) (papers and citations listed here)\n* [DBLP](http://dblp.uni-trier.de/pers/hd/h/Hammond:Jeff_R=) ([see also](http://dblp.uni-trier.de/pers/hd/h/Hammond:Jeff))\n* [ORCID](http://orcid.org/0000-0003-3181-8190)\n* [ResearchId](http://www.researcherid.com/rid/G-8607-2013)\n* [ResearchGate](https://www.researchgate.net/profile/Jeff_Hammond/) (some papers listed here)\n* [Mendeley](https://www.mendeley.com/profiles/jeff-hammond/) (some papers listed here)\n* [Figshare](http://figshare.com/authors/Jeff%20Hammond/439849) (mostly empty right now)\n\n### Software\n\n* [Github](https://github.com/jeffhammond) (very useful)\n* [Bitbucket](https://bitbucket.org/jeffscience) (useful)\n* [Ohloh](https://www.ohloh.net/accounts/jeff_science) (very useful)\n* [Gitlab](https://gitlab.com/u/jeffhammond) (not useful)\n* [Sourceforge](https://sourceforge.net/users/jeffhammond) (not useful)\n* [OpenMP](http://openmp.org/twiki/bin/view/Main/JeffHammond) (not useful)\n\n### Random Facts\n\n* Google Scholar says I have an h-index of 18 (this is not necessarily exact value measured using only official publications but is closely correlated with the exact value).\n* My Erdos number is 4 (via Jim Demmel).\n* My [Erdos-Bacon number](http://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Bacon_number) is infinity because I have not acted in any movies.\n\n## Publications\n\nSpecific software packages that these papers have involved are denoted by **Software Name** in front of the citation.\n\nNote that my contribution to these packages varies from \"nearly everything\" to \"supervised student developer\" to \"literally nothing\".  In most cases, the relevant version control system will give you all the details, if you're interested.\n\n### Reviews and High-level Presentations\n\nVenkatram Vishwanath, Thomas Uram, Lisa Childers, Hal Finkel, Jeff Hammond,  Kalyan Kumaran, Paul Messina and Michael E. Papka.\n_DOE ASCR Workshop on Software Productivity for eXtreme-Scale Science (SWP4XS)_, Rockville, Maryland, January 13-14, 2014.\n[Toward improved scientific software productivity on leadership facilities: An Argonne Leadership Computing Facility View](http://www.orau.gov/swproductivity2014/papers/vishwanath_v.pdf)\n\nJeff R. Hammond.\n_ACM XRDS_ 19 (3), Spring 2013.\n[Challenges and methods in large-scale computational chemistry applications](http://doi.acm.org/10.1145/2425676.2425687) (invited and proof-read but not refereed in the traditional sense)\n\nBill Allcock, Anna Maria Bailey, Ray Bair, Charles Bacon, Ramesh Balakrishnan, Adam Bertsch, Barna Bihari, Brian Carnes, Dong Chen, George Chiu, Richard Coffey, Susan Coghlan, Paul Coteus, Kim Cupps, Erik W. Draeger, Thomas W. Fox, Larry Fried, Mark Gary, Jim Glosli, Thomas Gooding, John Gunnels, John Gyllenhaal, Jeff Hammond, Ruud Haring, Philip Heidelberger, Mark Hereld, Todd Inglett, K.H. Kim, Kalyan Kumaran, Steve Langer, Amith Mamidala, Rose McCallen, Paul Messina, Sam Miller, Art Mirin, Vitali Morozov, Fady Najjar, Mike Nelson, Albert Nichols, Martin Ohmacht, Michael E. Papka, Fabrizio Petrini, Terri Quinn, David Richards, Nichols A. Romero, Kyung Dong Ryu, Andy Schram, Rob Shearer, Tom Spelce, Becky Springmeyer, Fred Streitz, Bronis de Supinski, Pavlos Vranas, Bob Walkup, Amy Wang, Timothy Williams, and Robert Wisniewski.\n_Blue Gene/Q: Sequoia and Mira_ in\n[Contemporary High Performance Computing: From Petascale toward Exascale](http://www.taylorandfrancis.com/books/details/9781466568341/), edited by Jeffrey S. Vetter.\n\nJeff R. Hammond.\n_IEEE-TCSC Blog_, August 6th, 2012.\n[Challenges for Interoperability of Runtime Systems in Scientific Applications](https://www.ieeetcsc.org/activities/blog/challenges_for_interoperability_of_runtime_systems_in_scientific_applications) (invited and proof-read but not refereed in the traditional sense)\n\n### Matrix and Tensor Computations\n\n**TTC:**\nP Springer, JR Hammond, P Bientinesi.\narXiv:1603.02297 (2016).\n[TTC: A high-performance Compiler for Tensor Transpositions](http://arxiv.org/abs/1603.02297)\n([Source Code](https://github.com/HPAC/TTC))\n\n**CTF:**\nEdgar Solomonik, Devin Matthews, Jeff Hammond, John Stanton and James Demmel.\n_Journal of Parallel and Distributed Computing_ (2014).\n[A massively parallel tensor contraction framework for coupled-cluster computations](http://dx.doi.org/10.1016/j.jpdc.2014.06.002)\n([Preprint](http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-143.html))\n([Source Code](https://github.com/devinamatthews/aquarius/))\n\n**BLIS:**\nT. M. Smith, R. van de Geijn, M. Smelyanskiy, J. R. Hammond, and F. G. Van Zee. \n_Proceedings of the 28th IEEE International Parallel and Distributed Processing Symposium (IPDPS)._\nPhoenix, Arizona, May 2014.\n[Anatomy of High-Performance Many-Threaded Matrix Multiplication](http://dx.doi.org/10.1109/IPDPS.2014.110)\n([Preprint](http://www.cs.utexas.edu/users/flame/pubs/blis3_ipdps14.pdf)).\n_Also known as_\nFLAME Working Note #71. The University of Texas at Austin, Department of Computer Science. \nTechnical Report TR-13-20. 2013.\n[Opportunities for Parallelism in Matrix Multiplication](http://www.cs.utexas.edu/users/flame/pubs/FLAWN71.pdf)\n([Home Page](https://code.google.com/p/blis/))\n([GitHub Source](https://github.com/flame/blis))\n\nP. Ghosh, J. R. Hammond, S. Ghosh, and B. Chapman,\n_4th International Workshop on. Performance Modeling, \nBenchmarking and Simulation of High Performance Computer Systems_ (PMBS13).\nWorkshop at SC13, Denver, Colorado, USA, November 2013.\n[Performance analysis of the NWChem TCE for different communication patterns](http://link.springer.com/chapter/10.1007/978-3-319-10214-6_14)\n([Preprint](http://www.dcs.warwick.ac.uk/~sdh/pmbs13papers/pmbs1655.pdf))\n\n**TCE-IE:**\nD. Ozog, J. R. Hammond, J. Dinan, P. Balaji, S. Shende and A. Malony.\n_International Conference on Parallel Processing_ (ICPP).\nEcole Normale Superieure de Lyon, Lyon, France, October 1-4, 2013. \n[Inspector-Executor Load Balancing Algorithms for Block-Sparse Tensor Contractions](http://dx.doi.org/10.1109/ICPP.2013.12) \n([Preprint](http://www.mcs.anl.gov/publication/inspector-executor-load-balancing-algorithms-block-sparse-tensor-contractions)).\n([Related poster](http://nic.uoregon.edu/ics2013/ICS_posters/poster14.pdf) from [ICS](http://dx.doi.org/10.1145/2464996.2467282).)\n\n**CTF:**\nEdgar Solomonik, Devin Matthews, Jeff Hammond and James Demmel.\n_Proc. 27th Intl. Parallel and Distributed Processing Symp (IPDPS)._\nBoston, Massachusetts, May 2013.\n[Cyclops Tensor Framework: reducing communication and eliminating load imbalance in massively parallel contractions](http://dx.doi.org/10.1109/IPDPS.2013.112)\n([Preprint](http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-210.html))\n([Source Code](http://ctf.eecs.berkeley.edu/))\n\n**CTF:**\nEdgar Solomonik, Jeff Hammond and James Demmel.\nElectrical Engineering and Computer Sciences,\nUniversity of California at Berkeley,\nTechnical Report No. UCB/EECS-2012-29, March 9, 2012.\n[A preliminary analysis of Cyclops Tensor Framework](http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-29.html).\n\n**Elemental:**\nJ. Poulson, B. Marker, J. R. Hammond, N. A. Romero, and R. van de Geijn.\n_ACM Trans. Math. Software_, 39 (2012).\n[Elemental: A New Framework for Distributed Memory Dense Matrix Computations](http://dx.doi.org/10.1145/2427023.2427030). \n([Preprint](http://elemental.googlecode.com/files/Elemental-rev2.pdf))\n([Source Code](https://github.com/elemental/Elemental))\n\n### MPI, Global Arrays, ARMCI, OpenSHMEM and other PGAS\n\n**UPCFock:**\nD. Ozog, A. Kamil, Y. Zheng, P. Hargrove, J. R. Hammond, A. Malony, W. de Jong, and K. Yelick.\n_Proc. 30th Intl. Parallel and Distributed Processing Symp (IPDPS)._ \nChicago, Ilinois, May 2016.\n[A Hartree-Fock Application using UPC++ and the New DArray Library](http://www.ipdps.org/)\n\nKarthikeyan Vaidyanathan, Dhiraj D. Kalamkar, Kiran Pamnany, Jeff R. Hammond, Pavan Balaji, Dipankar Das, Jongsoo Park, Balint Joo.\n_The International Conference for High Performance Computing, Networking, Storage and Analytics (SC15)._\nAustin, TX, November 15-20, 2015.\n[Improving Concurrency and Asynchrony in Multithreaded MPI Applications Using Software Offloading](http://dx.doi.org/10.1145/2807591.2807602)\n\n**PRK:**\nR. F. Van der Wijngaart, A. Kayi, J. R. Hammond, G. Jost, T. St. John, S. Sridharan, T. G. Mattson, J. Abercrombie, and J. Nelson.\nTo appear at _International Supercomputing Conference (ISC)._\nJune 20-22, 2016. Frankfurt, Germany.\n[Comparing runtime systems with exascale ambitions using the Parallel Research Kernels](http://www.isc-hpc.com/research-papers.html)\n\n**PRK:**\nRob Van der Wijngaart, Srinivas Sridharan, Abdullah Kayi, Gabriele Jost, Jeff Hammond, Tim Mattson, and Jacob Nelson.\n_The 9th International Conference on Partitioned Global Address Space Programming Models (PGAS)._\nSeptember 17-18, 2015. Washington, D.C.\n[Using the Parallel Research Kernels to study PGAS models](http://dx.doi.org/10.1109/PGAS.2015.24)\n([Slides](http://hpcl.seas.gwu.edu/pgas15/slides/PRK_for_PGAS2015_public.pdf))\n([Source Code](https://github.com/ParRes/Kernels))\n\n**Casper:**\nMin Si, Antonio J. Pena, Jeff R. Hammond, Pavan Balaji, and Yutaka Ishikawa.\n_IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)._\nMay 4-7, 2015, Shenzhen, China.\n**Scalable Computing Challenge Finalist.**\n[Scaling NWChem with Efficient and Portable Asynchronous Communication in MPI RMA](http://dx.doi.org/10.1109/CCGrid.2015.48)\n([Preprint](http://www.mcs.anl.gov/papers/P5312-0315.pdf))\n\n**Casper:**\nMin Si, Antonio J. Pena, Jeff Hammond, Pavan Balaji, Masamichi Takagi, Yutaka Ishikawa.\n_Proc. 29th Intl. Parallel and Distributed Processing Symp (IPDPS)._ \nHyderabad, India, May 2015.\n[Casper: An Asynchronous Progress Model for MPI RMA on Many-Core Architectures](http://dx.doi.org/10.1109/IPDPS.2015.35).\n([Preprint](http://www.mcs.anl.gov/publication/casper-asynchronous-progress-model-mpi-rma-many-core-architectures))\n([Source Code](http://git.mpich.org/soft/dev/casper.git/))\n\nJeff Hammond.\nOpenSHMEM User Group (OUG2014), October 7, 2014, Eugene, OR.\n[Towards a matrix-oriented strided interface in OpenSHMEM](http://nic.uoregon.edu/pgas14/oug_submissions/oug2014_submission_4.pdf).\n([Source Code](https://github.com/jeffhammond/oshmpi/tree/armci-strided/tests/extensions/aput))\n\n**BigMPI:** \nJeff R. Hammond, Andreas Schaefer, and Rob Latham.\nWorkshop on Exascale MPI at Supercomputing Conference 2014 (ExaMPI14), New Orleans, LA, November 17, 2014.\n[To INT_MAX... and beyond! Exploring large-count support in MPI](http://dx.doi.org/10.1109/ExaMPI.2014.5)\n([Preprint 1](https://github.com/jeffhammond/BigMPI-paper/raw/master/exampi14_resubmission_3.pdf))\n([Preprint 2](http://www.mcs.anl.gov/publication/intmax-and-beyond-exploring-large-count-support-mpi))\n([Source Code](https://github.com/jeffhammond/BigMPI))\n\nDavid Ozog, Allen Malony, Jeff Hammond and Pavan Balaji.\n_20th IEEE International Conference on Parallel and Distributed Systems (ICPADS)._\nHsinchu, Taiwan, December 16 - 19, 2014.\n[WorkQ: A Many-Core Producer/Consumer Execution Model Applied to PGAS Computations](http://dx.doi.org/10.1109/PADSW.2014.7097863) \n([Preprint 1](http://ix.cs.uoregon.edu/~ozog/pubs/ICPADS-2014.pdf))\n([Preprint 2](http://www.mcs.anl.gov/~balaji/pubs/2014/icpads/icpads14.workq.pdf)).\n\n**OSHMPI:**\nJ. R. Hammond, S. Ghosh, and B. M. Chapman,\naccepted to _First OpenSHMEM Workshop: Experiences, Implementations and Tools._\n[Implementing OpenSHMEM using MPI-3 one-sided communication](http://www.csm.ornl.gov/workshops/openshmem2013/documents/ImplementingOpenSHMEM%20UsingMPI-3.pdf)\n([Online](http://dx.doi.org/10.1007/978-3-319-05215-1_4))\n([Preprint](https://github.com/jeffhammond/oshmpi/blob/master/docs/iwosh-paper.pdf?raw=true))\n([Source Code](https://github.com/jeffhammond/oshmpi))\n\nV. Morozov, J. Meng, V. Vishwanath, J. R. Hammond, K. Kumaran and M. Papka. \n_Parallel Processing Workshops (ICPPW), 41st International Conference,_ September 2012, Pittsburgh, Pennsylvania\n[ALCF MPI Benchmarks: Understanding Machine-Specific Communication Behavior](http://dx.doi.org/10.1109/ICPPW.2012.7)\n([IEEE link](http://doi.ieeecomputersociety.org/10.1109/ICPPW.2012.7)) ([Slides](http://www.mcs.anl.gov/events/workshops/p2s2/2012/slides/Morozov-P2S2-MPI_benchmark.pdf))\n\n**OSPRI:**\nJ. R. Hammond, J. Dinan, P. Balaji, I. Kabadshow, S. Potluri, and V. Tipparaju, \n_The 6th Conference on Partitioned Global Address Space Programming Models (PGAS)._ Santa Barbara, CA, October 2012.\n[OSPRI: An Optimized One-Sided Communication Runtime for Leadership-Class Machines](https://sites.google.com/a/lbl.gov/pgas12/home/contributed-papers) \n([Preprint](http://www.mcs.anl.gov/publication/ospri-optimized-one-sided-communication-runtime-leadership-class-machines)).\n\n**ARMCI-MPI:**\nJ. Dinan, P. Balaji, J. R. Hammond, S. Krishnamoorthy, and V. Tipparaju,\n_Proc. 26th Intl. Parallel and Distributed Processing Symp (IPDPS)._ Shanghai, China, May 2012.\n[Supporting the Global Arrays PGAS Model Using MPI One-Sided Communication](http://dx.doi.org/10.1109/IPDPS.2012.72) \n([Preprint](http://www.mcs.anl.gov/publication/supporting-global-arrays-pgas-model-using-mpi-one-sided-communication))\n([Source Code](http://wiki.mpich.org/armci-mpi/index.php/Main_Page))\n\nJ. Dinan, S. Krishnamoorthy, P. Balaji, J. R. Hammond, M. Krishnan, V. Tipparaju and A. Vishnu,\nin _Recent Advances in the Message Passing Interface_ (Lecture Notes in Computer Science, Volume 6960/2011, pp. 282-291), edited by Y. Cotronis, A. Danalis, D. S. Nikolopoulos and J. Dongarra.\n[Noncollective Communicator Creation in MPI](http://www.springerlink.com/content/t1p532u8l1273x2m/)\n([Preprint](http://www.mcs.anl.gov/publication/noncollective-communicator-creation-mpi)).\n\n**TAU-ARMCI:**\nJ. R. Hammond, S. Krishnamoorthy, S. Shende, N. A. Romero and A. D. Malony, _Concurrency and Computation: Practice and Experience_ (DOI: 10.1002/cpe.1881).\n[Performance Characterization of Global Address Space Applications: A Case Study with NWChem](http://onlinelibrary.wiley.com/doi/10.1002/cpe.1881/abstract) \n([Preprint](http://www.cs.uoregon.edu/Research/paraducks/publ/htbin/bibify.cgi?cmd=show&coll=JOUR&id=CaC11&data_present=no))\n\n### Software for Multi-level Memories\n\n**MEMKIND:**\nChristopher Cantalupo, Vishwanath Venkatesan, Jeff R. Hammond, and Simon Hammond.\nSubmitted.\n[User Extensible Heap Manager for Heterogeneous Memory Platforms and Mixed Memory Policies]() ([Preprint](http://memkind.github.io/memkind/memkind_arch_20150318.pdf))\n([Source](https://github.com/memkind/memkind))\n\n### Performance Engineering and Application Scalability\n\n**GTFOCK:**\nEdmond Chow, Xing Liu, Sanchit Misra, Marat Dukhan, Mikhail Smelyanskiy, Jeff R. Hammond, Yunfei Du, Xiang-Ke Liao and Pradeep Dubey.\nInternational Journal of High Performance Computing Applications, accepted.\n[Scaling up Hartree-Fock Calculations on Tianhe-2](http://dx.doi.org/10.1177/1094342015592960)\n([Preprint](http://www.cc.gatech.edu/~echow/pubs/ijhpca-2015.pdf))\n\n**GTFOCK:**\nEdmond Chow, Xing Liu, Mikhail Smelyanskiy, and Jeff R. Hammond.\n_J. Chem. Phys._ *142*, 104103 (2015).\n[Parallel scalability of Hartree-Fock calculations](http://dx.doi.org/10.1063/1.4913961)\n([Preprint](http://www.cc.gatech.edu/~echow/pubs/hf-jcp-2015.pdf))\n([Source 1](https://code.google.com/p/gtfock/))\n([Source 2](https://github.com/hpcgarage/OptErd))\n\n**MADNESS:**\nRobert J. Harrison, Gregory Beylkin, Florian A. Bischoff, Justus A. Calvin, George I. Fann, Jacob Fosso-Tande, Diego Galindo, Jeff R. Hammond, Rebecca Hartman-Baker, Judith C. Hill, Jun Jia, Jakob S. Kottmann, M-J. Yvonne Ou, Laura E. Ratcliff, Matthew G. Reuter, Adam C. Richie-Halford, Nichols A. Romero, Hideo Sekino, William A. Shelton, Bryan E. Sundahl, W. Scott Thornton, Edward F. Valeev, Alvaro Vazquez-Mayagoitia, Nicholas Vence, Yukina Yokoi.\n[MADNESS: A Multiresolution, Adaptive Numerical Environment for Scientific Simulation](http://arxiv.org/abs/1507.01888)\n\n**MADNESS:**\nAlvaro Vazquez-Mayagoitia, W. Scott Thornton, Jeff R. Hammond, Robert J. Harrison.\n_Annual Reports in Computational Chemistry_ *10*, pp. 3-24 (2014).\n[Quantum Chemistry Methods with Multiwavelet Bases on Massive Parallel Computers](https://dx.doi.org/10.1016/B978-0-444-63378-1.00001-X)\n\n**Harvey:**\nAmanda Peters Randles, Vivek Kale, Jeff Hammond, William D. Gropp and Efthimios Kaxiras.\n_Proc. 27th Intl. Parallel and Distributed Processing Symp (IPDPS)._ Boston, Massachusetts, May 2013.\n[Performance Analysis of the Lattice Boltzmann Model Beyond Navier-Stokes](http://dx.doi.org/10.1109/IPDPS.2013.109) ([Preprint](http://people.seas.harvard.edu/~apeters/papers/ipdps_lbm.pdf))\n\n### Resilience\n\n**GVR:**\nA. Chien, P. Balaji, P. Beckman, N. Dun, A. Fang, H. Fujita, K. Iskra, Z. Rubenstein, Z. Zheng, R. Schreiber, J. Hammond, J. Dinan, I. Laguna, D. Richards, A. Dubey, B. van Straalen, M. Hoemmen, M. Heroux, K. Teranishi, A. R. Siegel.\n_Submitted.  2015._\n[Versioned Distributed Arrays for Resilience in Scientific Applications: Global View Resilience]() ([Preprint](http://www.mcs.anl.gov/papers/P5271-0115.pdf))\n\nSean Hogan, Jeff R. Hammond and Andrew A. Chien.\n_Fault-Tolerance at Extreme Scale (FTXS)._ Boston, MA.  June, 2012. \n[An Evaluation of Difference and Threshold Techniques for Efficient Checkpoints](http://dx.doi.org/10.1109/DSNW.2012.6264674). ([Preprint](http://www.cs.uchicago.edu/research/publications/techreports/TR-2012-07)) ([Slides](http://institute.lanl.gov/resilience/workshops/ftxs2012/FTXS2012_Sean_Hogan.pdf))\n\n### Statistical sampling and molecular dynamics\n\n**LAMMPS:**\nRolf Isele-Holder, Wayne Mitchell, Jeff Hammond, Axel Kohlmeyer and Ahmed Ismail, _J. Chem. Theory Comput._ **9** (12), 5412-5420 (2013).\n[Reconsidering Dispersion Potentials: Reduced Cutoffs in Mesh-Based Ewald Solvers Can Be Faster Than Truncation](http://pubs.acs.org/doi/abs/10.1021/ct4004614)\n\n**LAMMPS-Ensembles:**\nLuke Westby, Mladen Rasic, Adrian Lange and Jeff R. Hammond.  See [LAMMPS-Ensembles](https://github.com/jeffhammond/HPCInfo/wiki/LAMMPS#Ensembles) on my Wiki for more information.\n\n**NEUS:**\nA. Dickson, M. Maienshein-Cline, A. Tovo-Dwyer, J. R. Hammond and A. R. Dinner, _J. Chem. Theory Comput._ **7**, 2710 (2011).\n[Flow-dependent unfolding and refolding of an RNA by nonequilibrium umbrella sampling](http://pubs.acs.org/doi/abs/10.1021/ct200371n). ([Preprint](http://arxiv.org/abs/1104.5180))\n\n### Quantum chemistry on accelerators\n\n#### GPUs\n\n_Eugene has incorporated all of the GPU coupled-cluster codes into [PSI4](http://www.psicode.org/).  See [Github](https://github.com/edeprince3/gpu_dfcc) for details._\n\nA. E. DePrince III, J. R. Hammond, and C. D. Sherrill,\n_Iterative Coupled-Cluster Methods on Graphics Processing Units_ in\nin [Electronic Structure Calculations on Graphics Processing Units: From Quantum Chemistry to Condensed Matter Physics](http://www.wiley.com/WileyCDA/WileyTitle/productCd-1118661788.html), \nedited by Ross Walker and Andreas Goetz (Wiley, 2016).\n\nA. E. DePrince III, J. R. Hammond and S. K. Gray, \n[Proceedings of SciDAC 2011](http://press.mcs.anl.gov/scidac2011/), Denver, CO, July 10-14, 2011.\n[Many-body quantum chemistry on graphics processing units](http://www.mcs.anl.gov/uploads/cels/papers/scidac11/final/deprince_eugene.pdf).\n\nA. E. DePrince III and J. R. Hammond, _Symposium on Application Accelerators in High-Performance Computing (SAAHPC)_ Knoxville, TN, USA, 19-21 July 2011.\n[Quantum chemical many-body theory on heterogeneous nodes](http://dx.doi.org/10.1109/SAAHPC.2011.28). ([Slides](http://saahpc.ncsa.illinois.edu/presentations/deprince.pdf))\n\nA. E. DePrince III and J. R. Hammond\n_J. Chem. Theory Comput._ **7**, 1287 (2011)\n[Coupled Cluster Theory on Graphics Processing Units I. The Coupled Cluster Doubles Method](http://pubs.acs.org/doi/abs/10.1021/ct100584w).\n\nA. E. DePrince III and J. R. Hammond, _Symposium on Application Accelerators in High-Performance Computing (SAAHPC)_, Knoxville, TN, USA, 13-15 July 2011.\n[Evaluating one-sided programming models for GPU cluster computations](http://saahpc.ncsa.illinois.edu/10/papers/paper_43.pdf).\n\n#### Intel Xeon Phi (aka MIC)\n\n**NWChem-MIC:** \nEdoardo Apra, Karol Kowalski, Jeff R. Hammond, and Michael Klemm.\n_NWChem: Quantum Chemistry Simulations at Scale_ in\n[High Performance Parallelism Pearls](http://store.elsevier.com/High-Performance-Parallelism-Pearls/James-Reinders/isbn-9780128021187/), edited by James Reinders and James Jeffers (Morgan Kaufmann, 3 Nov. 2014).\n([Safari Books Online](https://www.safaribooksonline.com/library/view/high-performance-parallelism/9780128021187/B9780128021187000170.xhtml)) \n([Google Books Online](https://books.google.com/books?id=R-99BAAAQBAJ&lpg=PA287&ots=VMAjQdy8xU&dq=%22NWChem%3A%20Quantum%20Chemistry%20Simulations%20at%20Scale%22&pg=PA287#v=onepage&q=%22NWChem:%20Quantum%20Chemistry%20Simulations%20at%20Scale%22&f=false)) ([Summary at TechEnablement](http://www.techenablement.com/nwchem-quantum-chemistry-simulations-scale/))\n\n**NWChem-MIC:**\nJeff Hammond, Priyanka Ghosh, David Ozog, Cyrus Karshenas, and Karol Kowalski.  _work in progress_ (I gave a talk on the preliminary results at SIAM CSE13.)\n\n### Coupled-cluster response theory and NWChem\n\n[NWChem 101](https://github.com/jeffhammond/NWChem101) - incomplete version of what I hope will be a crash course in how to use NWChem like an expert.  Obviously, this is not a refereed publication.\n\n**NWChem:**\n[Coupled-cluster response theory: parallel algorithms and novel applications](http://dx.doi.org/10.6084/m9.figshare.967859)\n(my dissertation).\n\n**NWChem:**\nH. Hu, Y.-F. Zhao, J. Hammond, E. Bylaska, E. Apra, H.J.J. van Dam, J. Li, N. Govind, and K. Kowalski,\n_Chem. Phys. Lett._ (2015).\n[Theoretical studies of the global minima and polarizabilities of small lithium clusters](http://dx.doi.org/10.1016/j.cplett.2015.11.049)\n\n**NWChem:**\nK. Kowalski, J. R. Hammond, W. A. de Jong, P.-D. Fan, M. Valiev, D. Wang and N. Govind, in _Computational Methods for Large Systems: Electronic Structure Approaches for Biotechnology and Nanotechnology_, edited by J. R. Reimers (Wiley, March 2011, Hoboken). [Coupled-Cluster Calculations for Large Molecular and Extended Systems](http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470487887.html)\n\n**NWChem:**\nK. Kowalski, S. Krishnamoorthy, O. Villa, J. R. Hammond, and N. Govind, _J. Chem. Phys._ **132**, 154103 (2010).\n[Active-space completely-renormalized equation-of-motion coupled-cluster formalism: Excited-state studies of green fluorescent protein, free-base porphyrin, and oligoporphyrin dimer](http://dx.doi.org/10.1063/1.3385315)\n\n**NWChem:**\nJ. R. Hammond, N. Govind, K. Kowalski, J. Autschbach and S. S. Xantheas, _J. Chem. Phys._ **131**, 214103 (2009).\n[Accurate dipole polarizabilities for water clusters N=2-12 at the coupled-cluster level of theory and benchmarking of various density functionals](http://dx.doi.org/10.1063/1.3263604)\n\n**NWChem:**\nJ. R. Hammond and K. Kowalski, _J. Chem. Phys._ **130**, 194108 (2008).\n[Parallel computation of coupled-cluster hyperpolarizabilities](http://dx.doi.org/10.1063/1.3134744)\n\n**NWChem:**\nK. Kowalski, J. R. Hammond, W. A. de Jong and A. J. Sadlej, _J. Chem. Phys._ **129**, 226101 (2008).\n[Coupled cluster calculations for static and dynamic polarizabilities of C60](http://dx.doi.org/10.1063/1.3028541)\n\n**NWChem:**\nJ. R. Hammond, W. A. de Jong and K. Kowalski, _J. Chem. Phys._ **128**, 224102 (2008).\n[Coupled cluster dynamic polarizabilities including triple excitations](http://dx.doi.org/10.1063/1.2929840)\n\n**NWChem:**\nK. Kowalski, J. R. Hammond and W. A. de Jong, _J. Chem. Phys._ **127**, 164105 (2007).\n[Linear response coupled cluster singles and doubles approach with modified spectral resolution of the similarity transformed Hamiltonian](http://dx.doi.org/10.1063/1.2795708)\n\n**NWChem:**\nJ. R. Hammond, K. Kowalski and W. A. de Jong, _J. Chem. Phys._ **127**, 144105 (2007).\n[Dynamic polarizabilities of polyaromatic hydrocarbons using coupled-cluster linear response theory](http://dx.doi.org/10.1063/1.2772853)\n\n**NWChem:**\nJ. R. Hammond, M. Valiev, W. A. de Jong and K. Kowalski, _J. Phys.  Chem. A_ **111**, 5492 (2007).\n[Calculations of properties using a hybrid coupled-cluster and molecular mechanics approach](http://pubs.acs.org/cgi-bin/abstract.cgi/jpcafh/2007/111/i25/abs/jp070553x.html)\n\n### Chemistry Applications\n\nSameer Varma, Mohsen Botlani, Jeff R. Hammond, H. Larry Scott, Joseph P.R.O. Orgel, Jay D. Schieber,\n_Proteins: Structure, Function, and Bioinformatics_ (2015).\n[Effect of Intrinsic and Extrinsic Factors on the Simulated D-band Length of Type I Collagen](http://dx.doi.org/10.1002/prot.24864)\n\nR. S. Assary, P. C. Redfern, J. R. Hammond, J. Greeley and L. A. Curtiss, _Chem. Phys. Lett._, **497 (1-3)**, 123 (2010).  [Predicted Thermochemistry for Chemical Conversion of 5-Hydroxymethyl Furfural](http://dx.doi.org/10.1016/j.cplett.2010.07.082)\n\nR. S. Assary, P. C. Redfern, J. R. Hammond, J. Greeley and L. A. Curtiss, _J. Phys. Chem. B_, **114**, 9002 (2010).  [Computational Studies of the Thermochemistry for Conversion of Glucose to Levulinic Acid](http://pubs.acs.org/doi/abs/10.1021/jp101418f)\n\nR. K. Chaudhuri, J. R. Hammond, K. F. Freed, S. Chattopadhyay and U. S.  Mahapatra, _J. Chem. Phys._ **129**, 064101 (2008).\n[Reappraisal of cis effect in 1,2-dihaloethenes: An improved virtual orbital multireference approach](http://link.aip.org/link/?JCPSA6/129/064101/1)\n\nM. Lingwood, J. R. Hammond, D. A. Hrovat, J. M. Mayer, and W. T. Borden, _J. Chem. Theory Comp._ **2**, 740 (2006).\n[MPW1K, rather than B3LYP, should be used as the functional for DFT calculations on reactions that proceed by proton-coupled electron transfer (PCET)](http://pubs.acs.org/cgi-bin/abstract.cgi/jctcce/2006/2/i03/abs/ct050282z.html)\n\n### RDM Theory\n\nJ. R. Hammond and D. A. Mazziotti, _Bulletin of the American Physical Society_ **52 (1)** (March 2007). \n[Variational reduced-rensity-matrix theory applied to the hubbard model](http://meetings.aps.org/Meeting/MAR07/Event/60611). (This was first reported results on the 2D Hubbard model, which been the subject of ongoing interest (e.g. by http://prl.aps.org/abstract/PRL/v108/i21/e213001, http://prl.aps.org/abstract/PRL/v108/i20/e200404, and http://arxiv.org/abs/1207.4847).)\n\nJ. R. Hammond and D. A. Mazziotti, _Phys. Rev. A_ **73**, 062505 (2006). \n[Variational reduced-density-matrix calculation of the one-dimensional Hubbard model](http://link.aps.org/abstract/PRA/v73/e062505).\n\nJ. R. Hammond and D. A. Mazziotti, _Phys. Rev. A_ **73**, 012509 (2006). \n[Variational reduced-density-matrix calculations on small radicals: a new approach to open-shell ab initio quantum chemistry](http://link.aps.org/abstract/PRA/v73/e012509).\n\nJ. R. Hammond and D. A. Mazziotti, _Phys. Rev. A_ **71**, 062503 (2005). \n[Variational two-electron reduced-density-matrix theory: Partial 3-positivity conditions for N-representability](http://link.aps.org/abstract/PRA/v71/e062503).\n\n## Projects\n\n### Active\n\n* Standards activities: Programming parallel computers should be vendor-agnostic.  [MPI Forum](http://www.mpi-forum.org/docs/docs.html), [OpenMP](http://openmp.org/), and [OpenSHMEM](http://openshmem.org/) are of primary interest right now.\n\n* [Parallel Research Kernels](https://github.com/ParRes/Kernels) - Rob van der Wijngaart and Tim Mattson developed a set of parallel kernels, which are currently being used for programming models research.\n\n* [NWChem](http://www.nwchem-sw.org/) - I developed the coupled-cluster response property capability, among other features, during graduate school.  Static partitioning (load-balancing), threading, vectorization and accelerator integrations for NWChem are currently under investigation.\n\n* [BigMPI](https://github.com/jeffhammond/BigMPI) - MPI for large messages.  See link for details.\n\n* [ARMCI-MPI](http://wiki.mpich.org/armci-mpi/index.php/Main_Page) - Jim Dinan developed a portable, high-performance implementation of ARMCI using MPI-2 RMA.  I wrote the MPI-3 implementation and maintain the project.\n\n* [OSHMPI](https://github.com/jeffhammond/oshmpi) -  OpenSHMEM over MPI-3.  See link for details.\n\n* [MADNESS](https://github.com/m-a-d-n-e-s-s/madness) - I work on portability and new platform support in the low-level runtime.\n \n* [DALEC](https://github.com/jeffhammond/dalec) - A new distributed array library using MPI-3.  This is totally a work-in-progress and not even the slightest bit ready for third-party inspection right now.\n\n### Archives\n\n* [NSF SI2-SSI: A Linear Algebra Software Infrastructure for Sustained Innovation in Computational Chemistry and other Sciences](http://www.nsf.gov/awardsearch/showAward?AWD_ID=1148485&HistoricalAwards=false) (as part of the [FLAME team](http://www.cs.utexas.edu/~flame/web/team.html)).\n\n* [DOE X-Stack: X-Tune](http://ctop.cs.utah.edu/x-tune/) (follow link for details)\n\n* [DOE X-Stack: GVR](https://xstackwiki.modelado.org/GVR) (follow link for details)\n\n* [CORAL](https://asc.llnl.gov/CORAL-benchmarks) and Blue Gene/Q were my primary projects at ALCF.\n\n* I was involved in both ASCR Leadership Computing Challenge (ALCC) and (INCITE) projects in computer science and chemistry.\n\n* [OSPRI](https://github.com/jeffhammond/ospri) (follow link for details).  This project was rendered unnecessary by MPI-3 RMA, although there are a number of interesting low-level performance tests in this repo.\n\n* [Global Arrays](http://www.emsl.pnl.gov/docs/global/ ) - I contributed features and tried to maintain the Blue Gene/P port until converting wholly to the use of [ARMCI-MPI](http://wiki.mpich.org/armci-mpi/index.php/Main_Page).  Global Arrays is developed at [Pacific Northwest National Laboratory](http://www.pnl.gov/).\n\n* [BG/Q ESP](https://www.alcf.anl.gov/projects/accurate-numerical-simulations-chemical-phenomena-involved-energy-production-and-storage) - Robert, Curt, Ed and I have a Blue Gene/Q Early Science Program project entitled ''Accurate Numerical Simulations of Chemical Phenomena Involved in Energy Production and Storage with MADNESS and MPQC.''\n\n* [Unistack](https://wiki.mcs.anl.gov/unistack/) - Unified runtime systems for parallel programming models.\n\n* [MPQC](https://github.com/ValeevGroup/mpqc) - I helped port and optimize MPQC for Blue Gene/P, with kind support from [Curt Janssen](http://www.linkedin.com/pub/curtis-janssen/7/454/440) and [Ed Valeev](http://www.files.chem.vt.edu/chem-dept/valeev/), who are the lead developers of this code.\n\n* [A1](https://github.com/jeffhammond/a1) was the predecessor to OSPRI.  The hardware it targeted (Blue Gene/P) no longer exists.\n\n* [NEUS](https://code.google.com/p/parallelsampling/) - A massively-parallel implementation of non-equilibrium umbrella sampling.  I contribute the computer science portion in a collaboration with the [Dinner group](http://dinner-group.uchicago.edu/).  This project is funded with an [UC-ANL Strategic Collaborative Initiative grant](http://www.uchicago.edu/research/news/20102010_argonne.shtml).\n\n* GPU-CC - Eugene DePrince developed a coupled-cluster code for GPUs with help from me.  This code is now part of [PSI4](http://www.psicode.org/).  This project belongs to Eugene now although I'm still making use of lessons learned from it.\n\n* [TAU](http://www.cs.uoregon.edu/research/tau)-[ARMCI](http://www.emsl.pnl.gov/docs/parsoft/armci/) - I contributed to the development of TAU profiling capability for the ARMCI communication library.  This was a joint project with [Sriram Krisnamoorthy](http://hpc.pnl.gov/people/sriram/) and the [Sameer Shende] (http://ix.cs.uoregon.edu/~sameer/) that is now complete.\n\n* CECC - Chemistry Exascale Codesign Center.  If this project had been funded, I would never have left DOE.\n", 
  "id": 17043837
}