{
  "read_at": 1462558006, 
  "description": "quick'n'dirty notes on stuff I stumbled upon which didn't make it into my blog (yet)", 
  "README.md": "# https://sivers.org/book/FluentForever\nfirst 625: http://fluent-forever.com/wp-content/uploads/2014/05/625-List-Thematic.pdf\n* start off with \"minimal pair testing\" (e.g. having to differentiate sounds like niece and knees)\n* use Google images, not translations\n* to memorize genders(/tones/...) imagine the masculine terms exploding, feminine catching fire, neuter shattering like glass (similar for tone colors)\n* good at remembering when images are violent/sexual/funny\n* cloze card types for functional words like \"of\", \"what\", ...\n* for 10 ways to form plural, pick 10 nouns and use the person-action-object system (Tiger essen Fleisch)\n* also use this to learn verb/noun/adjective/adverb patterns\n* submit sentences to lang8 and put corrections in flash cards\n* TV series are easier than films (read ahead on Wikipedia, no subs)\n* card types should also ask for mnemonic (e.g. \"What's a phrase that includes [word]?\")\n\n# http://arxiv.org/pdf/1511.06444.pdf\nuniversality in halting time for spin glasses and deep neural networks\n\n# http://arxiv.org/pdf/1301.3537.pdf\nsuper short abstract:\n* pooling operators are for local invariance\n* 1 layer of convolutions is for learning 1 one-parameter-group invariance (or rather: convolutions leave already learned group invariance untouched and the training learns the invariance)\n* deep networks are basically doing group factorisation that's stable w.r.t. perturbations of the group\n\nmy notes:\n\n* the problem: find signal representation $\\Phi$ (e.g. classifier) that is\n\t* invariant under some transformation group $G$ (e.g. rotation), i.e. $\\Phi(x)=\\Phi(gx)$ for all $g\\in G$ and\n\t* is \"stable\" to perturbations (i.e. transformations $h$ that are \"close\" to the transformation group $G$ which can be thought of as a low dimensional manifold), i.e. $||\\Phi(\\varphi(h,x))-\\Phi(x)||\\leq C||x||d(h,G)$\n* approach: take convolutional networks and think of them in terms of signal processing, i.e.\n  * inputs $x\\in X$ = signals $x\\in L^2(\\Omega)$\n  * convolution kernels = filter bank $\\{\\psi_\\lambda\\}_\\lambda, \\lambda\\in\\Lambda_1$, e.g. for a filter bank corresponding to an expansion in a funtions Fourier series, convolution with $\\psi_\\lambda$ yields $\\langle x,e^{2\\pi i\\lambda/N}$\n  * the first layer maps $x(\\cdot)\\mapsto \\{x*\\psi_\\lambda(\\cdot)\\}_\\lambda =: z^{(1)}(\\cdot,\\lambda)$ (which is the set of **convolutions** with kernels $\\psi_\\lambda$, applies some operator $M$ (the **activation function** which acts piece-wise) and then applies some **pooling operator** $P$, which, in terms of signal processing, can be thought of as low-pass filter (low-pass because chosing max-pooling can be thought of as eliminating high frequency oscillations), followed by downsampling (see Wikipedia for a quick explanation)\n* the special case of one-parameter transformation groups $G=\\{U_t\\}_{t\\in\\mathbb R}, U_t\\in L^2(\\Omega), \\lim U_t z = U_{t_0}z, U_{t+s}=U_tU_s$ (e.g. translations, frequency transpositions, dilations):\n  * we want to find a canonical way of describing the action of a group element of a one-parameter family; this is given by Stone's theorem which (under reasonable assumptions) states that there is some s.a. $A$ such that $U_t=\\exp(itA)$ (spectral theorem!)\n  * since $A$ is self adjoint there is a change of basis given by $O$ that diagonalizes $A$, i.e. (in the finite dimensional case) $OAO^{-1} = \\text{diag}(\\lambda_1,\\dots,\\lambda_n)$\n  * hence we can write the group action $U_tz = O^{-1}(\\exp(it\\text{diag}(\\lambda_1,\\dots,\\lambda_n))Oz)$, implying that the group action is a linear phase change in the basis that diagonalizes $A$\n  * choosing $M$ to take the complex modulus yields a representation that is invariant under the action of $\\{U_t\\}_t$\n* \"defining\" perturbations of group elements:\n  * using the change of basis given by $O$ we can write the group action as some linear phase change; if we also apply the inverse Fourier transform this becomes a translation operator $T_s:z(\\cdot)\\mapsto z(\\cdot -s)$, which lets us measure deformations ($\\tilde T_s: z(\\cdot)\\mapsto z(\\cdot -\\tau(s))$ such that $d(\\tilde T_s,T_s)\\ll 1$) in a convenient way by analyzing the regularity of $\\tau$\n<!--  * thus the key to obtaining group invariant representations is not to find die eigenvectors of $A$ (yielding the change of basis given by $O$), but rather measurements that are \"close\" to diagonalising $A$ and are localised where deformations occur(?) - this can be done with convolutions with compactly supported filters-->\nI do not quite understand how this motivates the use of filters(/kernels) acting locally for group factorisation, but as soon as I do this will be updated.\n\n# Properties of the Fourier transform\n* diagonalises the derivative (i.e. turns it into a multiplication operator if expressed in its basis), which in turn let's us define pseudo-differential operators via non-polynomial symbols\n* Fourier transform is like a projection onto eigenspaces of Laplace operator - in a compact domain (with suitable regularity) those are discrete - get a sum = Fourier _series_\n\t* using the above idea one can generalize Fourier transforms not only to higher dimensional setting and manifolds, but also to graph settings (see graph Laplacian)\n* translation = phase change (think of a shift in 1D and what this does to the coefficients of a Fourier _series_)\n  * in a space-time metric (having signature 1,1,1,-1) we can \"place\" a particle at some point in space-time by multiplying the creation operator with some exponential in the right basis (i.e. the one of the Fourier transform) where the spatial part has the inverse sign of the time part because of this metric\n* smoothness of the original function corresponds to decay in the Fourier domain (and vice versa), which can be made precise by the following statement about a random variable's characteristic function (which is pretty much it's Fourier transform)\n  * $u({x: |x|>=2/u})<=1/u \\int_{-u}^u (1-\\hat u(t))dt$\n  * in particular gives the following implication (in the appropriate setting): \"characteristic function continuous in 0\" - \"measure is tight\" (where tightness is useful since it enables application of Helly's selection theorem, which is more or less the Banach-Alaoglu theorem for probability measures (latter would give weak-* convergence for signed, unnormed measures)\n* https://terrytao.wordpress.com/2009/04/06/the-fourier-transform/ for a more general take on the subject\n\n\n# Braess' paradox (http://ist.ac.at/fileadmin/user_upload/pdfs/Talks/2016/02/Talk_Timme.pdf)\n* the paradox: given some (directed) graph with a flow (e.g. traffic network) removing edges could improve the \"overall situation\" (e.g. on average you do not drive as long as before if a street is closed)\n* note that this also implies the converse: adding a street doesn't necessarily make the traffic situation better overall\n* one take on this paradox (from a dynamical systems point of view): knowing the capacity of all the edges and the nodes one gets a system of constraints for every closed loop in order to satisfy some stability condition, but if one introduces another edge that divides a circle in two we get two systems of constraints that may not be compatible - no more stable solution (= traffic jam)\n\n# http://www.huffingtonpost.com/2015/05/13/andrew-ng_n_7267682.html\n* \"The idea is that innovation is not these random unpredictable acts of genius, but that instead one can be very systematic in creating things that have never been created before. [...] learn a lot, read a lot, talk to experts.\"\n* on early influences: moved around, visited many different colleges and interned at different labs - many different points of view\n* don't \"follow your passion\" (which usually gets amended to \"follow your passion of all the things that happen to be a major at the university you're attending\"); often \"you first become good at something, and then you become passionate about it. And I think most people can become good at almost anything. So when I think about what to do with my own life, what I want to work on, I look at *two criteria*. The first is whether it's an *opportunity to learn*. Does the work on this project allow me to learn new and interesting and useful things? The second is the *potential impact*. **The world has an infinite supply of interesting problems. The world also has an infinite supply of important problems. I would love for people to focus on the latter.**\"\n* \"one pattern of mistakes I've made in the past, hopefully much less now, is doing projects where you do step one, you do step two, you do step three, and then you realize that step four has been impossible all along\"\n* \"\"if you seriously study half a dozen papers a week and you do that for two years, after those two years you will have learned a lot. [...] But that sort of investment, if you spend a whole Saturday studying rather than watching TV, there's no one there to pat you on the back or tell you you did a good job. Chances are what you learned studying all Saturday won't make you that much better at your job the following Monday. There are very few, almost no short-term rewards for these things. But it's a fantastic long-term investment. [...] People that count on willpower to do these things, it almost never works because willpower peters out. Instead I think people that are into creating habits -- you know, studying every week, working hard every week -- those are the most important. Those are the people most likely to succeed.\"\n* \"I don't work on preventing AI from turning evil for the same reason that I don't work on combating overpopulation on the planet Mars\", more urgent: \"[...] when the U.S. transformed from an agricultural to a manufacturing and services economy, we had people move from one routine task, such as farming, to a different routine task, such as manufacturing or working call service centers. A large fraction of the population has made that transition, so they've been okay, they've found other jobs. But many of their jobs are still routine and repetitive. The challenge that faces us is to find a way to scalably teach people to do non-routine non-repetitive work. Our education system, historically, has not been good at doing that at scale. The top universities are good at doing that for a relatively modest fraction of the population. But a lot of our population ends up doing work that is important but also routine and repetitive. That's a challenge that faces our educational system.\"\n* \"One thing about speech recognition: most people don't understand the difference between 95 and 99 percent accurate. Ninety-five percent means you get one-in-20 words wrong. That's just annoying, it's painful to go back and correct it on your cell phone. Ninety-nine percent is game changing. If there's 99 percent, it becomes reliable.\"\n\n# http://www.paulgraham.com/hs.html\ngreat text, you should read it!\n* if you don't have a clear goal (which is usually the case) work forward from promising situations instead\n* if you have to choose, take those options that will give you the most promising range of options afterward\n* work on hard problems (\"Writing novels is hard. Reading novels isn't. Hard means worry: if you're not worrying that something you're making will come out badly, or that you won't be able to understand something you're studying, then it isn't hard enough.\") - also get to know interesting people and get big ideas\n* \"Put in time how and on what? Just pick a project that seems interesting: to master some chunk of material, or to make something, or to answer some question. Choose a project that will take less than a month, and make it something you have the means to finish. Do something hard enough to stretch you, but only just, especially at first. If you're deciding between two projects, choose whichever seems most fun. If one blows up in your face, start another. Repeat till, like an internal combustion engine, the process becomes self-sustaining, and each project generates the next one.\"\n* \"Don't disregard unseemly motivations. One of the most powerful is the desire to be better than other people at something.\"\n\n# https://terrytao.wordpress.com/2008/08/07/on-time-management/\n* \"Another thing is that my ability to do any serious mathematics fluctuates greatly from day to day; sometimes I can think hard on a problem for an hour, other times I feel ready to type up the full details of a sketch that I or my coauthors already wrote, and other times I only feel qualified to respond to email and do errands, or just to take a walk or even a nap. I find it very helpful to organise my time to match this fluctuation: for instance, if I have a free afternoon, and feel inspired to do so, I might close my office door, shut off the internet, and begin typing on a languishing paper; or if not, I go and work on a week's worth of email, referee a paper, write a blog article, or whatever else seems suited to my current levels of energy and enthusiasm. It is fortunate in mathematics that a large fraction of one's work (with the notable exception of teaching, which one then has to build one's schedule around) can be flexibly moved from one time slot to another in this manner. [A corollary to this is that one should deal with tasks before they become so urgent that they have to be done immediately, thus disrupting one's time flexibility.]\"\n* \"A half-hearted system is probably worse than no system at all. A corollary to this is not to try to make an overly ambitious system ab nihilo that one is unlikely to follow faithfully; it is probably better to let such systems evolve over time.\"\n* \"Sometimes one should abandon one's own rules and allow for serendipity. There have been many times, for instance, when I had planned to work on something during my lunch hour (grabbing something quick to eat), when I was interrupted by a colleague or visitor to go out to eat. It has often happened that I got a lot more out of that lunch (mathematically or otherwise) than I would have back at the office, though not in the way I would have anticipated.\"\n\n<!--\n========================================================\n=====================YET TO WRITE=======================\n========================================================\n# http://arxiv.org/pdf/1405.4537v1.pdf\n* def: stream is a map $\\gamma$ from a totally ordered set $I$ to some state space\n* there is a canonical way to convert discrete streams to continuous paths (path: $I$ interval and some regularity conditions like right continuity)\n* from now on \"wlog\": $\\gamma:[J_-,J_+]\\rightarrow E$ will continuously map an interval $J$ to some Banach space $E$\n* def: bounded $p$-variation ($p\\geq 1$) iff $sup_{\\dots<u_i < u_{i+1}<\\dots\\in J}\\sum_i\\|\\gamma_{u_{i+1}}-\\gamma_{u_i}\\|^q <\\infty$ for $q=1$ and $q=p$\n========================================================\n===================TO BE CONTINUED======================\n========================================================\n-->\n\n# http://math.ucr.edu/home/baez/rosetta.pdf\ngreat paper for everybody who is interested in (yet not familiar with) category theory - summary may follow\n\n# uses of De  (Zhong Wen )\n* possessive particle: Wo De Nu  = my daughter\n* attributive (connecting adjective and noun): Hong Se De Cai  = red vegetables \n\n# http://arxiv.org/pdf/cond-mat/0611023v1.pdf\n* distribution of eigenvalues of the Hessian of a critical point is a shifted semicircle\n* e.g. global minimum - left of SCL is at 0, the bigger its energy, the more it is shifted to the left)\n\n# asking yes/no questions (Zhong Wen )\n* there are two ways of doing so, either with\n  * Ma : e.g. Chi Bao Liao Ma ? (have you eaten?), or\n  * XBu X: Ni Yao Bu Yao Qu Bei Jing ? (Do you or don't you want to go to Beijing?) which is equivalent to Ni Yao Qu Bei Jing Ma ?\n\n# past tense: Liao  vs Guo  (Zhong Wen )\n* Guo : applicable if action is repeatable and is finished\n* Liao : applicable for events which started in the past and continue to the present\n* e.g. Ta Qu Guo Mei Guo .vs Ta Qu Mei Guo Liao . (Guo -she is not there anymore, Liao -she is still there)\n\n# expressing prior/posterior actions with Yi Qian /Yi Hou  (Zhong Wen )\n* as \"previously\"/\"after\":\n  * Ta Yi Qian Zhu Zai Mei Guo . (He (previously) lived in America.)\n  * Ta Yi Hou Hui Qu Mei Guo . or Yi Hou Ta Hui Qu Mei Guo . (He will go to America (afterwards).) -- note that Yi Hou  can be before or after the subject!\n* relatively prior to/after something else:\n  * [action]Yi Qian [prior action], e.g. Ta Shui Jue Yi Qian Xi Huan Kan Shu .(He likes to read before going to bed. (literally - He sleep before, likes read.))\n  * [action]Yi Hou [later action], e.g. Ta Xia Ke Yi Hou Yao Hui Jia Chi Fan .(After class, he will return home to eat. (literally - He class is over after, going to return home eat a meal.))\n\n# http://math.stackexchange.com/questions/766479/what-is-spectrum-for-laplacian-in-mathbbrn, http://math.stackexchange.com/questions/790401/spectrum-of-laplace-operator\ntwo great articles covering why the spectrum of the Laplace operator (on $\\mathbb R^N$) is $(-[?],0]$ which, by looking more closely, also explains why \"unbounded domain\" implies \"uncountable basis of eigenfunctions\"\n\n# http://cs231n.github.io/convolutional-networks/\ngreat introduction to convolutional neural networks\n\n# http://stackoverflow.com/questions/7536465/create-a-2d-array-with-a-nested-loop\nwhy you want to use `[[None for j in xrange(3)] for i in xrange(3)]` instead of `[[None]*3]*3`\n\n# DFT\n* DFT in $N$ dimensions can be written as $F_N=1/\\sqrt N (\\omega_N^{kl})_{k,l=0}^{N-1}$, where $\\omega_N=e^{2\\pi i/N}$ and this matrix is a Vandermonde matrix over the roots of unity\n* multidimensional DFT consists of iterated sums which commute - can write it as $F_{N_2}(F_{N_1}(X_{i,j})_{j=0}^{N_1-1})_{j=0}^{N_2-1}$\n* Vandermonde matrix = evaluation of a polynomial at points generating the Vandermonde matrix - upsampling = (e^2pinm/2N)_{n,m=0}^{n=N-1,m=2N-1} (Vandermonde matrix with twice as many rows as a square one - evaluates the polynomial described by the given vector at twice as many points of the unit circle (- to convert a signal of frequency a to one with frequency b you first need to upsample it to LCM(a,b))\n* real signal x - DFT(x) has entries that are complex conjugated to each other (usually mirrored around half the length of the signal) since they are just the $L^2(\\mathbb C)$ inner product $(\\langle x,e_k\\rangle)_k$\n\n# linearisation of a differential operator\nhttp://math.stackexchange.com/questions/1677181/how-do-you-linearize-a-differential-operator-to-get-its-symbol for an example\n* used (i.a.) to define the symbol of a differential operator $D$\n* how to do it:\n  * usually linearize around a solution $\\tilde u, D\\tilde u=0$\n  * idea is to look at how $D$ behaves on functions $u$ that are close to the point one is linearising around (i.e. $\\tilde u$), so one does the substitution $u=\\tilde u+\\varepsilon v$\n  * rearranging terms like a power series in $\\varepsilon$ and using that $D\\tilde u=0$ one gets equations with $\\tilde u$ and $v$ which define an operator $A_{\\tilde u}$ acting on $v$, which satisfies $A_{\\tilde u}v = 0$ and is linear!\n* note that the linearisation depends on the solution just as the linearisation of a scalar valued function depends on the point $x_0$ one wants to linearize around and that the origin shifts accordingly, i.e. the input $v$ is much like $x-x_0$ and not $x$\n\n<!-- \n========================================================\n=====================YET TO WRITE=======================\n========================================================\n# http://www-etud.iro.umontreal.ca/~sordonia/pdf/sigir2013_sordoni.pdf\n* problem: model dependencies of words\n  * either: need additional features (\"computer\", \"architecture\" and \"computer architecture\" are completely different entries)\n  * or: model it as joint probabilities (less improvements than expected, huge computational effort)\n======================================================== \n=======================WRITE IT=========================\n========================================================\n-->\n\n# http://arxiv.org/pdf/1411.1792v1.pdf\nfirst few layers of CNNs with similar purposes tend to be very similar (Gabor filters) - can copy them\n\n# http://arxiv.org/pdf/1301.3583v4.pdf\nincreasing the size of neural networks only yields diminishing returns\n\n# http://arxiv.org/pdf/1312.5851v5.pdf\nhow to implement the convolutions of CNNs with the FFT (feasible for large number of feature maps)\n\n# green, white, black and Oolong tea\nhttp://cooking.stackexchange.com/questions/26002/what-is-the-difference-between-green-white-and-black-tea for quick introduction (also Oolong tea)\n* fermentation: green&white tea leaves are quickly heated (typically steamed in Japan, while pan-fired in China) after harvesting to reduce oxidation\n* white tea\n  * minimally processed to leave downy hair intact\n  * not oxidized - does not develop as much flavor, color, or caffeine\n* green tea\n  * history\n    * starting from 17th century (Europeans started importing tea) until mid 19th century green tea was about as popular as black tea (in Europe)\n    * originally (2000 bc) used as medicine in China\n    * in the 8th century Lu Yu  (Lu Yu) wrote Cha Jing  (chajing), the first known monograph on tea where he described tea being steamed and formed into tea bricks for storage and trade - to prepare tea one had to pulverize it before brewing - Mo Cha  (matcha)\n    * only later it was established\n  * rolling wilted leaves breaking cell walls to speed release of aromatic substances\n* types of tea\n  * China\n    * Zhu Cha  (gunpowder): rolled into little round pellets (green or Oolong tea); shiny pellets indicate freshness, little pellets are considered a mark of higher quality tea\n    * Long Jing Cha  (Long Jing tea): renowned for its high quality\n    * Huang Shan Mao Feng  ((Huangshan) Mao Feng): mild-flavored, very popular tea\n    * Mo Li Hua Cha  (Jasmine tea): subtly sweet and highly fragrant, stored with blossoms to acquire their scent\n    * Bai Mu Dan  (Bai mudan): one of the most well-known white teas\n  * Japan\n    * Jian Cha  (Sencha): most popular tea in Japan (80degC, 1 min, 1.5 tablespoons (7-8 grams) per litre)\n    * Jing Cha  (Kukicha): blend made of stems, stalks, and twigs (80degC, 40sec to 1min, 4 teaspoons per litre)\n    * Mo Cha  (Matcha): mentioned above\n    * Yu Lu  (Gyokuro): grown under the shade rather than the full sun since the more sun, the more Catechin (bitter)\n* aromatized tea: additional flavors, but basically loses its ability of being brewed more than once\n\n# showing emphasis with Shi ...De  (Zhong Wen )\n* encapsulate the part to be stressed with Shi ...De , e.g. Wo (Shi )Lai Tai Bei Zhao Peng You De . (I came *to Taipei* to visit a friend.)\n* in positive sentences Shi  may be omitted\n* only for events in the past\n\n# $\\{x : |x|_i<\\varepsilon, i\\in I\\}$ does not have to be an open set\n* take product topology (cylinder sets $\\{x : |x|_i<\\varepsilon, i\\in J\\}$ for $J$ being a finite subset of $I$ are a neighbourhood basis of $0$)\n* if $I$ is not finite, no element in the neighbourhood basis is contained in the set in the heading (if the norms $|\\cdot|_i$ do not trivially depend on each other) - it is not an open set\n* examples: locally convex spaces in PDEs\n\n# marking direct objects with Ba  (Zhong Wen )\n* used like [subject]Ba \\[object\\][transitive verb]\n* e.g. Ta Ba Wo De Bei Zi Da Po Liao . (He broke my cup.)\n\n# how to get compact sets in non Euclidean settings\n* (one) idea: given some compact inclusion $i$ of space $X$ into $Y$, find some bounded set $B\\subset X$ and consider $i(B)$\n* this is useful for locally convex spaces of all sorts and works well with machinery like the Rellich-Kondrachov theorem\n\n# exp of involutions\n* for any involution $a:a^2=I$, where $I$ is the identity, one has $exp(ca)=cosh(c)+a sinh(c)$ for scalar $c$, which can be seen by looking at the power series\n* can also be used for sin/cos with exp=i*sin+cos\n* can also be written as $cosh(c)(1+a tanh(c))$ which can be seen as \"low temperature expansion\" (think about $c$ as some temperature close to zero or more mathematically as some small $\\vareps>0$)\n\n# geometric interpretation of the (conditional) expectation\n* introduce conditional expectation $\\mathbb E[X|\\mathcal G]$ as follows:\n  * let $X$ be an integrable random variable on some probability space $(\\Omega,\\mathcal A, P)$ \n  * let $\\mathcal G\\subset\\mathcal A$ be a sub-$\\sigma$-algebra\n  * conditional expectation of $X$ given $\\mathcal G$, $\\mathbb E[X|\\mathcal G]$ is *any* random variable $Y$ s.t.\n    * $Y$ is $\\mathcal G$-measurable\n    * $\\forall A\\in\\mathcal G: \\mathbb E[X\\bf 1_A]=\\mathbb E[Y\\bf 1_A]$\n* it can be seen that such a $Y$ exists and, up to $P$-null equivalence, is uniquely determined by those conditions\n* moreover we have all the ``typical'' properties of expectations like\n  * positivity: $X\\geq 0$ implies $\\mathbb E[X|\\mathcal G]\\geq 0$ $P$-a.s.\n  * linearity\n  * monotonicity\n  * Jensen's inequality: $\\phi(\\mathbb E[X|\\mathcal G])\\leq\\mathbb E[\\phi(X)|\\mathcal G]$, $P$-a.s. for $\\phi$ convex and $X$ s.t. $\\mathbb E|X|<\\infty$, $\\mathbb E|\\phi(X)|<\\infty$\n  * importantly: $X$ being $\\mathcal G$-measurable, ($\\mathbb E|X|,\\mathbb E|Y|<\\infty$) implies $\\mathbb E[XY|\\mathcal G]=X\\mathbb E[Y|\\mathcal G]$, $P$-a.s., which, after identifying the deterministic ``random'' variable $X(\\omega)=1 \\forall\\omega$ with $1\\in\\mathbb R$, gives the second axiom for an expectation in terms of free probability, i.e. $\\mathbb E[1|\\mathcal G]=1$ (the first one being linearity)\n* one interpretation of this conditional expectation is a geometric one which can be made precise under suitable technical assumptions ($X\\in L^2(\\Omega,\\mathcal A,P)$): $\\mathbb E[X\\mathcal G]$ is the orthogonal projection of $X$ from $L^2(\\Omega,\\mathcal A,P)$ on $L^2(\\Omega,\\mathcal G,P)$\n* to put this in words: start with some random variable (/function of degrees of freedom determined by $\\mathcal A$) $X$, then find the function $\\tilde X$ in the space of functions with fewer degrees of freedom (i.e. those given by $\\mathcal G$ which has to be a subset of $\\mathcal A$) that is ``closest'' to the original one\n* to illustrate what is meant by ``degrees of freedom'' it is instructive to look at the example $\\mathcal G=\\{\\emptyset,\\Omega\\}$; to be $\\mathcal G$ measurable now means that the pre-image of every open set in the domain of $X$ has to be an element of $\\mathcal G$, which, for this $\\mathcal G$, implies that there are only constant functions - the second property in the definition now gives that this constant value has to be the classical expectation\n* conversely, if we take a bigger $\\sigma$-algebra, we give ourselves more degrees of freedom for $\\tilde X$ and can, in a sense stay closer to the original $X$, which, from a probabilistic point of view, just means that more randomness is retained - so if you think of some filtration, encoding e.g. the ``knowledge'' after flipping a coin for the $n$-th time, and the random variables $S_n=X_1+\\dots+X_n$ being the sum of all outcomes up to the $n$-th flip, conditioning on the first element of the filtration (i.e. the $\\sigma$-algebra $\\mathcal G_1=\\sigma(\\{(0,\\{0,1\\},\\{0,1\\},\\dots),(1,\\{0,1\\},\\{0,1\\},\\dots)\\})$) just means that $X_1$ is measurable, so $\\mathbb E[X_1+X_2+\\dots+X_n|\\mathcal G_1]=\\mathbb E[X_1|\\mathcal G_1]+\\mathbb E[X_2+\\dots+X_n|\\mathcal G_1]$ by linearity and $E[X_1|\\mathcal G_1]$ simplifies to $X_1$, whereas $\\mathbb E[X_2+\\dots+X_n|\\mathcal G_1]$ simply becomes the usual expectation $1/2$\n\n# when are Chernoff bounds sharp?\n* Chernoff-type bounds are something of the form $P(X\\geq a)\\leq\\mathbb E[e^{tX}]/e^{ta}\\forall t>0$\n* this can easily be seen by writing $P(X\\geq a)\\leq P(e^{tX}\\geq e^{ta})$ and applying Markov's inequality\n* a geometric interpretation:\n  * write the probability of some event $A$ as expectation of its indicator function $\\bf 1_A$\n  * bounding the probability of $X\\geq a$ with $e^{tX}\\geq e^{ta}$ should be thought of as some exponential function touching the outer left part of the indicator function - which is, a priori, obviously not a bound that one expects to be sharp\n* in the proof of Cramer's theorem (LDP), however, we only look at random variables that are ``exponentially'' concentrated around their mean (that's why we need large deviations after all), so the intuition is that the Chernoff bound is sharp since the whole mass is concentrated just around the point where the exponential function touches the indicator function\n\n", 
  "id": 51511541
}