{
  "read_at": 1462548353, 
  "description": "scripts and configuration files for Edinburgh neural MT submission to WMT 16 shared translation task", 
  "README.md": "Scripts for Edinburgh Neural MT systems for WMT 16\n==================================================\n\nThis repository contains scripts and an example config used for the Edinburgh Neural MT submission (UEDIN-NMT)\nfor the shared translation task at the 2016 Workshops on Statistical Machine Translation (http://www.statmt.org/wmt16/).\n\nThe scripts will facilitate the reproduction of our results, and serve as additional documentation (along with the system description paper)\n\n\nOVERVIEW\n--------\n\n- We built translation models with Nematus ( https://www.github.com/rsennrich/nematus )\n- We used BPE as subword segmentation to achieve open-vocabulary translation ( https://github.com/rsennrich/subword-nmt )\n- We automatically back-translated in-domain monolingual data into the source language to create additional training data. The data is publicly available here: http://statmt.org/rsennrich/wmt16_backtranslations/\n- More details about our system will appear in the (upcoming) system description paper\n\nSCRIPTS\n-------\n\n- preprocessing : preprocessing scripts for Romanian that we found helpful for translation quality.\n                  we used the Moses tokenizer and truecaser for all language pairs.\n\n- sample : sample scripts that we used for preprocessing, training and decoding. We used mostly the same settings for all translation directions,\n           with small differences in vocabulary size. Dropout was enabled for EN<->RO, but disabled otherwise.\n\n\n- r2l : scripts for reranking the output of the (default) left-to-right decoder with a model that decodes from right-to-left.\n\n\nLICENSE\n-------\n\nThe scripts are available under the MIT License.\n\nPUBLICATIONS\n------------\n\nThe Edinburgh Neural MT submission to WMT 2016 is described in:\n\n TBD\n\nIt is based on work described in the following publications:\n\nDzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio (2015):\n    Neural Machine Translation by Jointly Learning to Align and Translate, Proceedings of the International Conference on Learning Representations (ICLR).\n\nRico Sennrich, Barry Haddow, Alexandra Birch (2015):\n    Neural Machine Translation of Rare Words with Subword Units. arXiv preprint.\n\nRico Sennrich, Barry Haddow, Alexandra Birch (2015):\n    Improving Neural Machine Translation Models with Monolingual Data. arXiv preprint.", 
  "id": 57060514
}