{
  "read_at": 1462548921, 
  "description": "Research code for using generative neural networks in Estimation of Distribution Algorithms (EDAs)", 
  "README.md": "README of eda-suite on github\n\n# Overview\n1. What is EDA suite\n2. How can I use it\n3. Publications and configurations\n4. Sources and further reading\n\n# Contents\n##1. What is EDA suite##\nEDA suite is a tool I use for my research at the University of Mainz, Germany.\nIt is meant as a tool to test different probabilistic models in the \ncontext of estimation of distribution algorithms (EDA) (see [1]).\n\nIt comprises\n* a main EDA file (eda.m) with the basic loop\n* a set of benchmark problems (fitnessXXX.m)\n* a number of probabilistic models, currently:\n * a Restricted Boltzmann Machine (RBM) [2]\n * the Bayesian Optimization Algorithm (BOA) [3]\n * Population-based incremental learning  (PBIL) [4]\n * Denoising Autoencoders (DAE) [5]\n * Deep Boltzmann Machine (DBM) [6]  * with code form R. Salakhutdinov\n * Generative Adversarial Network (GAN) [7]\n * Factored Higher-Order RBM [8]\n* some basic selection functions\n \n##2. How can I use it?##\n * I used GNU/Octave for all experiments, mostly v3.2.4 and 3.8.1\n   (you might need to install some extra packages, especially for BOA) \n * Start Octave, go to the ./src/ directory\n * adjust the EDA parameters (including the choice of problem)\n    in the file set_eda_params.m\n * adjust the selection parameters in the file set_selection_params.m\n * choose a model which you want to use, and change the appropriate \n   .m-file, e.g. set_rbm_params.m\n * load all settings into the ocatave-workspace, and start the EDA,\n   as follows:\n\n```\nset_eda_params;\nset_selection_params;\nset_ae_params;  %in case you want to use the denoising autoencoder\n[mean_fitness, best_fitness, found_in_iteration,cputime_until_best]=eda(eda_params,selection_params,model_params)\n```\n##3. Publications and configurations\nThe following publications include experiments performed with this code (see individual comments).\n\nThe corresponding configuration files can be found in the directory '.configs-papers/'\n\nTitle|Comment|Config subfolder|URL\n--- | --- | --- | --- \nDenoising Autoencoders for Fast Combinatorial Black Box Optimization (GECCO'15 + submission on arXiv)|slightly different autoencoder code was used|`gecco15-dae-eda-fast/`|[ACM](http://dl.acm.org/citation.cfm?doid=2739482.2764691) and [arXiv](http://arxiv.org/abs/1503.01954)\nDeep Boltzmann Machines in Estimation of Distribution Algorithms for Combinatorial Optimization||`dbm-eda/`|[arXiv](http://arxiv.org/abs/1509.06535)\nGenerative Adversarial Networks in Estimation of Distribution Algorithms for Combinatorial Optimizatoin||`gan-eda/`|[arXiv](http://arxiv.org/abs/1509.09235)\n\n##4. Sources and further reading\n[1] P. Larranaga and J.A. Lozano. Estimation of Distribution Algorithms: A New Tool for Evolutionary Computation, Kluwer Academic Pub, 2002\n\n[2] Hinton, G. E. A Practical Guide to Training Restricted Boltzmann Machines, 2006 Techical Report \n\n[3] Pelikan, M., Hierarchical Bayesian Optimization Algorithm, Springer 2005\n\n[4] Baluja, S. Population-Based Incremental Learning: A Method for Integrating Genetic Search Based Function Optimization and Competitive Learning Carnegie Mellon University, 1994\n\n[5] Vincent, P.; Larochelle, H.; Bengio, Y. & Manzagol, P.-A. Extracting and Composing Robust Features with Denoising Autoencoders Proceedings of the 25th international conference on Machine learning, 2008, 1096-1103\n\n[6] Salakhutdinov, R. & Hinton, G. E. A Better Way to Pretrain Deep Boltzmann Machines Advances in Neural Information Processing Systems, 2012, 2447-2455\n\n[7] Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua; Generative Adversarial Nets; In: Advances in Neural Information Processing Systems (NIPS 2014), 2672-2680\n\n[8] Memisevic, Roland, and Geoffrey E. Hinton. \"Learning to represent spatial transformations with factored higher-order Boltzmann machines.\" Neural Computation 22.6 (2010): 1473-1492.\n", 
  "id": 42593180
}