{
  "README": "darch\n\nThe darch package is built on the basis of the code from G. E. Hinton and R. R. Salakhutdinov\n(available under Matlab Code for deep belief nets (http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html\n\"Matlab for science paper\", last visit: 23.06.2015).\n\nThis package is for generating neural networks with many layers (deep architectures) and train them with the method introduced by the publications \"A fast learning algorithm for deep belief nets\" (G. E. Hinton, S. Osindero, Y. W. Teh) and \"Reducing the dimensionality of data with neural networks\" (G. E. Hinton, R. R. Salakhutdinov). This method includes a pre training with the contrastive divergence method published by G.E Hinton (2002) and a fine tuning with common known training algorithms like backpropagation or conjugate gradient, as well as more recent techniques like dropout and maxout.\n\nVisit https://github.com/maddin79/darch for releases, information, bug reports\netc.\n\nCopyright (C) 2013-2016 Martin Drees and contributors\n\nReferences:\n\nHinton, G. E., S. Osindero, Y. W. Teh, A fast learning algorithm for deep belief nets,\nNeural Computation 18(7), S. 1527-1554, DOI:\n[10.1162/neco.2006.18.7.1527](http://dx.doi.org/10.1162/neco.2006.18.7.1527), 2006.\n\nHinton, G. E., R. R. Salakhutdinov, Reducing the dimensionality of data with neural\nnetworks, Science 313(5786), S. 504-507, DOI:\n[10.1126/science.1127647](http://dx.doi.org/10.1126/science.1127647), 2006.\n\nHinton, G. E., Training products of experts by minimizing contrastive divergence,\nNeural Computation 14(8), S. 1711-1800, DOI:\n[10.1162/089976602760128018](http://dx.doi.org/10.1162/089976602760128018), 2002.\n\nHinton, Geoffrey E. et al. (2012). \"Improving neural networks by preventing coadaptation of feature detectors\". In: Clinical Orthopaedics and Related Research abs/1207.0580. URL : [arxiv.org](http://arxiv.org/abs/1207.0580).\n\nGoodfellow, Ian J. et al. (2013). \"Maxout Networks\". In: Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1319-1327. URL : [jmlr.org](http://jmlr.org/proceedings/papers/v28/goodfellow13.html).\n\nDrees, Martin (2013). \"Implementierung und Analyse von tiefen Architekturen\nin R\". German. Master's thesis. Fachhochschule Dortmund.\n\nRueckert, Johannes (2015). \"Extending the Darch library for deep\narchitectures\". Project thesis. Fachhochschule Dortmund.\nURL: [saviola.de](http://static.saviola.de/publications/rueckert_2015.pdf).", 
  "read_at": 1462556924, 
  "description": "Create deep architectures in the R programming language", 
  "README.md": "[![Downloads from the RStudio CRAN mirror](http://cranlogs.r-pkg.org/badges/darch)](http://cran.rstudio.com/package=darch)\n\n\ndarch\n=====\n\n#### Create deep architectures in the R programming language\n\n\n### Installation\n\nThe latest stable version of `darch` (0.10.0) can be installed from CRAN using\n\n```R\ninstall.packages(\"darch\")\n```\n\nWhen using [devtools](https://github.com/hadley/devtools/), the latest git version (identifiable by a version number ending in something greater than or equal to 9000 and by the fact that it is regularly broken) can be installed using\n\n```R\ninstall_github(\"maddin79/darch\")\n```\n\nor, if you want the latest stable version,\n\n```R\ninstall_github(\"maddin79/darch@v0.10.0\")\n```\n\n\nThen, use `?darch` to view its documentation or `example(\"darch\")` to load some examples (these will not directly be executed, but provide `example.*` functions).\n\n### About\n\nThe darch package is built on the basis of the code from G. E. Hinton and R. R. Salakhutdinov\n(available under [Matlab Code for deep belief nets](http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html\n\"Matlab for science paper\") : last visit: 12.11.2015).\n\nThis package is for generating neural networks with many layers (deep architectures) and train them with the method introduced by the publications \"A fast learning algorithm for deep belief nets\" (G. E. Hinton, S. Osindero, Y. W. Teh) and \"Reducing the dimensionality of data with neural networks\" (G. E. Hinton, R. R. Salakhutdinov). This method includes a pre training with the contrastive divergence method published by G.E Hinton (2002) and a fine tuning with common known training algorithms like backpropagation or conjugate gradient, as well as more recent techniques like dropout and maxout.\n\nCopyright (C) 2013-2016 Martin Drees and contributors\n\n#### References\nHinton, G. E., S. Osindero, Y. W. Teh, A fast learning algorithm for deep belief nets,\nNeural Computation 18(7), S. 1527-1554, DOI:\n[10.1162/neco.2006.18.7.1527](http://dx.doi.org/10.1162/neco.2006.18.7.1527), 2006.\n\nHinton, G. E., R. R. Salakhutdinov, Reducing the dimensionality of data with neural\nnetworks, Science 313(5786), S. 504-507, DOI:\n[10.1126/science.1127647](http://dx.doi.org/10.1126/science.1127647), 2006.\n\nHinton, G. E., Training products of experts by minimizing contrastive divergence,\nNeural Computation 14(8), S. 1711-1800, DOI:\n[10.1162/089976602760128018](http://dx.doi.org/10.1162/089976602760128018), 2002.\n\nHinton, Geoffrey E. et al. (2012). \"Improving neural networks by preventing coadaptation of feature detectors\". In: Clinical Orthopaedics and Related Research abs/1207.0580. URL : [arxiv.org](http://arxiv.org/abs/1207.0580).\n\nGoodfellow, Ian J. et al. (2013). \"Maxout Networks\". In: Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1319-1327. URL : [jmlr.org](http://jmlr.org/proceedings/papers/v28/goodfellow13.html).\n\nDrees, Martin (2013). \"Implementierung und Analyse von tiefen Architekturen\nin R\". German. Master's thesis. Fachhochschule Dortmund.\n\nRueckert, Johannes (2015). \"Extending the Darch library for deep\narchitectures\". Project thesis. Fachhochschule Dortmund.\nURL: [saviola.de](http://static.saviola.de/publications/rueckert_2015.pdf).\n", 
  "id": 11685918
}