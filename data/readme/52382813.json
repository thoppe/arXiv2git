{
  "read_at": 1462557521, 
  "description": "Extract Text and Data from PDF Documents", 
  "README.md": "# pdftools\n\n[![Build Status](https://travis-ci.org/ropensci/pdftools.svg?branch=master)](https://travis-ci.org/ropensci/pdftools)\n[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/ropensci/pdftools?branch=master&svg=true)](https://ci.appveyor.com/project/jeroenooms/pdftools)\n[![Coverage Status](https://codecov.io/github/ropensci/pdftools/coverage.svg?branch=master)](https://codecov.io/github/ropensci/pdftools?branch=master)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/pdftools)](http://cran.r-project.org/package=pdftools)\n[![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/pdftools)](http://cran.r-project.org/web/packages/pdftools/index.html)\n[![Github Stars](https://img.shields.io/github/stars/ropensci/pdftools.svg?style=social&label=Github)](https://github.com/ropensci/pdftools)\n\n## Introduction\n\nScientific articles are typically locked away in PDF format, a format designed primarily for printing but not so great for searching or indexing. The new pdftools package allows for extracting text and metadata from pdf files in R. From the extracted plain-text one could find articles discussing a particular drug or species name, without having to rely on publishers providing metadata, or pay-walled search engines.\n\nThe pdftools slightly overlaps with the Rpoppler package by Kurt Hornik. The main motivation behind developing pdftools was that Rpoppler depends on glib, which does not work well on Mac and Windows. The pdftools package uses the poppler c++ interface together with Rcpp, which results in a lighter and more portable implementation.\n\n\n## Installation\n\nOn Windows and Mac the binary packages can be installed directly from CRAN:\n\n```r\ninstall.packages(\"pdftools\")\n```\n\nInstallation on Linux requires the poppler development library. On Debian/Ubuntu:\n\n```\nsudo apt-get install libpoppler-cpp-dev\n```\n\nOn Fedora or CentOS:\n\n```\nsudo yum install poppler-cpp-devel\n```\n\nIf you want to install the package from source on Mac OS-X you need brew:\n\n```\nbrew install poppler\n```\n\n## Getting started\n\nThe `?pdftools` manual page shows a brief overview of the main utilities. The most important function is `pdf_text` which returns a character vector of length equal to the number of pages in the pdf. Each string in the vector contains a plain text version of the text on that page.\n\n```r\nlibrary(pdftools)\ndownload.file(\"http://arxiv.org/pdf/1403.2805.pdf\", \"1403.2805.pdf\", mode = \"wb\")\ntxt <- pdf_text(\"1403.2805.pdf\")\n\n# first page text\ncat(txt[1])\n\n# second page text\ncat(txt[2])\n```\n\nIn addition, the package has some utilities to extract other data from the PDF file. The `pdf_toc` function shows the table of contents, i.e. the section headers which pdf readers usually display in a menu on the left. It looks pretty in JSON:\n\n```r\n# Table of contents\ntoc <- pdf_toc(\"1403.2805.pdf\")\n\n# Show as JSON\njsonlite::toJSON(toc, auto_unbox = TRUE, pretty = TRUE)\n```\n\nOther functions provide information about fonts, attachments and metadata such as the author, creation date or tags.\n\n\n```r\n# Author, version, etc\ninfo <- pdf_info(\"1403.2805.pdf\")\n\n# Table with fonts\nfonts <- pdf_fonts(\"1403.2805.pdf\")\n```\n\n## Bonus feature: rendering pdf\n\nA bonus feature on most platforms is rendering of PDF files to bitmap arrays. The poppler library provides all functionality to implement a complete PDF reader, including graphical display of the content. In R we can use `pdf_render_page` to render a page of the PDF into a bitmap, which can be stored as e.g. png or jpeg.\n\n```r\n# renders pdf to bitmap array\nbitmap <- pdf_render_page(\"1403.2805.pdf\", page = 1)\n\n# save bitmap image\npng::writePNG(bitmap, \"page.png\")\njpeg::writeJPEG(bitmap, \"page.jpeg\")\nwebp::write_webp(bitmap, \"page.webp\")\n```\n\nThis feature is still experimental and currently does not work on Windows.\n\n## Limitations\n\nData scientists are often interested in data from tables. Unfortunately the pdf format is pretty dumb and does not have notion of a table (unlike for example HTML). Tabular data in a pdf file is nothing more than strategically positioned lines and text, which makes it difficult to extract the raw data.\n\n```r\ntxt <- pdf_text(\"http://arxiv.org/pdf/1406.4806.pdf\")\n\n# some tables\ncat(txt[18])\ncat(txt[19])\n```\n\nPdftools usually does a decent job in retaining the positioning of table elements when converting from pdf to text. But the output is still very dependent on the formatting of the original pdf table, which makes it very difficult to write a generic table extractor. But with a little creativity you might be able to parse the table data from the text output of a given paper.\n\n\n[![](http://ropensci.org/public_images/github_footer.png)](http://ropensci.org)\n", 
  "id": 52382813
}