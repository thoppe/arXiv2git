{
  "read_at": 1462548238, 
  "description": "Generating adversarial examples on MNIST using Theano", 
  "README.md": "# Generating adversarial examples for MNIST\n\nDone as coursework for Mathematics in Action (2016) at the University of Edinburgh.  \nSee the report for results.\n\nReproducing results from Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. \"Explaining and harnessing adversarial examples.\" arXiv preprint arXiv:1412.6572 (2014).\n\nCode based on Theano tutorials: https://github.com/Newmu/Theano-Tutorials  \n\nMNIST data from Yann LeCun's website: http://yann.lecun.com/exdb/mnist/   \n\n## Usage  \n1. To train the simple neural network, set TRAINING to True. The weights are then saved with cPickle and for following runs can be loaded from file by setting TRAINING back to False. \n2. The magnitude of adversarial noise added can be set as the eps parameter.\n\n## Comments\nOne thing to point out from the report is that once the L1 and L2 regularization were added to the model, they were tested on the adversarial examples generated from the network without them. I realized this only after submitting. Hence the results in the report do not directly show how regularized network handles its own adversarial examples. By running it again it seems that the correct results are: \n\n| Dataset | Epsilon | Accuracy |\n| --- | --- | --- |\n| l1+l2 adversarial | 0.10 | 79.36%  |\n| l1+l2 adversarial | 0.25 | 76.30%  |\n\nBut I did these hastily and they do sound like a very good improvement, so perhaps check them before believing them.\n\n\n\n\n\n", 
  "id": 57208636
}