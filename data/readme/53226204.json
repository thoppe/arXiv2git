{
  "read_at": 1462554908, 
  "description": "Python implementation of \"Deep Residual Learning for Image Recognition\" (http://arxiv.org/abs/1512.03385 - MSRA, winner team of the 2015 ILSVRC and COCO challenges).", 
  "README.md": "# Deep Residual Learning for Image Recognition\n\nImplementation of [\"Deep Residual Learning for Image Recognition\", Kaiming\nHe, Xiangyu Zhang, Shaoqing Ren, Jian Sun](http://arxiv.org/abs/1512.03385) in [PyFunt](https://github.com/dnlcrl/PyFunt) (a simple Python + Numpy DL framework).\n\nAlso inspired by [this implementation in Lua + Torch](https://github.com/gcr/torch-residual-networks).\n\nThe network operates on minibatches of data that have shape (N, C, H, W)\nconsisting of N images, each with height H and width W and with C input\nchannels. It has, like in the reference paper, (6*n)+2 layers,\ncomposed as below:\n\n\t\t\t                                        (image_dim: 3, 32, 32; F=16)\n\t\t\t                                        (input_dim: N, *image_dim)\n\t\t\t INPUT\n\t\t\t    |\n\t\t\t    v\n\t\t\t+-------------------+\n\t\t\t|conv[F, *image_dim]|                    (out_shape: N, 16, 32, 32)\n\t\t\t+-------------------+\n\t\t\t    |\n\t\t\t    v\n\t\t\t+-------------------------+\n\t\t\t|n * res_block[F, F, 3, 3]|              (out_shape: N, 16, 32, 32)\n\t\t\t+-------------------------+\n\t\t\t    |\n\t\t\t    v\n\t\t\t+-------------------------+\n\t\t\t|res_block[2*F, F, 3, 3]  |              (out_shape: N, 32, 16, 16)\n\t\t\t+-------------------------+\n\t\t\t    |\n\t\t\t    v\n\t\t\t+---------------------------------+\n\t\t\t|(n-1) * res_block[2*F, 2*F, 3, 3]|      (out_shape: N, 32, 16, 16)\n\t\t\t+---------------------------------+\n\t\t\t    |\n\t\t\t    v\n\t\t\t+-------------------------+\n\t\t\t|res_block[4*F, 2*F, 3, 3]|              (out_shape: N, 64, 8, 8)\n\t\t\t+-------------------------+\n\t\t\t    |\n\t\t\t    v\n\t\t\t+---------------------------------+\n\t\t\t|(n-1) * res_block[4*F, 4*F, 3, 3]|      (out_shape: N, 64, 8, 8)\n\t\t\t+---------------------------------+\n\t\t\t    |\n\t\t\t    v\n\t\t\t+-------------+\n\t\t\t|pool[1, 8, 8]|                          (out_shape: N, 64, 1, 1)\n\t\t\t+-------------+\n\t\t\t    |\n\t\t\t    v\n\t\t\t+-------+\n\t\t\t|softmax|                                (out_shape: N, num_classes)\n\t\t\t+-------+\n\t\t\t    |\n\t\t\t    v\n\t\t\t OUTPUT\n\nEvery convolution layer has a pad=1 and stride=1, except for the dimension\nenhancning layers which has a stride of 2 to mantain the computational\ncomplexity.\nOptionally, there is the possibility of setting m affine layers immediatley before the softmax layer by setting the hidden_dims parameter, which should be a list of integers representing the numbe of neurons for each affine layer.\n\nEach residual block is composed as below:\n\n\t          Input\n\t             |\n\t     ,-------+-----.\n\tDownsampling      3x3 convolution+dimensionality reduction\n\t    |               |\n\t    v               v\n\tZero-padding      3x3 convolution\n\t    |               |\n\t    `-----( Add )---'\n\t             |\n\t          Output\n\nAfter every layer, a batch normalization with momentum .1 is applied.\n\n\n## Requirements\n\n- [Python 2.7](https://www.python.org/)\n- [numpy](www.numpy.org/)\n- [pyfunt](https://github.com/dnlcrl/PyFunt)\n- [pydatset](https://github.com/dnlcrl/PyDatSet)\n\n\nAfter you get Python, you can get [pip](https://pypi.python.org/pypi/pip) and install all requirements by running:\n\t\n\tpip install -r requirements.txt\n\n## Usage\n\nIf you want to train the network on the CIFAR-10 dataset, simply run:\n\n\tpython train.py --help\n\t\nOtherwise, you have to get the right train.py for MNIST or SFDDD datasets, they are respectively on the mnist and sfddd git branches:\n\n- train.py for MNIST: https://github.com/dnlcrl/PyResNet/blob/mnist/train.py\n\n- train.py for SFDDD: https://github.com/dnlcrl/PyResNet/blob/sfddd/train.py\n\n## Experiments Results\n\nYou can view all the experiments results in the [./docs directory](https://github.com/dnlcrl/PyResNet/tree/master/docs). Main results are shown below:\n\n###  [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)\n\nbest error: 9.59 % (accuracy: 0.9041) with a 20 layers residual network (n=3):\n\n[![CIFAR-10 results](https://github.com/dnlcrl/PyResNet/blob/master/docs/imgs/cifar.png)](https://github.com/dnlcrl/PyResNet/blob/master/docs/CIFAR-10%20Experiments.ipynb)\n\n###  [MNIST](http://yann.lecun.com/exdb/mnist/)\n\nbest error: 0.36 % (accuracy: 0.9964) with a 32 layers residual network (n=5):\n\n[![MNIST results](https://github.com/dnlcrl/PyResNet/blob/master/docs/imgs/mnistres.png)](https://github.com/dnlcrl/PyResNet/blob/master/docs/MNIST%20Experiments.ipynb)\n\n###  [SFDDD](https://www.kaggle.com/c/state-farm-distracted-driver-detection)\n\nbest error: 0.25 % (accuracy: 0.9975 %) on a subset (1000 samples) of the train data (~21k images) with a 44 layers residual network (n=7), resizing the images to 64x48, randomly cropping 32x32 images for training and cropping a 32x32 image from the center of the original images for testing. Unfortunately I got more than 2% error on Kaggle's results (composed of ~80k images).\n\t\nWIP\n\n## TODOs:\n\n- regenerate plots with english labels\n\n- experimentat elastic distortion as data augmentation function on MNIST\n\n- experiment other data augmentation functions on SFDDD\n\n- implementation of the second version of residual networks, as explained in [\"Identity Mappings in Deep Residual Networks\" by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun](http://arxiv.org/pdf/1603.05027v1.pdf) \n", 
  "id": 53226204
}