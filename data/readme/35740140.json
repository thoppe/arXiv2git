{
  "id": 35740140, 
  "read_at": 1462558810, 
  "README.rst": "The escalator package\n=========\n\nPython implementation of a reliable leaderboard for machine learning competitions\n\nMIT Licensed. See ``LICENSE.md``.\n\nInstallation\n------------\n\nThis is a development version. For now, use\n::\n    git clone https://github.com/nerdcha/escalator.git\n    cd escalator\n    python setup.py devel\n\nBackground\n----------\n\nSee this `blog\npost <http://blog.mrtz.org/2015/03/09/competition.html>`__ for a\ndiscussion on the problem of overfitting to the public leaderboard in a\ndata science competition. The official code repository for `this\npaper <http://arxiv.org/abs/1502.04585>`__ is the `Julia version <https://github.com/mrtzh/Ladder.jl>`__;\nthis is a Python re-implementation.\n\nHere's a bibtex reference:\n\n::\n\n    @article{BH15,\n      author    = {Avrim Blum and Moritz Hardt},\n      title     = {The Ladder: {A} Reliable Leaderboard for Machine Learning Competitions},\n      journal   = {CoRR},\n      volume    = {abs/1502.04585},\n      year      = {2015},\n      url       = {http://arxiv.org/abs/1502.04585},\n      timestamp = {Mon, 02 Mar 2015 14:17:34 +0100},\n      biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BlumH15},\n      bibsource = {dblp computer science bibliography, http://dblp.org}\n    }\n\nIf you use the code, we encourage you to cite the paper.\n\n\nExamples\n--------\n\nThis implementation is designed with single-user data science projects in mind,\nto allow you to repeatedly evaluate candidate models while guarding against overfitting.\nIt also supports team-by-team evaluation, as in a competition leaderboard.\n\nSee escalator.sanity_check.py for usage; more examples will come soon.\n\n\n", 
  "description": "Python implementation of Blum and Hardt's Ladder algorithm"
}