{
  "read_at": 1462552976, 
  "description": "List of automatic music tagging research articles that are evaluated against MagnaTagATune Dataset", 
  "README.md": "# magnatagatune-list\nList of automatic music tagging (using audio) research articles that are evaluated against MagnaTagATune Dataset\n\n### Download\nMagnaTagATune is kindly hosted by MIRG of City University London. Visit [here](http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset) to download mp3s and others.\n\n### Folders and files\nAfter download three files, ` $ cat mp3.zip.* > mp3_all.zip` to merge them, then unzip it by `unzip mp3_all.zip`.\nYou then got 16 folders, '0' to '9' and then 'a' to 'f'. \n\nMany works based on splitting the folders as 13:1:3 for training/validatin/testing. However it might be the best choice since each folders do NOT have same tag distributions. Minimum line is to shuffle training set, and would like to recommend to shuffle all of them and then split. Still you might need the same splitting to comparison, as in my case :-(\n\n\n### Tags (sorted by popularity)\n\n#### Top 50\n\n` guitar, classical, slow, techno, strings, drums, electronic, rock, fast, piano, ambient, beat, violin, vocal, synth, female, indian, opera, male, singing, vocals, no vocals, harpsichord, loud, quiet, flute, woman, male vocal, no vocal, pop, soft, sitar, solo, man, classic, choir, voice, new age, dance, male voice, female vocal, beats, harp, cello, no voice, weird, country, metal, female voice, choral `\n\n#### Top 50 by categories\n\n* genre: ` classical, techno, electronic, rock, indian, opera, pop, classic, new age, dance, country, metal `\n* instrument: ` guitar, strings, drums, piano, violin, vocal, synth, female, male, singing, vocals, no vocals, harpsichord, flute, no vocal, sitar, man, choir, voice, male voice, female vocal, harp, cello, femal voice, choral `\n* mood: ` slow, fast, ambient, loud, quiet, soft, weird `\n* etc: ` beat, solo, beats `\n\n#### Top 51-188\n\n` electro, drum, male vocals, jazz, violins, eastern, female vocals, instrumental, bass, modern, no piano, harpsicord, jazzy, string, baroque, foreign, orchestra, hard rock, electric, trance, folk, chorus, chant, voices, classical guitar, spanish, heavy, upbeat, no guitar, acoustic, male singer, electric guitar, electronica, oriental, funky, tribal, banjo, dark, medieval, man singing, organ, blues, irish, no singing, bells, percussion, no drums, woman singing, noise, spacey, singer, female singer, middle eastern, chanting, no flute, low, strange, calm, wind, lute, heavy metal, different, punk, oboe, celtic, sax, flutes, talking, women, arabic, hard, mellow, funk, fast beat, house, rap, not english, no violin, fiddle, female opera, water, india, guitars, no beat, chimes, drone, male opera, trumpet, duet, birds, industrial, sad, plucking, girl, silence, men, operatic, horns, repetitive, airy, world, eerie, deep, hip hop, space, light, keyboard, english, not opera, not classical, not rock, clapping, horn, acoustic guitar, disco, orchestral, no strings, old, echo, lol, soft rock, no singer, jungle, bongos, reggae, monks, clarinet, scary, synthesizer, female singing, piano solo, no voices, woodwind, happy, viola, soprano, quick, clasical `\n\n#### histogram of tags\n\nSo the dataset is unbalanced. \n\n![histogram](https://github.com/keunwoochoi/magnatagatune-list/blob/master/tags_histogram.png?raw=true)\n\n### Proposed tag preprocessing\nI wrote code to merge these synonyms.\n\n    synonyms = [['beat', 'beats'],\n\t\t\t\t['chant', 'chanting'],\n\t\t\t\t['choir', 'choral'],\n\t\t\t\t['classical', 'clasical', 'classic'],\n\t\t\t\t['drum', 'drums'],\n\t\t\t\t['electro', 'electronic', 'electronica', 'electric'],\n\t\t\t\t['fast', 'fast beat', 'quick'],\n\t\t\t\t['female', 'female singer', 'female singing', 'female vocals', 'female voice', 'woman', 'woman singing', 'women'],\n\t\t\t\t['flute', 'flutes'],\n\t\t\t\t['guitar', 'guitars'],\n\t\t\t\t['hard', 'hard rock'],\n\t\t\t\t['harpsichord', 'harpsicord'],\n\t\t\t\t['heavy', 'heavy metal', 'metal'],\n\t\t\t\t['horn', 'horns'],\n\t\t\t\t['india', 'indian'],\n\t\t\t\t['jazz', 'jazzy'],\n\t\t\t\t['male', 'male singer', 'male vocal', 'male vocals', 'male voice', 'man', 'man singing', 'men'],\n\t\t\t\t['no beat', 'no drums'],\n\t\t\t\t['no singer', 'no singing', 'no vocal','no vocals', 'no voice', 'no voices', 'instrumental'],\n\t\t\t\t['opera', 'operatic'],\n\t\t\t\t['orchestra', 'orchestral'],\n\t\t\t\t['quiet', 'silence'],\n\t\t\t\t['singer', 'singing'],\n\t\t\t\t['space', 'spacey'],\n\t\t\t\t['string', 'strings'],\n\t\t\t\t['synth', 'synthesizer'],\n\t\t\t\t['violin', 'violins'],\n\t\t\t\t['vocal', 'vocals', 'voice', 'voices'],\n\t\t\t\t['strange', 'weird']]`\n\nI'm not 100% sure if these should be merged.\n\n\t\t\t\t['opera', 'operatic'],\n\t\t\t\t['hard', 'hard rock'],\n\t\t\t\t\n\n----------\n# Papers\nThis list is based on google scholar, list of papers that cited the dataset, [google scholar search result](https://scholar.google.co.kr/scholar?hl=en&as_sdt=2005&sciodt=0,5&cites=10394255617419929029&scipsc=)\n\n### 2016\nnot yet\n\n### 2015\nAccording to [this](https://scholar.google.co.kr/scholar?hl=en&as_sdt=2005&sciodt=0%2C5&cites=10394255617419929029&scipsc=&as_ylo=2015&as_yhi=2015),\n * U Sandouk et al., arxiv: text-processing.\n * U Sandouk et al., arxiv: text-processing.\n * E Colautti et al. : ...written in French but doesn't seem related to audio\n\n(and,)\n\n * J Nam et al., arxiv: [pdf](http://arxiv.org/abs/1508.04999)\n   * title: A deep bag-of-features model for music auto-tagging\n   * algorithm: bag-of-features (mel-specgram, onsets) --> PCA --> RBM for feature extraction --> DNN + stackedRBM\n   * set: 14600/1629/6499 for training/validation/test\n   * tags: used 160 tags\n   * (selected) result\n\n\n| algorithm        | Deep-BoF (proposed)   | P  2011  | P Hamel 2011 | P Hamel 2012 |\n| ------------- |:-------------:|:-------------:|:-------------:|:-------------:| \n| AUC tag      | 0.888 | 0.845 | 0.861 | 0.870 |\n| AUC clip      | 0.956 | 0.938 | 0.943 | 0.949 |\n| Precision@3  | 0.511 | 0.449 | 0.467 | 0.481 |\n| Precision@6| 0.358 | 0.430 | 0.327 | 0.339 |\n| Precision@9 | 0.275 | 0.249 | 0.255 | 0.263 |\n| Precision@12 | 0.225 | 0.205 | 0.211 | 0.216 |\n| Precision@15 | 0.190 | 0.175 | 0.181 | 0.184 |\n\n### 2014\nAccording to [this](https://scholar.google.co.kr/scholar?hl=en&as_sdt=2005&sciodt=0%2C5&cites=10394255617419929029&scipsc=&as_ylo=2014&as_yhi=2014),\n * D Lim at al., JMLR: about metrics\n * S Duan et al.: survey of tagging techniques\n * G Sageder et al.: not used. onlysubset of MagnaTagATune is used to verity the proposed feature selection that is based on ismir2014 DB.\n\n(and,)\n\n * S Dieleman et al.: [pdf](http://www.redes.unb.br/lasp/files/events/ICASSP2014/papers/p7014-dieleman.pdf)\n   * title: End-to-end learning for music audio\n   * algorithm: end-to-end setting from audio (and spectrogram for comparison), 1d conv - MP - 1d conv - MP - fc layers\n   * set: 16-folds, 1-12/13/14-6 for training/validation/test\n   * (selected) result\n\n| filter length | stride   | AUC (spectrograms)  | AUC (raw audio) |\n| ------------- |:-------------:|:-------------:|:-------------:|\n| 1024      | 1024 | 0.8690 | 0.8366 |\n| 256      | 256 | 0.8815 | 0.8487 |\n\n * A Oord et al.: [pdf](http://www.terasoft.com.tw/conf/ismir2014/proceedings/T007_118_Paper.pdf)\n   * title: Transfer learning by supervised pre-training for audio-based music classification\n   * algorithm: transfering learned MLP into problems based on other dataset.\n   * network: k-means feature learning --> MLP\n   * result\n\n| model | nmse   | mean average precision (mAP)  |\n| ------------- |:-------------:|:-------------:|:-------------:|\n| linear regression      | 0.965 | 0.823 | 0.0099 |\n| MLP (1 hidden layer)      | 0.939 | 0.841 | 0.0179 |\n| MLP (2 hidden layers)      | 0.924 | 0.837 | 0.0179 |\n\n| task | AUC   |\n| ------------- |:-------------:|\n| tag (top 50 tags)      | <0.88 |\n| tag (all 188 tags)      | <0.86 |\n\n * SN Tran et al.: [pdf](http://mirg.city.ac.uk/blog/wp-content/uploads/2013/09/rbm-features-for-music-similarity.pdf): about similarity\n   * title: feature preprocessing with RBMs for music similarity learning\n * F Gouton et al.: [pdf](http://arxiv.org/abs/1410.0001)\n   * title: on evaluation validity in music autotagging\n   * used [MagTag5k](http://tl.di.fc.ul.pt/t/magtag5k.zip), which is subset of MagnaTagATune. Now the link is broken.\n\n \n### 2013\nAccording to [this](https://scholar.google.co.kr/scholar?hl=en&as_sdt=2005&sciodt=0%2C5&cites=10394255617419929029&scipsc=&as_ylo=2013&as_yhi=2013),\n * D Wolff et al. : similarity\n * J Watson et al., arxiv: proposing an algorithm for ranking prediction.\n   * set: 16k for training, 160 tags (probably top-160 popular tags)\n   * feature: 104-dim MFCC, 100-dim embedding dimension\n   * result\n\n| algorithm        | precision@1           | precision@3  |\n| ------------- |:-------------:| -----:|\n| k-nn      | 39.4% | 28.6% |\n| k-nn (Wsabie space)      | 45.2% | 31.9% |\n| Wsabie      | 48.7% | 37.5% |\n| Affinity Weighted Embedding      | 52.7% | 39.2% |\n\n * JA Burgoyne et al., ismir: just mentioning.\n\n(and,)\n\n * S Dieleman et al., ismir: [pdf](https://biblio.ugent.be/publication/4152117/file/4324540.pdf)\n   * title: Multiscale Approaches To Music Audio Feature Learning\n   * result\n\n| algorithm        | average AUC           | \n| ------------- |:-------------:| \n| Laplacian 1 frame      | 0.898 |\n| Multiresolution spectrograms      | 0.888 |\n   \n  \n\n * J Stastny et al.: [pdf](http://www.researchgate.net/profile/Jii_Fejfar/publication/257563651_Audio_data_classification_by_means_of_new_algorithms/links/5432b2d00cf225bddcc7c68a.pdf)\n   * title: audio data classification by means of new algorithms\n   * Not on tag prediction\n \n\n### 2012\nAccording to [this](https://scholar.google.co.kr/scholar?hl=en&as_sdt=2005&sciodt=0%2C5&cites=10394255617419929029&scipsc=&as_ylo=2012&as_yhi=2012),\n * D wolff et al.: just mentioning.\n * Y Li et al.: not about the DB\n * M Levy: can't get the pdf of it, seems like internal document in the school.\n * L Barrington, PhD thesis: probably not focusing on automatic tagging\n * P Hamel, PhD thesis: written in french. according to its list of contents it would be same results as in the author's ismir paper.\n(and,)\n \n * J NAM et al.,: mirex 2012 submission\n   * title: mirex 2012 submission: audio classification using sparse feature learning\n   * algorithm: RBM + SVM\n   * result\n    * AROC (ranking): 0.7244,\n    * F-measure (annotation): 0.1123\n\n### 2011\nAccording to [this](https://scholar.google.co.kr/scholar?hl=en&as_sdt=2005&sciodt=0%2C5&cites=10394255617419929029&scipsc=&as_ylo=2011&as_yhi=2011),\n * AJ Quinn et al., SIGCHI: a general survey on human computation\n * YH Yang et al. IEEE ASLP: just mentining. experiment was performed on smaller private dataset\n * P Hamel et al.:\n   * title: temporal pooling and multiscale learning for automatic annotation and ranking of music audio\n   * features: MFCC\n   * set: 14600/1629/6499 for training/validation/test\n   * tags: not specified, probably using all 160 tags.\n   * comparison: against four algirithms [19][17][18][9] (see the references in the paper), all of which use MFCC or 'cepstral transform that is closely related to MFCCs' + spectral features (spectrao centroid, rolloff, flux). They also used GMM of tags, VQ, PAMIR, or individual SVM. \n   * features: 128 MFCC --> 120 principle components, which is called PMSC\n     * then mean,var,max,min, 3rd and 4th centered moments -pooling\n  * model 1: MLP with L2 regularisation and cross-entropy for loss function. \n  * model 2: multi-time-scale learning \n  * evaluation metric: AUC\n  * result:\n\ntable 1. \n\n| algorithm        | AUC           | average training time  |\n|:-------------:|:-------------:|:-----:|\n| MFCC(20)      | 0.77+-0.04 | 5.9gh |\n| MEL-spectrum(128)      | 0.853+-0.008 | 5.2h |\n| PMSC(120)      | 0.876+-0.004 | 1.5h |\n\ntable 2.\n\n| measure | Manzagol | Zhi | Mandel | Marsyas | mel-spec+PFC | PMSC+PFC | PSMC+MTSL |\n| ------------- |:-------------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n| average AUC-tag | 0.750 | 0.673 | 0.821 | 0.831 | 0.820 | 0.845 | 0.861 |\n| average AUC-clip | 0.810 | 0.748 | 0.886 | 0.933 | 0.930 | 0.938 | 0.943 |\n| precision@3 | 0.255 | 0.224 | 0.323 | 0.440 | 0.430 | 0.449 | 0.467 |\n| precision@6 | 0.194 | 0.192 | 0.245 | 0.314 | 0.305 | 0.320 | 0.327 |\n| precision@9 | 0.159 | 0.168 | 0.197 | 0.244 | 0.240 | 0.249 | 0.255 |\n| precision@12 | 0.136 | 0.146 | 0.167 | 0.201 | 0.198 | 0.205 | 0.211 |\n| precision@15 | 0.119 | 0.127 | 0.145 | 0.172 | 0.170 | 0.175 | 0.181 |\n\nleft four: rsults from MIREX 2009\n\n * J Weston et al., JNMR:\nhttp://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37179.pdf\n * D Wolff, ismir: about similarity metric\n * J Weston et al., arXiv: same as above JNMR, but with tables at within text. \nhttp://arxiv.org/pdf/1105.5196.pdf\n * M Mann et al., ismir: mood predction in reduced dimensions.\n * D Wolff et al., : about similarity\n * T Aizenberg et al.: about melodic salience modelling\n\n(and,)\n\n * S Dieleman et al., ismir 2011:\n   * title: audio-based music classification with a pretrained convolutional network\n\n\n### 2010\nAccording to [this](https://scholar.google.co.kr/scholar?hl=en&as_sdt=2005&sciodt=0%2C5&cites=10394255617419929029&scipsc=&as_ylo=2010&as_yhi=2010),\n\n * E Law et al., MLKD: learning to tag from open voca labels\n * M Mandel et al.: learning tags that vary withnin a song\n * A Quinn et al.: not related.\n * E Law et al., ECML: learning to tag using noisy labels\n * Y Panagakis et al.: sparse multi-label linear embedding NNTF for automatic music tagging\n * F Maillet: about music recommendation\n\n(and,)\n\n * K Seyerlehner et al.: [pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.227.6265&rep=rep1&type=pdf)\n  * title: Automatic music tag classification based on block-level\n  * metric: f-score and G-mean (geometric mean of true negative rate and recall)\n  * result:\n\n| feature set | f-score | g-mean | f-score (S2) | g-mean(S20 |\n| ------------- |:-------------:|:-----:|:-----:|:-----:|\n| SAF | 0.3775 | 0.6101 | 0.3962 | 0.6252 |\n| BLF-PCA | 0.4163 | 0.6410 | 0.4201 | 0.6439 |\n\n| feature set | avg. f-score | avg. g-mean | avg. f-score (S2) | avg. g-mean(S20 |\n| ------------- |:-------------:|:-----:|:-----:|:-----:|\n| SAF | 0.1777 | 0.3573 | 0.1932 | 0.3784 |\n| BLF-PCA | 0.2136 | 0.4019 | 0.2185 | 0.4081 |\n\n * J Bergstra et al., ismir 2010: [pdf](https://www.researchgate.net/profile/Michael_Mandel/publication/220723009_Scalable_Genre_and_Tag_Prediction_with_Spectral_Covariance/links/02e7e520c4c292aee4000000.pdf)\n    * used few selected tags only\n\n* SR Ness et al.: [pdf](Improving automatic music tag annotation using stacked generalization of probabilistic svm outputs)\n  * title: Improving automatic music tag annotation using stacked generalization of probabilistic svm outputs\n  * audio features (+Affinity) + SVM\n  * result\n\nglobal average (table 2)\n\n| algorithm | precision | recall | accuracy | F-score |\n| ------------- |:-------------:|:-----:|:-----:|:-----:|\n| Audio SVM | 0.307 | 0.315 | 0.969 | 0.311 |\n| Affinity SVM | 0.351 | 0.354 | 0.971 | 0.353 |\n\naffinity svm - per-tag evaluation (table 4) (table 3 is ommited as table 4 outperforms in overall)\n\n| number of tags | precision | recall | accuracy | F-score |\n| ------------- |:-------------:|:-----:|:-----:|:-----:|\n| 20 | 0.418 | 0.691 | 0.856 | 0.518 |\n| 30 | 0.346 | 0.671  | 0.862  | 0.453  |\n| 40 | 0.394  | 0.397 | 0.914 | 0.395 |\n| 50 | 0.369| 0.372 | 0.923 | 0.371 |\n| 100 | 0.259 | 0.262 | 0.951 | 0.260 |\n| all (188) | 0.184 | 0.186 | 0.971 | 0.185 |\n\n\n### 2009\nN/A\n\n\n\n", 
  "id": 50314670
}