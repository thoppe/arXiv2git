{
  "read_at": 1462558637, 
  "description": "Reference implementation for Deep Unsupervised Learning using Nonequilibrium Thermodynamics", 
  "README.md": "# Diffusion Probabilistic Models\n\nThis repository provides a reference implementation of the method described in the paper:<br>\n> Deep Unsupervised Learning using Nonequilibrium Thermodynamics.<br>\n> Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya.<br>\n> International Conference on Machine Learning. 2015<br>\n> http://arxiv.org/abs/1503.03585\n\nThis implementation builds a generative model of data by training a Gaussian diffusion process to transform a noise distribution into a data distribution in a fixed number of time steps.\nThe mean and covariance of the diffusion process are parameterized using deep supervised learning.\nThe resulting model is tractable to train,\neasy to exactly sample from,\nallows the probability of datapoints to be cheaply evaluated,\nand allows straightforward computation of conditional and posterior distributions.\n\n## Using the Software\n\nIn order to train a diffusion probabilistic model on the default dataset of MNIST, simply install dependencies (see below), and then run\n``python train.py``.\n\n### Dependencies\n\n1. Install `Blocks` and its dependencies following [these instructions](http://blocks.readthedocs.org/en/latest/setup.html)\n2. Setup `Fuel` and download MNIST following [these instructions](https://github.com/mila-udem/fuel/blob/master/docs/built_in_datasets.rst).\n\nAs of October 16, 2015 this code requires the bleeding edge, rather than stable, versions of both Blocks and Fuel. (thanks to David Hofmann for pointing out that the stable release will not work due to an interface change)\n\n\n### Output\n\nThe objective function being minimized is the bound on the negative log likelihood in bits per pixel, minus the negative log likelihood under an identity-covariance Gaussian model. That is, it is the negative of the number in the rightmost column in Table 1 in the paper.\n\nLogging information is printed to the console once per training epoch, including the current value of the objective on the training set.\n\nFigures showing samples from the model, parameters, gradients, and training progress are also output periodically (every 25 epochs by default -- see ``train.py``).\n\nThe samples from the model are of three types -- standard samples, samples inpainting the left half of masked images, and samples denoising images with Gaussian noise added (by default, the signal-to-noise ratio is 1). This demonstrate the straightforward way in which inpainting, denoising, and sampling from a posterior in general can be performed using this framework.\n\nHere are samples generated by this code after 825 training epochs on MNIST, trained using the command `run train.py`:<br>\n<img src=\"https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models/blob/master/samples-_t0000_epoch0825.png\" width=\"500\">\n\nHere are samples generated by this code after 1700 training epochs on CIFAR-10, trained using the command `run train.py --batch-size 200 --dataset CIFAR10 --model-args \"n_hidden_dense_lower=1000,n_hidden_dense_lower_output=5,n_hidden_conv=100,n_layers_conv=6,n_layers_dense_lower=6,n_layers_dense_upper=4,n_hidden_dense_upper=100\"`:<br>\n<img src=\"https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models/blob/master/samples-_t0000_epoch1700.png\" width=\"500\">\n\n\n## Miscellaneous\n\n**Different nonlinearities** - In the paper, we used softplus units in the convolutional layers, and tanh units in the dense layers.\nIn this implementation, I use leaky ReLU units everywhere.\n\n**Original source code** - This repository is a refactoring of the code used to run the experiments in the published paper.\nIn the spirit of reproducibility, if you email me a request I am willing to share the original source code.\nIt is poorly commented and held together with duct tape though.\nFor most applications, you will be better off using the reference implementation provided here.\n\n**Contact** - I would love to hear from you. Let me know what goes right/wrong! <jaschasd@google.com>\n", 
  "id": 35744089
}