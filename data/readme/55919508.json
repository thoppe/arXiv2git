{
  "read_at": 1462548545, 
  "description": "A deep learning powered bot capable of classifying images into user-specified categories", 
  "README.md": "Classification Bot\n------------------\n\n[![Join the chat at https://gitter.im/AntreasAntoniou/DeepClassificationBot](https://badges.gitter.im/AntreasAntoniou/DeepClassificationBot.svg)](https://gitter.im/AntreasAntoniou/DeepClassificationBot?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\nWelcome to the Classification Bot codebase. Classification Bot is an attempt of simplifying the collection, extraction and preprocessing of data as well as providing an end to end pipeline for using them to train large deep neural networks.\n\nThe system is composed of scrapers, data extractors, preprocessors, deep neural network models using [Keras](https://github.com/fchollet/keras) provided by [Francois Chollet](https://github.com/fchollet) and an easy to use deployment module.\n\n## Installation\nMake sure you have a GPU as the training is very compute intensive\n\n1. (OSX) Install gcc: `brew install gcc`\n2. Install CUDA_toolkit 7.5\n3. Install cuDNN 4\n4. Install Theano, using `sudo pip install git+git://github.com/Theano/Theano.git`\n5. Install OpenCV\n6. Install hdf5 library (libhdf5-dev)\n7. Make sure you have Python 2.7.6 and virtualenv installed on your system\n8. Install Python dependencies\n\n```\n$ virtualenv --python=python2 --system-site-packages env\n$ . env/bin/activate\n$ pip install -r requirements.txt\n```\n\n## Training and deploying\n\n### To download images\n\nUse `google_image_scraper.py` to download images. It takes a .csv file of the categories you want, and downloads a number of images per line.\n\nThe first line of the .csv file will be ignored.\n\nThe number of images per category is configurable. We suggest a number between 200-1000:\n\n```\n$ google_image_scraper.py -n 200 yourfilehere.csv\n```\n\n#### Easy Mode:\n(For users that have a list of categories available at hand):\n\n1. Create a .csv file with one category per line of what you want the scraper to search for.\n2. Now let's download some images! Run `python google_image_scraper.py yourfilehere.csv`\n\n#### Hacker Mode:\n (For users that know an online repo that has their categories and want to fetch them, or if their categories are too many and you MUST automate the procedure, or if you much rather code stuff rather than copy and paste)\n\n1. Write a script that can fetch your categories using Wikipedia or any other resource you would like. For an example look at `examples/anime_show_extractor.py` to see what we used to get our categories.\n2. Have your script create a .csv file with the categories you require.\n3. Then run `python google_image_scraper.py yourfilehere.csv`\n\n\n##### Note: Some information on google_image_scraper.py\n\nWhen downloading images make sure you set the number of images to the number of images per class, we suggest a number between 200-1000.\n\n### To extract and preprocess data ready for training\n\n1. Once you have your data ready, run `python train.py extract_data` to get all of your data ready and saved in HDF5 files.\n\n### To train your network\n\n1. Once all of the above have been met then you are ready to train your network, by running `python train.py --run` to load data from HDF5 files or `python train.py --run --extract_data` to extract data and train in one procedure.\n2. If you want to continue training a model, you can. After each epoch the weights are saved. If you want to continue training simply run `python train.py --run --continue`\n\n\n### Deploying a model\n\n1. Once your training has finished and a good model has been trained then you can deploy your model.\n2. To deploy a model on a single URL image use `python deploy.py --URL [URL_LINK]`\n3. To deploy a model on a folder full of images use `python deploy -image_folder path/to/folder`\n4. To deploy a model on a single file use `python deploy -image_file path/to/file`\n\nOnce deployed the model should return the top 5 predictions on each image in a nice string formatted view: e.g.\n\n```\nImage Name: Tengen.Toppa.Gurren-Lagann.full.174481.jpg\nCategories:\n0. Gurren Lagann: 0.999914288521\n1. Kill La Kill: 7.29278544895e-05\n2. Naruto: 4.92283288622e-06\n3. Redline: 2.71744352176e-06\n4. Cowboy Bebop: 1.41406655985e-06\n_________________________________________________\n```\n\n### Things for you to try\n\n1. Create your own classifiers\n2. Try different model architectures (Hint: go to google scholar or arxiv and search for GoogLeNet, VGG-Net, AlexNet, ResNet and follow the waves :) )\n\n## Twitter bot\n\n`bot.py` provides a Twitter bot that provides an interface for querying the classifier.\n\n### Running the bot locally\n\n#### Prerequisites\n\n* A classifier\n* [A Twitter app](https://apps.twitter.com/) registered under the bot account\n* Consumer key and secret for that app\n* [Your access token and secret for that app](https://dev.twitter.com/oauth/overview/application-owner-access-tokens)\n\nCopy `bot.ini.example` to `bot.ini` and overwrite with your key/secret and token/secret.\n\n#### Run it\n\n```\n$ python bot.py -c bot.ini --debug\n```\n\n`python bot.py --help` will list all available command line options.\n\n### Deploying to Google Compute Engine\n\nThis repo comes with the necessary support files for deploying the Twitter bot\nto a dedicated GCE container-optimized instance.\n\n#### Prerequisites\n\n* A classifier\n* `bot.ini` with Twitter credentials (see above)\n* [Docker](https://www.docker.com/) tools and an account on a docker registry\n* [Google Cloud SDK](https://cloud.google.com/sdk/#Quick_Start)\n* [A Google Cloud Platform project](https://cloud.google.com/compute/docs/linux-quickstart#set_up_a_google_cloud_platform_project)\n\n#### Build and register your own docker image\n\n`classificationbot/base:latest` comes with all the dependencies installed.\nIf you've modified the code and added a new dependency,\nmake a new Docker image based on the dockerfiles in this repo.\n\n`dockerfiles/bot/Dockerfile` will contain the bot and the classifier when built.\n\nThis repo's associated images are built with these commands:\n\n```\n$ docker build -t classificationbot/base:latest -f dockerfiles/base/Dockerfile .\n$ docker push classificationbot/base:latest\n\n$ docker build -t classificationbot/ci:latest -f dockerfiles/ci/Dockerfile .\n$ docker push classificationbot/ci:latest\n\n$ docker build -t classificationbot/bot:latest -f dockerfiles/bot/Dockerfile .\n$ docker push classificationbot/bot:latest\n```\n\nWhen you've registered your own image, update the `image` value in `etc/containers.yaml`.\n\n#### Creating and deleting your instance\n\n`tasks.py` provides a handy shortcut for creating a small instance\nwith the Docker image specified in `etc/containers.yaml`.\nTwitter credentials are pulled from `bot.ini` and stored as instance metadata.\n\n```\n$ python tasks.py create_instance\n$ python tasks.py delete_instance\n```\n\n#### When something goes wrong\n\nOr when you want to see if it's working for yourself:\n\n```\n## SSH into your instance\n$ gcloud compute ssh --zone us-central1-a bot\n\n## Wait until our container comes up:\nyou@bot:~$ sudo watch docker ps\n\n## If it appears to be stuck, check kubelet's log:\nyou@bot:~$ sudo less /var/log/kubelet.log\n\n## Once it's up, you can drop into its shell:\nyou@bot:~$ sudo docker exec -it $(sudo docker ps --filter=ancestor=classificationbot/bot -q) bash\n\n## And run supervisorctl to check the bot.py process\n# supervisorctl\n\n## You can run it manually too:\n# cd /opt/bot/\n# python bot.py --mock --debug\n```\n\n## Special Thanks\nSpecial thanks to Francois Chollet (fchollet) for building the superb [Keras](https://github.com/fchollet/keras) deep learning library.\nWe couldn't have brought a project ready to be used by non-machine learning people if it wasn't for the ease of use of Keras.\n\nSpecial thanks to https://github.com/shuvronewscred/ for building the image scraper we adapted for our project.\nOriginal source code can be found at https://github.com/shuvronewscred/google-search-image-downloader\n", 
  "id": 55919508
}