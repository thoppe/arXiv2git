{
  "read_at": 1462549228, 
  "description": "bidirectional lstm", 
  "README.md": "# bidirectional_RNN\nbidirectional lstm\n\nThis repo demonstrates how to use [mozi](https://github.com/hycis/Mozi.git) to build a deep bidirectional RNN/LSTM with mlp layers before and after the LSTM layers\n\nThis repo can be used for the deep speech paper from Baidu\n\nDeep Speech: Scaling up end-to-end speech recognition\narXiv:1412.5567, 2014\nA. Hannun etc\n\n\n<!-- ![BiLSTM](images/illustration.png \"Title\" {width=40px height=400px}) -->\n<img src=\"item_lstm.png\" height=\"250\">\n\nThe figure above shows the structure of the Bidirectional LSTM, whereby you have one forward LSTM and one backward LSTM running in reverse time and with their features concatenated at the output layer, thus enabling informations from both past and future to come together.\n\n```python\ndef train():\n    max_features=20000\n    maxseqlen = 100 # cut texts after this number of words (among top max_features most common words)\n    batch_size = 16\n    word_vec_len = 256\n    iter_class = 'SequentialRecurrentIterator'\n    seq_len = 10\n\n    data = IMDB(pad_zero=True, maxlen=100, nb_words=max_features, batch_size=batch_size,\n                train_valid_test_ratio=[8,2,0], iter_class=iter_class, seq_len=seq_len)\n\n    print('Build model...')\n    model = Sequential(input_var=T.matrix(), output_var=T.matrix())\n    model.add(Embedding(max_features, word_vec_len))\n\n    # MLP layers\n    model.add(Transform((word_vec_len,))) # transform from 3d dimensional input to 2d input for mlp\n    model.add(Linear(word_vec_len, 100))\n    model.add(RELU())\n    model.add(BatchNormalization(dim=100, layer_type='fc'))\n    model.add(Linear(100,100))\n    model.add(RELU())\n    model.add(BatchNormalization(dim=100, layer_type='fc'))\n    model.add(Linear(100, word_vec_len))\n    model.add(RELU())\n    model.add(Transform((maxseqlen, word_vec_len))) # transform back from 2d to 3d for recurrent input\n\n    # Stacked up BiLSTM layers\n    model.add(BiLSTM(word_vec_len, 50, output_mode='concat', return_sequences=True))\n    model.add(BiLSTM(100, 24, output_mode='sum', return_sequences=True))\n    model.add(LSTM(24, 24, return_sequences=True))\n\n    # MLP layers\n    model.add(Reshape((24 * maxseqlen,)))\n    model.add(BatchNormalization(dim=24 * maxseqlen, layer_type='fc'))\n    model.add(Linear(24 * maxseqlen, 50))\n    model.add(RELU())\n    model.add(Dropout(0.2))\n    model.add(Linear(50, 1))\n    model.add(Sigmoid())\n\n    # build learning method\n    decay_batch = int(data.train.X.shape[0] * 5 / batch_size)\n    learning_method = SGD(learning_rate=0.1, momentum=0.9,\n                          lr_decay_factor=1.0, decay_batch=decay_batch)\n\n    # Build Logger\n    log = Log(experiment_name = 'MLP',\n              description = 'This is a tutorial',\n              save_outputs = True, # log all the outputs from the screen\n              save_model = True, # save the best model\n              save_epoch_error = True, # log error at every epoch\n              save_to_database = {'name': 'Example.sqlite3',\n                                  'records': {'Batch_Size': batch_size,\n                                              'Learning_Rate': learning_method.learning_rate,\n                                              'Momentum': learning_method.momentum}}\n             ) # end log\n\n    # put everything into the train object\n    train_object = TrainObject(model = model,\n                               log = log,\n                               dataset = data,\n                               train_cost = mse,\n                               valid_cost = error,\n                               learning_method = learning_method,\n                               stop_criteria = {'max_epoch' : 100,\n                                                'epoch_look_back' : 5,\n                                                'percent_decrease' : 0.01}\n                               )\n    # finally run the code\n    train_object.setup()\n    train_object.run()\n```\n", 
  "id": 38167939
}