{
  "read_at": 1462548409, 
  "description": "Awesome deep learning based NLP papers and survey, also some awesome machine learning/vision material", 
  "README.md": "# awesome-deep-nlp\n\n## Attention  \n\n1. ICLR15, Neural Machine Translation by Jointly Learning to Align and Translate , Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.   \n\n1. ACL15, Encoding Source Language with Convolutional Neural Network for Machine Translation , Fandong Meng, Zhengdong Lu, Mingxuan Wang, Hang Li, Wenbin Jiang, Qun Liu\n\n1. ACL15, A Hierarchical Neural Autoencoder for Paragraphs and Documents , Jiwei Li, Minh-Thang Luong, Dan Jurafsky  \n\n1. EMNLP15, A Neural Attention Model for Sentence Summarization, Alexander M. Rush, Sumit Chopra and Jason Weston  \n\n1. EMNLP15 short, Not All Contexts Are Created Equal: Better Word Representations with Variable Attention6, Wang Ling.\n\n1. NAACL15 , Two/Too Simple Adaptations of Word2Vec for Syntax Problems  \n\n1. NIPS14, Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu. Recurrent Models of Visual Attention.\n\n1. ICLR15, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate.  \n\n1. ACL15, Fandong Meng, Zhengdong Lu, Mingxuan Wang, et al. Encoding Source Language with Convolutional Neural Network for Machine Translation.  \n\n1. ACL15, Jiwei Li, Minh-Thang Luong, Dan Jurafsky. A Hierarchical Neural Autoencoder for Paragraphs and Documents. \n\n1. EMNLP15, Alexander M. Rush, Sumit Chopra, Jason Weston. A Neural Attention Model for Sentence Summarization. \n\n1. EMNLP15, Wang Ling, Lin Chu-Cheng, Yulia Tsvetkov, et al. Not All Contexts Are Created Equal: Better Word Representations with Variable Attention\n\n1. End-to-End Attention-based Large Vocabulary Speech Recognition\n\n1. NIPS14 ws, End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results http://arxiv.org/abs/1412.1602\n\n1. NIPS15, Attention-Based Models for Speech Recognition \n\n1. EMNLP15, Effective Approaches to Attention-based Neural Machine Translation \n\n1. ICML15, Kelvin Xu, Jimmy Ba, Ryan Kiros, et al. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. \n\n1. Karol Gregor, Ivo Danihelka, Alex Graves, et al. DRAW: A Recurrent Neural Network For Image Generation. \n\n1. NIPS15, Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, et al. Teaching Machines to Read and Comprehend. \n\n1. NIPS15, Lei Jimmy Ba, Roger Grosse, Ruslan Salakhutdinov, Brendan Frey. Learning Wake-Sleep Recurrent Attention Models.\n\n\n## IR\n\n1. **DSSM**   \n\n\n\tLearning deep structured semantic models for web search using clickthrough data, cikm2013 [Paper](http://www.msr-waypoint.net/pubs/198202/cikm2013_DSSM_fullversion.pdf) [code1](https://github.com/mranahmd/dssm-wemb-theano) [code2](https://github.com/outstandingcandy/dssm)\n2. **CDSSM**   \nA latent semantic model with convolutional-pooling structure for information retrieval, msr, CIKM2014\n3. **ARC-I**  \nconvolutional neural network architectures for matching natural language sentences, NIPS2014\n3. **ARC-II**  \nconvolutional neural network architectures for matching natural language sentences, NIPS2014\n3. **RAE**  \nDynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection, NIPS2011\n3. **Deep Match**  \nA deep architecture for matching short texts, NIPS,2013\n3. **CNTN**  \nConvolutional Neural Tensor Network Architecture for Community-based Question Answering, IJCAI2015\n3. **CNNPI**  \nconvolutional neural network for paraphrase identification, NAACL2015\n3. **MultiGranCNN**  \nMultiGranCNN: An architecture for general matching of text chunks on multiple levels of granularity, ACL2015\n\n3. **CLSTM**   \n\tContextual LSTM (CLSTM) models for Large scale NLP tasks, Google, Arxiv201602\n3. **CLSM**   \nA latent semantic model with convolutional-pooling structure for information retrieval, cikm2014\n4. **Recurrent-DSSM**  \nPalangi, H., Deng, L., Shen, Y., Gao, J., He, X., Chen, J., Song, X., and Ward, R. Learning sequential semantic representations of natural language using recurrent neural networks. In ICASSP, 2015.\n\n4. **LSTM-DSSM**  \nSEMANTIC MODELLING WITH LONG-SHORT-TERM MEMORY FOR INFORMATION RETRIEVAL,ICLR2016, workshop\n\n4. **DCNN: Dynamic convolutional neural network**  \na convolutional neural network for modeling sentences, acl2014\nconvolutional neural network architectures for matching natural language sentences, nips2014, noah\n3. **BRAE: bilingually-constrained recursive auto-encoders**  \nbilingually-constrained phrase embeddings for machine translation, acl2014, long paper\n6. **LSTM-RNN**  \nDeep sentence embedding using lstm networks: analysis and application to information retrieval, 201602\n7. **SkipThought**  \nSkip thought vectors, \n9. **Bidirectional LSTM-RNN**  \nBi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation, 201602, Arxiv\n10. **MV-DNN**  \nA multi-view deep learning approach for cross domain user modeling in recommendation systems, WWW2015\n\n## Knowledge Graph\n1. UM\n3. LFM\n5. SE\n6. SME\n7. RESCAL\n8. NTN\n8. TransE  \n   Translating Embedding for Modeling Multi-relational Data\n8. TransH\n9. TransR\n10. TransM\n11. TransG\n12. KG2E\n13. PTransE\n6. TransA@CAS\n7. TransA@THU\n8. STransE\n\n## Multimodal/Transfer/Multitask/Ensemble/Semisupervised\n\n## Deep Generative Models\n\n\n\n\n", 
  "id": 56665891
}