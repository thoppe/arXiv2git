{
  "read_at": 1462558009, 
  "description": "Wikipedia vandalism dataset generator", 
  "README.md": "[![GoDoc](https://godoc.org/github.com/shuLhan/wvcgen?status.svg)](https://godoc.org/github.com/shuLhan/wvcgen)\n[![Go Report Card](https://goreportcard.com/badge/github.com/shuLhan/wvcgen)](https://goreportcard.com/report/github.com/shuLhan/wvcgen)\n\n# wvcgen\n\nThis is Wikipedia vandalism dataset generator and library for working with it.\n\n* [Overview](#overview)\n* [How To Use](#how-to-use)\n  * [Installation](#installation)\n  * [PAN-WVC-2010](#pan-wvc-2010)\n    * [Creating Unified Dataset](#creating-unified-dataset)\n    * [Cleaning Wiki Revisions](#cleaning-wiki-revisions)\n    * [Generating Features](#generating-features)\n  * [PAN-WVC-2011](#pan-wvc-2011)\n    * [Creating Unified Dataset WVC-2011](#creating-unified-dataset-wvc-2011)\n    * [Cleaning WVC-2011 Revisions](#cleaning-wvc-2011-revisions)\n    * [Generating WVC-2011 Features](#generating-wvc-2011-features)\n* [List of Features](#list-of-features)\n  * [Metadata](#metadata)\n  * [Text](#text)\n  * [Language](#language)\n  * [Misc](#misc)\n* [Extending The Feature](#extending-the-feature)\n* [References](#references)\n\n---\n\n## Overview\n\nThis repository does not provide the full Wikipedia vandalism dataset provided\nby uni-weimar.de but provide the script to work with dataset, for example\ndiff-ing old and new revisions, creating new dataset, and computing the\nfeatures.\n\nFor anyone who know Wikipedia Vandalism Corpus, the original dataset only\ncontain the classification but does not provide the feature values (one must\ncreate one to work with it, like this repository do).\n\nThis program contain implementation of features from the original Mola-Velasco\nwork [1], including 4 metadata features, 11 text features, and 12 language\nfeatures.\n\nI hope this can speeding up research on Wikipedia vandalism in the future.\n\nIf you have any question or problem with the program, ask or report it in the\nissue page.\n\nThis project is written using [Go lang](https://golang.org).\n\n## How To Use\n\n### Installation\n\n* [Install Go](https://golang.org/doc/install) from binary distribution or\n  using your OS package management.\n* Download this package,\n\n  ```\n  $ go get github.com/shuLhan/wvcgen\n  ```\n\n### PAN-WVC-2010\n\n* Change working directory to this package (located in\n  `$GOPATH/src/github.com/shuLhan/wvcgen)`\n  ```\n  $ cd $GOPATH/src/github.com/shuLhan/wvcgen\n  ```\n\n* Download the full dataset from [uni-weimar.de\n  site](http://www.uni-weimar.de/medien/webis/corpora/corpus-pan-wvc-10/pan-wikipedia-vandalism-corpus-2010.zip)\n  ```\n  $ wget http://www.uni-weimar.de/medien/webis/corpora/corpus-pan-wvc-10/pan-wikipedia-vandalism-corpus-2010.zip\n  ```\n\n* Extract the zip file\n  ```\n  $ unzip pan-wikipedia-vandalism-corpus-2010.zip\n  ```\n\n* Rename the extracted directory from `pan-wikipedia-vandalism-corpus-2010` to\n  `pan-wvc-2010`\n  ```\n  $ mv pan-wikipedia-vandalism-corpus-2010 pan-wvc-2010\n  ```\n\n* Move all files in `pan-wvc-2010/article-revisions/partXX/` to\n  `pan-wvc-2010/revisions`\n  ```\n  $ cd pan-wvc-2010\n  $ mkdir -p revisions\n  $ find article-revisions -name \"*.txt\" -exec mv '{}' revisions/ \\;\n  ```\n\n#### Creating Unified Dataset\n\n* Change working directory to `cmd/unified-wvc2010`\n* Run `main.go` script to merge and create new dataset\n  ```\n  $ go run main.go\n  ```\n\n  which will create file `unified-wvc2010.dat` that combine file\n  `pan-wvc-2010/edits.csv` with `pan-wvc-2010/gold-annotations.csv` and add\n  two new fields. List of attributes in unified dataset are,\n\n  * editid\n  * class\n  * oldrevisionid\n  * newrevisionid\n  * edittime\n  * editor\n  * articletitle\n  * editcomment\n  * deletions\n  * additions\n\nThe new fields are `deletions` and `additions` which contain diff of old\nrevision with new revision at words level. Attribute `deletions` contain\ndeleted text in old revision, and attribute `additions` contain inserted text\nin new revision.\n\nOne can customize the output of dataset by editing the `unified-wvc2010.dsv`\nconfiguration and run the merge script again.\n\n#### Cleaning Wiki Revisions\n\nCleaning wiki text revision, which is located in `revisions` directory, is\nrequired to speeding up processing features.\n\n* Change working directory to `cmd/wikiclean`\n* Create directory where the output of cleaning will be located,\n  ```\n  $ mkdir -p ../../pan-wvc-2010/revisions_clean\n  ```\n* Run `main.go` script to clean revisions file\n  ```\n  $ go run main.go ../../pan-wvc-2010/revisions ../../pan-wvc-2010/revisions_clean\n  ```\n\n  The first parameter is the input location where the revision text to be\n  cleaning up, the second parameter is location where new revision that has\n  been cleaned up will be written.\n\n#### Generating Features\n\nAfter one of PAN WVC dataset has been merged and cleaned up one can compute the\nvandalism features by runnning `main.go` script in root of repository.\n\n    $ go run main.go wvc2010_features.dsv\n\nGenerated feature values will be written to file `wvc2010_features.dat`.\n\nOne can customize the input and which features should be computed by editing\nfile `wvc2010_features.dsv`, which contains,\n* `Input` option point to the input file (the unified data set)\n* `InputMetadata` contains fields in input file,\n* `Output` option point the file where result of features computation will be\n  written,\n* `OutputMetadata` contain list of features that will computed. The name for\n  feature is described below.\n\n### PAN-WVC-2011\n\n* Download the full dataset from [uni-weimar.de\n  site](http://www.uni-weimar.de/medien/webis/corpora/corpus-pan-wvc-11/pan-wikipedia-vandalism-corpus-2011.zip)\n  ```\n  $ wget http://www.uni-weimar.de/medien/webis/corpora/corpus-pan-wvc-11/pan-wikipedia-vandalism-corpus-2011.zip\n  ```\n\n* Extract the zip file\n  ```\n  $ unzip pan-wikipedia-vandalism-corpus-2011.zip\n  ```\n\n* Rename the extracted directory from `pan-wikipedia-vandalism-corpus-2011` to\n  `pan-wvc-2011`\n  ```\n  $ mv pan-wikipedia-vandalism-corpus-2011 pan-wvc-2011\n  ```\n\n* Create unified directory for English revisions\n  ```\n  $ mkdir -p pan-wvc-2011/revisions\n  ```\n\n* Move all files in `pan-wvc-2011/article-revisions-en/partXX/` to\n  `pan-wvc-2011/revisions`. For example, using `find` tool on Linux,\n  ```\n  $ cd pan-wvc-2011\n  $ find article-revisions-en -name \"*.txt\" -exec mv '{}' revisions/ \\;\n  ```\n\n#### Creating Unified Dataset WVC-2011\n\n* Change working directory to `cmd/unified-wvc2011`\n\n* Run `main.go` script to merge and create new dataset\n  ```\n  $ go run main.go\n  ```\n\nThe unified dataset will contain new two fields `additions` and `deletions`,\nwhich is the diff of old revision with new revision.\n\n#### Cleaning WVC-2011 Revisions \n\nCleaning wiki text revision, which is located in `revisions` directory, is\nrequired to speeding up processing features.\n\n* Create directory where the output of cleaning will be located,\n  ```\n  $ mkdir -p pan-wvc-2011/revisions_clean\n  ```\n\n* Change working directory to `cmd/wikiclean`\n\n* Run `main.go` script to clean revisions file\n  ```\n  $ go run main.go ../../pan-wvc-2011/revisions ../../pan-wvc-2011/revisions_clean\n  ```\n\n  The first parameter is the input location where the revision text to be\n  cleaning up, the second parameter is location where new revision that has\n  been cleaned up will be written.\n\n#### Generating WVC-2011 Features\n\nAfter one of PAN WVC dataset has been merged and cleaned up one can compute the\nvandalism features by runnning `main.go` script in root of repository.\n\n    $ go run main.go wvc2011_features.dsv\n\nGenerated feature values will be written to file `wvc2011_features.dat`.\n\n\n## List of Features\n\nFeature implementation is located in directory `feature`.\n\nList of features is implemented from paper by Mola-Velasco (2010) [1].\n\n### Metadata\n\n* \"anonim\": give a value '1' if an editor is anonymous or '0' otherwise.\n* \"comment_length\": length of character in the comment supplied with an edit.\n* \"size_increment\": compute the size different between inserted text minus\n  deletion.\n* \"size_ratio\": length of new revision divided by length of old revision.\n\n### Text\n\n* \"upper_lower_ratio\": ratio of uppercase to lowercase in inserted text.\n* \"upper_to_all_ratio\": ratio of uppercase to all character in inserted text.\n* \"digit_ratio\": ratio of digit to all character in inserted text.\n* \"non_alnum_ratio\": ratio of non alpha-numeric to all character in inserted\n  text.\n* \"char_diversity\": length of inserted text power of (1 / number of unique\n  character).\n* \"char_distribution_insert\": the distribution of character using\n  Kullback-Leibler divergence algorithm.\n* \"compress_rate\": compute the compression rate of inserted text using LZW.\n* \"good_token\": compute number of good or known Wikipedia token in inserted\ntext.\n* \"term_frequency\": compute frequency of inserted word in new revision.\n* \"longest_word\": the length of longest word in inserted text.\n* \"longest_char_seq\": length of the longest sequence of the same character in\n  inserted text.\n\n### Language\n\n* \"words_vulgar_frequency\" compute frequency of vulgar words in inserted text.\n* \"words_vulgar_impact\" compute increased of vulgar words in new revision.\n* \"words_pronoun_frequency\" compute frequency of colloquial and slang pronoun\n  in inserted text.\n* \"words_pronoun_impact\" compute increased of pronoun words in new revision.\n* \"words_bias_frequency\" compute frequency of colloquial words with high bias\n  in inserted text.\n* \"words_bias_impact\" compute increased of biased words in new revision.\n* \"words_sex_frequency\" compute frequency of sex-related, non-vulgar words in\n  inserted text.\n* \"words_sex_impact\" compute increased of sex-related words in new revision.\n* \"words_bad_frequency\" compute frequency of bad words, colloquial words or\nbad writing words.\n* \"words_bad_impact\" compute increased of bad words in new revision.\n* \"words_all_frequency\" compute frequency of vulgar, pronoun, bias, sex, and\nbad words in inserted text.\n* \"words_all_impact\" compute the increased of vulgar, pronoun, bias, sex, and\n  bad words in new revision.\n\n### Misc\n\n* \"class\": convert the classification from text to numeric. The \"regular\" class\nwill become 0 and the \"vandalism\" will become 1.\n\n## Extending The Feature\n\nThe feature directory provide implementation of above features, where one can\nsee how the feature works.\nOne must familiar with Go language to work with it.\n\nThere is also a template (named `template.go`) which can be copied to create a\nnew feature.\n\nIn this section we will see how to create new feature to compute the length of\ninserted text with feature name is `insert_length`.\n\nFirst, Copy `feature/template.go` to new name, for example\n`feature/insert_length.go`\n\nCreate new type using `Feature` as base type,\n\n\ttype InsertLength Feature\n\nand then register it to global features list including feature value type and\nfeature name that will be used later,\n\n\tfunc init() {\n\t\tRegister(&InsertLength{}, tabula.TInteger, \"insert_length\")\n\t}\n\nCreate a function `Compute` using our InsertLength type with the first\nparameter is input dataset,\n\n\tfunc (ftr *InsertLength) Compute(dataset tabula.Dataset) {\n\t\t// Compute the feature value. See other features on how to get\n\t\t// input records and iterate on them.\n\t}\n\nTest your feature by adding it to `main_test.go`,\n\n\tfunc TestInsertLength(t *testing.T) {\n\t\tmain.Generate(\"insert_length\", fInputDsv)\n\t}\n\nand to test it, run with,\n\n\t$ go test -v -run TestInsertLength -timeout 40m main_test.go -args wvc2010_features_test.dsv\n\nIf its works as intended add it to `wvc2010_features.dsv` or your own feature\nfile.\n\n## References\n\n[1] M. Mola-Velasco, \"Wikipedia vandalism detection through machine learn-\ning: Feature review and new proposals: Lab report for pan at clef 2010,\" ArXiv\npreprint arXiv:1210.5560, 2012.\n", 
  "id": 51240845
}