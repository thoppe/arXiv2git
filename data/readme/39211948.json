{
  "read_at": 1462546388, 
  "description": "Simple javascript neural net", 
  "README.md": "# C.-Elegance\n\nHas nothing to do with C, or being elegant.\nIt's just a hilarious pun on the worm used in biological testing.\n\n\nFilled with vanilla neural nets, implemented in javascript.\n\nGenerate sine network\n![](https://github.com/FlyingSpringrol/C.-Elegance/blob/master/Neurons.png)\n\nStill need to implement:\n   * momentum,\n   * softmax layer,\n   * Use of matrices for weight + bias containers\n   * Cross-entropy training option\n   * Gradient checking\n   * Other activation functions (linear and Tanh)\n   * Reduced learning rates over time,\n   * Random dropout.\n   * Regularization?\n   * Batch Training\n\nList of possible hyperparameters (design choices and variables) for networks:\n   * Batch vs online\n   * Learning rate (different for biases and weights?)\n   * Epochs\n   * Network size (hidden neurons)\n   * Error functions\n   * Bias initialization\n   * Weight initialization\n\nThings I still need to do:\nWrite a module that trains multiple different neural nets with slightly different\nhyper-parameters, and selects the best one.\n\n#### The future of Neural networks\n\nWe need to spend more time extracting features than classifying, the output of a neural network should almost be identical to the way we as humans percieve it\n(a collection of hierarchical, queriable features). The network should be able to:\n   a) Recognize sub-objects, associate this object with similar sub-objects stored in memory\n   b) Recognize textures, associate with similar textures in memory\n   c) Hold objects in memory (use recurrent networks)\n   d)\n\nHow to get there:\n   Hand code feature extraction?\n   Auto-encoders (variational and standar) and the discovery of local correlation of inputs?\n\n##Articles and links for more information about neural nets:\n\nGreat overview of neural nets:\nhttps://github.com/cazala/synaptic/wiki/Neural-Networks-101\nhttp://neuralnetworksanddeeplearning.com/\n\nReview of state-of-the-art performance and other details:\nhttp://arxiv.org/pdf/1207.0580.pdf\n\nComprehensive review page on performance of models on different tests:\nhttp://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n\nBatch vs online training:\n\nhttp://axon.cs.byu.edu/papers/Wilson.nn03.batch.pdf\n\nOverfitting+Generalization:\nhttp://www.mathworks.com/help/nnet/ug/improve-neural-network-generalization-and-avoid-overfitting.html\n\nRandom dropout:\nhttp://arxiv.org/pdf/1207.0580.pdf\n\nHyper-parameters:\n\nhttps://web.stanford.edu/class/ee373b/nninitialization.pdf\n\nBiological Neural Systems:\n\nPlanaria nervous system\nhttp://www.scholarpedia.org/article/Planaria_nervous_system\n\nSpiking neural nets\nhttp://www.neuron.yale.edu/neuron/static/papers/jcns/brette2007.pdf\n\n####Deep Belief nets\nhttps://www.cs.toronto.edu/~hinton/nipstutorial/nipstut3.pdf\n\nG. Hinton's site:\nhttp://www.cs.toronto.edu/~hinton/\n\nDeeplearning.net:\nhttp://deeplearning.net/tutorial/lenet.html\n\n####Convolutional Neural nets:\nStanford tutorial+class links:\nhttp://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/\nhttp://cs.stanford.edu/people/karpathy/convnetjs/\nhttp://cs231n.github.io/\nhttp://ufldl.stanford.edu/tutorial/\n\n####Auto-encoders\nhttp://jmlr.csail.mit.edu/proceedings/papers/v27/baldi12a/baldi12a.pdf\n\n####Machine Learning Links:\nhttp://www.cs.berkeley.edu/~jordan/courses/294-fall09/\n\nOther:\nhttp://arxiv.org/pdf/1411.6369v1.pdf\n###Ready made models:\n\nhttp://pybrain.org/\n\nhttp://caffe.berkeleyvision.org/\n\nhttp://cs.stanford.edu/people/karpathy/convnetjs/\n\nhttp://lasagne.readthedocs.org/en/latest/index.html\n\nhttp://deeplearning.net/software/pylearn2/\n\n\nImage Datasets:\n\nhttp://www.image-net.org/download.php\nhttp://archive.ics.uci.edu/ml/datasets.html\n", 
  "id": 39211948
}