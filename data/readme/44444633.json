{
  "read_at": 1462556150, 
  "description": "", 
  "README.md": "# RNNProteins\nWorld record on cb513 dataset, available at:\nhttp://www.princeton.edu/~jzthree/datasets/ICML2014/\n\nuse train.py when data is unzipped in data folder\n\n## Reproducing results\n\n### Installation\nPlease refer to [lasagne's](https://github.com/Lasagne/Lasagne/wiki/From-Zero-to-Lasagne-on-Ubuntu-14.04) for installation and setup of GPU environment on an Ubuntu 14.04 machine.\n\n\n### Getting repository\nGo to desired folder for repo and type in terminal.\n>> git clone https://github.com/alrojo/RNNProteins.git\n\n### Training models\n\n\n## Best network elaborated\nhttps://github.com/alrojo/RNNProteins/blob/master/configurations/avg1.py\n\n1. InputLayer\n2. 3x ConvLayer(InputLayer, filter_size=3-5-7) + Batch Normalization\n4. DenseLayer1([ConcatLayer, InputLayer]) + Batch Normalization\n5. LSTMLayerF(DenseLayer1, Forward)\n6. LSTMLayerB([DenseLayer1, LSTMLayerF], Backward)\n7. DenseLayer2([LSTMLayerF, LSTMLayerB], dropout=0.5)\n8. OutLayer(DenseLayer2)\n\nGradients are further normalized if too large and probabilities cutted. RMSProps is used and L2=0.0001\n\n## Project elaboration: Start Juli 2015 - still ongoing\n\nThis project is a continuation of Soren Sonderby's (github.com/skaae) previous results on CB513: http://arxiv.org/abs/1412.7828, supervised under Ole Winther (cogsys.imm.dtu.dk/staff/winther/).\n\nMy project was to recreate Soren's results and test: Convolutional layers across time, L2, \"vertical\" links (feeding forward LSTM to backwards LSTM), batchnormalization, different optimizers etc.\n\nIt took me approximately 3 months (with grid search of 200-300 models) before I managed to achieve similar results to Soren (apperently a DropoutLayer in the DenseLayer before the first LSTM messed with the model performance, which is why it took so long to get Soren's results).\n\nAfter achieving similar performance I started applying the various \"new\" techniques to my neural network. It took another 100-150 models gridsearching various combination which led to the model in \"Best network elaborated\", which increased performance by 1.5% compared to Sorens results. Notice the use of skip layer and that the convolutions are all on the input.\n\nI would like to test BN-RNN by baidu (http://arxiv.org/abs/1512.02595) and used Batch Normalization after the LSTM when lasagne starts to support masked Batch Norms.\n\nThe article will be submitted as a Methods paper to Nature together with some other research from Ole Winther's lab. My first draft is in the article folder.\n\nNext up: Make a 10 model average and finish the ImageNet12 article like drawings of the final network.\n", 
  "id": 44444633
}