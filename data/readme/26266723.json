{
  "read_at": 1462553774, 
  "description": "Code for reproducing results of NIPS 2014 paper \"Semi-Supervised Learning with Deep Generative Models\"", 
  "README.md": "NIPS'14-SSL\n==========\n\nCode for reproducing some key results of our NIPS 2014 paper on semi-supervised learning (SSL) with deep generative models.\n\nD.P. Kingma, D.J. Rezende, S. Mohamed, M. Welling  \n**Semi-Supervised Learning with Deep Generative Models**  \nAdvances in Neural Information Processing Systems 27 (**NIPS 2014**), Montreal  \n[http://arxiv.org/abs/1406.5298](http://arxiv.org/abs/1406.5298)\n\nPlease cite this paper when using this code for your research.\n\n_Warning_: This code is far from fully commented.\n\nFor questions and bug reports, please send me an e-mail at _dpkingma[at]gmail.com_.\n\n## Prerequisites\n\n1. Make sure that recent versions installed of:\n\t- Python (version 2.7 or higher)\n\t- Numpy (e.g. `pip install numpy`)\n\t- Theano (e.g. `pip install Theano`)\n\n2. Set `floatX = float32` in the `[global]` section of Theano config (usually `~/.theanorc`). Alternatively you could prepend `THEANO_FLAGS=floatX=float32 ` to the python commands below. \n\n3. Clone this repository, e.g.:\n```sh\ngit clone https://github.com/dpkingma/nips14-ssl.git\n```\n\n4. Set an environment variable `ML_DATA_PATH` that points to subdirectory `data/`. For example, if you checked out this repo to your home directory:   \n```sh\nexport ML_DATA_PATH=\"$HOME/nips14-ssl/data\"\n```\n\n# Qualitative results\n\n## Flying through latent space of M2 model\nTo generate movies of flying through latent-space of the M2 model, run:\n```sh\npython run_flying.py [dataset] 1 output.mkv\n```\nwhere `dataset` is 'mnist' or 'svhn', and `target_filename` is the filename to save the movie file to. NOTE: This script requires ffmpeg to be installed.\n\n## Analogies\nRun:\n```sh\npython run_analogies.py [dataset] 1\n```\n\n# Quantitative results\n\n## Learning M1 model\n\nTo train model M1 (a standard Variational Auto-Encoder / DLGM with sperical Gaussian latent space):\n```sh\npython run_gpulearn_z_x.py [dataset]\n```\nThe M1 model does not incorporate class label, but is used in the paper's experiments for feature extration.\n\n## Learning M1+M2 model, partially observed labels\n\nTo run the semi-supervised learning experiments with model M1+M2:\n```sh\npython run_2layer_ssl.py [n_labels] [seed]\n```\nwhere `n_labels` is the number of labels, and `seed` is the random seed for Numpy. To reproduce the experimental results in the paper, the number of labels should be in (100,600,1000,3000). The random seed can be any integer. Each experiment will run for 3000 epochs; since this code is not GPU-optimized, running many epochs might take a few days to complete. However, it is often not necessary to run the the algorithm for so many epochs to produce good results.\n\n## Learning M2 model with fully observed labels\n\nFor training a generative model with all labels:\n```sh\npython run_gpulearn_yz_x.py [dataset]\n```\nwhere `dataset` is 'mnist', 'svhn', 'norb' or 'norb_reshuffled'.\n\n\n## Evaluate test-set error of models trained with all labels\n\nFor evaluating the test-set classification error using already trained generative models of MNIST and SVHN:\n```sh\npython run_sl.py [dataset]\n``` \nThis iteratively builds, for each test-set image, an importance-sampled estimate of the posterior probability distribution over the class labels. This is an expensive procedure, but may be speed up by using fitting an inference model to the posterior distribution of class labels (which wasn't done in this case).\n\n\n\n\n", 
  "id": 26266723
}