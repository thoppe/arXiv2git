{
  "read_at": 1462558485, 
  "description": "TrackademiX - Insight Engineering Project", 
  "README.md": "Trackademix\n===========\n\n[Trackademix](http://trackademix.com) was developed as a project for [Insight Data Science](http://insightdataengineering.com) September through October 2014.\n\n## Intro\n\n**Trackademix** tracks scientific contributions and collaborations and portrays a more detailed picture \nthan the standard author-document-date, author-document-date, etc. approach can. \n**Trackademix** examines the web of collaborators with whom a particular author has worked over the years.\nThis paints a broader picture than a simple listing of the co-authors on any given document.\n\nFurthermore, as an author publishes his work, its relevance is typically determined by the number of citations made thereto.\nA simple count of the citations is not as informative as examining a chronological history of the citations.\nFor example, two authors may have been cited 300 times over the last five years.\nThe first author had one paper cited 250 times and two others cited 25 times each.\nThe second author has six (6) papers cited 50 times each.\nThe citation profile is very different in both cases; **Trackademix** allows us to visualize the citation history. \n**TrackademiX** application harvests available information from the [ArXiv](http://arxiv.org), \n[INSPIRE HEP](http://inspirehep.net) and \n[NASA ADS](http://adsabs.harvard.edu) databases, \nfilters out information unlikely to be part of the author''s scientific contributions history\n\n## The Query Variables\n\n**TrackademiX** provides the following variables for querying.\n\n- *Citation History* -- The documents for the author in question are scanned for citation history. \nThe documents must have less than 10 authors to be considered;\ndocuments with more than 10 authors are considered large collaborations.\nThe citations arising from such a large collaboration would dilute the actual contributions of the scientist.\nThe citation dates are noted and histogrammed in bins corresponding to one month.\nFor example, if an author''s work is cited on Jan 26, 2012 and Jan 2, 2012, \nthe entry corresponding to Jan-2012 will reflect those two citations.\nThe next image shows the visualization of the citation history for the reknown physicist, Stephen Hawking. \nThe citation history below shows how often his works are cited per month, starting from 1992 until 2014.\n\n![Alt Text](https://github.com/jsvirzi/insight/blob/master/images/citations_ui.png \"Citations\")\n\n- *Collaborators* -- The documents for the author in question are scanned for co-authors.\nAs is the case for *Citation History*, only documents with less than 10 authors are considered.\nAll the co-authors from the selected documents are considered ''collaborators''.\nThe frequency of collaboration is noted, indicating how many times the two authors have worked together.\n\n![Alt Text](https://github.com/jsvirzi/insight/blob/master/images/collaborators.png \"Collaborators\")\n\n- *Submissions* -- The documents that an author has contributed are listed.\nOnly documents with less than 10 authors are considered.\n\n![Alt Text](https://github.com/jsvirzi/insight/blob/master/images/publications.png \"Submissions\")\n\n# The Data Pipeline\n\n![Alt Text](https://github.com/jsvirzi/insight/blob/master/images/pipeline.png \"Data Pipeline\")\n\nThe pipeline has a batch component, which is intended to be processed weekly (specifically on Sunday).\nThe batch processing retrieves metadata information from the ArXiv via a REST API.\nIf recent works have cited older works, the records are updated in our database.\nThe batch component is intended to have eventual consistency on a *weekly* timescale.\n\nThe pipeline has two real-time components, both of which use STORM to process the requests.\nThe first real-time component scans the ArXiv records for recent activity,\nand performs informational updates to a temporary database.\nThis temporary database is eventually superceded by the database created during the batch processing.\nThe second real-time component is an on-demand query which will process the information when the user clicks\nthe ''submit'' button in the User Interface.\n\n## Batch \n\nThe details of the data pipeline for the batch process are as follows:\n\n- Kafka is used to collect data coming from RESTful API calls\n- A custom web crawler harvests information from Kafka and scrapes additional information from different web-based databases\n(ADS, INSPIRE, etc.)\n- Relevant information is filtered and subsequently stored into HDFS\n- PIG is used to merge information from the different databases stored in HDFS \n- MR Job Python scripts are used to process the different query variables using Map/Reduce algorithms\n- Hive is used to import the MapReduce jobs into HBase\n- Flask serves to query HBase and send the results as HTML/JavaScript pages to the browser\n\n## Real-Time\n\nThe real-time data pipeline shares the same data collection technology as the batch process,\nexcept that it has been implemented with STORM.\n- Kafka feeds the STORM ''spouts''\n- The STORM ''bolts'' perform the scraping\n- The ''bolts'' use Happybase to deliver the results into HBase\n- Flask serves to query HBase and send the results as HTML/JavaScript pages to the browser\n\n## On-Demand\n\nThe real-time data pipeline can be used to service on-demand queries via the user interface.\nThe information is processed from scratch.\nNote this can take a long time to process due to latencies in html page requests from external websites.\n\n![Alt Text](https://github.com/jsvirzi/insight/blob/master/images/generic_ui_ondemand.png \"Query implementing On-Demand Data Acquisition\")\n\n", 
  "id": 24040854
}