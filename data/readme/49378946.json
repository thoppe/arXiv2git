{
  "read_at": 1462552492, 
  "description": "Backpropagate derivatives through the Cholesky decomposition", 
  "README.md": "Differentiating the Cholesky decomposition\n==========================================\n\nTo find out about the different ways to differentiate expressions or code\ncontaining a Cholesky decomposition, please see the companion\n[overview on arXiv](http://arxiv.org/abs/1602.07527).\n\nThis directory contains a reverse-mode routine written in FORTRAN 77,\nmodeled after the fast LAPACK Cholesky routine `DPOTRF`, which uses blocked\nlevel-3 BLAS routines. LAPACK's implementation is widely used, such as by\nNumPy, Octave, and R. The file `dpofrt.f` in this repository is a new\ncompanion routine, which takes derivatives with respect to a Cholesky\ndecomposition from `dpotrf.f` and replaces them with derivatives with\nrespect to elements of the original positive definite input matrix.\n\nThe Python and Matlab directories show how to link this Fortran code,\nbut also provide pure Octave/Matlab and Python versions, that are still\nreasonably fast. The Matlab directory has a simple Gaussian Process demo,\nalso warning how many GP codes are inefficient.\n\n\nHow to use the derivative routine\n---------------------------------\n\nThe matlab and python sub-directories demonstrate how to compile this\nroutine for Matlab/Octave, call it, and check consistency. The matlab\ndirectory has a toy Gaussian process demo, to show how the new routines\ncould be used within a larger gradient computation. The Matlab and Python\ndirectories also include native implementataions. The Python is probably\nthe easiest to read, and also implements all of the forward- and\nreverse-mode updates discussed in the [note on arXiv](http://arxiv.org/abs/1602.07527).\n\nWhat follows is a high level description of how a reverse-mode routine\ncould be used in general, although see also the [overview on\narXiv](http://arxiv.org/abs/1602.07527).\n\nAssume you wish to differentiate a scalar function `f(chol(A(x)))` with\nrespect to a vector of input values `x`. `A` is positive-definite matrix\nthat depends on the inputs, and `L = chol(A)` is its lower-diagonal\nCholesky decomposition. If you can compute derivatives `L_bar = df/dL`, the\ncode provided here will convert them into `A_bar = df/dA`. These\nderivatives should in turn be passed on to reverse-mode/backpropagation\ncode that can compute `x_bar = df/dx`, the final required result.\n\n(NB don't compute the 3-dimensional array with elements `dA_{ij}/dx_k`\n-- use backpropagation to compute `df/dx_k` from `A_bar = df/dA` without\nproducing large intermediate objects! A lot of Gaussian process code in the\nwild does create large intermediate objects, or has inefficient loops that\nforward propagate some of the gradient computation. The demo in the Matlab\ndirectory includes a comparison of the different approaches.)\n\nWe're only providing the core primitive for pushing derivatives through the\nCholesky decomposition. You'll have to do the rest of the differentiation\nby hand, or provide this routine as a useful primitive to an automatic\ndifferentiation package.\n\nA great review of how to differentiate linear algebra is:\n\n[An extended collection of matrix derivative results for forward and\nreverse mode automatic differentiation, Mike B Giles (2008)](https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf)\n\nGiles provides pseudo-code and simple Matlab code for pushing derivatives\nthrough the Cholesky decomposition. However, this algorithm doesn't use\nblock matrix operations like LAPACK routines do, so is comparatively slow.\n\nTools that can do reverse-mode differentiation of the Cholesky (some faster\nthan others), include:\n- [AutoGrad](https://github.com/HIPS/autograd). Can differentiate multiple times.\n- [GPFlow](https://github.com/GPflow/GPflow)\n- [Stan](http://mc-stan.org/)\n- [Theano](http://deeplearning.net/software/theano/)\n\n\nMore details on the horrible FORTRAN\n------------------------------------\n\nLAPACK sticks to FORTRAN 77 and 6-letter function names. For the moment,\nwe've been masochistic and done the same. The last two or three letters of\na LAPACK routine represent the operation, and we have reversed these\nletters to indicate a reverse operation.\n\nThe idea is that lots of projects already use LAPACK. If our routines are\njust like those, in principle it should be possible to build them with\nexisting tool-chains for many use cases. However, many people use binary\nreleases of LAPACK, and working out how to compile new routines against\nBLAS and LAPACK is not easy for everyone. One option is to try to get\nroutines like the one in this directory shipped pre-compiled with binary\ndistributions of projects like Octave and SciPy. Another option is to\nreimplement the routines in other languages. The Python version, the\npseudo-code in the [arXiv note](http://arxiv.org/abs/1602.07527), or the\nMatlab version may be easier to read than the FORTRAN.\n\nIf you are using the FORTRAN, it's important to check the size of integers\nused by BLAS and LAPACK: they are traditionally 32-bit even on 64-bit\nmachines, but builds with 64-bit integers are becoming more common. If your\nBLAS and LAPACK binaries use 64-bit indexes, make sure to use the relevant\ncompiler flag (possibly `-fdefault-integer-8` or `-i8`), or use the derived\nfiles `dpotrf_ilp64.f` and `dpo2ft_ilp64.f`. See `make_ilp64.sh` for more\ndetails. \n\n\nStatus and open issues\n----------------------\n\nI've thrown this up in the hope it will be useful. It is not heavily tested\nand there are some known rough edges. I should probably make these github\nissues...\n\nI've only built it on my own linux machine. Tweaks to the Matlab/Octave and\nPython demos that help them work on other platforms would be appreciated.\n\nCurrently the routine only lets you specify a lower-triangular Cholesky\ndecomposition. Fixing that to allow upper-triangular matrices would be a\nquick job.\n\nThe unblocked routine `dpo2ft.f` is neat in that it only touches one\ntriangle of the input/output matrix of derivatives. The blocked routine\n`dpofrt.f` has a couple of messy parts, where it creates unneeded values in\nthe other triangle of the matrix, and then overwrites them with zeros at\nthe end. This code seems needlessly inefficient, but I didn't immediately\nsee how to make it better, given the standard BLAS routines that exist.\nHowever, I'm sure these parts could be improved.\n\nIs there any demand to allow the Cholesky decomposition to be in one\ntriangle and the gradients to be in another. That is, to have two `UPLO`\narguments one for `A` and one for `C`? If the messy details around the\ndiagonal (previous paragraph) could be resolved, maybe the Cholesky\ndecomposition and gradients could optionally be packed into one `Nx(N+1)`\narray, saving memory. Would it be worth the hassle?\n\nIn the end I think we should have \"LARMPACK\", a library containing the\nreverse-mode functions for everything in LAPACK. This project is an\nexperiment, starting with one LAPACK routine, and seeing how that goes\nbefore working on the rest. I don't think differentiating the rest of the\nroutines should be hard (in principle it can be done automatically,\nalthough some manual work can make the results neater).\n\n", 
  "id": 49378946
}