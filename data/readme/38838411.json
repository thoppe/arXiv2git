{
  "read_at": 1462546296, 
  "description": "Answer Sentence Selection using Deep Learning", 
  "README.md": "Deep Learning for Answer Sentence Selection Reconstruction\n==========================================================\n\nThis work started as an attempt to reproduce Yu et al.'s http://arxiv.org/abs/1412.1632\n\nUsed word embeddings: pre-trained GloVe vectors from http://nlp.stanford.edu/projects/glove/\n\nSo far implemented:\n  * Bag of words + basic gradient descent learning classification\n  * Bag of words + basic gradient descent learning classification + word counts logistic regression\n\nDevelopment Instructions\n------------------------\n\nFor sentence selection development,\nused dataset: TREC-based originally by Wang et al., 2007; in the form\nby Yao et al., 2013 as downloaded from https://code.google.com/p/jacana/\n\nPreprocessing (not required):\n  * Run save.py first with updated filepath constants (const.py) if you have different dataset (requires jacana formating)\n\nTrain and test:\n  * Run train.py for training from TREC TRAIN dataset and testing from TREC TEST dataset\n  * train.py generates truth.txt and res.txt, to evaluate using the official trec_eval tool, run\n\n\ttrec_eval -a truth.txt res.txt\n\nTODO:\n  * CNN instead of bag of words unigram averaging for aggregate embeddings. \n\nResults (evaluated using stock TREC scripts):\n\n|                 | MRR    | MAP    |\n|-----------------|--------|--------|\n| TRAIN           | 0.7312 | 0.6551 |\n| TRAIN-ALL       | 0.7308 | 0.6566 |\n| TRAIN+count     | 0.7763 | 0.7165 |\n| TRAIN-ALL+count | 0.8128 | 0.7258 |\n\n\nProperty selection in yodaqa/moviesC:\n-------------------------------------\n\nFolow these steps if you want to retrain currently used weights:\n\n  * Gather input data (labelled tuples) according to the instructions\n    in YodaQA data/ml/embsel/README.md.\n\n  * Run './std_run.sh -p PATH' (PATH is the directory of dumped yodaqa files).\n    You can alter the training constants in basicgrad.py and train.py.\n\n  * If you are happy with the results, you copy the generated file data/Mbtemp.txt\n    to yodaqa src/main/resources/cz/brmlab/yodaqa/analysis/rdf/Mbprop.txt\n\nIn summary, use this:\n\n\t./std_run.sh -p ../yodaqa/data/ml/embsel/propdata\n\tcp data/Mbtemp.txt ../yodaqa/src/main/resources/cz/brmlab/yodaqa/analysis/rdf/Mbprop.txt\n\n### Snapshot of results based on curated:\n\n(With a random 1:1 train:test split of the original curated-train.)\n\n**Used dataset:**  \n\n\ttrain questions: 270 train sentences: 19624\t(generated with curated-measure.sh train)\n\ttest questions: 222 test sentences: 17561\t(generated with curated-measure.sh train)\n\t2.7902739024% of the properties contains correct answers\n\trandom test mrr = 0.0475542678953\n\n**Current results:**  \n\n\tMMR after unigram learning train: 0.600856454434\n\tMMR after unigram learning test: 0.582881935037\n\n\nSentence selection on yodaqa/curated:\n-------------------------------------\n\nFolow these steps if you want to retrain currently used weights:\n\n  * Gather input data (labelled tuples) according to the instructions\n    in YodaQA data/ml/embsel/README.md.\n\n  * Run './std_run.sh -p PATH' (PATH is the directory of dumped yodaqa files).\n    You can alter the training constants in basicgrad.py and train.py.\n\n  * If you are happy with the results, you copy the generated file data/Mbtemp.txt\n    to yodaqa src/main/resources/cz/brmlab/yodaqa/analysis/passextract/Mb.txt\n\nIn summary, use this (with YodaQA's f/sentence-selection branch):\n\n\t./std_run.sh ../yodaqa/data/ml/embsel/sentdata\n\tcp data/Mbtemp.txt ../yodaqa/src/main/resources/cz/brmlab/yodaqa/analysis/passextract/Mb.txt\n\n### Snapshot of results based on curated:\n\n(With a random 1:1 train:test split of the original curated-train.)\n\n**Used dataset:**  \n\n\ttrain questions: 186 train sentences: 43843\t(generated with curated-measure.sh train)\n\ttest questions: 429 test sentences: 88779\t(generated with curated-measure.sh test)\n\t5.21294450264% of the properties contains correct answers\n\trandom test mrr = 0.0760195275186\n\n**Current results:**  \n\nbaseline (clue1+0.25*clue2):\n\n\tMRR unigram+clues train 0.249327071552\n\tMRR unigram+clues test 0.29659580682\n\nglove only:  \n\n\tMMR after unigram learning train: 0.224787152966\n\tMMR after unigram learning test: 0.222749753007\n\nglove+clue1:  \n\n\tMRR unigram+clues train 0.358206351223\n\tMRR unigram+clues test 0.388948882077\n\n", 
  "id": 38838411
}