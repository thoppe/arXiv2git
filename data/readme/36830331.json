{
  "read_at": 1462549166, 
  "description": "", 
  "README.md": "# A Hierarchical Neural Autoencoder for Paragraphs and Documents\n\nImplementations of the three models presented in the paper \"A Hierarchical Neural Autoencoder for Paragraphs and Documents\" by Jiwei Li, Minh-Thang Luong and Dan Jurafsky, ACL 2015\n\n## Requirements:\nGPU \n\nmatlab >= 2014b\n\nmemory >= 4GB\n\n\n\n## Folders\nStandard_LSTM: Standard LSTM Autoencoder\n\nhier_LSTM: Hierarchical LSTM Autoencoder \n\nhier_LSTM_Attention: Hierarchical LSTM Autoencoder with Attention \n\n## DownLoad [Data](http://cs.stanford.edu/~bdlijiwei/data.tar)\n- `dictionary`: vocabulary\n- `train_permute.txt`: training data for standard Model. Each line corresponds to one document/paragraph\n- `train_source_permute_segment.txt`: source training data for hierarchical Models. Each line corresponds to one sentence. An empty line starts a new document/sentence. Documents are reversed. \n- `test_source_permute_segment.txt`: target training data for hierarchical Model.\n\nTraining roughly takes 2-3 weeks for standard models and 4-6 weeks for hierarchical models on a K40 GPU machine.\n\n\nFor any question or bug with the code, feel free to contact jiweil@stanford.edu\n\n```latex\n@article{li2015hierarchical,\n    title={A Hierarchical Neural Autoencoder for Paragraphs and Documents},\n    author={Li, Jiwei and Luong, Minh-Thang and Jurafsky, Dan},\n    journal={arXiv preprint arXiv:1506.01057},\n    year={2015}\n}\n```\n", 
  "id": 36830331
}