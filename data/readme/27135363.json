{
  "read_at": 1462553888, 
  "description": "Scripts for comparing the SSPSC (Simple Step Population Size Change) and StSI (Structured Symmetric Island) models", 
  "README.md": "SSPSC vs StSI\n=============\n\nThese python scripts were used to perform the simulations described in our \npaper *Demographic inference using genetic data from a single individual: \nseparating population size variation from population structure* published in Theoretical Population and Biology.\nYou can fin the original paper at the following link:\n  <a href=http://www.sciencedirect.com/science/article/pii/S0040580915000581>\n  Mazet O., Rodriguez W., Chikhi L. (2015)</a> and a preprint in biorxiv \n  <a href=http://arxiv.org/abs/1412.1243>  </a> see these links for theoretical details).\n\nThe code may be used in order to reproduce the analyses performed in the paper\nand in the Supplementary Materials. \n\nThe code works fine under:\n* Python 2.7\n\nYou will also need:\n\n* numpy (version 1.9.2)\n* scipy (version 0.15.1)\n\nThe file *experiment_settings.txt* allows to specify the parameters that you may\nwant to use for simulate the values.\n\nThe script *experiment.py* can be modified if you want to change the name of\nthe output file. It is recommended to run many experiments in parallel in order\nto save time. To make an experiment just do \n\n*./experiments.py*\n\nOne single experiment produce two files. One contains the simulated T2 values\n(i.e. the values of the coalescence times of two individuals under the actual\n  model) and the other file contains the results of the experiment.\n\n  Explanation of the output file\n  ------------------------------\n\n  The output file (named by default *experiment_OUT.txt*) contains 10 columns:\n\n  * Original_variable: Depending on the model used to produce the T2 values\n  this could be SSPSC (Single Step Population Size Change model) or StSI\n  (Structured Symmetrical Island model). The model can be changed in the\n  configuration file. If everything goes fine, the method\n  should be able to identify the model used to produce data by doing the\n  analysis of the simulated values of $T_2$.\n\n  * real_parameters: The parameters of the model used for simulate data.\n\n  * number_of_observations: The number of $T_2$ values simulated by the original\n  model.\n\n  * log-likelihood_of_real_params: The likelihood of the real parameters (the\n    parameters of the model used for simulate the data), computed from the $T_2$\n    values simulated by the model itself.\n\n  * Estim_params_T2_SSPSC: The estimated parameters (by Maximum Lileklihood\n    Estimation) assuming the $T_2$ values are coming from a SSPSC model.\n\n  * log-likelihood_T2_SSPSC: The likelihood of the parameters estimated in the\n  previous column computed from the $T_2$ values.\n\n  * p-value_T2_SSPSC: Assuming that simulated $T_2$ values come from a SSPSC\n  model with the parameters estimated in the previous step, we do a KS-test\n  (*Kolmogorov-Smirnov* test) in order to see if the model fits the data.\n\n  Now, the same process is repeated, but assuming data come from a StSI model\n\n  * Estim_params_T2_StSI: The estimated parameters (by Maximum Lileklihood\n    Estimation) assuming the $T_2$ values are coming from a StSI model.\n\n  * log-likelihood_T2_StSI: The likelihood of the parameters estimated in the\n  previous column computed from the $T_2$ values.\n\n  * p-value_T2_StSI: Assuming that simulated $T_2$ values come from a StSI\n  model with the parameters estimated in the previous step, we do a KS-test\n  (*Kolmogorov-Smirnov* test) in order to see if the model fits the data.\n\n  At this stage, we expect the KS test will reject the wrong model and will not\n  reject the right one. For example, if data where simulated under the SSPSC\n  model, the p-value_T2_StSI should be low (let's say, lower than 0.05) while\n  the p-value_T2_SSPSC should not be too low (let's say, higher than 0.05).\n  Given that in some cases, a KS test will not be enough to distinguish both\n  models, an AIC (*Akaike Information Criterion*) approach may be used (you can\n  find some explanations in\n    <a href=https://en.wikipedia.org/wiki/Akaike_information_criterion>\n    Wikipedia</a>). In our case, given that we are comparing two model that have\n    the same number of parameters, using the AIC is equivalent to make a choise\n    based just in the likelihood of estimated parameters under both models.  \n\n  * AIC_selected_model: If it is 0 means that the selected model was the SSPSC\n  and if it is 1, means that the selected model was the StSI.\n\n  * AIC_relative_prob: Give some measure of how the chosen model is more\n  likely to explain the data than the other.\n\n\nYou may do as many experiments as you want. Then you can process the results\nwith you favorite statistics software. I used some python scripts coded by\nmyself (./lib/results_handler.py). Sorry if they are not well documented, I\nwill explain how to use it soon.\n", 
  "id": 27135363
}