{
  "read_at": 1462555857, 
  "description": "The project SamuKnows can distinguish the sub-processes taking place in the sensory input. This project is merged from the SamuBrain and the SamuVocab projects.", 
  "README.md": "# SamuKnows\n\nThe project SamuKnows can distinguish the sub-processes taking place in the sensory input. \nThis project is merged from the [SamuBrain](https://github.com/nbatfai/SamuBrain) and the \n[SamuVocab](https://github.com/nbatfai/SamuVocab) projects.\n\n## SamuKnows, exp. 8, cognitive mental organs: MPU (Mental Processing Unit), acquiring higher-order knowledge\n\n### SamuBrain\n\nCurrently I am working on a manuscript titled \"Samu in His Prenatal Development\" where I want to establish a definition of a mathematical machine for learning. It is for this reason that I have made various experiments on the subject.\n\nThe project called SamuBrain is an implementation of a version of the definition in question. In this experiment, I have been investigating the possibility of developing a \"cognitive mental organ\" which is called Mental Processing Unit (or briefly MPU) in the terminology of the sources of this project.\n\nAn MPU consisting of two lattices, one input and one output lattice. The input lattice (called reality) represents the perception of the agent. Each cell of the output lattice (called Samu's predictions) is equipped with \na [COP](http://arxiv.org/abs/1108.2865)-based\n[SAMU](http://arxiv.org/abs/1511.02889)  engine to predict the next state of the corresponding input cell. Three different inputs are shown to the agent in the experiment:\n\n1. 5 gliders move in the input lattice in accordance with Conway's Game of Life (https://github.com/nbatfai/SamuLife)\n2. 9 simple \"pictures\" are shown (https://github.com/nbatfai/SamuStroop)\n3. a simple \"film\" is shown (https://github.com/nbatfai/SamuMovie)\n\nIn the project SamuBrain, the agent must learn and recognize these complex patterns. It is shown in video at https://youtu.be/_W0Ep2HpJSQ\n\n### SamuKnows\n\nHere the video of SamuMovie is divided into its three subcomponents: the house, the car and the man. \nFirst I teach the agent to recognize these standalone subcomponents. \nThen the agent should detect the house and the moving car and man in the original SamuMovie video.\n\n## Usage\n\n### vSamuBrain\n\nUsing the selection mechanism of SamuVocab for detecting sub-processes looks like a dead-end so I return to the original SamuBrain: [https://youtu.be/MLOeNNqd2Nw](https://youtu.be/MLOeNNqd2Nw)\n\n```\ngit clone https://github.com/nbatfai/SamuKnows.git\ncd SamuKnows/\ngit checkout vSamuBrain\n~/Qt/5.5/gcc_64/bin/qmake SamuLife.pro\nmake\n./SamuKnows 2>out\n```\n\n```\ntail -f out|grep \"HIGHER-ORDER NOTION MONITOR\"\n```\n\n```\ntail -f out|grep SENSITIZATION\n```\n\n\n## Previous other experiments\n\nSamu (Nahshon)\nhttp://arxiv.org/abs/1511.02889,\nhttps://github.com/nbatfai/nahshon\n\n---\n\nSamuLife\nhttps://github.com/nbatfai/SamuLife,\nhttps://youtu.be/b60m__3I-UM\n\nSamuMovie\nhttps://github.com/nbatfai/SamuMovie,\nhttps://youtu.be/XOPORbI1hz4\n\nSamuStroop\nhttps://github.com/nbatfai/SamuStroop,\nhttps://youtu.be/6elIla_bIrw,\nhttps://youtu.be/VujHHeYuzIk\n\nSamuBrain\nhttps://github.com/nbatfai/SamuBrain\n\nSamuCopy\nhttps://github.com/nbatfai/SamuCopy\n\n---\n\nSamuTicker\nhttps://github.com/nbatfai/SamuTicker\n\nSamuVocab\nhttps://github.com/nbatfai/SamuVocab\n\n--- \n\nSamuCam\nhttps://github.com/nbatfai/SamuCam\n\n![samucam1-nandi4](https://cloud.githubusercontent.com/assets/3148120/14001514/91fbb354-f146-11e5-9a0a-5d551bee494a.png)\n\n\nRobopsychology One\nhttps://github.com/nbatfai/Robopsychology", 
  "id": 53354989
}