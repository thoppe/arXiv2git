{
  "read_at": 1462552903, 
  "description": "An attempt to extract style and content from music in a separable way", 
  "README.md": "IDEA: use a combination of convolutional and recurrent networks to\nderive a set a features for a song classification task (e.g. predicting\ngenre/style or mood)\n\nThen use variations of these features along with new songs as input to\nalter the style of the song, similar to http://arxiv.org/abs/1508.06576\nfor artistic images.\n\n\nPlan is to split a song into 5-10 second chunks (enough to keep a\ncoherent picture of the timbre and style of the music) and use a deep\nconvolutional network to generate a 1-dimensional embedding (currently\nI have a small version of GoogleNet/Inception written out, but I may need\na lot more data for that).\n\nThese embeddings are then sequentially fed into a recurrent neural\nnetwork with either LSTM or GRU cells. Output can either be a\nconcatenation of all the RNN outputs or the last output.\n\nTwo initial things to try: inputting song as a waveform and doing 1-D\nconvolution, or inputting song as a spectrogram. It seems that ideally\nwith enough data the waveform should work, and the network should be\nable to \"learn\" how to take the FFT. However, this seems implausible\ngiven the amount of data I currently have (~7000 songs). Plus, humans\nmore or less have the FFT hardwired into our inner ears, and don't use\nextensive brainpower to compute it.\n\nSongs currently come from the MIREX competition, but I think I can get a\nlot more using the freesound api, and almost all music I get is going to\ncome prelabeled with some sort of metadata, and even if it's not I can\nlook up the artist/track on an internet database to retrieve the\nmetadata.\n\n\nI've created a time-distributed 1D convolutional layer in Keras in the\nfile `keras_extra_layers.py`\n", 
  "id": 49546349
}