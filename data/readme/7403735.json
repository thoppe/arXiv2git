{
  "read_at": 1462551362, 
  "description": "The Cloudbreak Hadoop-based Genomic Structural Variation Caller", 
  "README.md": "#cloudbreak\n\nCloudbreak is a Hadoop-based structural variation (SV) caller for Illumina\npaired-end DNA sequencing data. Currently Cloudbreak calls genomic insertions\nand deletions; we are working on adding support for other types of SVs.\n\nCloudbreak contains a full pipeline for aligning your data in the form of FASTQ\nfiles using alignment pipelines that generate many possible mappings for every\nread, in the Hadoop framework. It then contains Hadoop jobs for computing\ngenomic features from the alignments, and for calling insertion and deletion\nvariants from those features.\n\nYou can get Cloudbreak by downloading a pre-packaged release from the \"releases\"\ntab in the GitHub repository, or by building from source as described below.\n\nYou can read more about how Cloudbreak works in our paper, which is currently\navailable as a preprint at http://arxiv.org/abs/1307.2331.\n\n##Building From Source\n\nTo build the latest version of Cloudbreak, clone the GitHub repository. You'll\nneed to install Maven to build the executables. (http://maven.apache.org/)\nEnter the top level directory of the Cloudbreak repository and type the command:\n\n`mvn package`\n\nThis should compile the code, execute tests, and create a distribution file,\n `cloudbreak-$VERSION-dist.tar.gz` in the `target/` directory. You can then copy\n that distribution file to somewhere else on your system, unpack it with\n `tar -xzvf cloudbreak-$VERSION-dist.tar.gz` and access the Cloudbreak jar file\n  and related scripts and properties files.\n\n##Dependencies\n\nCloudbreak requires a cluster Hadoop 0.20.2 or Cloudera CDH3 to run (the older\nmapreduce API). If you don't have a Hadoop cluster, Cloudbreak can also use the\nApache Whirr API to automatically provision a cluster on the Amazon Elastic\nCompute Cloud (EC2). See the section on using WHIRR below.\n\nCurrently Cloudbreak requires that the Hadoop cluster have the native libraries to\nsupport Snappy compression installed. This comes standard with many Hadoop distributions,\nincluding Cloudera's distribution and AMIs for use on EC2. However, if you are running\nan older installation of Apache's Hadoop distribution, you may not have snappy installed.\nIf so, you will have to install snappy to run Cloudbreak, either by reinstalling from\nthe distribution package or as an add-on using the hadoop-snappy package:\n\nhttps://code.google.com/p/hadoop-snappy/\n\nWe are working to fix this dependency on snappy so that those without snappy support can\nrun Cloudbreak (without the benefits of its fast compression algorithms, of course).\n\nIf you wish to run alignments using Cloudbreak, you will need one of the following\nsupported aligners:\n\n* BWA (Recommended): http://bio-bwa.sourceforge.net/\n* GEM: http://algorithms.cnag.cat/wiki/The_GEM_library\n* RazerS 3: http://www.seqan.de/projects/razers/\n* Bowtie2: http://bowtie-bio.sourceforge.net/bowtie2/index.shtml\n* Novoalign: http://www.novocraft.com\n\n##User Guide\n\nYou can use Cloudbreak in several different ways, depending on whether you want\nto start with FASTQ files and use Hadoop to help parallelize your alignments, or if you already\nhave an aligned BAM file and just want to use Cloudbreak to call variants. In addition,\nthe workflow is slightly different depending on whether you want to run on a local\nHadoop cluster or want to run using a cloud provider like Amazon EC2. Later in this\nfile, we've listed a set of scenarios to describe options for running the Cloudbreak\npipeline. Find the scenario that best fits your use case for more details on how to\nrun that workflow. For each scenario, we have created a template script that contains\nall of the steps and parameters you need, which you can modify for your particular data set.\n\n##Running on a cloud provider like Amazon EC2 with Whirr\n\nCloudbreak has support for automatically deploying a Hadoop cluster on\ncloud providers such as Amazon EC2, transferring your data there, running the Cloudbreak algorithm, and\ndownloading the results.\n\nOf course, renting compute time on EC2 or other clouds costs money, so please be\nfamiliar with the appropriate usage and billing policies of your cloud provider\nbefore attempting this.\n\nWE ARE NOT RESPONSIBLE FOR UNEXPECTED CHARGES THAT YOU MIGHT INCUR ON EC2 OR\nOTHER CLOUD PROVIDERS.\n\nMany properties that affect the cluster created can be set in the file\n`cloudbreak-whirr.properties` in this distribution. You will need to edit this file\nto set your AWS access key and secret access key (or your credentials for other\ncloud provider services), and to tell it the location of the public and\nprivate SSH keys to use to access the cluster. You can also control the number\nand type of nodes to include in the cluster. The default settings in the file\ncreate 15 nodes of type m1.xlarge, which is sufficient to fully process a 30X\nsimulation of human chromosome 2, including read alignment and data transfer time,\nin under an hour. We have only tested this capability using EC2; other cloud providers\nmay not work as well. You can also direct Whirr to use Amazon EC2's spot instances, which are\ndramatically cheaper than on-demand instances, although they carry the risk of\nbeing terminated if your price is out-bid. Using recent spot pricing, it cost\nus about $5 to run the aforementioned chromosome 2 simulation. We recommend\nsetting your spot bid price to be the on demand price for the instance type you\nare using to minimize the chance of having your instances terminated.\n\nPlease consult Amazon's EC2 documentation and the documentation for Whirr for\nmore information on how to configure and deploy clusters in the cloud.\n\n##Running on a Small Example Data Set\n\nTo facilitate testing of Cloudbreak, we have publicly hosted the reads from the simulated\ndata example described in the Cloudbreak manuscript on a bucket in Amazon's S3 storage\n service at s3://cloudbreak-example/. We have also provided an example script that creates\n a cluster in Amazon EC2, copies the data to the cluster, runs the full Cloudbreak\n  workflow including alignments with BWA, and copies the variant calls back to the\n  local machine before destroying the cluster. The script, called `Cloudbreak-EC2-whirr-example.sh`\n  is in the scripts directory of the Cloudbreak distribution. Of course, you will still\n  need to edit the `cloudbreak-whirr.properties` file with your EC2 credentials, and verify\n  that the cluster size, instance types, and spot price are to your liking before\n  executing the example.\n\n###Scenario 1: Compute alignments in Hadoop, using a local Hadoop cluster\n\nTo install aligner dependencies for use by Cloudbreak, first generate the index\nfor the genome reference you would like to run against. Then, copy all of the\nrequired index files, and the executable files for the aligner into HDFS using\nthe `hadoop dfs -copyFromLocal` command. For BWA you will need all of the index files\ncreated by running `bwa index`. You will also need an 'fai' file for the reference,\ncontaining chromosome names and lengths, generated by `samtools faidx`.\n\nIf your reference file is `reference.fa`, and `bwa aln` has created the files\n\n    reference.fa.amb\n    reference.fa.ann\n    reference.fa.bwt\n    reference.fa.pac\n    reference.fa.sa\n\nand `reference.fa.fai` as described above, issue the following\ncommands to load the necessary files into HDFS:\n\n    hdfs -mkdir indices/\n    hdfs -mkdir executables/\n    hdfs -copyFromLocal reference.fa.amb indices/\n    hdfs -copyFromLocal reference.fa.ann indices/\n    hdfs -copyFromLocal reference.fa.bwt indices/\n    hdfs -copyFromLocal reference.fa.pac indices/\n    hdfs -copyFromLocal reference.fa.sa indices/\n    hdfs -copyFromLocal reference.fa.fai indices/\n    hdfs -copyFromLocal /path/to/bwa/executables/bwa executables/\n    hdfs -copyFromLocal /path/to/bwa/executables/xa2multi.pl executables/\n\nThe basic workflow is:\n\n1. Load the FASTQ files into HDFS\n2. Run one of the Cloudbreak alignment commands to align your reads\n3. Create a readGroup file to describe the location and insert size characteristics of your reads, and copy it into HDFS.\n4. Run the GMM fitting feature generation step of the Cloudbreak process.\n5. Extract deletion calls from the features created in step 4.\n6. Copy the deletion calls from HDFS to a local directory.\n5. Extract insertion calls from the features created in step 4.\n6. Copy the insertion calls from HDFS to a local directory.\n7. Optionally, export the alignments back into a BAM file in your local filesystem.\n\nWe have created a script to run through the full process of executing the\nCloudbreak pipeline from FASTQ files to insertion and deletion calls. The\nscript is named `Cloudbreak-full.sh` and can be found in the scripts directory\nof the Cloudbreak distribution. To customize the script for your needs, copy\nit to a new location and edit the variables in the first three sections:\n\"EXPERIMENT DETAILS\", \"LOCAL FILES AND DIRECTORIES\", and\n\"HDFS FILES AND DIRECTORIES\".\n\n###Scenario 2: Call variants on existing alignments, using a local Hadoop cluster\n\nFor this scenario you don't need to worry about having an aligner executable or\naligner-generated reference in HDFS. You will however, need a chromosome length\n'fai' file, which you can generate by running `samtools faidx` on your reference\nFASTA files and then copying to HDFS:\n\n    hdfs -copyFromLocal reference.fa.fai indices/\n\nAfter that, the workflow is:\n\n1. Load your BAM file into HDFS and prepare it for Cloudbreak\n2. Create a readGroup file to describe the location and insert size characteristics of your reads.\n3. Run the GMM fitting feature generation step of the Cloudbreak process.\n4. Extract deletion calls from the features created in step 3.\n5. Copy the deletion calls from HDFS to a local directory.\n6. Extract insertion calls from the features created in step 3.\n7. Copy the insertion calls from HDFS to a local directory.\n\nTo prepare alignments for Cloudbreak, they must be sorted by read name. You can then use the\n `readSAMFileIntoHDFS` Cloudbreak command.\n\nA templates for this scenario is available in the script `Cloudbreak-variants-only.sh`\nlocated in the scripts directory of the Cloudbreak distribution.\n\n###Scenario 3: Compute alignments in Hadoop, using a cloud provider like EC2\n\nFirst, see the section \"Running on a Cloud Provider like Amazon EC2 with Whirr\" above, and modify the file\n`cloudbreak-whirr.properties` to include your access credentials and the appropriate cluster\nspecifications. After that, the workflow is similar to the workflow described for scenario #1\nabove, with the additional first steps of copying your reads and dependency files to the cloud and\ncreating a cluster before processing begins, and then destroying the cluster after processing has\ncompleted.\n\nYou can see an example workflow involving EC2 by examining the script\n`Cloudbreak-EC2-whirr.sh`. This begins by transferring your reads to Amazon S3. It then\nuses Apache Whirr to launch an EC2 Hadoop cluster, copies the necessary executable files\nto EC2, and runs the algorithm.\n\n###Scenario 4: Call variants on existing alignments, using a cloud provider like EC2\n\nAgain, please read the section \"Running on a Cloud Provider like Amazon EC2 with Whirr\" above to learn how to\nupdate the `cloudbreak-whirr.properties` file with your credentials and cluster specifications. After that,\nfollow the template in the script `Cloudbreak-EC2-whirr-variants-only.sh` to create a workflow\ninvolving calling variants in the cloud.\n\n##Output Files\n\nThe output from running Cloudbreak using one of the scripts above will be found in the files named\n\n    READ_GROUP_LIBRARY_dels_genotyped.bed\n    READ_GROUP_LIBRARY_ins_genotyped.bed\n\nwhere READ_GROUP and LIBRARY are the names of the reads in your experiment. The\nformat of the files is tab-delimited with the following columns:\n\n*  CHROMOSOME: The chromosome of the deletion call\n*  START: The start coordinate of the deletion call\n*  END: The end coordinate of the deletion call\n*  NUMBER: The cloudbreak identifier of the deletion call\n*  LR: The likelihood ratio of the deletion (higher indicates a call more likely to be true)\n*  TYPE: Either \"INS\" or \"DEL\"\n*  W: The average weight of the estimated GMM mixing parameter alpha, used in genotyping\n*  GENOTYPE: The predicted genotype of the call\n\n##Contact information\n\nPlease contact cwhelan at gmail.com with any questions on running cloudbreak.\n\n##Reference Guide\n\nAll of Cloudbreak's functionality is contained in the executable jar file in the\ndirectory where you unpacked the Cloudbreak distribution. Use the 'hadoop'\ncommand to run the jar file to ensure that the necessary Hadoop dependencies\nare available to Cloudbreak.\n\nTo invoke any Cloudbreak command, use a command line in this format:\n\n`hadoop cloudbreak-${project.version}.jar [options] [command] [command options]`\n\nWhere `command` is the name of the command, `command options` are the arguments specific\nto that command, and `options` are general options, including options for how to run\nHadoop jobs. For example, if you'd like to specify 50 reduce tasks\nfor one of your commands, pass in `-Dmapred.reduce.tasks=50` as one of the general options. \n\nEach command is detailed below and its options are listed below. You can view this information by typing\n`hadoop jar cloudbreak-${project.version}.jar` without any additional parameters.\n\n\n        readPairedEndFilesIntoHDFS      Load paired FASTQ files into HDFS\n          Usage: readPairedEndFilesIntoHDFS [options]\n      Options:\n            *     --HDFSDataDir                  HDFS directory to load reads into\n                  --clipReadIdsAtWhitespace      Whether to clip all readnames at\n                                                 the first whitespace (prevents trouble\n                                                 with some aligners)\n                                                 Default: true\n                  --compress                     Compression codec to use on the\n                                                 reads stored in HDFS\n                                                 Default: snappy\n            *     --fastqFile1                   File containing the first read in\n                                                 each pair\n            *     --fastqFile2                   File containing the second read in\n                                                 each pair\n                  --filesInHDFS                  Use this flag if the BAM file has\n                                                 already been copied into HDFS\n                                                 Default: false\n                  --filterBasedOnCasava18Flags   Use the CASAVA 1.8 QC filter to\n                                                 filter out read pairs\n                                                 Default: false\n                  --outFileName                  Filename of the prepped reads in\n                                                 HDFS\n                                                 Default: reads\n                  --trigramEntropyFilter         Filter out read pairs where at\n                                                 least one read has a trigram entropy less\n                                                 than this value. -1 = no filter\n                                                 Default: -1.0\n\n        readSAMFileIntoHDFS      Load a SAM/BAM file into HDFS\n          Usage: readSAMFileIntoHDFS [options]\n      Options:\n            *     --HDFSDataDir   HDFS Directory to hold the alignment data\n                  --compress      Compression codec to use for the data\n                                  Default: snappy\n                  --outFileName   Filename to give the file in HDFS\n                                  Default: alignments\n            *     --samFile       Path to the SAM/BAM file on the local filesystem\n\n        bwaPairedEnds      Run a BWA paired-end alignment\n          Usage: bwaPairedEnds [options]\n      Options:\n            *     --HDFSAlignmentsDir    HDFS directory to hold the alignment data\n            *     --HDFSDataDir          HDFS directory that holds the read data\n            *     --HDFSPathToBWA        HDFS path to the bwa executable\n                  --HDFSPathToXA2multi   HDFS path to the bwa xa2multi.pl executable\n            *     --maxProcessesOnNode   Ensure that only a max of this many BWA\n                                         processes are running on each node at once.\n                                         Default: 6\n                  --numExtraReports      If > 0, set -n and -N params to bwa sampe,\n                                         and use xa2multi.pl to report multiple hits\n                                         Default: 0\n            *     --referenceBasename    HDFS path of the FASTA file from which the\n                                         BWA index files were generated.\n\n        novoalignSingleEnds      Run a Novoalign alignment in single ended mode\n          Usage: novoalignSingleEnds [options]\n      Options:\n            *     --HDFSAlignmentsDir            HDFS directory to hold the\n                                                 alignment data\n            *     --HDFSDataDir                  HDFS directory that holds the read\n                                                 data\n            *     --HDFSPathToNovoalign          HDFS path to the Novoalign\n                                                 executable\n                  --HDFSPathToNovoalignLicense   HDFS path to the Novoalign license\n                                                 filez\n                  --qualityFormat                Quality score format of the FASTQ\n                                                 files\n                                                 Default: ILMFQ\n            *     --reference                    HDFS path to the Novoalign\n                                                 reference index file\n            *     --threshold                    Quality threshold to use for the -t\n                                                 parameter\n\n        bowtie2SingleEnds      Run a bowtie2 alignment in single ended mode\n          Usage: bowtie2SingleEnds [options]\n      Options:\n            *     --HDFSAlignmentsDir       HDFS directory to hold the alignment\n                                            data\n            *     --HDFSDataDir             HDFS directory that holds the read data\n            *     --HDFSPathToBowtieAlign   HDFS path to the bowtie2 executable\n            *     --numReports              Max number of alignment hits to report\n                                            with the -k option\n            *     --reference               HDFS path to the bowtie 2 fasta\n                                            reference file\n\n        gemSingleEnds      Run a GEM alignment\n          Usage: gemSingleEnds [options]\n      Options:\n            *     --HDFSAlignmentsDir     HDFS directory to hold the alignment data\n            *     --HDFSDataDir           HDFS directory that holds the read data\n            *     --HDFSPathToGEM2SAM     HDFS path to the gem-2-sam executable\n            *     --HDFSPathToGEMMapper   HDFS path to the gem-mapper executable\n            *     --editDistance          Edit distance parameter (-e) to use in the\n                                          GEM mapping\n                                          Default: 0\n            *     --maxProcessesOnNode    Maximum number of GEM mapping processes to\n                                          run on one node simultaneously\n                                          Default: 6\n            *     --numReports            Max number of hits to report from GEM\n            *     --reference             HDFS path to the GEM reference file\n                  --strata                Strata parameter (-s) to use in the GEM\n                                          mapping\n                                          Default: all\n\n        razerS3SingleEnds      Run a razerS3 alignment\n          Usage: razerS3SingleEnds [options]\n      Options:\n            *     --HDFSAlignmentsDir   HDFS directory to hold the alignment data\n            *     --HDFSDataDir         HDFS directory that holds the read data\n            *     --HDFSPathToRazerS3   HDFS path to the razers3 executable file\n            *     --numReports          Max number of alignments to report for each\n                                        read\n            *     --pctIdentity         RazerS 3 percent identity parameter (-i)\n                                        Default: 0\n            *     --reference           HDFS path to the reference (FASTA) file for\n                                        the RazerS 3 mapper\n            *     --sensitivity         RazerS 3 sensitivity parameter (-rr)\n                                        Default: 0\n\n        mrfastSingleEnds      Run a novoalign mate pair alignment\n          Usage: mrfastSingleEnds [options]\n      Options:\n            *     --HDFSAlignmentsDir   HDFS directory to hold the alignment data\n            *     --HDFSDataDir         HDFS directory that holds the read data\n            *     --HDFSPathToMrfast    HDFS path to the mrfast executable file\n            *     --reference           HDFS path to the mrfast reference index file\n                  --threshold           MrFAST threshold parameter (-e)\n                                        Default: -1\n\n        exportAlignmentsFromHDFS      Export alignments in SAM format\n          Usage: exportAlignmentsFromHDFS [options]\n      Options:\n                  --aligner        Format of the alignment records\n                                   (sam|mrfast|novoalign)\n                                   Default: sam\n            *     --inputHDFSDir   HDFS path to the directory holding the alignment\n                                   reccords\n\n        GMMFitSingleEndInsertSizes      Compute GMM features in each bin across the genome\n          Usage: GMMFitSingleEndInsertSizes [options]\n      Options:\n                  --aligner                            Format of the alignment\n                                                       records (sam|mrfast|novoalign)\n                                                       Default: sam\n                  --chrFilter                          If filter params are used,\n                                                       only consider alignments in the\n                                                       region\n                                                       chrFilter:startFilter-endFilter\n                  --endFilter                          See chrFilter\n                  --excludePairsMappingIn              HDFS path to a BED file. Any\n                                                       reads mapped within those intervals\n                                                       will be excluded from the\n                                                       processing\n            *     --faidx                              HDFS path to the chromosome\n                                                       length file for the reference genome\n            *     --inputFileDescriptor                HDFS path to the directory\n                                                       that holds the alignment records\n                  --legacyAlignments                   Use data generated with an\n                                                       older version of Cloudbreak\n                                                       Default: false\n                  --mapabilityWeighting                HDFS path to a BigWig file\n                                                       containing genome uniqness scores. If\n                                                       specified, Cloudbreak will weight reads\n                                                       by the uniqueness of the regions\n                                                       they mapped to\n                  --maxInsertSize                      Maximum insert size to\n                                                       consider (= max size of deletion\n                                                       detectable)\n                                                       Default: 25000\n                  --maxLogMapqDiff                     Adaptive quality score cutoff\n                                                       Default: 5.0\n                  --maxMismatches                      Max number of mismatches\n                                                       allowed in an alignment; all other\n                                                       will be ignored\n                                                       Default: -1\n                  --minCleanCoverage                   Minimum number of spanning\n                                                       read pairs for a bin to run the\n                                                       GMM fitting procedure\n                                                       Default: 3\n                  --minScore                           Minimum alignment score (SAM\n                                                       tag AS); all reads with lower AS\n                                                       will be ignored\n                                                       Default: -1\n            *     --outputHDFSDir                      HDFS path to the directory\n                                                       that will hold the output of the\n                                                       GMM procedure\n                  --resolution                         Size of the bins to tile the\n                                                       genome with\n                                                       Default: 25\n                  --startFilter                        See chrFilter\n                  --stripChromosomeNamesAtWhitespace   Clip chromosome names from\n                                                       the reference at the first\n                                                       whitespace so they match with alignment\n                                                       fields\n                                                       Default: false\n\n        extractDeletionCalls      Extract deletion calls into a BED file\n          Usage: extractDeletionCalls [options]\n      Options:\n            *     --faidx                Chromosome length file for the reference\n            *     --inputHDFSDir         HDFS path to the GMM fit feature results\n                  --medianFilterWindow   Use a median filter of this size to clean\n                                         up the results\n                                         Default: 5\n            *     --outputHDFSDir        HDFS Directory to store the variant calls\n                                         in\n                  --resolution           Size of the bins to tile the genome with\n                                         Default: 25\n            *     --targetIsize          Mean insert size of the library\n                                         Default: 0\n            *     --targetIsizeSD        Standard deviation of the insert size of\n                                         the library\n                                         Default: 0\n                  --threshold            Likelihood ratio threshold to call a\n                                         variant\n                                         Default: 1.68\n\n        extractInsertionCalls      Extract insertion calls into a BED file\n          Usage: extractInsertionCalls [options]\n      Options:\n            *     --faidx                Chromosome length file for the reference\n            *     --inputHDFSDir         HDFS path to the GMM fit feature results\n                  --medianFilterWindow   Use a median filter of this size to clean\n                                         up the results\n                                         Default: 5\n                  --noCovFilter          filter out calls next to a bin with no\n                                         coverage - recommend on for BWA alignments, off for\n                                         other aligners\n                                         Default: true\n            *     --outputHDFSDir        HDFS Directory to store the variant calls\n                                         in\n                  --resolution           Size of the bins to tile the genome with\n                                         Default: 25\n            *     --targetIsize          Mean insert size of the library\n                                         Default: 0\n            *     --targetIsizeSD        Standard deviation of the insert size of\n                                         the library\n                                         Default: 0\n                  --threshold            Likelihood ratio threshold to call a\n                                         variant\n                                         Default: 1.68\n\n        copyToS3      Upload a file to Amazon S3 using multi-part upload\n          Usage: copyToS3 [options]\n      Options:\n            *     --S3Bucket   S3 Bucket to upload to\n            *     --fileName   Path to the file to be uploaded on the local\n                               filesystem\n\n        launchCluster      Use whirr to create a new cluster in the cloud using whirr/cloudbreak-whirr.properties\n          Usage: launchCluster [options]\n        runScriptOnCluster      Execute a script on one node of the currently running cloud cluster\n          Usage: runScriptOnCluster [options]\n      Options:\n            *     --fileName   Path on the local filesystem of the script to run\n\n        destroyCluster      Destroy the currently running whirr cluster\n          Usage: destroyCluster [options]\n        summarizeAlignments      Gather statistics about a set of alignments: number of reads, number of mappings, and total number of mismatches\n          Usage: summarizeAlignments [options]\n      Options:\n                  --aligner        Format of the alignment records\n                                   (sam|mrfast|novoalign)\n                                   Default: sam\n            *     --inputHDFSDir   HDFS path of the directory that holds the\n                                   alignments\n\n        exportGMMResults      Export wig files that contain the GMM features across the entire genome\n          Usage: exportGMMResults [options]\n      Options:\n            *     --faidx          Local path to the chromosome length file\n            *     --inputHDFSDir   HDFS path to the directory holding the GMM\n                                   features\n            *     --outputPrefix   Prefix of the names of the files to create\n                  --resolution     Bin size that the GMM features were computed for\n                                   Default: 25\n\n        dumpReadsWithScores      Dump all read pairs that span the given region with their deletion scores to BED format (debugging)\n          Usage: dumpReadsWithScores [options]\n      Options:\n                  --aligner                            Format of the alignment\n                                                       records (sam|mrfast|novoalign)\n                                                       Default: sam\n            *     --inputFileDescriptor                HDFS path to the directory\n                                                       that holds the alignment records\n                  --maxInsertSize                      Maximum possible insert size\n                                                       to consider\n                                                       Default: 500000\n                  --minScore                           Minimum alignment score (SAM\n                                                       tag AS); all reads with lower AS\n                                                       will be ignored\n                                                       Default: -1\n            *     --outputHDFSDir                      HDFS path to the directory\n                                                       that will hold the output\n            *     --region                             region to find read pairs\n                                                       for, in chr:start-end format\n                  --stripChromosomeNamesAtWhitespace   Clip chromosome names from\n                                                       the reference at the first\n                                                       whitespace so they match with alignment\n                                                       fields\n                                                       Default: false\n\n        debugReadPairInfo      Compute the raw data that goes into the GMM fit procedure for each bin (use with filter to debug a particular locus)\n          Usage: debugReadPairInfo [options]\n      Options:\n                  --aligner                 Format of the alignment records\n                                            (sam|mrfast|novoalign)\n                                            Default: sam\n            *     --chrFilter               Print info for alignments in the region\n                                            chrFilter:startFilter-endFilter\n            *     --endFilter               see chrFilter\n                  --excludePairsMappingIn   HDFS path to a BED file. Any reads\n                                            mapped within those intervals will be excluded\n                                            from the processing\n            *     --faidx                   HDFS path to the chromosome length file\n                                            for the reference genome\n            *     --inputFileDescriptor     HDFS path to the directory that holds\n                                            the alignment records\n                  --mapabilityWeighting     HDFS path to a BigWig file containing\n                                            genome uniqness scores. If specified,\n                                            Cloudbreak will weight reads by the uniqueness of\n                                            the regions they mapped to\n                  --maxInsertSize           Maximum insert size to consider (= max\n                                            size of deletion detectable)\n                                            Default: 500000\n                  --minScore                Minimum alignment score (SAM tag AS);\n                                            all reads with lower AS will be ignored\n                                            Default: -1\n            *     --outputHDFSDir           HDFS directory to hold the output\n                  --resolution              Size of the bins to tile the genome with\n                                            Default: 25\n            *     --startFilter             see chrFilter\n\n        findAlignment      Find an alignment record that matches the input string\n          Usage: findAlignment [options]\n      Options:\n            *     --HDFSAlignmentsDir   HDFS path to the directory that stores the\n                                        alignment data\n            *     --outputHDFSDir       HDFS path to the directory in which to put\n                                        the results\n            *     --read                Read name or portion of the read name to\n                                        search for\n\n        sortGMMResults      Sort and merge GMM Results (use with one reducer to get all GMM feature results into a single file\n          Usage: sortGMMResults [options]\n      Options:\n            *     --inputHDFSDir    HDFS path to the directory holding the GMM\n                                    features\n            *     --outputHDFSDir   Directory in which to put the results\n\n\n\n", 
  "id": 7403735
}