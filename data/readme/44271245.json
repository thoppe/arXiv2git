{
  "read_at": 1462556535, 
  "description": "Mathematical namespace discovery", 
  "README.md": "# Pipeline for Mathematical namespace discovery\n\n- https://github.com/alexeygrigorev/namespacediscovery is just a bunch of ipython notebooks - we need to make the code from there runnable\n\ninput:\n\n- [Mathematical Language Processing](https://github.com/TU-Berlin/project-mlp) for extracting identifiers from wikipedia (and arXiv)\n- [Java Language Processing](https://github.com/alexeygrigorev/JLP) for extracting variable declarations from Java\n\noutput:\n\n- namespaces (like [here](http://0agr.ru/wiki/index.php/Discovered_namespaces))\n\n\n## Running It\n\n    git clone https://github.com/alexeygrigorev/namespacediscovery-pipeline.git\n    cd namespacediscovery-pipeline/src\n    python pipeline.py\n\n\nModify `luigi.cfg` to set different configuration parameters \n\nYou need to at least change the following parameters:\n\n- `[MlpResultsReadTask]/mlp_results` - path to the output of mlp\n- `[MlpResultsReadTask]/categories_processed` - path to the category information\n- (optional) `[DEFAULT]/intermediate_result_dir` - path to directory where pre-calculated results will be stored \n\n\nOther parameters (`[DEFAULT]` section): \n\n- `isv_type` identifier vector space model, can be `nodef`, `weak` or `strong`\n- `vectorizer_dim_red` type of dimentionality reduction, can be `none`, `svd`, `nmf` or `random`\n- `clustering_algorithm`, now only `kmeans` is implemented \n\n\n## Dependencies \n\n- python2\n- numpy \n- scipy\n- scikit-learn\n- nltk\n- python-Levenshtein\n- fuzzywuzzy\n- rdflib\n- luigi \n\n\nfor PyData stack libraries such as numpy, scipy, scikit-learn and nltk \nit's best to use [anaconda](http://docs.continuum.io/anaconda/install) \ninstaller \n\nNot all dependencies come pre-installed with anaconda, use `pip` to install them:\n\n    pip install python-Levenshtein\n    pip install fuzzywuzzy\n    pip install luigi\n    pip install rdflib\n\n\nWe also need to download some data for nltk: the list of stopwords and the model \nfor tokenization. Run it in the python console to install them:\n\n    import nltk\n    nltk.download('stopwords')\n    nltk.download('punkt')\n\n\nsee [SETUP.md](SETUP.md) for an example how to set up the environment\n\n\n## Datasets \n\nWe use the following datasets as input:\n\n\n- mlp ... \n- dbpedia category information\n\n\nClassification schemes:\n\n- MSC (downloaded from http://cran.r-project.org/web/classifications/MSC.html)\n- PACS (downloaded from https://github.com/teefax/pacsparser)\n- ACM (Can be downloaded from http://www.acm.org/about/class/class/2012 and has parsable skos format http://dl.acm.org/ft_gateway.cfm?id=2371137&ftid=1290922&dwn=1)\n\nThe classification schemes datasets are already available in the `data` directory.\n\n", 
  "id": 44271245
}