{
  "README": "Author: ilyasu@,  woj.zaremba@gmail.com\n\nThis is not an official Google project.\n\nFirst, install 'strict' (luarocks install strict)\nSecond, make sure that torch lives in /usr/bin/torch (or edit exprs/run_utils_new.py to point to the right torch path)\n\nTo run the gradient checker on the new layers, type\n\n> luarocks install totem\n> torch rlntm_pkgs/test_individual_layers.lua\n\n\nTo run a full gradient checker (which includes exact expectations for Reinforce), type:\n> torch rlntm_pkgs/RL_check_grad.lua\n\n\n\nTo train a model that reverses a list, where the input head is forced to move forward, type:\n> python exprs/train_reverse.py\n\nIt takes about 10,000 parameter updates to converge, which sadly takes 3 days on my workstation\n(GPU not required).\n\nIf you are not willing to wait for 3 days, please see the output of the included printout log: \n> less rlntm_runs/train_repeatCopy-1/train_reverse-1.stdout\n\nThe printout shows the evolution of the execution traces of the RL-NTM, which \nare fun to inspect.\n\n\n\nTo train a model that solves the repeat copy task, type:\n\n> python exprs/train_repeat_copy.py\n\nIt sometimes converges in as little as 1,000 parameter updates. If you\nare not willing to wait for a day, please see the output of the\nincluded printout log: \n> less rlntm_runs/train_repeatCopy-1/train_repeatCopy-1.stdout\n\n\n\n\nWARNING:  These experiments are fragile.  It is possible that they will not succeed with \nthe provided random seeds if the random number generation library is different. \nWe observed that between 30-50% of the random seeds produce the desired result, so you may need\nto tweak it a modest number of times.\n\n", 
  "read_at": 1462554054, 
  "description": "An implementation of the RL-NTM from http://arxiv.org/abs/1505.00521", 
  "id": 46893659
}