{
  "id": 35792686, 
  "read_at": 1462558862, 
  "README.rst": "=======\nrevrand \n=======\n\n.. image:: https://travis-ci.org/NICTA/revrand.svg?branch=master\n   :target: https://travis-ci.org/NICTA/revrand\n\n.. image:: https://codecov.io/github/NICTA/revrand/coverage.svg?branch=master\n    :target: https://codecov.io/github/NICTA/revrand?branch=master\n\n------------------------------------------------------------------------------\nA library of scalable Bayesian generalised linear models with *fancy* features\n------------------------------------------------------------------------------\n\nThis library implements various Bayesian linear models (Bayesian linear\nregression) and generalised linear models. A few features of this library are:\n\n- A fancy basis functions/feature composition framework for combining basis\n  functions like radial basis function, sigmoidal basis functions, polynomial\n  basis functions etc.\n- Basis functions that can be used to approximate Gaussian processes with shift\n  invariant covariance functions (e.g. square exponential) when used with\n  linear models [1]_, [2]_, [3]_.\n- Non-Gaussian likelihoods with Bayesian generalised linear models using a\n  modified version of the nonparametric variational inference algorithm\n  presented in [4]_.\n- Large scale learning using stochastic gradient descent (ADADELTA).\n\n\nQuickstart\n----------\n\nTo install, simply run ``setup.py``:\n\n.. code:: console\n\n   $ python setup.py install\n\nor install with ``pip``:\n\n.. code:: console\n\n   $ pip install git+https://github.com/nicta/revrand.git\n\nRefer to `docs/installation.rst <docs/installation.rst>`_ for advanced \ninstallation instructions.\n\nHave a look at some of the `demos <demos/>`_, e.g.: \n\n.. code:: console\n\n   $ python demos/demo_regression.py\n\nOr,\n\n.. code:: console\n\n   $ python demos/demo_glm.py\n\n\nBayesian Linear Regression Example\n..................................\n\nHere is a very quick example of how to use Bayesian linear regression with\noptimisation of the likelihood noise, regulariser and basis function\nparameters. Assuming we already have training noisy targets ``y``, inputs \n``X``, and some query inputs ``Xs`` (as well as the true noiseless function\n``f``):\n\n.. code:: python\n\n    import matplotlib.pyplot as pl\n    import numpy as np\n    from revrand.basis_functions import LinearBasis, RandomRBF\n    from revrand.regression import learn, predict\n    from revrand.btypes import Parameter, Positive\n\n    ...\n    \n    # Concatenate a linear basis and a Random radial basis (GP approx)\n    init_lenscale = Parameter(1.0, Positive())  # init val and bounds \n    basis = LinearBasis(onescol=True) \\\n        + RandomRBF(nbases=300, Xdim=X.shape[1], init_lenscale)\n\n    # Learn regression parameters and predict\n    params = regression.learn(X, y, basis)\n    Eys, Vfs, Vys = regression.predict(Xs, basis, *params) \n\n    # Training/Truth\n    pl.plot(X, y, 'k.', label='Training')\n    pl.plot(Xs, f, 'k-', label='Truth')\n\n    # Plot Regressor\n    Sys = np.sqrt(Vys)\n    pl.plot(Xs, Eys, 'g-', label='Bayesian linear regression')\n    pl.fill_between(Xs, Eys - 2 * Sys, Eys + 2 * Sys, facecolor='none',\n                    edgecolor='g', linestyle='--', label=None)\n\n    pl.legend()\n\n    pl.grid(True)\n    pl.title('Regression demo')\n    pl.ylabel('y')\n    pl.xlabel('x')\n    pl.show()\n\nThis script will output something like the following,\n\n.. image:: blr_demo.png\n\n\nBayesian Generalised Linear Model Example\n.........................................\n\nThis example is very similar to that above, but now let's assume our targets\n``y`` are drawn from a Poisson likelihood, or observation, distribution which\nis a function of the inputs, ``X``. The task here is to predict the mean of the\nPoisson distribution for query inputs ``Xs``, as well as the uncertainty\nassociated with the prediction.\n\n.. code:: python\n\n    import matplotlib.pyplot as pl\n    import numpy as np\n    from revrand.basis_functions import RandomRBF\n    from revrand.glm import learn, predict_moments, predict_interval\n\n    ...\n    \n    # Random radial basis (GP approx)\n    init_lenscale = Parameter(1.0, Positive())  # init val and bounds \n    basis = RandomRBF(nbases=100, Xdim=X.shape[1], init_lenscale)\n\n    # Set up the likelihood of the GLM\n    llhood = likelihoods.Poisson(tranfcn='exp')  # log link\n\n    # Learn regression parameters and predict\n    params = learn(X, y, llhood, basis)\n    Eys, _, _, _ = predict_moments(Xs, llhood, basis, *params) \n    y95n, y95x = predict_interval(0.95, Xs, llhood, basis, *params)\n\n    # Training/Truth\n    pl.plot(X, y, 'k.', label='Training')\n    pl.plot(Xs, f, 'k-', label='Truth')\n\n    # Plot GLM SGD Regressor\n    pl.plot(Xs, Eys, 'b-', label='GLM mean.')\n    pl.fill_between(Xs, y95n, y95x, facecolor='none',\n                    edgecolor='b', linestyle='--', label=None)\n\n    pl.legend()\n\n    pl.grid(True)\n    pl.title('Regression demo')\n    pl.ylabel('y')\n    pl.xlabel('x')\n    pl.show()\n\nThis script will output something like the following,\n\n.. image:: glm_demo.png\n\n\nLarge-scale Learning with Stochastic Gradients\n..............................................\n\nBy default the GLM uses stochastic gradients to learn all of its\nparameters/hyperparameters and does not require any matrix inversion, and so it\ncan be used to learn from large datasets with lots of features\n(regression.learn uses L-BFGS and requires a matrix inversion). We can also use\nthe GLM to approximate and scale up regular Bayesian linear regression. For\ninstance, if we modify the Bayesian linear regression example from before,\n\n.. code:: python\n\n    ...\n\n    from revrand import glm, likelihoods\n\n    ...\n\n    # Set up the likelihood of the GLM\n    llhood = likelihoods.Gaussian(var_init=Parameter(1., Positive()))\n\n    # Learn regression parameters and predict\n    params = glm.learn(X, y, llhood, basis)\n    Ey_g, Vf_g, Eyn, Eyx = glm.predict_moments(Xtest, llhood, base, *params)\n\n    ...\n\n    # Plot GLM SGD Regressor\n    Sy_g = np.sqrt(Vy_g)\n    pl.plot(Xpl_s, Ey_g, 'm-', label='GLM')\n    pl.fill_between(Xs, Ey_g - 2 * Sy_g, Ey_g + 2 * Sy_g, facecolor='none',\n                    edgecolor='m', linestyle='--', label=None)\n\n    ...\n\nThis script will output something like the following,\n\n.. image:: glm_sgd_demo.png\n\nWe can see the approximation from the GLM is pretty good - this is because it\nuses a mixture of diagonal Gaussians posterior (thereby avoiding a full matrix\ninversion) to approximate the full Gaussian posterior covariance over the\nweights. This also has the advantage of allowing the model to learn multi-modal\nposterior distributions when non-Gaussian likelihoods are required.\n\n\nFeature Composition Framework\n.............................\n\nWe have implemented an easy to use and extensible feature-building framework\nwithin revrand. You have already seen the basics demonstrated in the above\nexamples, i.e. concatenation of basis functions,\n\n.. code:: python\n\n    >>> X = np.random.randn(100, 5)\n    >>> N, d = X.shape\n    >>> base = LinearBasis(onescol=True) + RandomRBF(Xdim=d, nbases=100)\n    >>> lenscale = 1.\n    >>> Phi = base(X, lenscale)\n    >>> Phi.shape\n    (100, 206)\n\nThere are a few things at work in this example:\n\n- Both ``LinearBasis`` and ``RandomRBF`` are applied to all of ``X``, and the\n  result is concatenated.\n- ``LinearBasis`` has pre-pended a column of ones onto ``X`` so a subsequent\n  algorithm can learn a \"bias\" term.\n- ``RandomRBF`` is actually approximating a radial basis *kernel* function,\n  [3]_, so we can approximate how a kernel machine functions with a basis\n  function!  This also outputs ``2 * nbases`` number of basis functions.\n- Hence the resulting basis function has a shape of \n  ``(N, d + 1 + 2 * nbases)``.\n\nWe can also use *partial application* of basis functions, e.g.\n\n\n.. code:: python\n\n    >>> base = LinearBasis(onescol=True, apply_ind=slice(0, 2)) \\\n        + RandomRBF(Xdim=d, nbases=100, apply_ind=slice(2, 5))\n    >>> Phi = base(X, lenscale)\n    >>> Phi.shape\n    (100, 203)\n\nNow the basis functions are applied to seperate dimensions of the input, ``X``.\nThat is, ``LinearBasis`` takes dimensions 0 and 1, and ``RandomRBF`` takes the\nrest, and again the results are concatenated.\n\nFinally, if we use these basis functions with any of the algorithms in this\nrevrand, *the parameters of the basis functions are learned* as well! So\nreally in the above example ``lenscale = 1.`` is just an initial value for\nthe kernel function length-scale!\n\n\nUseful Links\n------------\n\nHome Page\n    http://github.com/nicta/revrand\n\nDocumentation\n    http://nicta.github.io/revrand\n\nIssue tracking\n    https://github.com/nicta/revrand/issues\n\nBugs & Feedback\n---------------\n\nFor bugs, questions and discussions, please use \n`Github Issues <https://github.com/NICTA/revrand/issues>`_.\n\n\nReferences\n----------\n\n.. [1] Yang, Z., Smola, A. J., Song, L., & Wilson, A. G. \"A la Carte --\n   Learning Fast Kernels\". Proceedings of the Eighteenth International\n   Conference on Artificial Intelligence and Statistics, pp. 1098-1106,\n   2015.\n.. [2] Le, Q., Sarlos, T., & Smola, A. \"Fastfood-approximating kernel\n   expansions in loglinear time.\" Proceedings of the international conference\n   on machine learning. 2013.\n.. [3] Rahimi, A., & Recht, B. \"Random features for large-scale kernel\n   machines.\" Advances in neural information processing systems. 2007. \n.. [4] Gershman, S., Hoffman, M., & Blei, D. \"Nonparametric variational\n   inference\". arXiv preprint arXiv:1206.4665 (2012).\n\nCopyright & License\n-------------------\n\nCopyright 2015 National ICT Australia.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n", 
  "description": "A library of scalable Bayesian generalised linear models with fancy features"
}