{
  "read_at": 1462558998, 
  "description": "Tensor Factorization via Matrix Factorization", 
  "README.md": "Tensor Factorization via Matrix Factorization\n=============================================\n\nTensor factorization is a key subroutine in several\nrecent algorithms for learning latent variable models using the method of moments. \nThis general technique is applicable to a broad class of models,\nsuch as:\n\n* Mixtures of Gaussians\n* Topic models (e.g. latent Dirichlet allocation)\n* Hidden Markov models\n\nHowever, techniques for factorizing tensors are not as well-developed as \nmatrix factorization techniques. The algorithms implemented here instead \ntransform the problem of finding the CP decomposition of a\ntensor to the problem of jointly diagonalizing a set of matrices.\n\nThese ideas have been proposed and analyzed in the following publications:\n\n```\nV. Kuleshov, A. Chaganty, and P. Liang. Tensor Factorization via Matrix Factorization. AISTATS 2015\nV. Kuleshov, A. Chaganty, and P. Liang. Simultaneous Diagonalization: the asymmetric, low-rank, and noisy settings. ArXiv Technical report.\n```\n\n## Installation\n\n### Requirements\n\nThe algorithms have been implemented in MATLAB and make extensive use of:\n\n* MATLAB Tensor Toolbox 2.5\n* Tensorlab 2.02\n\nThese libraries are available for free for academic use.\n\nUnfortunately, it seems like the current version of Octave (3.8.2)\ndoes not support the Tensor Toolbox, which means our code cannot be \nused in Octave.\n\n### Setup\n\nTo install this package, simply clone the git repo:\n\n```\ngit clone [...];\ncd tenfact;\n```\n\nYou must then make sure that Tensorlab and the Tensor Toolbox can be seen from \nMATLAB (i.e. make sure to run `addpath` on their paths).\n\n## Contents\n\nThe main algorithms are in `/bin`. The exact scripts are:\n\n* `jacobi.m`: Jacobi algorithm for simultaneous matrix diagonalization.\n* `qrj1d.m`: QRJ1D algorithm for the non-orthogonal case.\n* `tenfact.m`: Orthogonal tensor factorization using the `OJD0/OJD1` algorithms from the paper.\n* `no_tenfact.m`: Non-orthogonal tensor factorization using the `NOJD0/NOJD1` algorithms.\n* `tpm.m`: Our implementation of the tensor power method.\n\nThe root folder contains files for reproducing the synthetic experiments \nfrom the paper.\n\n## Reproducing experiments from the paper\n\nTo reproduce the orthogonal experiments, use the script `run_ortho_comparison.m`. This \nsets certain global parameters (e.g., the dimension, the rank, the noise level, etc.)\nand for each level of noise, it performs a series of synthetic \nexperiments (implemented in `run_ortho_experiment.m`). \n\nEach synthetic experiment is defined by:\n\n* The tensor dimension `p`\n* The tensor rank `k`\n* The noise level `epsilon`\n* The number of experiment repetitions `tries`\n* An output file `outputfile`\n\nThe results reported in `outputfile` are averged over all the repetitions.\n\nThe scripts `run_nonortho_comparison.m` and `run_nonortho_experiment.m` perform the \nsame analysis for non-orthogonal tensors.\n\nThe result of one experiment (in `outputfile`) has the following format:\n```\nnojd0   0.142636        128.800000\nnojd1   0.135456        244.100000\nals     0.200180        404.800000\nlath    0.235587        12.000000\nnls     0.154352        167.800000\n```\n\nThe first column is the name of the algorithm used, which is one of:\n\n* `ojd0/ojd1`: our orthogonal tensor factorization algorithms\n* `nojd0/nojd1`: our non-orthogonal tensor factorization algorithms\n* `tpm`: the tensor power method\n* `als`: alternating least squares\n* `nls`: non-linear least squares\n* `lath`: Lathauwers algorithm\n\nThe implementations of most of these algorithms are taken from Tensorlab.\n\nThe second column in the output file is the error of the algorithm.\nThe last column measures the running time. For the joint diagonalization \nalgorithms, it measures the total number of sweeps by the JD subroutine. \nFor the TPM, it measures the total number of multiplications. See the \ndocumentation in Tensorlab for the other algorithms.\n\n## Feedback\n\nPlease send feedback to [Volodymyr Kuleshov](http://www.stanford.edu/~kuleshov)\n", 
  "id": 35061205
}