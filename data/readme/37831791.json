{
  "read_at": 1462549314, 
  "description": "DRAW implementation with deepy", 
  "README.md": "[![Requirements Status](https://requires.io/github/uaca/deepy-draw/requirements.svg?branch=master)](https://requires.io/github/uaca/deepy-draw/requirements/?branch=master)\n[![GPL3](https://img.shields.io/badge/license-GPL3-blue.svg)](https://github.com/uaca/deepy-draw/blob/master/LICENSE)\n\n# Another implementation of DRAW with deepy framework\n\nPaper: http://arxiv.org/pdf/1502.04623\n\n### Note\n\nCore components in the implementation are copied from https://github.com/jbornschein/draw, thanks for the great work of https://github.com/jbornschein.\n\n### Tricky parts in the model\n\n- Differential filter functions that can zoom in and zoom out an image to get a glimpse\n- Q Sampler\n - Differential sampling function to get a sample from distributions\n\n### What does the sampler do, an analysis from computation graph\n\n- Prior distribution P_z\n - This distribution generates latent variables used in image generation\n- mean and deviation transform networks\n - These two networks transform inputs to mean and deviation vectors to form a distribution Q(z|x)\n - The goal for training these two network is to make Q(z|x) close to prior P_z\n  - KL(Q(z|x) || P_z) is to evaluate how close these two distributions are\n- But if we sample a latent variable from Q(z|x), another goal is to restore the original input x from z\n - So this means the model try to map any input to a similar latent vector\n - but it has to maintain a tiny difference so as to decode it back to x\n- Then if we sample from P_z, we are sampling from Q(z|x) which we don't know what the x is\n\n\n### Experiment on MNIST\n\n```bash\npython mnist_training.py\n```\n\n### MNIST Animation (work in progress)\n\n![](https://github.com/zomux/deepy-draw/raw/master/plots/mnist-animation.gif)\n", 
  "id": 37831791
}