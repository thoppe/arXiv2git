{
  "read_at": 1462545702, 
  "description": "Instructions and script to use the Solr search engine", 
  "README.md": "# Solr-tools\n\nThese instructions should help you set up your own instance of the\nSolr search engine and this repository provides some scripts to help\nimport data into your new Solr instance.\n\n## Install Solr\n\nAs of this writing the newest version of Solr is 5.3, but you can\ncheck from the [Solr web site][1] if there is a newer version and\nchange the commands below accordingly.\n\n    wget http://www.nic.funet.fi/pub/mirrors/apache.org/lucene/solr/5.3.0/solr-5.3.0.tgz\n    tar xvf solr-5.3.0.tgz \n\n### Run Solr directly (Linux and OS X)\n\nSolr can now be started with:\n\n    ./solr-5.3.0/bin/solr start\n    \nIt should now be up and running, just go to <http://localhost:8983> to check it out.\n\nSolr can be stopped with:\n\n    ./solr-5.3.0/bin/solr stop -all\n\nand the current status can be checked with:\n\n    ./solr-5.3.0/bin/solr status\n\n### Install and auto-start Solr (Linux only)\n\n    umask 022\n    sudo ./solr-5.3.0/bin/install_solr_service.sh solr-5.3.0.tgz \n\nThe `umask` is just to make sure the installation will be readable for\nall users. After this Solr should already be up and running, just go\nto <http://localhost:8983> to check it out.\n\nIn the future you can start and stop Solr in the normal way:\n\n    sudo service solr start\n    sudo service solr stop\n\nThe Solr files have been installed in the `/opt/solr` directory, for\nexample the executable binaries can be found in `/opt/solr/bin`.\n\n## Importing Wikipedia\n\nFor importing from a Wikipedia XML dump there exists a ready-made\nconfiguration to make things easier. First you need to edit the files\nin the\n[`wikipedia_config`](https://github.com/HIIT/solr-tools/tree/master/wikipedia_config)\ndirectory that you get when checking out this repository. In\nparticular change the `url` to the correct location of the\n`enwiki-latest-pages-articles.xml` file in\n[`arxiv_config/conf/data-config.xml`](https://github.com/HIIT/solr-tools/blob/master/wikipedia_config/conf/data-config.xml).\n\nOnce you are done you can initialise the core in Solr:\n\n    sudo -u solr /opt/solr/bin/solr create_core -c enwiki -d ~/solr-tools/wikipedia_config\n\nAfter this you can start importing the data by going to the web UI at\n<http://localhost:8983/solr/#/enwiki/dataimport> and press Execute.\n\nIf you mess everything up and wish to delete everything from the core you can run:\n\n    curl http://localhost:8983/solr/enwiki/update?commit=true -d '<delete><query>*:*</query></delete>'\n\nFor other data sets you have to do things the more complicated way.\nRead below for more detailed information.\n\n## Create a new database in Solr\n\nFirst you have to decide if you want to use a fixed schema for the\ndatabase, or use the new schemaless mode. A schema is a description of\nthe fields and data types of each record in the database, together\nwith information on how they are processed and indexed. For more\nserious work you probably want to use a schema. However, we will first\ndescribe how to setup a schemaless database, this is useful for\ntesting and learning Solr when you want to get started quickly.\n\n### Setup without schema\n\nLet's create a new database named `arxiv-cs`. You have to run the\ncreate command as the `solr` user like this:\n\n    sudo -u solr /opt/solr/bin/solr create_core -c arxiv-cs -d data_driven_schema_configs\n\nThis creates a new core, which is the Solr term for a database. You\ncan have many cores in the same Solr instance. The `-c arxiv-cs`\noption gives a name to the core, you can of course give it any name\nyou want. The `-d data_driven_schema_configs` option means that you do\nnot need to specify a schema for the database, but that it will derive\nit automatically from the uploaded data.\n\n### Setup with schema\n\nIn this section we will setup a database with schema. Note that this\nis an alternative to the schemaless setup described in the previous\nsection. (You can try them both at the same time, but then you have to\ngive them different names.)\n\nTo make your life easier I have created a directory with the starting\npoint already configured for arXiv data in the directory\n[`arxiv_config`](https://github.com/HIIT/solr-tools/tree/master/arxiv_config).\nThe file\n[`arxiv_config/conf/schema.xml`](https://github.com/HIIT/solr-tools/blob/master/arxiv_config/conf/schema.xml)\nis the most important file which sets the schema. The\n[format is documented in the Solr wiki][2], but it isn't too hard to\nunderstand by just looking at the existing file as an example.\n\nFor example:\n\n    <field name=\"title\" type=\"text_en\" indexed=\"true\" stored=\"true\"/>\n\ncreates a field called \"title\" which will contain English text\n(`text_en`), and which is indexed and stored in the database (for huge\ndata fields you might only want to have them indexed but not stored as\nsuch). The `text_en` field type is defined later in `schema.xml` in a\n`<fieldType>` declaration. You can see that it specifies a tokenizer,\nstop word filtering, and Porter stemming among other things.\n\nThis example:\n\n    <field name=\"keyword\" type=\"text_general\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/>\n\ncreates a \"keyword\" field of the type `text_general` (again check the\ndefinition later in the file). The most interesting thing here is the\n`multiValued=\"true\"` part which specifies that you can give multiple\nkeywords in your data.\n\nThen:\n\n    <field name=\"text\" type=\"text_en\" indexed=\"true\" stored=\"false\" multiValued=\"true\" />\n    <copyField source=\"title\" dest=\"text\"/>\n    <copyField source=\"author\" dest=\"text\"/>\n    <copyField source=\"abstract\" dest=\"text\"/>\n\nsets up a field \"text\" to which title, author and abstract fields are\ncopied to. This is a special field that is used as the default target\nfor text searches. You can also search the other indexed field, but\nthey have to be targetted explicitly.\n\nFinally, after you have made any changes to the schema you can create\nthe new database (Solr core):\n\n    sudo -u solr /opt/solr/bin/solr create_core -c arxiv-cs-s -d ~/solr-tools/arxiv_config\n\nHere you might have to change the last argument to match the path\nwhere you have cloned this repository.\n\n## Fetch and import data to Solr\n\nIn this git repository are some example scripts to get data from arXiv\nand convert them to Solr's format. You can modify these scripts to\nwork for your own data format. In short the final XML format for\nuploading to Solr looks a bit like this:\n\n    <add>\n      <doc>\n        <field name=\"id\">oai:arXiv.org:0704.0002</field>\n        <field name=\"last_modified\">2008-12-13T00:00:00Z</field>\n        <field name=\"set\">cs</field>\n        <field name=\"title\">Sparsity-certifying Graph Decompositions</field>\n        <field name=\"author\">Streinu, Ileana</field>\n        <field name=\"author\">Theran, Louis</field>\n        <field name=\"subject\">Mathematics - Combinatorics</field>\n        <field name=\"abstract\">We describe a new algorithm, etc etc</field>\n        <field name=\"comment\">To appear in Graphs and Combinatorics</field>\n        <field name=\"keyword\">algorithmic solutions</field>\n        <field name=\"keyword\">graph</field>\n        <field name=\"keyword\">graph algorithms</field>\n        <field name=\"url\">http://arxiv.org/abs/0704.0002</field>\n      </doc>\n      <doc>\n        <!-- another document here -->\n      </doc>\n      <!-- and so on ... ->\n    </add>\n\nSo each field has a name which matches the field names from the\n`schema.xml`, the content of the tag is the value of the field.\nMulti-valued fields can be repeated several times.\n\n### Harvesting data from arXiv\n\nTo harvest data from arXiv you can use the `harvest-arxiv.py` script,\nto see a list of options run:\n\n    ./harvest-arxiv.py -h\n\nAt the moment it downloads only from the `cs` (Computer Science) set,\nbut you can change this and other details by editing the first lines\nof the script.  To download everything, just run:\n\n    mkdir output_dir\n    ./harvest-arxiv.py -d output_dir\n\nThe script is a bit slow since arXiv mandates a short delay between\neach fetch as not to overload the server. If later want to fetch\neverything that has been updated since a particular date you can run\nlike this:\n\n    mkdir output_dir_more\n    ./harvest-arxiv.py -d output_dir_more -f 2015-08-31\n\nNext you need to convert each file into a format understood by Solr:\n\n    ./arxiv_to_solr.py output_dir*/*.xml\n\nThis will generate a huge `solr.xml` file. You can do this in smaller\nsteps as well so you don't have to create a huge file in one go. It\ncan be uploaded in pieces to the Solr database.\n\n### Uploading to Solr\n\nFinally upload your XML file to Solr. If you have used the harvesting\nscripts from the previous section it will be the `solr.xml` file.\n\n    /opt/solr/bin/post -c arxiv-cs solr.xml\n\nYou can safely upload the files many times, and upload again when new\nfiles are available since they have a unique `id` and any new record\nwill overwrite the old one (i.e. you won't get duplicates).\n\nNow you can head over to the web interface and make a query, just\nselect the database in the pull-down list on the left or go directly\nto: <http://localhost:8983/solr/#/arxiv-cs/query>. Now type the query\nin the `q` field and select the format of the result (e.g. json or\nxml). Click \"Execute query\" when you are done, now you will see the\nquery result and the URL which you can use in your application, for\nexample:\n<http://localhost:8983/solr/arxiv-cs/select?q=computer+vision&wt=json&indent=true>.\n\nA useful option to add is `fl=*,score` which causes Solr to return the\nrelevance score of each result.\n\n[1]: http://lucene.apache.org/solr/\n[2]: https://wiki.apache.org/solr/SchemaXml\n", 
  "id": 41677999
}