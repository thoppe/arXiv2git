{
  "read_at": 1462548174, 
  "description": "LocNet: Improving Localization Accuracy for Object Detection", 
  "README.md": "## *LocNet: Improving Localization Accuracy for Object Detection*\n\n### Introduction\n\nThis code implements the following CVPR 2016 paper (accepted for oral presentation):    \n**Title:**      \"LocNet: Improving Localization Accuracy for Object Detection\"    \n**Authors:**     Spyros Gidaris, Nikos Komodakis    \n**Institution:** Universite Paris Est, Ecole des Ponts ParisTech    \n**ArXiv Link:**  http://arxiv.org/abs/1511.07763   \n**Code:**        https://github.com/gidariss/LocNet    \n\n**Abstract:**  \n\"We propose a novel object localization methodology with the purpose of boosting the localization accuracy of state-of-the-art object detection systems. Our model, given a search region, aims at returning the bounding box of an object of interest inside this region. To accomplish its goal, it relies on assigning conditional probabilities to each row and column of this region, where these probabilities provide useful information regarding the location of the boundaries of the object inside the search region and allow the accurate inference of the object bounding box under a simple probabilistic framework.  \nFor implementing our localization model, we make use of a convolutional neural network architecture that is properly adapted for this task, called LocNet. We show experimentally that LocNet achieves a very significant improvement on the mAP for high IoU thresholds on PASCAL VOC2007 test set and that it can be very easily coupled with recent state-of-the-art object detection systems, helping them to boost their performance. Finally, we demonstrate that our detection approach can achieve high detection accuracy even when it is given as input a set of sliding windows, thus proving that it is independent of region proposal methods.\"   \n\n\n### Citing LocNet\n\nIf you find LocNet useful in your research, please consider citing:   \n\n> @inproceedings{gidaris2016locnet,  \n  title={LocNet: Improving Localization Accuracy for Object Detection},  \n  author={Gidaris, Spyros and Komodakis, Nikos},   \n  booktitle={Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on},  \n  year={2016}  \n}  \n\n### License\nThis code is released under the MIT License (refer to the LICENSE file for details).  \n\n### Requirements\n\n**Hardware:**  \nFor training the LocNet models or testing the LocNet object detection pipeline you will require a  GPU with at least 6 Gbytes of memory.\n\n**Software:**  \n1. Modified version of Caffe developed to supprot LocNet and installed with the cuDNN library [[link](https://github.com/gidariss/caffe_LocNet)].  \n2. MATLAB (tested with R2014b)  \n  \n**Optional:**  \nThe following packages are necessary for using the [EdgeBox](http://research.microsoft.com/apps/pubs/default.aspx?id=220569) or [Selective Search](http://koen.me/research/selectivesearch/) bounding box proposas algorithms:     \n1. Edge Boxes code [[link](https://github.com/pdollar/edges)].    \n2. The image processing MATLAB toolbox of Piotr Dollar (used for the Edge Boxes) [[link](http://vision.ucsd.edu/~pdollar/toolbox/doc/index.html)].         \n3. Selective search code [[link](http://huppelen.nl/publications/SelectiveSearchCodeIJCV.zip)].       \n   \n**Note:** we provide the bounding box proposals of PASCAL images and hence installing the above packages is not necessary for training or testing models on this dataset. However, they are necessary for running the demo code on images other than the one that is provided by us. \n\n### Installation (sufficient for the demo)\n\n1. Download and install this modified version of [Caffe](https://github.com/gidariss/caffe_LocNet) developed to supprot LocNet. To clone Caffe on your local machine:\n    ```Shell\n\n    # $caffe_LocNet: directory where Caffe will be cloned  \n    git clone https://github.com/gidariss/caffe_LocNet $caffe_LocNet\n    ``` \n`$caffe_LocNet` is the directory where Caffe is cloned. After cloning Caffe follow the installation instructions [here](http://caffe.berkeleyvision.org/installation.html). **Note** that you have to install Caffe with the cuDNN library.   \n2. Clone the ***LocNet*** code in your local machine:\n    ```Shell\n\n    # $LocNet: directory where LocNet will be cloned  \n    git clone https://github.com/gidariss/LocNet $LocNet\n    ``` \nFrom now on, the directory where ***LocNet*** was cloned will be called `$LocNet`.  \n3. Create a symbolic link of [Caffe](https://github.com/gidariss/caffe_LocNet) installatation directory at `$LocNet/external/`:  \n    ```Shell\n\n    # $LocNet: directory where LocNet was cloned    \n    # $caffe_LoNet: directory where caffe was cloned    \n    ln -sf $caffe_LoNet $LocNet/external/caffe_LocNet    \n    ```      \n\n4.  open matlab from the `$LocNet/` directory:  \n    ```Shell  \n\n    cd $LocNet  \n    matlab  \n    ```      \n5.  Run the `LocNet_build.m` script on matlab command line\n    ```Shell\n\n    # matlab command line enviroment\n    >> LocNet_build    \n    ``` \n  Do not worry about the warning messages. They also appear on my machine.  \n\n### Download and use the pre-trained models\n\nDownload the tar.gz files with the pre-trained models:    \n**Recognition models:**     \n1. [Reduced MR-CNN recognition model](https://drive.google.com/file/d/0BwxkAdGoNzNTNFNKTzV3UnZGLW8/view?usp=sharing).     \n2. [Fast RCNN recognition model](https://drive.google.com/file/d/0BwxkAdGoNzNTMDJUMGRhWV9qV2s/view?usp=sharing) (re-implemented by us).   \n**Localization models:**  \n1. [LocNet In-Out model](https://drive.google.com/file/d/0BwxkAdGoNzNTcFpaYkFVN3FraUU/view?usp=sharing).         \n2. [LocNet Borders model](https://drive.google.com/file/d/0BwxkAdGoNzNTTF84MG5Xby1RRzA/view?usp=sharing).    \n3. [LocNet Combined model](https://drive.google.com/file/d/0BwxkAdGoNzNTS3FPOVlWSGZUVjQ/view?usp=sharing).   \n4. [CNN-based bounding box regression model](https://drive.google.com/file/d/0BwxkAdGoNzNTV0p2Vmh5YS1LRE0/view?usp=sharing).  \n\nUntar and unzip all of the above files on the following locations:\n\n   ```Shell\n   \n   # Recognition models:\n   $LocNet/models-exps/VGG16_Reduced_MRCNN # Reduced MR-CNN recognition model\n   $LocNet/models-exps/VGG16_FastRCNN # Fast RCNN recognition model\n   # VOC2012 structure:\n   $LocNet/models-exps/VGG16_LocNet_InOut # LocNet In-Out model\n   $LocNet/models-exps/VGG16_LocNet_Borders # LocNet Borders model\n   $LocNet/models-exps/VGG16_LocNet_Combined # LocNet Combined model\n   $LocNet/models-exps/VGG16_BBoxReg # CNN-based bounding box regression model\n   ```\n\nAll of the above models are based on the VGG16-Net and are trained on the union of VOC2007 train+val plus VOC2012 train+val datasets. Note that they are not the same as those used to report result in the paper and hense their performance is slightly different from them (around 0.2 mAP points difference more or less in the percentage scale).\n\n### Demo\n\nAfter having complete the basic installation, you will be able to run the demo of object detection based on the LocNet localization models. In order to do that, open the matlab from `$LocNet/` directory and then run the script `'demo_LocNet_object_detection_pipeline'` from the matlab command line enviroment:  \n    \n    \n    cd $LocNet\n    matlab\n    # matlab command line enviroment\n    >> demo_LocNet_object_detection_pipeline    \n    \n  \n**Note:** you will require a GPU with at least 6 Gbytes of memory in order to run the demo. \n\nIn order to play with the parameters of the object detection pipeline read and edit the [demo script](https://github.com/gidariss/LocNet/blob/master/code/examples/demo_LocNet_object_detection_pipeline.m):  \n    \n    # matlab command line enviroment\n    >> edit demo_LocNet_object_detection_pipeline.m   \n  \n### Installing and using the box proposals algorithms (Optional)\n\nFirst install 1) [Edge Boxes](https://github.com/pdollar/edges), 2) [Dollar's image processing MATLAB toolbox](http://vision.ucsd.edu/~pdollar/toolbox/doc/index.html) (used in Edge Boxes), and 3) [Selective Search](http://huppelen.nl/publications/SelectiveSearchCodeIJCV.zip). Then, create symbolic links of the installation directories at `$LocNet/external/`:       \n   \n    \n    # $edges: installation directory of Edge Boxes     \n    ln -sf $edges $LocNet/external/edges   \n    # $pdollar-toolbox: installation directory of Dollar's image processing MATLAB toolbox  \n    ln -sf $pdollar-toolbox $LocNet/external/pdollar-toolbox  \n    # $selective_search: installation directory of selective search code  \n    ln -sf $selective_search $LocNet/external/selective_search  \n    \nThe above packages are necessary in case you want to try the demo on an image of your choise.  \n\n### Downloading and preparing the PASCAL VOC2007 and VOC2012 datasets\n  \nIn order to run experiments (e.g. train or test models) on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) datasets you will need to download and prepare the corresponding data. For that purpose you will have to:\n\n1. Download the VOC datasets and VOCdevkit:\n   ```Shell\n   \n   # VOC2007 DATASET\n    wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar # VOC2007 train+val set\n    wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar # VOC2007 test set\n    wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar # VOC2007 devkit\n    # VOC2012 DATASET\n    wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar # VOC2012 train+val set\n    wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCdevkit_18-May-2011.tar  # VOC2012 devkit\n   ```\n2. Untar the VOC2007 tar files in a directory named `$datasets/VOC2007/VOCdevkit` and the VOC2012 tar files in a directory named `$datasets/VOC2012/VOCdevkit`:\n   ```Shell\n   \n   mkdir $dataset\n   # VOC2007 data:\n    mkdir $datasets/VOC2007\n    mkdir $datasets/VOC2007/VOCdevkit\n    tar xvf VOCtrainval_06-Nov-2007.tar  -C $datasets/VOC2007/VOCdevkit\n    tar xvf VOCtest_06-Nov-2007.tar -C $datasets/VOC2007/VOCdevkit\n    tar xvf VOCdevkit_08-Jun-2007.tar -C $datasets/VOC2007/VOCdevkit\n   # VOC2012 data:\n    mkdir $datasets/VOC2012\n    mkdir $datasets/VOC2012/VOCdevkit\n    tar xvf VOCtrainval_11-May-2012.tar -C $datasets/VOC2012/VOCdevkit\n    tar xvf VOCdevkit_18-May-2011.tar -C $datasets/VOC2012/VOCdevkit\n   ```\n3. They should have the following structure:\n   ```Shell  \n   \n   # VOC2007 structure:\n   $datasets/VOC2007/VOCdevkit/ # VOC2007 development kit\n   $datasets/VOC2007/VOCdevkit/VOCcode/ # VOC2007 development kit code\n   $datasets/VOC2007/VOCdevkit/VOC2007/ # VOC2007 images, annotations, etc \n   # VOC2012 structure:\n   $datasets/VOC2012/VOCdevkit/ # VOC2012 development kit\n   $datasets/VOC2012/VOCdevkit/VOCcode/ # VOC2012 development kit code\n   $datasets/VOC2012/VOCdevkit/VOC2012/ # VOC2012 images, annotations, etc \n   ```\n4. Create symlink of the `$datasets` directory at `$LocNet/datasets`:\n   ```Shell\n   \n   ln -sf $datasets $LocNet/datasets  \n   ```\n5. Download the pre-computed [Edge Box](https://drive.google.com/file/d/0BwxkAdGoNzNTT194VzhIak9KSk0/view?usp=sharing) and [Selective Search](https://drive.google.com/file/d/0BwxkAdGoNzNTRTdtWTliWC1QQnc/view?usp=sharing) proposals of PASCAL images (click the links) and place them on the following locations:\n   ```Shell\n\n    $LocNet/data/edge_boxes_data # Edge Box proposals  \n    $LocNet/data/selective_search_data # Selective Search proposals  \n   ```\n\n### Testing the (pre-)trained models on VOC2007 test set\n\nTo test the object detection pipeline with the (pre-)trained models on VOC2007 test do:\n    \n    # 1) open matlab from $LocNet directory\n    cd $LocNet\n    matlab\n    # 2) run in the matlab command line enviroment\n    >> script_test_object_detection_pipeline_PASCAL('VGG16_Reduced_MRCNN','VGG16_LocNet_InOut','bbox_proposals','edge_boxes','gpu_id',1);    \n   \n\nThe above command will use the Reduced MR-CNN recognition model and the LocNet In-Out localization model located in the following directories:\n   ```Shell\n\n    $LocNet/models-exps/VGG16_Reduced_MRCNN # Reduced MR-CNN recognition model\n    $LocNet/models-exps/VGG16_LocNet_InOut # LocNet In-Out localization model\n   ```\n\nIn general, you can specify the recognition and localization models that will be tested by giving the directory names of the corresponding models as the 1st and 2nd input arguments in the `script_test_object_detection_pipeline_PASCAL` script. For more instructions on how to test the (pre-)trained models see the script:  \n   ```Shell\n    $LocNet/code/script_test_object_detection_pipeline_commands.m\n   ```\n\n### Training your own LocNet models in PASCAL VOC datasets\n\nTo train your own LocNet localization models on PASCAL VOC dataset:\n\n1. Downlaod the archive file of [VGG16-Net model](https://drive.google.com/file/d/0BwxkAdGoNzNTLUVnWEg5aWRtaFU/view?usp=sharing) pre-trained on ImageNet classification task and then unzip and untar it in the following location: `$LocNet/data/vgg16_pretrained`.\n2. Then   \n   ```Shell\n   \n    # 1) open matlab from $LocNet directory\n    cd $LocNet\n    matlab\n    # 2) run in the matlab command line enviroment\n    >> script_train_LocNet_PASCAL('VGG16_LocNet_InOut_model','gpu_id',1,'loc_type','inout');  \n   ```\nThe above command will train a LocNet In-Out localization model in the union of VOC2007 train+val and VOC2012 train+val datasets using both Selecive Search and Edge Box proposals. The trained model will be saved in the destination directory:  \n   ```Shell\n\n    $LocNet/models-exps/VGG16_LocNet_InOut_model \n   ```\nof which the directory name is the string that is given as 1st input argument in the script `script_train_LocNet_PASCAL`. For more instructions on how to train your own LocNet localization models see the script:\n   ```Shell\n    $LocNet/code/script_train_localization_models_commands.m\n   ```\n   \n**Note:** for training each of the LocNet models you will require a GPU with at least 6 Gbytes of memory.\n", 
  "id": 55430887
}