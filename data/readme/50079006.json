{
  "read_at": 1462552521, 
  "description": "Semantic Text Similarity Dataset Hub", 
  "README.md": "Semantic Text Similarity Dataset Hub\n====================================\n\nA typical NLP machine learning task involves classifying a sequence of tokens\nsuch as a sentence or a document, i.e. approximating a function\n\n\tf_1(s) [?] [0,1]\n\n(where *f_1* may determine a domain, sentiment, etc.).  But there is a large\nclass of problems that are often harder and involve classifying a *pair* of\nsentences:\n\n\tf_2(s1, s2) [?] [0,1]*c\n\n(where s1, s2 are sequences of tokens and c is a rescaling factor like c=5).\n\nTypically, the function *f_2* denotes some sort of **semantic similarity**,\nthat is whether (or how much) the two parameters \"say the same thing\".\n(However, the function could do something else - like classify entailment\nor contradiction or just topic relatedness.  We may include such datasets\nas well.)\n\nThis repo aims to gather a variety of standard datasets and tools for training\nand evaluating such models in a single place, with the base belief that it\nshould be possible to build generic models for *f_2* that aren't tailored to\nparticular tasks (and even multitask learning should be possible).\n\nMost of the datasets are pre-existing; text similarity datasets that may be\nredistributed (at least for research purposes) are included.  Always check\nthe licence of a particular dataset.  Some datasets may be original though,\nbecause we are working on many applied problems that pertain training such\na function...\n\nThe contents of dataset-sts and baseline results are described in the\npaper [Sentence Pair Scoring: Towards Unified Framework for Text Comprehension](http://arxiv.org/abs/1603.06127).\n\n**Pull requests welcome that extend the datasets, or add important comments,\nreferences or attributions.  Please let us know if we misread some licence\nterms and shouldn't be including something, we'll take that down right away!**\n\nPull request that include simple baselines for *f_2* models are also welcome.\n(Simple == they fit in a couple of screenfuls of code and are batch-runnable.\nPython is preferred, but not mandatory.)\n\n\nSoftware Tools\n--------------\n\nTo get started with simple classifiers that use task-specific code,\nlook at the **examples/** directory.\nTo get started with task-universal deep learning models, look at the\n**tools/**, **models/** and **tasks/** directory.\n\n  * **pysts/** Python module contains various tools for easy loading,\n    manipulation and evaluation of the dataset.\n\n  * **pysts/kerasts** the KeraSTS allows easy prototyping of deep learning\n    models for many of the included tasks using the Keras library.\n\n  * **examples/** contains a couple of simple, self-contained baselines\n    on various tasks.\n\n  * **models/** directory contains various strong baseline models using\n    the KeraSTS toolkit, including state-of-art neural networks\n\n  * **tasks/** directory contains model-independent interfaces to datasets\n    for various tasks (from Answer Sentence Selection to Paraphrasing)\n\n  * **tools/** directory contains tools that put models and tasks together;\n    training, evaluating, tuning and transferring models on tasks\n\nDatasets\n--------\n\nThis is for now as much a TODO list as an overview.\n\n### \"Paraphrasing\" Task\n\nThese datasets are about binary classification of independent sentence\n(or multi-sentence) pairs regarding whether they say the same thing;\nfor example if they describe the same event (with same data), ask the\nsame question, etc.\n\n  * [X] **data/para/msr/** MSR Paraphrase Dataset (TODO: pysts manipulation tools)\n\n  * [X] **data/para/askubuntu/** [AskUbuntu StackOverflow Similar Questions](https://github.com/taolei87/rcnn)\n\n  * [ ] [PPDB: The Paraphrase Database](http://www.cis.upenn.edu/~ccb/ppdb/)\n    contains only short phrase snippets, but tens of millions of pairs\n\n  * [ ] More [Stack Exchange](https://archive.org/details/stackexchange) data?\n    (some is also contained in the new STS datasets)\n\n### \"Semantic Text Similarity\" Task\n\nThese datasets consider the semantic similarity of independent pairs of texts\n(typically short sentences) and share a precise similarity metric definition\nof assigning a number between 0 to 5 to each pair denoting the level of\nsimilarity/entailment.\n\n  * [X] **data/sts/semeval-sts/** SemEval STS Task - multiple years, each covers a bunch of\n    topics that share the same precise similarity metric definition\n\n  * [X] **data/sts/sick2014/** SemEval SICK2014 Task\n\n  * [ ] [SemEval 2014 Cross-level Semantic Similarity Task](http://alt.qcri.org/semeval2014/task3/index.php?id=data-and-tools)\n    (TODO; 500 paragraph-to-sentence training items)\n\n### \"Entailment\" Task\n\nThese datasets classify independent pairs of \"hypothesis\" and \"fact\"\nsentences as entailment, contradiction or unknown.\n\n  * [X] **data/rte/sick2014/** SemEval SICK2014 Task also includes entailment data\n\n  * [X] **data/rte/snli/** [The Stanford Natural Language Inference (SNLI) Corpus](http://nlp.stanford.edu/projects/snli/)\n(570k pairs dataset for an RTE-type task).\n\n  * [ ] RTE Datasets up to RTE-3 http://nlp.stanford.edu/RTE3-pilot/ (TODO)\n\n### \"Answer Sentence Selection\" Task\n\nThese datasets concern a \"bipartite ranking\" task.  That is, each tuple\nof sentences is binary classified, but there are many different S1 sentences\ngiven the same S0 and the ultimate goal is to sort positive-labelled S1s\nabove negative-labelled S1s for each S0.\n\nTypically, S0 is a question and S1 are potentially-answer-bearing passages\n(in that case, identifying the actual answer might be an auxiliary task\nto consider; see anssel-yodaqa).  However, other scenarios are possible, like\nthe Ubuntu Dialogue Corpus where S1 are dialogue followups to S0.\n\n  * [X] **data/anssel/wang/** Answer Sentence Selection - original Wang dataset\n\n  * [X] **data/anssel/yodaqa/** Answer Sentence Selection - YodaQA-based\n\n  * [ ] [InsuranceQA Dataset](https://github.com/shuzi/insuranceQA)\n(used in recent IBM papers, 25k question-answer pairs; unclear licencing)\n\n  * [X] **data/anssel/wqmprop/** Property Path Selection (based on WebQuestions + YodaQA)\n\n  * [X] **data/anssel/ubuntu/** The Ubuntu Dialogue Corpus\ncontains pairs of sequences where the second sentence is a candidate for being\na followup in a community techsupport chat dialog.  10M pairs make this\nawesome.\n\n### \"Hypothesis Evidencing\" Task\n\nSimilar to the \"Answer Sentence Selection\" task, these datasets need to\nconsider a variety of S1 given a fixed S0 - the desired output should be\nhowever a judgement about S0 alone (typically true / false).\n\n  * [X] **data/hypev/argus/** Argus Dataset (Yes/No Question vs. News Headline)\n\n  * [X] [AI2 8th Grade Science Questions](http://allenai.org/data.html)\nare 641 school Science quiz questions (A/B/C/D test format), stemming from\n[The Allen AI Science Challenge](https://www.kaggle.com/c/the-allen-ai-science-challenge/)\nWe are going to produce a dataset that merges questions and answers in a single\nsentence, and pairs each with potential-evidencing sentences from Wikipedia and\nCK12 textbooks.  This will be probably the hardest dataset by far included in\nthis repo for some time.  (We may also want to include the Elementary dataset.)\n\n  * [ ] bAbI has a variety of datasets, especially re memory networks (memory\nrelevant to a given question), though with an extremely limited vocabulary.\n\n  * [ ] [Machine Comprehension Test (MCTest)](http://research.microsoft.com/en-us/um/redmond/projects/mctest/)\ncontains 300 children stories with many sentences and 4 questions each.\nA share-alike type licence.\n\n  * [ ] More \"Entrance Exam\" tasks solving multiple-choice school tests.\n\n\nOther Datasets\n--------------\n\nSome datasets are not universally available, but we may accept contributions\nregarding code to load them.\n\n### Non-Redistributable Datasets\n\nSome datasets cannot be redistributed.  Therefore, some scientists\nmay not be able to agree with the licence and download them, and/or may\ndecide not to use them for model development and research (if it is in\ncommercial setting), but only for some final benchmarks to benefit\ncross-model comparisons.  We *discourage* using these datasets.\n\n  * [ ] [Microsoft Research Video Description Corpus](http://research.microsoft.com/en-us/downloads/38cf15fd-b8df-477e-a4e4-a4680caa75af/)\n(video annotation task, 120k sentences in 2k clusters)\n\n  * [ ] [Microsoft Research WikiQA Corpus](http://research.microsoft.com/en-US/downloads/4495da01-db8c-4041-a7f6-7984a4f6a905/default.aspx)\n(Answer Selection task, 3k questions and 29k answers with 1.5k correct)\n\n  * [ ] [STS2013 Joint Student Response Analysis (RTE-8)](https://www.cs.york.ac.uk/semeval-2013/task7/index.php%3Fid=data.html)\n\n### Non-free Datasets\n\nSome datasets are completely non-free and not available on the internet,\ntherefore as strong believers in reproducible experiments and open science,\nwe *strongly discourage their usage*.\n\n  * [ ] [STS2013 Machine Translation](https://catalog.ldc.upenn.edu/LDC2013T18)\npairs translated and translated-and-postedited newswire headlines.\nPayment required.\n\n  * [ ] [TAC tracks RTE-4 to RTE-7](http://www.nist.gov/tac/data/).\nPrinted user agreement required.\n\n\nAlgorithm References\n--------------------\n\nHere, we refer to some interesting models for sentence pair classification.\nWe focus mainly on papers that consider multiple datasets or are hard to find;\nyou can read e.g. about STS winners on the STS wiki, about anssel/wang models\non the ACL wiki, about RTE models on the SNLI page.\n\n  * https://github.com/alvations/stasis contains several baselines and another\n    view of the datasets (incl. the CLSS task)\n  * https://github.com/ryankiros/skip-thoughts\n  * Standard memory networks (MemNN, MemN2N) are in fact *f_2* models at their\n    core; very similar to http://arxiv.org/abs/1412.1632\n\nLicence and Attribution\n-----------------------\n\nAlways check the licences of the respective datasets you are using!  Some of\nthem are plain CC-BY, others may be heavily restricted e.g. for non-commercial\nuse only.  Default licence for anything else in this repository is ASLv2 for\nthe code, CC-BY 4.0 for data.\n\nThere should be a paper on this conglomerate of datasets (and comparison of\n*f_2* metrics) to cite soon!  (As of Jan 2016.)  Watch this space for the\nreference when it's done.\n", 
  "id": 50079006
}