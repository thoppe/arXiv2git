{
  "read_at": 1462547781, 
  "description": "Sublinear memory optimization for deep learning, reduce GPU memory cost to train deeper nets", 
  "README.md": "# MXNet Memory Monger\n\nThis project contains a 150 lines of python script to give sublinear memory plans of deep neural networks.\nThis allows you to trade computation for memory and get sublinear memory cost,\nso you can train bigger/deeper nets with limited resources.\n\n## Reference Paper\n\n[Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/abs/1604.06174) Arxiv 1604.06174\n\n## How to Use\n\nThis code is based on [MXNet](https://github.com/dmlc/mxnet), a lightweight, flexible and efficient framework for deep learning.\n\n- Configure your network as you normally will do using symbolic API\n- Give hint to the allocator about the possible places that we need to bookkeep computations.\n  - Set attribute ```mirror_stage='True'```, see [example_resnet.py](example_resnet.py#L25)\n  - The memonger will try to find possible dividing points on the nodes that are annotated as mirror_stage.\n- Call ```memonger.search_plan``` to get an symbolic graph with memory plan.\n\n```python\nimport mxnet as mx\nimport memonger\n\n# configure your network\nnet = my_symbol()\n\n# call memory optimizer to search possible memory plan.\nnet_planned = memonger.search_plan(net)\n\n# use as normal\nmodel = mx.FeedForward(net, ...)\nmodel.fit(...)\n```\n\n## Write your Own Memory Optimizer\n\nMXNet's symbolic graph support attribute to give hint on whether (mirror attribute) a result\ncan be recomputed or not. You can choose to re-compute instead of remembering a result\nfor less memory consumption. To set output of a symbol to be re-computable, use\n```python\nsym._set_attr(force_mirroring='True')\n```\n\nmxnet-memonger actually use the same way to do memory planning. You can simply write your own memory\nallocator by setting the force_mirroring attribute in a smart way.\n", 
  "id": 56810814
}