{
  "read_at": 1462511588, 
  "description": null, 
  "README.md": "# autobw\n`autobw` is a simple library for automatically performing **a backwards pass, given only a forwards pass,** in Torch. \n A major advantage of this is that the neural network's **structure need not be fixed before runtime**. This allows for easy implementation of structures such as recurrent networks. See the example below.\n\n\nBackpropagation is often described as a method for propagating gradients through a computational graph. One way to implement it for graphs is to explicitly construct a graph given by the user, then evaluate the computational nodes in the order specified in the forward pass, then again but in reverse for the backward pass. \n\n## Install\n```\nluarocks install https://raw.githubusercontent.com/bshillingford/autobw.torch/master/autobw-scm-1.rockspec\n```\n\n## Details\nA method that's closer to how one may reason about a neural network is to explicitly write down a forward pass while recording the statements as they are being executed, then execute the statements' derivative computations (aka adjoint) in reverse. This is equivalent to specifying a computation graph, but more explicit, and allows the user to use **control-flow such as for loops and conditionals**.\n\nThis is similar to the approach taken by implementations of reverse-mode automatic differentiation, see e.g. <http://arxiv.org/abs/1502.05767>.\n\n## Examples:\nA simple example of computing `linear(x1) + x2 * sigmoid(x3)`, but **randomly** replacing `sigmoid(x3)` with `x3` sometimes:\n```lua\nlin = nn.Linear(5,5)\nadd = nn.CAddTable()\nmul = nn.CMulTable()\nsigm = nn.Sigmoid()\n\ntape = autobw.Tape()\n\n-------------- START OF FORWARD PASS --------------\n-- records the sequence of operations\ntape:begin()\ncoin_flip = torch.rand(1)[1]\nval1 = lin:forward(x1)\n\nif coin_flip > 0.5 then\n  maybe_sigmoid = sigm:forward(x3)\nelse\n  maybe_sigmoid = x3\nend\n\nresult = add:forward{val1, mul:forward{x2, maybe_sigmoid}}\ntape:stop()\n-------------- END OF FORWARD PASS --------------\n\n-- Play it back in reverse:\ntape:backward()\n\n-- Now, the gradients are in the four nn.Module objects as usual.\n```\n\nNote: I don't actually use the gradients at all here, and I don't set them to zero first, just to keep the example simple.\nSee also [our nngraph practical](https://github.com/oxford-cs-ml-2015/practical5/blob/master/practical5.pdf) for the equivalent in `nngraph`.\n\n\n### RNN Example\n\nSee the [examples folder](examples/) for a [fully functional rnn example](examples/rnn_example.lua) with toy data.\n\n### LSTM example\nThe LSTM example <https://github.com/oxford-cs-ml-2015/practical6> can easily be shortened by using this. We delete the backward pass, and simply play it back from the recorded forward pass:\n```lua\n-- setup autodiff\ntape = Tape() -- TODO: local\n\n-- do fwd/bwd and return loss, grad_params\nfunction feval(x)\n    if x ~= params then\n        params:copy(x)\n    end\n    grad_params:zero()\n    \n    ------------------ get minibatch -------------------\n    local x, y = loader:next_batch()\n\n    ------------------- forward pass -------------------\n    tape:begin() -----------------\n    local embeddings = {}            -- input embeddings\n    local lstm_c = {[0]=initstate_c} -- internal cell states of LSTM\n    local lstm_h = {[0]=initstate_h} -- output values of LSTM\n    local predictions = {}           -- softmax outputs\n    local loss = 0\n\n    for t=1,opt.seq_length do\n        embeddings[t] = clones.embed[t]:forward(x[{{}, t}])\n\n        -- we're feeding the *correct* things in here, alternatively\n        -- we could sample from the previous timestep and embed that, but that's\n        -- more commonly done for LSTM encoder-decoder models\n        lstm_c[t], lstm_h[t] = unpack(clones.lstm[t]:forward{embeddings[t], lstm_c[t-1], lstm_h[t-1]})\n\n        predictions[t] = clones.softmax[t]:forward(lstm_h[t])\n        loss = loss + clones.criterion[t]:forward(predictions[t], y[{{}, t}])\n    end\n    tape:stop() -----------------\n\n    ------------------ backward pass -------------------\n    tape:backward()\n\n    ------------------------ misc ----------------------\n    -- transfer final state to initial state (BPTT)\n    initstate_c:copy(lstm_c[#lstm_c])\n    initstate_h:copy(lstm_h[#lstm_h])\n\n    -- clip gradient element-wise\n    grad_params:clamp(-5, 5)\n\n    return loss, grad_params\nend\n```\n", 
  "id": 34748829
}