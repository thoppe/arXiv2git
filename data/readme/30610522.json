{
  "read_at": 1462551936, 
  "description": "Fastidious accounting of entropy streams into and out of optimization and sampling algorithms.", 
  "readme.md": "# Early Stopping is Nonparametric Variational Inference\n\n<img src=\"https://raw.githubusercontent.com/HIPS/maxwells-daemon/master/experiments/2015_03_02_funnel/2/dists.png\" width=\"400\">\n\nSource code for http://arxiv.org/abs/1504.01344\n\n### Abstract:\n\nWe show that unconverged stochastic gradient descent can be interpreted as a procedure that samples from a nonparametric variational approximate posterior distribution. This distribution is implicitly defined as the transformation of an initial distribution by a sequence of optimization updates. By tracking the change in entropy over this sequence of transformations during optimization, we form a scalable, unbiased estimate of the variational lower bound on the log marginal likelihood. We can use this bound to optimize hyperparameters instead of using cross-validation. This Bayesian interpretation of SGD suggests improved, overfitting-resistant optimization procedures, and gives a theoretical foundation for popular tricks such as early stopping and ensembling. We investigate the properties of this marginal likelihood estimator on neural network models.\n\nAuthors:\n[Dougal Maclaurin](mailto:maclaurin@physics.harvard.edu),\n[David Duvenaud](http://mlg.eng.cam.ac.uk/duvenaud/), and\n[Ryan P. Adams](http://people.seas.harvard.edu/~rpa/)\n\nFeel free to email us with any questions at (maclaurin@physics.harvard.edu), (dduvenaud@seas.harvard.edu).\n\nFor a look at some directions that didn't pan out, take a look at our early [research log](experiments/research-log.md).\nWe also played around with tracking the entropy of Hamiltonian Monte Carlo without accept/reject steps, but that didn't make it into the final version of the paper.\n", 
  "id": 30610522
}