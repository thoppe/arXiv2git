{
  "read_at": 1462547677, 
  "description": "Deep Residual Networks with 1K Layers", 
  "README.md": "# Deep Residual Networks with 1K Layers\n\nBy [Kaiming He](http://kaiminghe.com), [Xiangyu Zhang](https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=en), [Shaoqing Ren](http://home.ustc.edu.cn/~sqren/), [Jian Sun](http://research.microsoft.com/en-us/people/jiansun/).\n\nMicrosoft Research Asia (MSRA).\n\n## Table of Contents\n0. [Introduction](#introduction)\n0. [Notes](#notes)\n0. [Usage](#usage)\n\n\n\n## Introduction\n\nThis repository contains re-implemented code for the paper \"Identity Mappings in Deep Residual Networks\" (http://arxiv.org/abs/1603.05027). This work enables training quality **1k-layer** neural networks in a super simple way.\n\n*Acknowledgement*: This code is re-implemented by Xiang Ming from Xi'an Jiaotong Univeristy for the ease of release.\n\n**Seel Also:** Re-implementations of **ResNet-200 [a] on ImageNet** from Facebook AI Research (FAIR): https://github.com/facebook/fb.resnet.torch/tree/master/pretrained\n\nRelated papers:\n\n\t[a]\t@article{He2016,\n\t\t\tauthor = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n\t\t\ttitle = {Identity Mappings in Deep Residual Networks},\n\t\t\tjournal = {arXiv preprint arXiv:1603.05027},\n\t\t\tyear = {2016}\n\t\t}\n\t\n\t[b] @article{He2015,\n\t\t\tauthor = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n\t\t\ttitle = {Deep Residual Learning for Image Recognition},\n\t\t\tjournal = {arXiv preprint arXiv:1512.03385},\n\t\t\tyear = {2015}\n\t\t}\n\t\n\n\t\n## Notes\n\n0. This code is based on the implementation of Torch ResNets (https://github.com/facebook/fb.resnet.torch).\n\n0. The experiments in the paper were conducted in Caffe, whereas this code is re-implemented in Torch. We observed similar results within reasonable statistical variations.\n\n0. To fit the 1k-layer models into memory without modifying much code, we simply reduced the mini-batch size to 64, noting that results in the paper were obtained with a mini-batch size of 128. Less expectedly, the results with the mini-batch size of 64 are slightly better:\n\n\tmini-batch |CIFAR-10 test error (%): (median (mean+/-std))\n\t:---------:|:------------------:\n\t128 (as in [a]) | 4.92 (4.89+/-0.14)\n\t64 (as in this code)| **4.62** (4.69+/-0.20)\n\n0. Curves obtained by running this code with a mini-batch size of 64 (training loss: y-axis on the left; test error: y-axis on the right):\t\n![resnet1k](https://cloud.githubusercontent.com/assets/11435359/14414142/68714c82-ffc0-11e5-8b1b-657fdb3d96a6.png)\n\t\n## Usage\n\n0. Install Torch ResNets (https://github.com/facebook/fb.resnet.torch) following instructions therein.\n0. Add the file resnet-pre-act.lua from this repository to ./models.\n0. To train ResNet-1001 as of the form in [a]:\n```\nth main.lua -netType resnet-pre-act -depth 1001 -batchSize 64 -nGPU 2 -nThreads 4 -dataset cifar10 -nEpochs 200 -shareGradInput false\n```\n**Note**: ``shareGradInput=true'' is not valid for this model yet.\n", 
  "id": 55462616
}