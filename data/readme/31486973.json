{
  "read_at": 1462544884, 
  "description": "", 
  "README.md": "# Shallots\n\n\"Shining the light on the dark web, analysis of what is beneath the surface of the internet\"\n<br><a href=\"http://www.shallots.info/\">Analysis dashboard</a>\n\n<a href=\"http://www.shallots.info/\"><img src=\"/images/screenshot3.png\" width=500></a>\n\n<i>Summary</i>\n\nThe dark web has a lot of secrets. My goal is to give some insight in what is going on there. I'll describe the main topics that are discussed there and give insight in their meaning. I'll divide the clusters into 2 groups (legal/illegal) and find out if they behave like 2 separate islands or are actually connected. And last I'll geographically plot the countries talked about in relation to the clusters. It will start with crawled dark web html content and it will end with a website with visualizations. \n\n<i>Description</i>\n\nTor helps anonymous online communication. It is meant to provide safety for vulnerable internet users such as political activists. The downside, however, is that it facilitates criminals that use servers that can only receive connections through Tor, to make it hard to get identified. Those servers are called hidden services and can be accessed through an .onion address. \n\nNot much research has been done on what is going on in this \"dark web\". There was some content clustering, which showed that both legal and illegal content is available on websites. It is not clear how connected those 2 groups are. \n\n<i>Motivation</i>\n\nIn 2011 I first encountered the illegal side of the dark web. Since then it kept surprising me that tools and analysts that focus on the internet, normally don't take the dark web into account. They actually should in my opinion because this is especially the place where things can come to the surface since users feel save by the anonymizing function of Tor. \n\nIt is an ideal way to combine my interest in the dark web with my preference for NLP, social network analysis (SNA) and visualization into one project. And it can grow along the way, if there is time, looking further into insights I get during the analysis.\n\n<i>Numbers</i><br>\nThe current setup works wih 3350 crawled onion websites (2408 of which are classified as being in english)\nThere are 1117 distinct domains within the data, 743 of which being in English. \n\n<i>Data Sources</i><br>\nCrawled dark web data stored in mongoDB, crawled by the builder of Ahmia and OnionBot.\n<a href=https://github.com/juhanurmi/ahmia/tree/master/onionbot>OnionBot</a>\n\n<i>Details</i>\n\n<img src='images/projectplan.png' width = 800>\n\n<i>Process</i>\n<br>-Get the scraped html content stored in MongoDB\n<br>-Check the scraped data for correctness and completeness (+EDA)\n<br>-Detect language of the content and continue only with English content data\n<br>-Find .onion links in result (regex) and fill the relations table with that (id, id) in SQL\n<br>-Clear html from content\n<br>-Clean stopwords, lemmatize and vectorize. Do topic modeling with varying k (somewhere around k=10)\n<br>-Read cluster top x words to decide what the best descriptive word is, if not clear, change k\n<br>-Store manually decided name, legal/illegal in table with cluster\n<br>-NER on country names for visualization\n<br>-Create concept graph data of similar words with word2vec\n<br>-Create json files with relevant data for the viz\n<br>-Create website with data viz dashboard\n\n<br>-Visualize \n<ul>\n<li>Barchart with on click -> wordcloud</li>\n<li>Network grouped clusters with relations between them based on url references</li>\n<li>Map of the world with spectrum red-green based on legal/illegal</li>\n<li>Mouseover piechart on map</li>\n</ul>\n\n<i>Architecture & implementation</i>\n<br>-Python\n<br>-MongoDB\n<br>-PostgreSQL\n<br>-d3.js\n<br>-NER from stanford\n<br>-Gensim\n<br>-sklearn NMF\n\n<img src='images/dataflow.png' width=800>\n\n<br>Chart for data viz\n<img src='images/vizflow.png' width=800>\n\n<i>Challenges I ran into:</i>\n<br>-Getting it to work on amazon & storage > a lot of crashes\n\n<i>References</i>\n<br>https://blog.torproject.org/category/tags/crawling \n<br>http://arxiv.org/pdf/1308.6768v2.pdf\n<br>http://www.dis.uniroma1.it/~dasec/DASec_Pustogarov.pdf\n<br>https://www.gwern.net/docs/sr/2014-spitters.pdf\n<br>https://github.com/juhanurmi/ahmia/tree/master/onionbot\n\n<i>Future work</i>\n<br>Check the future_work.MD for my plans for future improvements.\n\n<i>Dependencies</i>\n<br>pip install pymongo\n<br>conda install psycopg2\n<br>pip install psycountry\n<br>pip install fuzzywuzzy\n<br>Geograpy2 -> install from <a href=https://github.com/Corollarium/geograpy2>github</a>, but comment out the reference to geograpy-nltk in the install script. \n<br>pip install python-Levenshtein\n<br>conda install gensim\n\n<i>Run it</i>\n<br>Be sure to already have crawled data in mongodb\n<br>Python shallots.py for preparing the data\n<br>Python index.py starting the dashboard\n", 
  "id": 31486973
}