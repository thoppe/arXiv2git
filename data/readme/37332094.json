{
  "read_at": 1462549608, 
  "description": "Output-sensitive H-Redundancy Removal", 
  "README.md": "# Output-sensitive *H*-Redundancy Removal\n\n## Some background on output-sensitive algorithms\n\nAn output-sensitive algorithm is an algorithm whose complexity depends on the\nsize of the output. Thus, for a fixed instance size *n*, the same algorithm can\nhave different running times. A classical example of a problem solved by output\nsensitive algorithms is the computation of the convex hull of a finite\nset of points in *R^d*.\n\n### A canonical output-sensitive algorithm\n\nClarkson [1] gives an output-sensitive algorithm to find the minima\nset of a set of elements according to some partial order.\nThis algorithm uses at most *2nA* comparisons where *A* is the cardinality of\nthe minima set.\n\n\n#### Pseudo Code\n\n**input** *<* , *S*\n\n  1. Let *M* be the empty set.\n  2. While there are candidate elements left in *S*:\n    1. Let *x* be one of the candidates and decide whether it\n       is dominated by one of the minima elements in *M* using predicate *<*.\n    2. If so, discard *x*.\n    3. Otherwise, scan the candidates for a minimum according to *<*. At this point the\n       candidate set is nonempty and at least one of its elements must be a\n       minimum. We scan the candidate list to find this element, remove it from *S*\n       and put it in *M*.\n\n  3. Output *M*.\n\n#### Run-time Analysis\n\nFor step **2.i**, at most *n.A* comparisons are used since we compare each element\nof *S* with each elements of the minima set which is of cardinality at most *A*\nduring the execution of the algorithm.\n\nFor step **2.iii**, for each executed loop we\nobtain a new minimum and increase the size of the constructed minima set by\n*1*, hence there are at most *A* loops execution, each of which loops over at\nmost *n* elements. Step **2.iii** uses thus at most *nA* comparisons.\n\nThe algorithm can be implemented so that it runs in place. The running time is\ndominated by the comparison time and thus the\ncomplexity of this algorihtm is *O(nA)*.\n\n\n## *H*-Redundancy Removal\n\nWe want to solve the following problem:\n\n> Given a set *H* of *n* half-spaces in *R^d* we want to filter out all half-spaces\n*h* in *H* such that the intersection of all half-spaces in *H* does not change\nif we remove *h* from *H*.\n\n### Naive Algorithm\n\nA simple way of solving the above problem is to solve *n* linear programs.\nFor each *h* we consider the set of linear constraints defined by the half-spaces in\n*H \\ h* and solve a linear program in *d* variables with these constraints and\nwith an objective function whose gradient is the inverse of the direction of half-space *h*. If\nthe objective value of the optimal solution of this linear program\nis different from the objective value of the optimal solution of the same\nlinear program with the addition of a new linear constraint defined by half-space\n*h* then *h* is nonredundant. Otherwise, *h* is redundant.\n\nLet *LP(d,n)* denote the complexity of solving a linear program in *d*\nvariables with *n* constraints, then the complexity of this algorithm is\n*n LP(d,n)*.\n\n### Clarkson's Algorithm\n\nClarkson [1] gives an algorithm that runs much faster when the number *s* of\nnonredundant half-spaces is small relative to *n*.\n\nWe make several assumptions on the input:\n\n  1. The polyhedron defined by *H* is full-dimensional.\n  2. No inequality is a positive multiple of another one.\n  3. We are given a vertex *z* in the interior of the polyhedron defined by *H*.\n\nThese assumptions can be guaranteed using *d LP(d,n)* processing time. See [2] for details.\n\n#### Pseudo Code\n\n**input** *H*, *z*\n\n  1. Let *M* be the empty set.\n  2. While there are candidate elements left in *H*:\n    1. Let *h* be one of the candidates and decide whether it\n       is redundant with respect to the set of half-spaces *M*.\n    2. If so, discard *h*.\n    3. Otherwise, scan the candidates for a nonredundant half-space. At this point the\n       candidate set is nonempty and at least one of its elements must be\n       nonredundant. We scan the candidate list to find this element, remove it from *H*\n       and put it in *M*.\n\n  3. Output *M*.\n\nNote the similarity between this algorithm and the minima finding algorithm we\nintroduced earlier.\n\n#### Run-time Analysis\n\nFor step **2.i**, we solve a linear program in *d* variables with at most\n*s+1* constraints. In the case where *h* is nonredundant with respect to *M* we\nobtain an optimal solution _x*_ lying on the boundary of *h*. There are *n*\nsuch steps since after each execution of the main loop we either find\na nonredundant half-space or discard one.\n\nFor step **2.iii**, we use the following *rayshoot* procedure which given a set of\nhyperplanes *H*, a vertex *z* and a direction *r* finds the first hyperplane\nfrom *H* to meet the ray shot from *z* in direction *r*. For our use case,\nwe choose _r = x* - z_ and hence the rayshoot procedure finds a nonredundant half-space.\nHere we give a\nsimplified version of the algorithm that assumes a candidate solution is already\nknown, which is the case for us. In the description below, *h* denotes this\ncandidate solution and *w >= 0* is the scalar such that *z + wr* lies on *h*.\n\n**input** *H*, *z*, *r*, *h*, *w*\n\n  1. For each candidate *g* in *H*:\n\n    1. Compute *l = eval( g , z ) / dot( g , r )*, where *eval* evaluates the\n       linear expression *g = a_0 + a_1 x_1 + ... + a_n x_n* for *x = z* and\n       where *dot(.,.)* denotes the dot product.\n\n    2. if *l >= 0* and *l < w* let *l* be the new *w* and *g* the new *h*  ;\n\nThe complexity of the rayshoot algorithm is *O(nd)*. After each execution of\nstep **2.iii** we find a new nonredundant half-space and thus there are at most\n*s* such steps.\n\nThe total complexity of the algorithm is thus *O(n LP(d,s) + snd)*.\n\n### Combinatorial Redundancy Detection\n\nFukuda et al. [3] show that is possible to solve *H*-redundancy removal given\nonly the signs of a linear program dictionary\n(*n* constraints in *n + d* dimension where *n* are slack variables)\nencoding the set of half-spaces to consider.\n\nThey prove that there exists a redundancy certificate for each redundant\nhalf-space. Similarly, they prove that there exists a nonredundancy certificate\nfor each nonredundant half-space.\n\nThe complexity of this new algorithm is\n\n> R(n,d,s) = O(d n s^(d-1) LP(d,s) + d s^d LP(d,n))\n\nThe algorithm is simpler in the case of general position\n\n> R(n,d,s) = O(n LP(d,s) + s LP(d,n))\n\nwhich for small *s* is similar to the complexity of Clarkson's algorithm.\n\nMoreover the algorithm only needs a combinatorial encoding of the\nset of half-spaces. This property allows the algorithm to generalize to other\ncombinatorial structures.\n\n\n## References\n\n  1. CLARKSON, Kenneth L. More output-sensitive geometric algorithms. In :\n*Foundations of Computer Science, 1994 Proceedings., 35th Annual Symposium on.*\nIEEE, 1994. p. 695-702.\n  2. [Lecture: Polyhedral Computation, Spring 2015](http://www-oldurls.inf.ethz.ch/personal/fukudak/lect/pclect/notes2015/PolyComp2015.pdf)\n  3. FUKUDA, Komei, GARTNER, Bernd, et SZEDLAK, May. Combinatorial Redundancy Detection. *arXiv preprint arXiv:1412.1241*, 2014.\n", 
  "id": 37332094
}