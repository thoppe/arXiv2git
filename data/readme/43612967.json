{
  "read_at": 1462556120, 
  "description": "A precise to-read list for recurrent neural network (RNN).", 
  "README.md": "# awesome-neat-rnn\nThis is a precise to-read list for recurrent neural network (RNN). \n\nOmit the long author lists; start with year, followed by title, jounral, and link. Newer papers go first. There are additional resources such as codes, interesting blog posts, cool articles, etc.\n\nMaintainer: [Johnny Ho] (https://github.com/johnny5550822)\n\n# Contributions\nThis repository originally is decided for keeping track of resources related to recurrent neural network. I hope everyone can contribue to it and make it better! So, please submit pull requests! For any questions, contact me (johnny5550822@g.ucla.edu)\n\n# Additional resources\nThis repository is created in order to follow preciseness and neatness. There is another repository that also provide excellent (with full author lists, etc) sources for recurrent neural network, please visit [awesome-rnn] (https://github.com/kjw0612/awesome-rnn). \n\n# Table of Content\n- [Software Package] (#software-package)\n- [Sample Codes] (#sample-codes)\n- [Blogs] (#blogs)\n- [Review] (#review) \n- [Tutorial] (#tutorial)\n- [Language modeling] (#language-modeling)\n- [Translation](#translation)\n- [Image Generation](#image-generation)\n- [Hand-writing](#hand-writing)\n- [Text Generation](#text-generation)\n- [Questions and Answers] (#questions-and-answers)\n- [Cell Type](#cell-type)\n- [Other](#other)\n\n## Software Package\n+ python, [[neon](https://github.com/NervanaSystems/neon)]\n+ python, [[chainer](https://github.com/pfnet/chainer)]\n+ torch, [[oxnn](https://github.com/tkocisky/oxnn)]\n+ torch, [[Element-research](https://github.com/Element-Research/rnn)]\n+ Deep Learning in general\n  + torch, [[dp](https://github.com/nicholas-leonard/dp)], a torch deep learning library. I think the example folder is the most useful, for example, CNN implementation there\n\n## Sample Codes\n+ torch, [[char-rnn](https://github.com/karpathy/char-rnn)]\n+ torch, [[learning_to_execute](https://github.com/wojciechz/learning_to_execute)]\n+ torch, [[Oxford practical 6](https://github.com/oxford-cs-ml-2015/practical6)]\n+ torch, [[Spatial Transformer Layer](https://github.com/moodstocks/gtsrb.torch)]\n+ Deep Learing in general\n  + torch, [[torch7 official tutorials](https://github.com/torch/tutorials)]\n  + torch, [[torch7 official demos](https://github.com/torch/demos)], have a lot of good examples\n  + torch, [[UCLA IPAM course on torch7](http://code.madbits.com/wiki/doku.php?id=start)]\n  + torch, [[A simplified example on CNN](https://github.com/hpenedones/luacnn)]\n  + torch, [[Kaggle CIFAR-10](https://github.com/nagadomi/kaggle-cifar10-torch7/)], codes for Kaggle CIFAR-10 competition (CNN)\n+ Lua In general\n  + Lua, [[Learn Lua in 15 Minutes](http://tylerneylon.com/a/learn-lua/)]\n+ GitXiv, summary of recent published git repositoris for research algorithm [[link](http://gitxiv.com/)]\n\n## Blogs\n+ torch7 blogs (some cool explanation and codes), [[blog](http://torch.ch/blog/index.html)]\n+ Up-to-date DL news from notey [[blog](http://www.notey.com/blogs/deep-learning)]\n+ What does DL think about your selfie? [[blog](http://karpathy.github.io/2015/10/25/selfie/)]\n+ The Unreasonable Effectiveness of Recurrent Neural Networks. [[blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)]\n\n## Review\n+ 2015 Deep Learning, Nature [[paper](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html)]\n\n## Tutorial\n+ 2015, Recurrent Neural Networks Tutorial [[link](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)]\n+ 2015, understanding LSTM [[links](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)]\n+ 2003 A tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the \"echo state network\" approach [[link](http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf)]\n\n## Language Modeling\n+ 2016 Long Short-Term Memory-Networks for Machine Reading, arXiv [[paper](http://arxiv.org/pdf/1601.06733v3.pdf)]\n+ 2015 Teaching Machines to Read and Comprehend, NIPS [[paper](http://arxiv.org/pdf/1506.03340v1.pdf)]\n+ 2015 Character-Aware Neural Language Models, arXiv [[paper](http://arxiv.org/pdf/1508.06615v2.pdf)]\n\n## Translation\n+ 2014 Sequence to Sequence Learning with Neural Networks, NIPS [[paper](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks)]\n+ 2014 On the Properties of Neural Machine Translation: Encoder-Decoder Approaches, arXiv [[paper](http://arxiv.org/abs/1409.1259)]\n+ 2014 Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, arXiv [[paper](http://arxiv.org/abs/1406.1078)]\n+ 2013 Recurrent Continuous Translation Models, EMNLP [[paper](http://nal.co/papers/KalchbrennerBlunsom_EMNLP13)]\n\n## Image Generation\n+ 2015 DRAW: A Recurrent Neural Network For Image Generation, arXiv [[paper](http://arxiv.org/abs/1502.04623]\n+ 2015 Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image Generation, arXiv [[paper](http://arxiv.org/abs/1506.03500)]\n+ 2015 Generative Image Modeling Using Spatial LSTMs, arXiv [[paper](http://arxiv.org/abs/1506.03478)]\n+ 2014 Recurrent Models of Visual Attention, arXiv [[paper](http://arxiv.org/abs/1406.6247)]\n\n## Hand-writing\n+ 2013 Generating Sequences With Recurrent Neural Networks, arXiv [[paper](http://arxiv.org/abs/1308.0850)]\n+ 2007 Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks, NIPS [[paper](http://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks)]\n\n## Text Generation\n+ 2011 Generating Text with Recurrent Neural Networks, ICML [[paper](http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf)]\n\n## Questions and Answers\n+ 2015 Ask Your Neurons: A Neural-based Approach to Answering Questions about Images [[paper](http://arxiv.org/pdf/1505.01121.pdf)]\n+ 2015 VQA: Visual Question Answering [[paper](http://arxiv.org/pdf/1505.00468.pdf)]\n+ 2015 Exploring Models and Data for Image Question Answering [[paper](http://arxiv.org/pdf/1505.02074.pdf)]\n+ 2015 Are you talking to a machine? Dataset and methods for multilingual image question answering [[paper](http://arxiv.org/pdf/1505.05612.pdf)]\n+ 2015 Teaching Machines to read and comprehand [[paper](http://arxiv.org/pdf/1506.03340.pdf)]\n+ 2015 Ask me anything: dynamic memory networks for natural language processing [[paper](http://arxiv.org/pdf/1506.07285.pdf)]\n\n## Cell Type\n+ 2014 Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, arXiv [[paper](http://arxiv.org/abs/1412.3555)]\n+ 1997 Long Short-Term Memory, Neural Computation [[paper](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6795963)]\n\n## Other\n+ Hyperparameters Optimization\n  + 2015 Gradient-based Hyperparameter Optimization through Reversible Learning, Arxiv [[paper](http://arxiv.org/abs/1502.03492)] \n+ CNN: something cool and worth to share here\n  + 2015 Spatial Transformer Networks, NIPS [[paper](http://arxiv.org/pdf/1506.02025.pdf)]\n  + 2015 Human-level control through deep reinforcement learning, Nature [[paper](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html)]\n+ 2007 Multi-Dimensional Recurrent Neural Networks [[paper](http://people.idsia.ch/~juergen/icann_2007.pdf)]\n\n\n\n\n\n", 
  "id": 43612967
}