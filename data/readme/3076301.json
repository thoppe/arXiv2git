{
  "read_at": 1462543305, 
  "description": "", 
  "README.md": "# ArXiv analysis\n\nRun [online variational LDA](http://arxiv.org/abs/1206.7051v1) on all the\nabstracts from the arXiv. The implementation is based on [Matt Hoffman's\nGPL licensed code](http://www.cs.princeton.edu/~mdhoffma/).\n\n## Usage\n\nYou'll need a [`mongod`](http://www.mongodb.org/) instance running on\nthe port given by the environment variable `MONGO_PORT` and a\n[`redis-server`](http://redis.io/) instance running on the port given by\nthe `REDIS_PORT` environment variable.\n\nThe code depends on the Python packages: `numpy`, `scipy`, `requests`,\n`pymongo` and `redis`.\n\n* `mkdir abstracts`\n* `./analysis.py scrape abstracts` -- scrapes all the metadata from the arXiv\n  [OAI interface](http://arxiv.org/help/oa/index) and saves the raw XML\n  responses as `abstracts/raw-*.xml`. This takes a _long time_ because of\n  the arXiv's flow control policies. It took me approximately 6 hours.\n* `./analysis.py parse abstracts/raw-*.xml` -- parses the raw responses and\n  saves the abstracts to a MongoDB database called `arxiv` in the collection\n  called `abstracts`.\n* `./analysis.py build-vocab` -- counts all the words in the corpus removing\n  anything with less than 3 characters and removing any stop words.\n* `./analysis.py get-vocab 100 5000 > vocab.txt` -- lists the vocabulary\n  skipping the first 100 most popular words and keeping 5000 words total.\n* `./analysis.py run vocab.txt` -- runs online variational LDA by randomly\n  selecting articles from the database. The topic distributions are stored\n  in the `lambda-*.txt` files. This will run forever so just kill it whenever\n  you feel like it.\n* `./analysis.py vocab.txt lambda-100.txt` -- list the topics and their most\n  common words at step 100.\n", 
  "id": 3076301
}