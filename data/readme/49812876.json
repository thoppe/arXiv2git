{
  "read_at": 1462553029, 
  "description": "German Credit Classifier", 
  "README.md": "# credit-classifier\n\n## Introduction \nThis is a small tech demonstration of analyzing credit data from Hamburg University. The analyzer can analyze some data collected by a bank giving a loan. The dataset consists of 1000 datapoints of categorical and numerical dataas well as a good credit vs bad credit metric which has been assigned by bank employees. The dataset is fully anonymized. The code in this repository contains a linear regression and a neural network machine learning models to try to predict the credit rating that a bank employee would assign given some datapoints.\n\n## Requirements\nPython<br>\nTensorFlow<br>\nScikit Learn\n\n## The dataset\nThe dataset consists of 1000 datatpoints each with 20 variables (dimensions) 7 are numerical and 13 are categorical. The categorical data is encoded according to terms which have meaning to the bankers such as current employment timeframe:\n\n* A71 : unemployed\n* A72 :       ... < 1 year\n* A73 : 1  <= ... < 4 years\n* A74 : 4  <= ... < 7 years\n* A75 :       .. >= 7 years\n\t      \nOr what kind of housing the person has:\n\n* A151 : rent\n* A152 : own\n* A153 : for free\n\t         \nOther data provided in the dataset contains numerical information such as:\n\n* Duration of the loan in months\n* Credit amount\n* Age in years\n\nSee the Description file attached to the repository for further details on the data. Source:\n\nProfessor Dr. Hans Hofmann<br>\nInstitut fur Statistik und Okonometrie<br>\nUniversitat Hamburg<br>\nFB Wirtschaftswissenschaften<br>\nVon-Melle-Park 5<br>\n2000 Hamburg 13\n\n### Dataset preprocessing  \n##### Categorical Data\nBecause the majority of this dataset is categorical data we must first digitize the categorical variables into numbers from which our ML (machine algorithms) can learn. The most common way of encoding categorical variables is One-Hot-Encoding<br>\n [One-Hot on Wikipedia](https://en.wikipedia.org/wiki/One-hot)<br>\n [scikit learn encoding categorical variable](http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features)<br>\n \nOne-Hot encoding is basically assigning a binary state machine state to each unique value in the set of categorical variables.For example if there are 3 categorical variables {A151, A152, A153} they will be encoded as {001, 010, 100}. This is important because since we cannot easily estimate how close or how far the variables should be to each other if they were on the same axis. The downside of One-Hot encoding is that it increases the number of dimensions and therefore increases the computational complexity of any models using the data. \n\n##### Binary Categorical Data\n\nAn exception to the above rule is when the categorical variable consists of only two unique values. In that case the difference can be encoded as either 0 vs 1 or -1 vs +1 with the latter being better for building ML models.\n\n##### Numerical Data\n\nNumerical data should be normalized even though it is not strictly neccessary, but it has been shown that normalization of numerical data can lead to faster training of neural network weights.\n\n#### Implementation of dataset preproccessing\nYou can find the implementation of my dataset preprocessor which does the above three operations in createNumericalData.py\n\n## Classification\n\n### Linear regression\nA linear regression simply tries to linearly separate the model by finding an equation of a line which can match some ideal parameters (to be found by a cost function). The linear regression can be modeled by finding a combination of **W** (weights) and b (bias term) which when combined can give us the line separating our data.\n\n**Y** = **X** * **w** + b\n\nWhere: **X** = {x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>} and **W** = {w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>}\n\nTherefore **Y** can be expanded to:\n\n**Y** = w<sub>1</sub>x<sub>1</sub> + w<sub>1</sub>x<sub>2</sub> + ... + w<sub>n</sub>x<sub>n</sub> + b\n\n#### Finding optimal w and b\nLet: w<sub>0</sub> = b <br>\nThe bias term b can be thought of as part of the weights vector with degree of zero. The optimal weights can be found using a classical method called least square error minimization. The sum of the square errors can be calculated with the following equation:\n\nE(w) =  **S**<sub>1</sub><sup>n</sup> (y<sub>i</sub> - x<sub>i</sub><sup>T</sup>w)<sup>2</sup> = (y-Xw)<sup>T</sup>(y-Xw)\n\nFinding the vetor w that minimizes the above formula is the solution that offers a best fit model. This can be solved for using a gradient descent method.\n\n#### Cost Matrix\nThe  dataset requires the use of a cost matrix (see below). In the credit/loan problem it is worse to predict a customer as good when he is bad (false positive) vs predicting a customer as bad when he is good (false negative).\n\n|          |Good |   Bad  |\n|:--------:|:---:|:------:|\n| **Good** | 0   |     1  |\n|  **Bad** | 5   |    0   |\n\nThis cost matrix has been incorporated into the naive least square errors function described above. The false positives are multiplied by 5 and false negatives are multiplied by 1.\n\nSee code in trainLinearRegression.py\n\n### Neural Network\n\n```\n            Hidden     Hidden\nInput       Layer1     Layer2\nLayer      +-----+    +-----+\n           |     |    |     |\n+-----+    |     +--> |     |\n|     +--> +-----+    +-----+\n|     |    +-----+    +-----+     Output\n+-----+    |     +--> |     |     Layer\n+-----+    |     |    |     |    +-----+\n|     |    +-----+    +-----+    |     |\n|     +--> +-----+    +-----+ +> |     |\n+-----+    |     |    |     |    +-----+\n+-----+    |     +--> |     |    +-----+\n|     |    +-----+    +-----+    |     |\n|     +--> +-----+    +-----+ +> |     |\n+-----+    |     |    |     |    +-----+\n+-----+    |     +--> |     |\n|     +--> +-----+    +-----+     2 neurons\n|     |    +-----+    +-----+\n+-----+    |     +--> |     |\n           |     |    |     |\n 59 n      +-----+    +-----+\n\n            124 n      124 n\n```\n\nThe neural network is built according to the above diagram. There are 59 neurons associated with the input variables (one for each feature in the adjusted dataset), there are two hidden layers of 124 neurons each and finally an output layer corresponding to the 2 classes (bad credit vs good credit). Each one of the neurons is connected with all neurons in the previous and next layers.\n\nThe network is initialized with random weights from a Gaussian distribution with {m = 0, s = 0.1}, a learning rate of 10<sup>-4</sup> and 3000 epochs of training for each fold and is trained by minimizing the cost function described using an Adam Optimizer [3].\n\nSee code in trainNeuralNet.py\n\n### 10-Fold cross validation\n\nTo use a fair comparison of the performance of the two networks I am using 10-Fold cross validation and only recording the values from the validation set. There should not be any overfitting in the \n\n## Results\n\n#### Linear model\n\nAverage validation accuracy of 10 runs: 0.793333<br>\nStandard deviation: 0.010506<br>\nConfusion matrix:\n\n|          |Good |   Bad  |\n|:--------:|:---:|:------:|\n| **Good** |558  |   72   |\n|  **Bad** |114  |  156   |\n\n\n#### Neural network model\nNearly perfect results from the neural network:\n\nAverage validation accuracy of 10 runs: 0.970778<br>\nStandard deviation: 0.045872<br>\nConfusion matrix:\n\n|          |Good |   Bad  |\n|:--------:|:---:|:------:|\n| **Good** |623.1|   6.9  |\n|  **Bad** |19.4 | 250.6  |\n\nThe very small standard deviations included in the results signify that the results are repeatable with respect to the folds used in k-fold cross validation.\n\n## Analysis \nNeural networks, even a simple one as above can both obtain a greater overall accuracy and minimize the number of false positives  when compared to linear regressions. Neural nets can be easily improved by adding more layers. The only downside of neural nets is that it is harder to describe which features are considered important by neural nets as opposed to by a single weight vector in a linear regression.\n\n\n## References \n[1] Sola, J., & Sevilla, J. (1997). Importance of input data normalization for the application of neural networks to complex industrial problems. Nuclear Science, IEEE Transactions on, 44(3), 1464-1468.<br>\n[2] Duan, K., Keerthi, S. S., Chu, W., Shevade, S. K., & Poo, A. N. (2003). Multi-category classification by soft-max combination of binary classifiers. In Multiple Classifier Systems (pp. 125-134). Springer Berlin Heidelberg.<br>\n[3] Kingma, D., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.[Link](http://arxiv.org/pdf/1412.6980v7.pdf)<br>\n\n\n\n\n", 
  "id": 49812876
}