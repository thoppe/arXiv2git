{
  "read_at": 1462555831, 
  "description": "", 
  "README.md": "# agile_grasp 2.0\n\n* **Author:** Andreas ten Pas (atp@ccs.neu.edu)\n* **Version:** 1.0.0\n* **ROS Wiki page:** not available yet\n* **Author's website:** [http://www.ccs.neu.edu/home/atp/](http://www.ccs.neu.edu/home/atp/)\n\n\n## 1) Overview\n\nThis package localizes antipodal grasps in 3D point clouds. AGILE stands for **A**ntipodal **G**rasp **I**dentification and \n**LE**arning. The reference for this package is: \n[High precision grasp pose detection in dense clutter](http://arxiv.org/abs/1603.01564). *agile_grasp 2.0* is an improved \nversion of our previous package, [agile_grasp](http://wiki.ros.org/agile_grasp).\n\nThe package already comes with pre-trained machine learning classifiers and can be used (almost) out-of-the-box with \nRGBD cameras such as the Microsoft Kinect and the Asus Xtion Pro as well as point clouds stored as *.pcd files.\n\n\n## 2) Requirements\n\n1. [ROS Indigo](http://wiki.ros.org/indigo) (installation instructions for [Ubuntu](http://wiki.ros.org/indigo/Installation/Ubuntu))\n2. [Lapack](http://www.netlib.org/lapack/) (install in Ubuntu: `$ sudo apt-get install liblapack-dev`) \n3. [OpenNI](http://wiki.ros.org/openni_launch) or a similar range sensor driver\n4. [Caffe](http://caffe.berkeleyvision.org/) \n\n\n## 3) Installation\n\n1. Install Caffe. [Instructions](https://github.com/BVLC/caffe/wiki/Install-Caffe-on-EC2-from-scratch-%28Ubuntu,-CUDA-7,-cuDNN%29) for Ubuntu 14.04. \n2. Compile Caffe as a cmake library ([https://github.com/BVLC/caffe/pull/1667](instructions)):\n\n   ```\n   $ cd caffe && mkdir cmake_build && cd cmake_build\n   $ cmake .. -DBUILD_SHARED_LIB=ON\n   $ make -j 12\n   $ ln -s ../build .\n   ```\n   \n   If the first line gives the error *Manually-specified variables were not used by the project: BUILD_SHARED_LIB*, \n   just run the first line again.\n3. Clone the agile_grasp 2.0 repository into your ros workspace: \n\n   ```\n   $ cd <location_of_your_ros_workspace>/src\n   $ git clone https://github.com/atenpas/agile_grasp2\n   ```\n4. Recompile your ROS workspace: \n\n   ```\n   $ cd ..\n   $ catkin_make\n   ```\n\n\n## 4) Detect Grasp Poses With a Robot\n\n1. Connect a range sensor, such as a Microsoft Kinect or Asus Xtion Pro, to your robot.\n2. Adjust the file *robot_detect_grasps.launch* for your sensor and robot hand.\n3. Run the grasp pose detection: \n   \n   ```\n   $ roslaunch agile_grasp2 robot_detect_grasps.launch\n   ```\n\n![Image Alt](readme/robot1.png)\n![Image Alt](readme/robot2.png)\n\n\n## 5) Detect Grasp Poses in a PCD File\n\n1. Have a *.pcd file available. Let's say it's located at */home/user/cloud.pcd*. \n2. Change the parameter *cloud_file_name* in the file *file_detect_grasps.launch* to */home/user/cloud.pcd*.\n3. Detect grasp poses: \n  \n   ```\n   roslaunch agile_grasp2 file_detect_grasps.launch\n   ```\n![Image Alt](readme/file1.png)\n\n\n## 6) Using Precomputed Normals\n\nThe ROS node, *grasp_detection*, can take point clouds with normals as input. The normals need to be stored in the \nnormal_x, normal_y, and normal_z fields. **Attention**: the only preprocessing operation that works with this is \nworkspace filtering, so avoid voxelization and manual indexing.\n\n\n## 7) Parameters\n\nThe most important parameters are cloud_type and cloud_topic to define the input point cloud, and workspace and \nnum_samples to define the workspace dimensions and the number of grasp hypotheses to be sampled. The other parameters \nonly need to be modified in special cases.\n\n#### Input\n* cloud_type: the type of the input point cloud. \n  * 0: *.pcd file\n  * 1: sensor_msgs/PointCloud2\n  * 2: agile_grasp/CloudSized\n  * 3: agile_grasp/CloudIndexed.\n* cloud_topic: the ROS topic from which the input point cloud is received.\n\n#### Visualization\n* plot_mode: what type of visualization is used (0: no visualization, 1: PCL, 2: rviz (not supported yet))\n* only_plot_output: set this to *false* to see additional visualizations.\n\n#### Grasp Hypothesis Search\n* workspace: the limits of the robot's workspace, given as a cube centered at the origin: [x_min, x_max, y_min, y_max, z_min, z_max] (in meters).\n* camera_pose: the camera pose as a 4x4 homogeneous transformation matrix.\n* num_samples: the number of grasp hypotheses to be sampled.\n* num_threads: the number of CPU threads to be used.\n* nn_radius_taubin: the neighborhood search radius for normal estimation (in meters).\n* nn_radius_hands: the neighborhood search radius for the grasp hypothesis search (in meters).\n* num_orientations: the number of hand orientations to be considered.\n* antipodal_mode: the output of the algorithm. 0: grasp hypotheses, 1: antipodal graps (prediction), 2: antipodal grasps (geometric).\n* normal_estimation_method: the method used to estimate normals. 0: Taubin quadric fitting, 1: PCL normal estimation.\n* voxelize: if the point cloud gets voxelized.\n* filter_half_grasps: if half-grasps are filtered out.\n* gripper_width_range: the aperture range of the robot hand: [aperture_min, aperture_max].\n\n#### Robot Hand Geometry\n* finger_width: the finger width.\n* hand_outer_diameter: the hand's outer diameter (aperture + finger_width).\n* hand_depth: the hand's depth (length of fingers).\n* hand_height: the hand's height.\n* init_bite: the initial amount that the hand extends into the object to be grasped.\n\n#### Classifier\n* images_directory: where images are stored (not used).\n* model_file: the Caffe prototxt file that specifies the network.\n* trained_file: the Caffe model file that contains the weights for the network.\n* label_file: a txt file that contains the label for each class.\n* min_score_diff: the minimum difference between the positive and the negative score for a grasp to be classfied as positive.\n\n#### Grasp Selection\n* num_selected: the number of selected grasps. If antipodal grasps are predicted/calculated, then the selected grasps will be \nthe highest-scoring grasps.\n\n#### Other\n* use_service: uses a ROS service instead of topics (untested).\n\n\n## 8) Citation\n\nIf you like this package and use it in your own work, please cite our [arXiv paper](http://arxiv.org/abs/1603.01564):\n\n```\n@misc{1603.01564,\nAuthor = {Marcus Gualtieri and Andreas ten Pas and Kate Saenko and Robert Platt},\nTitle = {High precision grasp pose detection in dense clutter},\nYear = {2016},\nEprint = {arXiv:1603.01564},\n} \n```\n", 
  "id": 53368274
}