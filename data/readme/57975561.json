{
  "read_at": 1462556736, 
  "description": "a trained attention-based summarization model", 
  "README.md": "# trained-ABS-model\na trained attention-based summarization neural network model based on the published\n[paper](http://arxiv.org/abs/1509.00685v2)\nand [code](https://github.com/facebook/NAMAS)\n\n## usage\n\nI used Github LFS (Large File Service) to store the large model binary:\nit is pretty awesome!\n\nSo you need to install [Git LFS](https://git-lfs.github.com/) and then just\nclone the repo.\n\nTo evaluate the model, clone the NAMAS repo (mine or the original) and run the `test_model.sh` script:\n```bash\n./test_model.sh paper-example.article.txt abs-model.th 10\n```\n\nConsult the original repo for more instructions.\n\n## how this model was trained\n\nThis model is trained using my [fork](https://github.com/falcondai/NAMAS) of\nthe original repo and here are some notable differences.\n\n- used the unannotated 5th edition gigaword dataset (https://catalog.ldc.upenn.edu/LDC2011T07).\n  The original repo used the annotated version (https://catalog.ldc.upenn.edu/LDC2012T21)\n  but didn't use the extra annotation aside from its tokenization.\n- I used NLTK ``word_tokenize`` to tokenize the text (which may result in\n  different tokenization from the annotated gigaword).\n- I dropped article-title pairs that contains non-ASCII characters because it\n  will cause errors in the tokenizer and it is known that gigaword contains\n  some articles in non-English (Spanish). it would be better to remove these\n  using the official list though.\n\nI got similar validation perplexity (**27.74**) as the number quoted in the\npaper (27.1) and here are some sample predictions (compare with ABS+ predictions\npresented in the paper):\n\n- detained iranian-american academic released from jail after posting bail \t\n- eu ministers gather for unprecedented conference on economic cooperation\n- death toll from school in haiti rises to #\n- australian fm congratulates new zealand as 'man of respected\n- two south africans upset at hong kong rugby coach\n- christian conservatives in the last two us presidential elections\n- un nuclear watchdog warns iran of possible new sanctions\n- thousands of kashmiris rally in support for cancer treatment\n- two us soldiers killed in iraq s restive province\n- davydenko retires from # nd round at sydney international\n- russia 's gazprom set up joint venture in northwest\n\n## extra notes\n\nI trained the model with the default hyperparameters but some of the\ndefault hyperparameters, as pointed out in [this issue](https://github.com/facebook/NAMAS/issues/8), do not match the ones\npresented in the paper.\n\n## reference\n\nRush, Alexander M., Sumit Chopra, and Jason Weston. \"A neural attention model for abstractive sentence summarization.\" arXiv preprint [arXiv:1509.00685](http://arxiv.org/abs/1509.00685v2) (2015).\n\n## author\n\nFalcon Dai (dai@ttic.edu)\n", 
  "id": 57975561
}