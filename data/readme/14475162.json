{
  "read_at": 1462556897, 
  "description": "", 
  "README.md": "tagger\n======\n\ncs221 final project\n\n\nExperiments we ran\n===================\nEach table contains the percentage of questions\nthat have correct tag percentages in the categories\n0, 0-25, 25-50, 50-75, and 75-100\n(see printResultsTable)\n\n- SVM vs Naive Bayes using unigrams\n\nSVM\n--------------\n0:       89.11\n1-25:    1.047\n25-50:   5.443\n50-75:   3.350\n75-100:  1.047\n--------------\n\nNaive Bayes\n--------------\n\n\n- data size\n- n-grams\n- question contains code markup (blockquote, LaTeX, grid of data)\n- If there's code, classify its language based on some library\n- question contains latex markup\n- unigrams (this is a huge one)\n- bigrams\n- trigrams?\n- length of description\n- length of question\n- Is there a high frequency of one pronoun?\n- questions contains numbers\n- misspellings\n- redundancy on question and description\n- is some text in a foreign language\n- poor grammar\n- number of question marks\n- confidence of asker (do they use weak phrases that inexperienced posters typically do, like \"Any\n- use of a TextBlock\n- Use of blockquote\n- human names (Jeff, Amanda)\n- are there many other questions that may be the same as this one? It probably has a popular tag.\n- is there emoji? :)\n- are sentences capitalized?\n\nFeature selection process\n    We first use filter feature selection to compute a score\n    for how likely each feature influences the output tag. The\n    purpose of this selection method was to get a general sense\n    of whether the feature could successfully inform the tags at\n    least a little. After we eliminated all the features that\n    had next to no effect on the tags, we'll further prune our\n    feature set by using forward feature search.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependencies\n============\n- scikit-learn\n\nNote\n====\nA CS221 instructor said, \"For the CS221 project, we\nhave strict policy that at most 2 students do the work.\nYou can share some of the code infrastructure with the\nCS229 project, but will need to clearly separate an\ninteresting problem on which only two of you will be\nworking for the CS221 project.\"\n\nAfter we try a few more classification schemes and test\nsome features, I will take a part of the\nproject to work on that will be separate from what\nBen, Guoxing, and I will be working on.\n\nPapers\n======\nHere's the treasure trove: http://scholar.google.com/scholar?espv=210&es_sm=91&um=1&ie=UTF-8&lr&q=related:b9kLL_qDc8XssM:scholar.google.com/\nhttp://arxiv.org/pdf/1202.4063.pdf\nhttp://arxiv.org/pdf/1208.3623v1.pdf\nreally good for background on SVM and text classification: http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf_\non kNN: http://clair.si.umich.edu/~radev/papers/tc.pdf\n\nSet up\n======\nDownload the data and split it.\nsudo pip install -U scikit-learn\ngit clone https://github.com/matplotlib/matplotlib.git\ncd matplotlib\npython setup.py build\npython setup.py install\n\nTesting and Continuous Integration\n==================================\nWhen changes are pushed to Github, the new code is tested on the complete training and test data sets.\n\ntagger CI (Jenkins):\nhttp://ci.perfmode.com/job/tagger/\n\nTODO(roseperrone): grant repo access to Jenkins server\n\nWhat we need to figure out\n==========================\nHow to determine how many tags to give a question.\n    - there needs to be at least one\n    - learn the variance of the number of tags\n    - learn what makes a question have many tags.\n        - e.g. History questions only have 1-3 tags,\n               but coding questions have 4-7\n    - learn what makes a question have more tags. Should it\n      simply be the tags that pass a certain threshold?\n\nI split up the training data using\nsplit -l 355100 Train.csv\n\nI split up the testing set using\nsplit -l 100000 Test.csv\n\nNote that the competitive success of our algorithm will\nlikely depend most on how we can apply domain knowledge\nto select our features. For example, we may want to predict\nhow many tags a question has, and use that to inform\nthe threshold for labeling.\n\nFeatures in order of how much we estimate the feature will contribute to classification:\ne.g.\n    - question contains code markup\n    - If there's code, classify its language based on some library\n    - question contains latex markup\n    - unigrams (this is a huge one)\n    - bigrams\n    - trigrams?\n    - length of description\n    - length of question\n    - Is there a high frequency of one pronoun?\n    - questions contains numbers\n    - misspellings\n    - redundancy on question and description\n    - is some text in a foreign language\n    - poor grammar\n    - number of question marks\n    - confidence of asker (do they use weak phrases that inexperienced posters typically do, like \"Any help would be appreciate(d)\" and \"My Question:\", \"Thank you.\", \"Thanks for any ideas\", \"I am clueless\"\n    - use of a TextBlock\n    - Use of blockquote\n    - human names (Jeff, Amanda)\n    - are there many other questions that may be the same as this one? It probably has a popular tag.\n    - is there emoji? :)\n    - are sentences capitalized?\n\n\n\nNote: could consider using a Random Forest algorithm to decrease runtime.\n", 
  "id": 14475162
}