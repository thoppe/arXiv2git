{
  "read_at": 1462551014, 
  "description": "Parsimonious Topic Models", 
  "README.md": "Parsimonious Topic Model\n\nFor details of the algorithm, please check the paper, Hossein Soleimani and David J. Miller,\n \"Parsimonious Topic Models with Salient Word Discovery\", arXiv:1401.6169.\n\n(C) Copyright 2014, Hossein Soleimani\n\t\t     David J. Miller\n\n\n This program is free program; you can redistribute it and/or modify it under the terms of \n the GNU General Public License as published by the Free Software Foundation; either \n version 2 of the License, or any later version. This program is distributed in the hope \n that it will be useful, but WITHOUT ANY WARRANTY; without even he implied warranty of \n MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License \n for more details.\n\n\n1) Compile the program in Linux-based system. \ntype: make \n\n2) Type \"./ptm\"\n\n3) Options:\n\n\t--task\t\t\ttraining/test, (default: training )\n\t--num_topics\tnumber of topics\n\t--directory\t\tdirectory to save the output\n\t--corpus\t\tcorpus file, in lda-c format; i.e. each line is of the form\n\t\t\t\t[L] [term_1]:[count] ... [term_L]:[count]\n\t\t\t\twhere L is the number of unique terms in the document, and the [count]\n\t\t\t\tassociated with each term is the number of times that term appears in the document.\n\t--init\t\t\tinitialization method. seeded/random/load\n\t\t\t\tseeded: see the paper for details of this method\n\t\t\t\trandom: random initialization\n\t\t\t\tload: load word probabilities and randomly initialize topic proportions\n\t--model\t\t\tname of the model to load\n\t--max_iter\t\tmaximum iterations after which we stop the EM algorithm. (default: 100)\n\t--convergence\tIf increase in the log-likelihood is less than \"convergence\", EM is terminated. (default: 5e-3)\n\t--save_lag\t\tSave the model at every \"save_lag\" step. (default: -1)\n\t--step\t\t\tNumber of topics to remove for next steps' initialization. See the paper for model\n\t\t\t\torder selection. (default 0)\n\t\t\n \n\n4) Output format:\n\tTraining phase saves the follwong files in the directory:\n\t\t\n\t\tfinal.alpha:\t\tContains topic proportions, where each line corresponds to\n\t\t\t\t\ta document in the format: [alpha_1] [alpha_2] ... [alpha_M]\n\t\t\t\t\twhere M is the number of topics\n\t\tfinal.v:\t\tBinary switches for topic proportions (i.e. v switches) in the same \n\t\t\t\t\tformat as in final.alpha.\n\t\tfinal.beta\t\tContains M+1 columns and N rows where each row corresponds to a term \n \t\t\t\t\t(N: total # unique words)\n\t\t\t\t\tFirst column is the shared model, and each of the next M columns indicates \n\t\t\t\t\tprobability of words under that topic.\n\t\tfinal.u\t\t\tContains u switches in M columns and N rows\n\t\tfinal.other\t\tFirst row is the number of topics and the second number of terms\n\t\tlikelihood.dat:\t\tContains bic, log-likelihood, and convergence values at each iteration of EM.\n\t\tnbar.txt:\t\tIndicates total number of topic-specific words at each iteration of EM. \n\n\tTest step saves the follwong files in the directory:\n\t\ttest-alpha:\t\tSimilar to final.alpha.\n\t\ttest-lhood:\t\tsimilar to likelihood.dat\n\n", 
  "id": 22218799
}