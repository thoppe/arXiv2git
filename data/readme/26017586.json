{
  "read_at": 1462511399, 
  "description": "Implementation of \"Variational Depth from Focus Reconstruction\" [Moeller 2014]", 
  "README.md": "variational-depth-from-focus\n============================\n\nCUDA implementation of: \nMoeller, Michael, et al. [Variational Depth from Focus Reconstruction](http://arxiv.org/pdf/1408.0173v2.pdf). arXiv preprint arXiv:1408.0173v2 (2014).\n\n>This paper deals with the problem of reconstructing a depth map from a sequence of differently focused images, also known as depth from focus or shape from focus. We propose to state the depth from focus problem as a variational problem including a smooth but nonconvex data fidelity term, and a convex nonsmooth regularization, which makes the method robust to noise and leads to more realistic depth maps. Additionally, we propose to solve the nonconvex minimization problem with a linearized alternating directions method of multipliers (ADMM), allowing to minimize the energy very efficiently. A numerical comparison to classical methods on simulated as well as on real data is presented.\n\n## Dependencies \nCMake, CUDA, OpenCV\n\n#### MacOSX (10.9, 10.10)\nWe installed OpenCV 2.4.10 and CMake 3 using brew\n\nmake sure brew is installed correctly\n```sh\nruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\nbrew doctor\n```\n\ninstall cmake and opencv\n```sh\nbrew install cmake\nbrew tap homebrew/science\nbrew install opencv\n```\n\nInstall CUDA 7.0, which comes with C++11 support and is compiled with libc++ which is the default on OSX since 10.9. This really simplifies the installation, because prior to 7.0, CUDA was built with libstdc++, so the OpenCV dependency would have to be built with libstdc++ as well (e.g. by passing the --with-cuda flag).\n\nhttp://developer.download.nvidia.com/compute/cuda/7_0/Prod/local_installers/cuda_7.0.29_mac.pkg\n\n\n#### Linux\nOur code was tested under Arch Linux with OpenCV 2.4.10, CMake 3.2.1 and CUDA 7.0.\nIt should compile and run successfully under your favorite linux distribution if it provides the above mentioned dependencies.\n\nPlease use the appropriate package manager of your distribution to install these packages; below is an example\nfor Arch Linux\n\n```sh\npacman -S opencv\npacman -S cmake\npacman -S cuda\n```\n\n#### Windows\nThe code was tested under Windows 8.1, OpenCV 2.4.10, CMake 3.2.1, CUDA 7.0 and assumes you have a working version of Visual Studio installed.In our case, we worked with Visual Studio Ultimate 2013. \n\nIf you do not have a version of Visual Studio, it is recommended that you install it first.\nOne can obtain a free version under the following link:\nhttps://www.visualstudio.com/en-us/products/visual-studio-express-vs.aspx\n\nDownload and install CUDA 7.0, which can be found under:  \nhttps://developer.nvidia.com/cuda-downloads  \n  \nNext, install OpenCV 2.4.10 from  \nhttps://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.10/opencv-2.4.10.exe/download  \nPlease remember in which path you installed OpenCV, since we have to add to the Windows PATH environment variable\n```\nyour/path/to/opencv/build/x64/vc12/bin\n```\nIf necessary, replace x64 with x86 for a 32-bit system and vc12 with vc11 for Visual Studio 12 or vc10 for Visual Studio 10.  \n  \nLastly, we need to install CMake, which can be obtained under:\nhttp://www.cmake.org/download/\n\n## Installation\nNow we are ready to build our code\n\n#### Mac OSX and Linux\nCheck out our repository, create a separate build-folder and then build the files using CMake.\n\n```sh\ncd ~/projects\ngit clone https://github.com/adrelino/variational-depth-from-focus.git\ncd variational-depth-from-focus\nmkdir build && cd build\ncmake ..\nmake\n```\n#### Windows \nCheck out the repository under some directory, e.g. \n```\nC:\\variational-depth-from-focus\n```\nStart up CMake and specify where the source code is found. In our example this would be\n```\nC:\\variational-depth-from-focus\n```\nNext, you have to specify, and if necessary create, a folder into which the binaries get generated.  \nWe choose to create and use\n```\nC:\\variational-depth-from-focus\\build\n```\nNow we need to set the compiler under Tools - Configure. Choose your compiler and select \"Use default native compilers\".\nClick \"Finish\" and CMake will try to generate the build files. If you get an error message about \"OpenCV_DIR-NOTFOUND\",\nclick on this message and specify the path to \n```\nyour/path/to/opencv/build\n```\nEverything should now work smoothly and then you can open the generated \"variational_depth_from_focus.sln\" in Visual Studio, which can be found in the build folder. In Visual Studio, unload all projects except \"vdff\" (by right-clicking them and\nchoosing \"Unload Project\"). Right-click \"vdff\" and select \"Set as Startup Project\".\n\nNext, we have to define the\"NOMINMAX\" preprocessor directive. Right-click again \"vdff\" and click \"Properties\".\nSelect on the left \"Configuration Properties\" - \"C/C++\" - \"Preprocessor\". On the right, click into \"Preprocessor Definitions\"\nand insert \n```\n;NOMINMAX\n```\nClick OK. Then select \"BUILD\" - \"Build solution\".\n\nThe project should now compile successful. If the compiler complains that it can not find \"dirent.h\", you have to download the windows implementation of the header file from here  \nhttp://www.softagalleria.net/dirent.php  \nand copy it into the include folder of your Visual Studio installation.  \nFor Visual Studio 2013 the path would be  \n```\nC:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\include\n```\n\n## Execution\n#### Quickstart\nIf you just run\n```sh\n./vdff\n```\nthen our code uses the included focus sequence with appropriate default parameters.\n\nThe focus sequence was generated from a textured image using the Defocus simulation code from\nhttp://www.sayonics.com/downloads.html\n\n![sim-in](https://github.com/adrelino/variational-depth-from-focus/blob/master/samples/sim/im_10.png)\n\nThe expected result looks as follows:\n\n![sim-out](https://github.com/adrelino/variational-depth-from-focus/blob/master/samples/results/sim.png)\n\n#### Advanced\nYou can evaluate your own image sequence by specifying its locaton to the executable via the \"-dir\" switch, e.g.\n```sh\n./vdff -dir your/path/to/imgs\n```\nAll pictures of your sequence have to reside in the specified folder; pictures located in sub-folders are\nignored. Currently supported file formats are JPEG, TIFF and PNG.\nAdditional parameters can be used, which are explained in more detail below.\n##### Parameters\nThe following parameters have to be preceded by a single dash and their arguments are given after a separating space.\nFor the boolean parameters we use 0 as false and 1 as true.\n\nParameter | Expected Type | Default Value |Explanation\n----------|---------------|---------------|-----------\ndir | string | ../samples/sim | specifies the location of your image sequence. All pictures have to be in the folder; no sub-folders are supported.\nsmoothGPU | bool | 1 | if enabled, the GPU is used to smooth the initial obtained MLAP estimates. (increases speed)\npageLocked | bool | 0 | if enabled, pageLocked-memory and cuda streams are used to speed up the creation of sharpness images\nminVal | int | -10 | minimum value of the range for which the polynomial approximations are calculated\nmaxVal | int | 10 | maximum value of the range for which the polynomial approximations are calculated\npolyDegree | int | 6 | specifies the degree of the polynomials we fit to the sharpness values\ndenomRegu | float | 0.3 | regularizer used to decrease importance of sharp edges\nnrIterations | int | 400 | nr. of iterations of the ADMM algorithm\nconvIterations | int | 0 | specifies at which number of iterations the proposed convergence scheme is used (see section 4 in the above paper). The default is to use it from the beginning.\nlambda | float | 1.0 | corresponds to the l used in the ADMM algorithm\ngrayscale | bool | 0 | convert image to grayscale before further processing\nexport | string | \"\" | if a string is supplied, the created depth map is exported to the desired absolute or relative file-path after the closing of the application. Exporting is currently only possible to .png or .jpg; if no suffix is supplied, the png format is assumed.\n\n### Datasets\nAll the datasets are provided as zip files, unzip them and then run with the -dir option.\n#### ARRI&reg; - Dataset\n![fokusfahrt_in](http://home.in.tum.de/~haarbach/fokus202.jpg)\n\nThe full dataset of the paper is available under the following link:  \n* http://in.tum.de/~haarbach/fokusfahrt_png_compr9.zip\n\nIt consists of 374 16-bit png files which have a total size of 3.86 GB. The result obtained with default parameters should look like:\n![fokusfahrt_full](https://github.com/adrelino/variational-depth-from-focus/blob/master/samples/results/fokusfahrt_png_compr9.png)\n\nSince this is quite large, we also offer a smaller set, which uses 8-bit jpg files with a compression setting of 95. With a total size of 198 MB it is considerably smaller and offers nearly the same results as the original dataset.\n\nThe small dataset can be found under:\n* http://in.tum.de/~haarbach/fokusfahrt_jpg_compr95.zip\n\nDue to the compression and loss of precision, you have to adjust the setting of the -denomRegu parameter a little; e.g. a setting of 1.0 is nearly identical to the results of the full dataset:\n![fokusfahrt_small](https://github.com/adrelino/variational-depth-from-focus/blob/master/samples/results/fokusfahrt_jpg_compr95_denomRegu1.png)\n\n#### Booksequence\n\nThe same author that provides the focus simulation code we used above also provides a real sequence with different noise levels on his website:\n* http://www.sayonics.com/sources/books_00.zip low noise\n* http://www.sayonics.com/sources/books_02.zip middle noise\n* http://www.sayonics.com/sources/books_05.zip high noise\n\n#### devCam sequences\n\nWe recently recorded a new sequences using a Nexus 5 device and the [devCam](https://users.soe.ucsc.edu/~rcsumner/devcam/) Android app, which allows to record a burst of images while varying the focal distance. \nThe datasets can be found under:\n* http://in.tum.de/~haarbach/balcony.zip (19 images, 8.7MB)\n![balcony_in](http://home.in.tum.de/~haarbach/balcony_0002.jpg)\n![balcony](https://raw.githubusercontent.com/adrelino/variational-depth-from-focus/master/samples/results/balcony.png)\n\n* http://in.tum.de/~haarbach/shelf.zip (20 images, <9MB)\n![shelf_in](http://home.in.tum.de/~haarbach/shelf-0000.jpg)\n![shelf](https://raw.githubusercontent.com/adrelino/variational-depth-from-focus/master/samples/results/shelf.png)\n\n* http://in.tum.de/~haarbach/alley.zip (20 images, <9MB)\n![alley_in](http://home.in.tum.de/~haarbach/alley-0000.jpg)\n![alley](https://raw.githubusercontent.com/adrelino/variational-depth-from-focus/master/samples/results/alley.png)\n\nWe plan to record further datasets in the future.\n\n## Record your own dataset\nIf you own a Nexus 5 or 6 device you may record your own sequences. For this you may download our [fork of devCam](https://github.com/adrelino/devCam), open it in Android Studio and run it on your device using the debug bridge.\n\nThe parameters of the image sequence to record, called a burst, are specified in a configuration file in json format. [This configuration file](https://github.com/adrelino/variational-depth-from-focus/blob/master/samples/sweep_focus_8-100cm.json) varies the focal distance from 8cm to 100cm in steps of 2cm and needs to be copied from the computer to the following folder on the SD card of the Android device:\n```sh\nPictures/devCam/Designs/\n```\nAfter the capture, a burst of images is saved in the folder\n```sh\nPictures/devCam/Captured/<capture design id>\n```\nFor copying the json file to the device and copying the captured image folder back to the PC one can use the MTP based [Android File Transfer](https://www.android.com/filetransfer/) on MAC. Note that if new folders/files don't show up, just restart the Android File Transfer program and if that doesn't help, restart the phone.\n\nSince varying the focal distance also changes the field of view, corresponding pixels will no longer be aligned. This effect can be removed by aligning all the images and optimizing the field of view for all images using the [align image stack](http://hugin.sourceforge.net/docs/manual/Align_image_stack.html) of [hugin](http://hugin.sourceforge.net/). This outputs tiff images, to save space one can convert them back to jpg using the compress application provided in our repository.\n```sh\ncd <vddf repo root>/Captured\nmkdir ../aligned\nmkdir ../aligned/<capture design id>\ncd <capture design id>\n/Applications/hugin/HuginTools/align_image_stack -m -a ../../aligned/<capture design id>/<capture design id>- `ls *.jpg | sort -n -t - -k 2 -v`\ncd ..\n../build/compress -compr 95 -indir ../aligned/<capture design id> -outdir ../samples/<capture design id> -type jpg -color 1 -anydepth 1 -debug 0\n```\nnote that when using the sweep_focus.json file which ships with the app instead of the one we provide above, the order of focal distances is reversed, meaning that the first captured image has the largest focal distance. In this case, just pass -r to sort so that the images will be cropped correctly and the resulting depth colormap is in the same order as in the other sequences.\n\nFor convenience, we provide the bash script [align.sh](https://github.com/adrelino/variational-depth-from-focus/blob/master/align.sh) which bundles all of the above tasks, and additionally calls the main vdff programm together with the export option to quickly check if a recorded burst gives a satisfying depth map.\n\nconsidered you copied the Captured folder to the root of this github repository, then you simply need to run\n```sh\ncd <vddf repo root>/Captured\n../align.sh <capture design id> <reverse>?\n```\nwhich aligns the images, optionally reversing them if a second argument is supplied, crops them, compresses them and stores them in the /samples/<capture design id> folder and then runs the main vdff programm with this sequence, exporting the resulting depth map to /samples/results/<capture design id>.png.\n", 
  "id": 26017586
}