{
  "read_at": 1462548292, 
  "description": "", 
  "README.md": "# arXiv GRAPH\n# Project Description\n\n## Description of the problem\n\nFor the 4th project at Metis, we had to explore an application of Natural Language Processing (NLP) and I decided to work towards improving the current [arXiV](http://arxiv.org) website.\n\nArXiv is largest archive for scientific papers and gives access to a sheer amount of information. With over 1 million e-prints in 6 different subjects and an ever-increasing number of papers published every month, it's getting quite hard for a researcher to look for papers that may be relevant to his / her research.\n\nSo my goal for this project is to allow a researcher to easily navigate throughout the papers without having to go through painfully long daily digests. \n\n## Getting the data\n\nIn order to get the ball rolling, we first need to get our hands dirty and download some data.\nLuckily for us, the [arXiv API](http://arxiv.org/help/api/user-manual) will avoid us scraping, which may result in them blocking our IP address.\n\nIn `get_the_data/get_arxiv_metadata.py` you will find a python parser for the API that saves the articles metadata in a [MongoDB](https://www.mongodb.org/) database. For the sake of simplicity I've only focused on the Astrophysics papers of 2015, which are about 11,000, but should should feel free to change the API parameters to retrieves whatever papers you're most interested in.\n\n## Recommender system\n\nFor every article I'm able to retrieve the title, the abstract, the category and the author list, which is more than enough to build a recommender system and in `recommender/tfidf_recommender.py` I show all the necessary steps to get to it, which can be summarized as follows:\n\n1. Read the data from the MongoDB database we've created in the previous section.\n2. Parse (with `ntlk`) the title and the abstract of every article in order to retain only the names and get rid of all the rest (adjectives, articles, adverbs, ...). Not only this has the advantage of speeding up the code by reducing the dimensionality of the problem, but it may also improve the accuracy.\n3. Build the document-term matrix, computed with the TF-IDF weighting scheme, using `sklearn` \n4. Recommendations are given using the cosine distance\n\n## Social network\n\nAnother interesting application that it's possible to do with the arXiv metadata, is the creation of a social network using the co-authors relations.\n(For this step I recommend retrieving papers for more than just one year, but that can be easily done if one adjusts the input parameters in `get_the_data/get_arxiv_metadata.py`).\n\nI created a graph where every node is represented by every author of each article in the database and the connections are established if two researchers have been co-authors of a paper.\n\nIn `social_network/compute_network.py` I show how to do that and the output `csv` file can be used together with the code for the [D3 visualization](http://bl.ocks.org/MatteoTomassetti/e6720a91d7a58489e583) I created on [Building Bl.ocks](http://blockbuilder.org) to show the social network (only the top 5 connections are considered) for a given researcher.\n\nYou should see something like this\n\n![](img/graph_example.png)\n\n## Live Demo\n\nUnfortunately, I don't have the resources to host a live demo on a web server, since the articles database can be quite heavy. \n\nHowever, if you would like to try it on your local machine, here are the files you need to generate and place in the `data/` folder in `test_website/`\n\n1. `processed_data_2015.npy` (see **line 62** in `recommender/tfidf_recommender.py`)\n2. `tfidf_sparse_matrix.npy` (see **line 76** in `recommender/tfidf_recommender.py`)\n3. `author_index.npy` (see **line 41** in `social_network/compute_network.py`)\n4. `authors_graph.npy` (see **line 42** in `social_network/compute_network.py`)\n\nOnce you have generated those files, run this command in the `test_website` folder\n\n`python arxivgraph.py`\n\nAnd go to `localhost:5000` to view the demo.\n\nYou should see something like this ![](img/live_demo_example.png).\n\nWhile you navigate through the website you can see the top 10 recommendations for each article and the degree of the connections among the different authors.\n\n\n## Bonus\n\nAt the end of `recommender/tfidf_recommender.py`, I also show how to build a Multinomail Naive Bayes classifier to model an article's category given the document-term matrix build at step 3.\nThe model is able to get very high accuracy, with an AUC of more than 0.9 for every category.\n\n![](img/roc_curve_all_categories.png)\n\n\n\n\n\n\n", 
  "id": 56249516
}