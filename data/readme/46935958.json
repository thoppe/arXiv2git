{
  "read_at": 1462554288, 
  "description": "Source code for \"Visual Language Modeling on CNN Image Representations\"", 
  "README.md": "# Visual Language Modeling on CNN Image Representations\n\n## Authors\n* [Hiroharu Kato (The University of Tokyo)](http://hiroharu-kato.com/)\n* [Tatsuya Harada (The University of Tokyo)](http://www.isi.imi.i.u-tokyo.ac.jp/~harada/)\n\n## Abstract\n  Measuring the naturalness of images is important to generate realistic images or to detect unnatural regions in images. Additionally, a method to measure naturalness can be complementary to Convolutional Neural Network (CNN) based features, which are known to be insensitive to the naturalness of images. However, most probabilistic image models have insufficient capability of modeling the complex and abstract naturalness that we feel because they are built directly on raw image pixels. In this work, we assume that naturalness can be measured by the predictability on high-level features during eye movement. Based on this assumption, we propose a novel method to evaluate the naturalness by building a variant of Recurrent Neural Network Language Models on pre-trained CNN representations. Our method is applied to two tasks, demonstrating that 1) using our method as a regularizer enables us to generate more understandable images from image features than existing approaches, and 2) unnaturalness maps produced by our method achieve state-of-the-art eye fixation prediction performance on two well-studied datasets.\n\n## Example\n\n### Image reconstruction\n\n#### Original image\n![Original image](./testdata/024_227.png)\n\n#### Reconstructed image from the fc7 feature of AlexNet (1000 dims)\n![Reconstructed image](./testdata/reconstructed.png)\n\n\n### Saliency map\n\n#### Original image\n![Original image](./testdata/024.jpg)\n\n#### Saliency map from CNN-VLM of conv5_4 layer of VGGNet\n![Saliency map](./testdata/saliency_map.png)\n\n## Full paper\n  http://arxiv.org/abs/1511.02872\n\n```tex\n  @article{kato2015cnnvlm,\n      author={Kato, Hiroharu and Harada, Tatsuya},\n      title={Visual Language Modeling on CNN Image Representations},\n      journal={arXiv preprint arXiv:1511.02872},\n      year={2015}\n  }\n```\n\n## Code\n\n### Requirements\n  * Python libraries\n    * numpy\n    * scipy\n    * scikit-learn\n    * theano\n  * Other libraries\n    * GPU and cudnn\n    * Caffe\\*\n    * ImageMagick\\*\n  * Dataset\n    * ILSVRC2012 classification Dataset\\*\n\n\\* not required for testing pre-trained models.\n\n### Testing pre-trained models\n```bash\npython run_pretrained_model.py\n```\nThis script downloads pre-trained CNN-VLM models of AlexNet / VGGNet and execute image reconstruction and compute saliency map. Loading pre-trained model takes tens of minutes and requires a lot of RAM.\n\nPre-trained models were compiled using CUDA 7.0 and CuDNN v3 for GTX 970 on Ubuntu 14.04.\n\n### Training CNN-VLM\nFirst, specify the path of ILSVRC2012 dataset in prepare_images.py. Then run\n```bash\npython run_all.py\n```\nThis may take several days.\n\n### Differences from the paper\n* CNN-VLMs are trained on the validation set of ImageNet.\n* The learning rate of a reconstructor is 2^20.\n[](* Image feature is standardized before reconstruction.)\n\n### Contact\n* My website: http://hiroharu-kato.com/\n* E-mail: hiroharu.kato.1989.10.13@gmail.com\n", 
  "id": 46935958
}