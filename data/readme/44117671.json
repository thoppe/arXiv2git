{
  "read_at": 1462556211, 
  "description": "Image annotation UIs (good for AMT tasks)", 
  "README.md": "# annotation-UIs\n\nThis repository contains image annotation UIs used for various projects at Stanford University.\n\nAuthors: [Olga Russakovsky](http://cs.cmu.edu/~orussako) (olga@cs.stanford.edu) and [Justin Johnson](http://cs.stanford.edu/people/jcjohns/)\n\n### Entrypoint\n\nThe entry point for the UIs is all_actions.html. You can open it in your browser to see a set of sample tasks and check out the UIs.\n\n### Annotating images on Amazon Machanical Turk\n\nThe simplest backend to use with these templates is [simple-amt](https://github.com/jcjohnson/simple-amt)\n\n**Important**: make sure to put in absolute rather than relative paths, e.g.,\n\n//image-net.org/path/to/you/file\n\nSearch for 'absolute' in the code -- most places should be marked with comments. You'll need to do it in\n- best_of_both_world/all_actions.html\n- best_of_both_worlds/instructions.html\n- best_of_both_worlds/task_header.js\n- whats_the_point/all_actions.html\n- all image paths\n\n### References\n\nIf you find the UIs useful in your research, please cite:\n\n**best_of_both_worlds**\n\nProject page: http://ai.stanford.edu/~olga/best_of_both_worlds\n\n\t@inproceedings{RussakovskyCVPR15,\n\tauthor = {Olga Russakovsky and Li-Jia Li and Li Fei-Fei}, \n\ttitle = {Best of both worlds: human-machine collaboration for object annotation},\t\n\tbooktitle = {CVPR},\n\tyear = {2015}\n\t} \n\n**whats_the_point**\n\nProject page: http://vision.stanford.edu/whats_the_point\n\n    @article{Bearman15,\n    author = {Amy Bearman and Olga Russakovsky and Vittorio Ferrari and Li Fei-Fei},\n    title = {What's the point: Semantic segmentation with point supervision},\n    journal = {ArXiv e-prints},\n    eprint = {1506.02106}, \n    year = {2015}\n    }\n\n", 
  "id": 44117671
}