{
  "read_at": 1462555000, 
  "description": "This package shows how to train a siamese network using Lasagne and Theano and includes network definitions for state-of-the-art networks including: DeepID, DeepID2, Chopra et. al, and Hani et. al.  We also include one pre-trained model using a custom convolutional network.", 
  "README.md": "# Siamese Net\n\n![images/prediction.png](images/prediction.png)\n\n# Introduction\n\nThe siamese network is a method for training a distance function discriminatively.  Its use is popularized in many facial verification models including ones developed by Facebook and Google.  The basic idea is to run a deep net on pairs of images describing either matched or unmatched pairs.  The same network is run separately for the left and right images, but the loss is computed on the pairs of images rather than a single image.  This is done by making use of the \"batch\" dimension of the input tensor, and computing loss on interleaved batches.  If the left image is always the even idx (0, 2, 4, ...) and the right image is always the odd idxs, (1, 3, 5, ...), then the loss is computed on the alternating batches: `loss = output[::2] - output[1::2]`, for instance.  By feeding in pairs of images that are either true or false pairs, the output of the networks should try to push similar matching pairs closer to together, while keeping unmatched pairs farther away.\n\nThis package shows how to train a siamese network using Lasagne and Theano and includes network definitions for state-of-the-art networks including: DeepID, DeepID2, Chopra et. al, and Hani et. al.  We also include one pre-trained model using a custom convolutional network.\n\nWe are releasing all of this to the community in the hopes that it will encourage more models to be shared and appropriated for other possible uses.  The framework we share here should allow one to train their own network, compute results, and visualize the results.  We encourage the community to explore its use, submit pull requests on any issues within the package, and to contribute pre-trained models.\n\n![images/embedding.png](images/embedding.png)\n\n# Package\n\nSiamese Network for performing training of a Deep Convolutional\nNetwork for Face Verification on the Olivetti and LFW Faces datasets.\n\nDependencies:\n\npython 3.4+, numpy>=1.10.4, sklearn>=0.17, scipy>=0.17.0, theano>=0.7.0, lasagne>=0.1, cv2, dlib>=18.18 (only required if using the 'trees' crop mode).\n\nPart of the package siamese_net:\nsiamese_net/\nsiamese_net/faces.py\nsiamese_net/datasets.py\nsiamese_net/normalization.py\nsiamese_net/siamese_net.py\n\nLook at the notebook file `siamese_net_example.ipynb` for how to use the pre-trained model to predict pairs of images or visualize layers of the model.\n\n![images/layers.png](images/layers.png)\n![images/gradient.png](images/gradient.png)\n\nAlso look at `siamese_net.py` for training your own model.  The default parameters will train a model on LFW without any face localization.\n\n```\n$ python3 siamese_net.py --help\nusage: siamese_net.py [-h] [-m MODEL_TYPE] [-of N_OUT] [-bs BATCH_SIZE]\n                      [-e N_EPOCHS] [-lr LEARNING_RATE] [-dp DROPOUT_PCT]\n                      [-norm NORMALIZATION] [-f FILENAME] [-path PATH_TO_DATA]\n                      [-hm HYPERPARAMETER_MARGIN]\n                      [-ht HYPERPARAMETER_THRESHOLD] [-ds DATASET]\n                      [-nl NONLINEARITY] [-fn DISTANCE_FN] [-cf CROP_FACTOR]\n                      [-sp SPATIAL] [-r RESOLUTION] [-nf NUM_FILES]\n                      [-gray B_CONVERT_TO_GRAYSCALE]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -m MODEL_TYPE, --model_type MODEL_TYPE\n                        Choose the Deep Network to use. [\"hani\"], \"chopra\", or\n                        \"custom\" (default: hani)\n  -of N_OUT, --output_features N_OUT\n                        Number of features in the final siamese network layer\n                        (default: 40)\n  -bs BATCH_SIZE, --batch_size BATCH_SIZE\n                        Number of observations per batch. (default: 100)\n  -e N_EPOCHS, --epochs N_EPOCHS\n                        Number of epochs to train for. (default: 5)\n  -lr LEARNING_RATE, --learning_rate LEARNING_RATE\n                        Initial learning rate to apply to the gradient update.\n                        (default: 0.0001)\n  -dp DROPOUT_PCT, --dropout_pct DROPOUT_PCT\n                        Percentage of connections to drop in between\n                        Convolutional layers. (default: 0.0)\n  -norm NORMALIZATION, --normalization NORMALIZATION\n                        Normalization of the dataset using either [\"-1:1\"],\n                        \"LCN\", \"LCN-\", or \"ZCA\". (default: -1:1)\n  -f FILENAME, --filename FILENAME\n                        Resulting pickle file to store results. If none is\n                        given, a filename is created based on the combination\n                        of all parameters. (default: None)\n  -path PATH_TO_DATA, --path_to_data PATH_TO_DATA\n                        Path to the dataset. If none is given it is assumed to\n                        be in the current working directory (default: None)\n  -hm HYPERPARAMETER_MARGIN, --hyperparameter_margin HYPERPARAMETER_MARGIN\n                        Contrastive Loss parameter describing the total free\n                        energy. (default: 2.0)\n  -ht HYPERPARAMETER_THRESHOLD, --hyperparameter_threshold HYPERPARAMETER_THRESHOLD\n                        Threshold to apply to the difference in the final\n                        output layer. (default: 5.0)\n  -ds DATASET, --dataset DATASET\n                        The dataset to train/test with. Choose from [\"lfw\"],\n                        or \"olivetti\" (default: lfw)\n  -nl NONLINEARITY, --nonlinearity NONLINEARITY\n                        Non-linearity to apply to convolution layers.\n                        (default: rectify)\n  -fn DISTANCE_FN, --distance_fn DISTANCE_FN\n                        Distance function to apply to final siamese layer.\n                        (default: l2)\n  -cf CROP_FACTOR, --cropfactor CROP_FACTOR\n                        Scale factor of amount of image around the face to\n                        use. (default: 1.0)\n  -sp SPATIAL, --spatial_transform SPATIAL\n                        Whether or not to prepend a spatial transform network\n                        (default: False)\n  -r RESOLUTION, --resolution RESOLUTION\n                        Rescale images to this fixed square pixel resolution\n                        (e.g. 64 will mean images, after any crops, are\n                        rescaled to 64 x 64). (default: 64)\n  -nf NUM_FILES, --num_files NUM_FILES\n                        Number of files to load for each person. (default: 2)\n  -gray B_CONVERT_TO_GRAYSCALE, --grayscale B_CONVERT_TO_GRAYSCALE\n                        Convert images to grayscale. (default: True)\n```\n\nExample output of training w/ default parameters:\n\n```\n$ python3 siamese_net.py\nNamespace(b_convert_to_grayscale=True, batch_size=100, crop_factor=1.0, dataset='lfw', distance_fn='l2', dropout_pct=0.0, filename=None, hyperparameter_margin=2.0, hyperparameter_threshold=5.0, learning_rate=0.0001, model_type='hani', n_epochs=5, n_out=40, nonlinearity='rectify', normalization='-1:1', num_files=2, path_to_data=None, resolution=64, spatial=False)\nDataset: lfw\nSpatial: 0\nBatch Size: 100\nNum Features: 40\nModel Type: hani\nNum Epochs: 5\nNum Files: 2\nLearning Rate: 0.000100\nNormalization: -1:1\nCrop Factor: 1\nResolution: 64\nHyperparameter Margin: 2.000000\nHyperparameter Threshold: 5.000000\nDropout Percent: 0.000000\nNon-Linearity: rectify\nGrayscale: 1\nDistance Function: l2\n\nWriting results to: results/dataset_lfw_transform_0_batch_100_lr_0.000100_model_hani_epochs_5_normalization_-1:1_cropfactor_1.00_nout_40_resolution_64_numfiles_2_q_2.00_t_5.00_d_0.00_nonlinearity_rectify_distancefn_l2_grayscale_1.pkl\n\nLoading dataset...\nPreprocessing dataset\nLoading data in siamese-net/lfw\nPerson: 5749/5749\n(11498, 1, 64, 64)\nInitializing Siamese Network...\n(11498, 1, 64, 64)\n\nEpoch 1 of 5 took 20.952s\n    training loss:          0.008983\n    validation loss:        0.007918\n    validation AUC:         0.64\n    validation F1:          0.69\n```\n\n... training will begin after downloading the dataset, pre-processing faces, and compilation (can take ~30 minutes!).  Each epoch will then take ~ 21 seconds using these default parameters using a GeForce GT 750M GPU.\n\n# References\n\nChopra, S., Hadsell, R., & Y., L. (2005). Learning a similiarty metric discriminatively, with application to face verification. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 349-356.\n\nDonahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., & Darrell, T. (2014). DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. arXiv Preprint. Retrieved from http://arxiv.org/abs/1310.1531\n\nEl-bakry, H. M., & Zhao, Q. (2005). Fast Object / Face Detection Using Neural Networks and Fast Fourier Transform, 8580(11), 503-508.\n\nHuang, G. B., Mattar, M. a., Lee, H., & Learned-Miller, E. (2012). Learning to Align from Scratch. Proc. Neural Information Processing Systems, 1-9.\n\nKhalil-Hani, M., & Sung, L. S. (2014). A convolutional neural network approach for face verification. High Performance Computing & Simulation (HPCS), 2014 International Conference on, (3), 707-714. doi:10.1109/HPCSim.2014.6903759\n\nKostinger, M., Hirzer, M., Wohlhart, P., Roth, P. M., & Bischof, H. (2012). Large scale metric learning from equivalence constraints. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, (Ldml), 2288-2295. doi:10.1109/CVPR.2012.6247939\n\nLi, H., & Hua, G. (2015). Hierarchical-PEP Model for Real-world Face Recognition, 4055-4064. doi:10.1109/CVPR.2015.7299032\n\nParkhi, O. M., Vedaldi, A., Zisserman, A., Vedaldi, A., Lenc, K., Jaderberg, M., ... others. (2015). Deep face recognition. Proceedings of the British Machine Vision, (Section 3).\n\nSun, Y., Wang, X., & Tang, X. (2014). Deep Learning Face Representation by Joint Identification-Verification. Nips, 1-9. doi:10.1109/CVPR.2014.244\n\nTaigman, Y., Yang, M., Ranzato, M., & Wolf, L. (2014). DeepFace: Closing the Gap to Human-Level Performance in Face Verification. Conference on Computer Vision and Pattern Recognition (CVPR), 8. doi:10.1109/CVPR.2014.220\n\nWheeler, F. W., Liu, X., & Tu, P. H. (2007). Multi-Frame Super-Resolution for Face Recognition. 2007 First IEEE International Conference on Biometrics: Theory, Applications, and Systems, 1-6. doi:10.1109/BTAS.2007.4401949\n\nYi, D., Lei, Z., Liao, S., & Li, S. Z. (2014). Learning Face Representation from Scratch. arXiv.\n\n# License\n\nParag K. Mital\nCopyright 2016 Kadenze, Inc.\nKadenze(R) and Kannu(R) are Registered Trademarks of Kadenze, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n\nApache License\n\nVersion 2.0, January 2004\n\nhttp://www.apache.org/licenses/\n\n# Kadenze\n\nKadenze is a creative arts MOOC working with institutions around the world to deliver affordable education in the arts.  Interested in working on problems in deep learning, signal processing, and information retrieval?  We're always looking for great people to join our team either as interns or potentially other roles. If you are interested in working with us, contact jobs@kadenze.com.\n\n![images/ds.png](images/ds.png)", 
  "id": 54579351
}